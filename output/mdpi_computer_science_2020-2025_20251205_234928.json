[
    {
        "title": "Control Applications with FPGA: Case of Approaching FPGAs for Students in an Intelligent Control Class",
        "authors": "byDu≈°an Fister,Alen JakopiƒçandMitja Truntiƒç",
        "journal": "Appl. Sci.2025,15(24), 12884; https://doi.org/10.3390/app152412884 (registering¬†DOI) - 5 Dec 2025",
        "abstract": "Experience shows that knowledge transfer and understanding of fundamental FPGA principles are greatly improved by exercising laboratory practices and manual hands-on operations. Hence, a case study was performed on two didactic platforms for students of intelligent control techniques that were upgraded with FPGAs to be involved in laboratory practices. Among others, platforms allow implementation of traditional linear control algorithms, such as PID, or modern non-linear control algorithms, such as fuzzy logic or artificial neural networks. Initially, the underlying physics can be carefully studied, and the mathematical model can be derived. Then, such a model can be discretized into its digital form, an appropriate controller can be designed, and its performance can be compared to the known benchmark. Controllers and control parameters can be practiced by students themselves, offering underlying potential for improving students‚Äô understanding of the fundamentals of FPGA.",
        "keywords": ":fuzzy logic controller; didactic tool; practicing laboratory works; understanding fundamental FPGA principlesfuzzy logic controller;didactic tool;practicing laboratory works;understanding fundamental FPGA principles",
        "full_content": {
            "Abstract": "Experience shows that knowledge transfer and understanding of fundamental FPGA principles are greatly improved by exercising laboratory practices and manual hands-on operations. Hence, a case study was performed on two didactic platforms for students of intelligent control techniques that were upgraded with FPGAs to be involved in laboratory practices. Among others, platforms allow implementation of traditional linear control algorithms, such as PID, or modern non-linear control algorithms, such as fuzzy logic or artificial neural networks. Initially, the underlying physics can be carefully studied, and the mathematical model can be derived. Then, such a model can be discretized into its digital form, an appropriate controller can be designed, and its performance can be compared to the known benchmark. Controllers and control parameters can be practiced by students themselves, offering underlying potential for improving students‚Äô understanding of the fundamentals of FPGA. Keywords:fuzzy logic controller; didactic tool; practicing laboratory works; understanding fundamental FPGA principlesfuzzy logic controller;didactic tool;practicing laboratory works;understanding fundamental FPGA principles",
            "Share and Cite": "MDPI and ACS StyleFister, D.;                     Jakopiƒç, A.;                     Truntiƒç, M.    \n        Control Applications with FPGA: Case of Approaching FPGAs for Students in an Intelligent Control Class.Appl. Sci.2025,15, 12884.\n    https://doi.org/10.3390/app152412884AMA StyleFister D,                                 Jakopiƒç A,                                 Truntiƒç M.        \n                Control Applications with FPGA: Case of Approaching FPGAs for Students in an Intelligent Control Class.Applied Sciences. 2025; 15(24):12884.\n        https://doi.org/10.3390/app152412884Chicago/Turabian StyleFister, Du≈°an,                                 Alen Jakopiƒç,                                 and Mitja Truntiƒç.        \n                2025. \"Control Applications with FPGA: Case of Approaching FPGAs for Students in an Intelligent Control Class\"Applied Sciences15, no. 24: 12884.\n        https://doi.org/10.3390/app152412884APA StyleFister, D.,                                 Jakopiƒç, A.,                                 & Truntiƒç, M.        \n        \n        (2025). Control Applications with FPGA: Case of Approaching FPGAs for Students in an Intelligent Control Class.Applied Sciences,15(24), 12884.\n        https://doi.org/10.3390/app152412884 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further detailshere.",
            "Article Metrics": "YesCitationsNo citations were found for this article, but you may check onGoogle ScholarNoArticle metric data becomes available approximately 24 hours after publication online.",
            "Citations": "No citations were found for this article, but you may check onGoogle Scholar"
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2076-3417/15/24/12884",
        "scraped_at": "2025-12-05 23:49:50"
    },
    {
        "title": "Environmental Management Study of Dairy Cattle Farming in the Peri-Urban Area of Algiers, Algeria",
        "authors": "byMounir Ghezal,Bahia Bouchafaa Hammadou,Karima Kouachi,Pierre Spiteri,Tilemachos KoliopoulosandFateh Mebarek-Oudina",
        "journal": "Sustainability2025,17(24), 10912; https://doi.org/10.3390/su172410912 (registering¬†DOI) - 5 Dec 2025",
        "abstract": "This study focuses on the search for the optimal value of the cost per liter of raw milk. The sample included 59 farms with different types of labor, containing 422 elements maintained in different accommodation conditions. The farms are located in an urban area in the country‚Äôs capital. This study was essentially based on mathematical methodology close to a variant of the Cobb‚ÄìDouglas function used by many economists. This made it possible to find expressions of the relationships linking different parameters involved in the evaluation of the optimal value of the cost per raw liter, as well as certain critical values of the number of elements to be determined. The results show that the variation in the cost per liter follows two levels; the first relates to a number of elements between one and ten, where the increase occurs in a linear and progressive manner. The second level concerns the range between 10 and 30 elements. It is characterized by a linear increase and is more accentuated than in the previous case. The results also suggest that a critical number indicates the separation between the two levels. Application of these wastes as fertilizers aligns with the EU Action Plan on the Circular Economy and can contribute to achieving SDGs 2 and 12.Keywords:circular economy;peri-urban area;profitability;raw milk;sustainable development goal 2;sustainable development goal 12",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "The combination of progressive economic development and population growth is intensifying the demand for essential resources like water, food, and energy. As societies aspire to improve living standards, markets respond by expanding the variety and volume of products they produce. This trend underscores the need for sustainable resource management and innovative solutions for profitability, creating more jobs so as to meet the growing needs while minimizing the environmental impact. In Africa, the livestock sector is one of the most dynamic components of the agricultural economy [1]. It represents a significant share of the economies of many countries, averaging 15% of the gross domestic product (GDP), with figures exceeding 40% in some regions [2]. In developing countries, this sector contributes to household income, food security, and nutrition [3]. In Algeria, livestock accounts for over 30% of the agricultural GDP across all types of farming [4], underscoring its vital role within the country‚Äôs agricultural sector. The essential contribution of dairy cattle farming is recognized for facilitating the establishment of more intensive and sustainable agricultural practices [5,6,7,8]. As of 2024, Algeria‚Äôs dairy herd was estimated at 908.412 head, with nearly 12% comprising modern imported dairy cattle (MDC), which account for 70% of the total cow milk production‚Äîapproximately 2.5 billion liters annually, translating to a per capita availability of 150 L/year, with 49% sourced locally and the remainder met through powdered milk imports. This consumption level is the highest in the Maghreb region [3]. Additionally, the local dairy sector has generated over 300.000 jobs nationwide, with significant state budget allocations dedicated to livestock development through various agricultural programs, including the rehabilitation program for the dairy sector launched in 1995, the National Agricultural and Rural Development Plan (NARDP) in 2001, NARDP in 2002, and Agricultural Renewal initiatives beginning in 2009. Since then, the local dairy sector has experienced real growth across all its components [9,10,11]. This development is currently falling short of the established objectives, despite a steady increase in consumption fueled by population growth and state support for consumer prices [12]. According to [13], imports of pregnant heifers have significantly risen from an average of 1.671 heads annually between 1964 and 1968 to 29.222 heads during the period from 2005 to 2009. This number surged further to 93.500 heads from 2009 to 2012 and ultimately reached 109.462 heads from 2012 to 2022. In total, over half a million cows were imported from 1964 to 2022; however, this influx has not produced the expected increase in local production. It is now clear that, despite considerable efforts, dairy farming in Algeria is struggling to progress. This sector contends with numerous constraints related to livestock management, particularly concerning animal feed [5,14,15,16], as well as challenges in reproduction control [17,18] and various external constraints influenced by territorial factors, including climate, agronomic potential, hydraulic infrastructure, and demographic aspects such as urbanization [10,19]. These cows indicate significant dysfunctions within Algeria‚Äôs dairy cattle farming and are critical determinants of the cost of producing one liter of milk, which appears inconsistent with prices in other countries [10,20]. To address these production deficiencies, it is essential to identify the multiple factors limiting the efficiency of dairy operations through a purely mathematical study. In this context, Algiers was selected as the study area due to its reputation for intensive dairy farms located on its outskirts, which play a vital role in the production systems of the Mitidja region. However, like many cities worldwide, Algiers has faced significant pressures from urban growth, leading to both a loss of agricultural space and a decline in agricultural coherence. Agriculture is now primarily practiced on the periphery and is increasingly shifting towards peri-urban practices, especially regarding livestock farming [21]. However, efficiency is defined as the result achieved in relation to the resources used to attain it [22]. Moreover, peri-urban agriculture, in its strict etymological sense, refers to agricultural activities situated on the outskirts of a city, irrespective of the type of production system used [23]. This article focuses primarily on examining the situation of dairy cattle farming. For this purpose, a representative sample of 59 diverse farms was taken from the peri-urban area of Algiers, creating opportunities for sustainability, profitability, and new jobs. These farms underwent technical and economic surveys, resulting in a substantial database for analysis through mathematical modeling based on statistical results. The sector is experiencing tangible changes that could force it to contract or even vanish within a shorter time frame if available space continues to diminish [24,25,26,27,28]. This study was essentially supported by the mathematical methodology, which has not been well discussed for the treatment of such an agricultural problem by specialists in this area of our country.",
            "2. Methodological Option and Statistical Modeling Study": "2.1. Study AreaWe propose some forms of production functions, widely used by specialists, which are proposed to express the quantitative relationship between the volume of production and the factors of production, such as the Cobb‚ÄìDouglas Production Function (CD), which is one of the most widely used functions in practice. The American economist P.H. Douglas and the American mathematician Charles Cobb developed it in 1928. It expresses the relationship between labor and capital. The production function (CES) is a non-linear homogeneous function. In 1961, the economists Chemery, Solow, Minha, and Arrow proposed this function by improving the CD function. The other functions include the Revankar function, proposed in 1971, the Transcendental function, proposed by Hater in 1957, and the Translog function. The function used in agricultural production is of the type:ùë¶=ùëì(ùë•1,ùë•2,ùë•3,ùë•4,‚Ä¶.,ùë•ùëõ).y=f(x1,x2,x3,x4,‚Ä¶.,xn).(1)The functionfexpresses the relationship between the proportionality between the quantity of production and the quantity of the production parameters. It can take forms or exponential functions, such as the Cobb‚ÄìDouglas function CD. Despite the very wide use of the DC production function, it is not the only one used by economists in the analysis and measurement of agricultural production. The other functions include the Wicksell function, which was proposed in 1961. In this study, we opted for a mathematical approach equivalent to that used to express agricultural production, which is an improved variant of the Cobb‚ÄìDouglas function.The objective of this study was to develop a mathematical model that can link the various parameters involved in the mathematical laws derived, which technical managers in this field may find beneficial for creating new jobs and supporting sustainability and opportunities for the circular economy. This research was conducted among 59 dairy cattle farmers situated around the city of Algiers, specifically in five municipalities, including Ouled Chebel, Birtouta, Tassala El Merdja, and Douira, from 2023 to 2024, as part of doctoral research (Figure 1). This region has been the focus of an extensive development program aimed at increasing raw milk production since the early 2000s, specifically from 2001 to the present. The farms included in this study were selected using a non-probability sampling method, specifically through convenience sampling. This approach involved choosing the most accessible and available participants. The majority of these farms specialize in dairy cattle farming, which significantly contributes to overall income through the revenue generated from milk production. These farms primarily raise cows, particularly specialized imported dairy breeds, such as Holstein and Montb√©liarde, as well as locally crossbred cows. Local breed cows are nearly nonexistent.Figure 1.(Map 1) Distribution of dairy production potential in Algiers Province. Source: Agricultural Services Directorate (DSA) of Algiers, 2024.A preliminary survey phase was conducted to assess the clarity and relevance of the questions prior to the actual survey phase. This initial assessment facilitated the design of the final questionnaire (Supplementary Materials) utilized during fieldwork, which addressed both the technical and socio-economic characteristics of the farms and their environments. Each farm underwent at least three evaluations: the first in the autumn, another at the beginning of spring when feed is abundant, and a third in mid-summer, which is considered the lean season.Once the database was established for the data analysis, the data were classified to facilitate economic calculations, which enabled the determination of production costs for each farmer and an assessment of their profitability. This information enhanced our database for subsequent mathematical and statistical analyses.The procedure began with assigning a registration number to each farm. In this initial phase, the farm‚Äôs registration number was defined as follows:MAT: 21316P001Where213 is the international code assigned to Algeria;016 is the code for Algiers Province, where the farms are located;P indicates the type of operator, with ‚ÄòP‚Äô denoting personal, ‚ÄòF‚Äô denoting family-owned, and ‚ÄòE‚Äô denoting external;001 indicates the rank of the operator within the specified type.The table references the existence of 18P, 32F, and 09E.The data collected in the field, as indicated inTable 1, cannot be directly utilized due to significant discrepancies in values for the same parameter; thus, we opted to work with averaged values for the cases studied.Table 1.Statistical data of farms.The key variables were identified based on their influence on the parameters involved in the production chains, established from the samples taken during this study.The mathematical modeling method unfolded as follows:The data are treated as mathematical functions of a single variable.A smoothing process is applied to the function to determine its shape across each interval of the chosen variable.Subsequently, a mathematical function that closely resembles the smoothed version is proposed.Using the smoothed curve data, certain data points deemed less realistic are corrected, allowing us to derive mathematical expressions that reflect the variations in the evolution curve of the studied parameters. InTable 2, in order to enhance the readability of the collected data, the following nomenclature is presented.The environmental management of farms considered in this study has not been the subject of mathematical modeling due to the current lack of certain data on the environmental aspect. It should be noted that the operations studied are managed by modest means and equipment. Regarding the supply of water and energy, the public authorities contribute financial aid to encourage operators to switch to green energy. In a future study, we will aim to present the environmental aspect of the exploitation of dairy cattle farms in Algeria.We are concerned with the efficiency of dairy cattle breeding activity. It should be noted that multiple factors determine this efficiency, in particular, the use of technology in dairy cattle breeding activity. On these farms, we recorded an average yield of 17 L/VL/Day (liters per dairy cow per day), close to that of the Maghreb countries, which is 20 L/VL/Day, but far from the milk yields recorded on European farms, which exceed 40 L/VL/Day [25]. On the other hand, the economic results observed show economic performance that can hardly be said to be weak. The production cost shows an archaic level of technical mastery; it depends on explanatory variables, such as food, which remains the most important item, with 56.89% of the total cost and 18.49%% of the labor costs. All these results explain the inefficiency of these dairy farms.Table 2.Nomenclature used to examine the case study. 2.2. Statistical and Modeling ApproachThis study applied the neoclassical production theory to assess the relationships between inputs, production costs, and milk output in peri-urban dairy farms. Total production costs were calculated following FAO (2017) standards, including fixed costs (infrastructure, depreciation, taxes) and variable costs (feed, labor, veterinary care, utilities). The costs were standardized per liter of milk.2.2.1. Production Function ModelingTo analyze the determinants of milk output and identify scale effects, we employed a modified Cobb‚ÄìDouglas production function. The explicit form used was:Q = AKŒ±LŒ≤FŒ≥(2)whereQ = milk production;K = capital (equipment and infrastructure);L = labor input;F = feed input;A, Œ±, Œ≤, and Œ≥ = parameters;The model was estimated using the log-linear transformation:ln(Q) = ln(A) + Œ±ln(K) + Œ≤ln(L) + Œ≥ln(F) + Œµ(3)Parameter estimation was performed using ordinary least squares (OLS). Coefficient significance was evaluated throught-tests, and model fit was assessed using R2, standard errors, and residual diagnostics. The Cobb‚ÄìDouglas form was selected due to its widespread use in agricultural production modeling and its ability to capture scale elasticity efficiently.2.2.2. Cost Curve ModelingEmpirical cost functions were derived by fitting polynomial models to observed production costs across different herd sizes. This curve-fitting approach allowed the identification of the following:A critical herd size Nc;Cost asymptotes, such as the optimal cost level Cop;Profitability thresholds across size categories.Model selection was based on least-square error minimization and graphical inspection of fit quality.The mathematical treatment of the different parameters characterizing the production of raw milk by dairy cows was performed using a mathematical modeling method according to the following steps: First step: The real values of the data were entered. Second step: The mathematical curve was traced, indicating the variation in the parameter studied represented in type graphs. Third step: A mathematical correction was applied in order to find a mathematical expression in polynomial form, represented in type graphs.Figure 2shows the curve indicating the variation in the cost price per liter of raw milk, including all charges, depending on the number of elements processed. We see that the shape of this curve can be divided into two zones; the first zone concerns a group bringing together between one and ten cows. This set is characterized by a cost price, which increases almost linearly and progressively, depending on the number of elements processed. The second zone, concerning the second set, brings together between ten and thirty cows. We see that the variation in the cost price also increases linearly, but in a more pronounced manner than in the previous case. The mathematical modeling of the variation in the cost price per liter of raw milk, including all charges, as a function of the number of elements processed, is presented inFigure 3. We notice the existence of a critical numberùëÅùëêNcthat indicates the separation between the two zones. The intersection of the two curves is then made at this point, for a critical value corresponding toùëÅùëê=12Nc=12.Figure 4indicates a proposal for a law of variation in the TCF function as a function of the number of cows, according to the following expression:ùë¶=103(ùë•2+7ùë•)y=103(x2+7x)(4)Figure 2.Variation in total production costs as a function of the number of cows (N).Figure 3.Model depicting the variation in total production costs based on the number of cows (N).Figure 4.Variation in the total cost function (TCF) in relation to the number of cows (N).Figure 5shows the variation in the cost of raw milk production depending on the number of cows treated. We see that its appearance can be divided into three zones; the first zone concerns a group of one to four cows. For this set, we notice that the cost price is very high for a single element processed; it then falls following an inverse law, that is to say that the cost price is inversely proportional to the number of cows. The second zone relates to a group of four to eight cows. In this area, the cost price follows an almost linear increase to reach approximately the same values as in the previous case. The third zone concerns a set that brings together between eight and thirty elements. In this zone, the cost price decreases following a law inverse to the number of cows. The decrease in the curve reaches an asymptotic limit value; this is the optimal valueùê∂ùëúùëùCop, seeFigure 6.Figure 7shows a curve indicating the variation in the profitability of the cost price of raw milk production as a function of the number of cows treated. We see that it can be divided into three zones; the first zone concerns a group of one to six cows. For this set, we notice that the profitability has negative valuesùê∂ùëùCp, for1<ùëÅùëù<71<Np<7. The second zone concerns a set that brings together six to twelve elements. In this zone, the profitability is positive; it follows an almost linear increase to reach values that tend to keep constant valuesùê∂ùë†ùë°Cst, for a level of values of the number of elements satisfying the following inequality:10<ùëÅùë†ùë°<2010<Nst<20. The third zone concerns a group of twenty to thirty cows. In this zone, the cost price relative to the gainùê∂ùëîCgincreases following a linear law forùëÅùëî>20Ng>20. SeeFigure 8.Figure 9indicates the profitability corrected for the cost price of raw milk production represented by the variable y as a function of the number of cows treated, represented by the variable x according to the following expression:ùë¶=2ùë•‚àí20y=2x‚àí20(5)Figure 5.Variation in production costs as a function of the number of cows (N).Figure 6.Curve modeling the variation in production costs as a function of the number of cows (N).Figure 7.Variation in profit as a function of the number of cows (N).Figure 8.Curve representing the variation in profit as a function of the number of cows (N).Figure 9.Adjusted curve modeling profit variation as a function of the number of cows (N).",
            "2.1. Study Area": "We propose some forms of production functions, widely used by specialists, which are proposed to express the quantitative relationship between the volume of production and the factors of production, such as the Cobb‚ÄìDouglas Production Function (CD), which is one of the most widely used functions in practice. The American economist P.H. Douglas and the American mathematician Charles Cobb developed it in 1928. It expresses the relationship between labor and capital. The production function (CES) is a non-linear homogeneous function. In 1961, the economists Chemery, Solow, Minha, and Arrow proposed this function by improving the CD function. The other functions include the Revankar function, proposed in 1971, the Transcendental function, proposed by Hater in 1957, and the Translog function. The function used in agricultural production is of the type:ùë¶=ùëì(ùë•1,ùë•2,ùë•3,ùë•4,‚Ä¶.,ùë•ùëõ).y=f(x1,x2,x3,x4,‚Ä¶.,xn).(1) The functionfexpresses the relationship between the proportionality between the quantity of production and the quantity of the production parameters. It can take forms or exponential functions, such as the Cobb‚ÄìDouglas function CD. Despite the very wide use of the DC production function, it is not the only one used by economists in the analysis and measurement of agricultural production. The other functions include the Wicksell function, which was proposed in 1961. In this study, we opted for a mathematical approach equivalent to that used to express agricultural production, which is an improved variant of the Cobb‚ÄìDouglas function. The objective of this study was to develop a mathematical model that can link the various parameters involved in the mathematical laws derived, which technical managers in this field may find beneficial for creating new jobs and supporting sustainability and opportunities for the circular economy. This research was conducted among 59 dairy cattle farmers situated around the city of Algiers, specifically in five municipalities, including Ouled Chebel, Birtouta, Tassala El Merdja, and Douira, from 2023 to 2024, as part of doctoral research (Figure 1). This region has been the focus of an extensive development program aimed at increasing raw milk production since the early 2000s, specifically from 2001 to the present. The farms included in this study were selected using a non-probability sampling method, specifically through convenience sampling. This approach involved choosing the most accessible and available participants. The majority of these farms specialize in dairy cattle farming, which significantly contributes to overall income through the revenue generated from milk production. These farms primarily raise cows, particularly specialized imported dairy breeds, such as Holstein and Montb√©liarde, as well as locally crossbred cows. Local breed cows are nearly nonexistent. Figure 1.(Map 1) Distribution of dairy production potential in Algiers Province. Source: Agricultural Services Directorate (DSA) of Algiers, 2024. A preliminary survey phase was conducted to assess the clarity and relevance of the questions prior to the actual survey phase. This initial assessment facilitated the design of the final questionnaire (Supplementary Materials) utilized during fieldwork, which addressed both the technical and socio-economic characteristics of the farms and their environments. Each farm underwent at least three evaluations: the first in the autumn, another at the beginning of spring when feed is abundant, and a third in mid-summer, which is considered the lean season. Once the database was established for the data analysis, the data were classified to facilitate economic calculations, which enabled the determination of production costs for each farmer and an assessment of their profitability. This information enhanced our database for subsequent mathematical and statistical analyses. The procedure began with assigning a registration number to each farm. In this initial phase, the farm‚Äôs registration number was defined as follows: MAT: 21316P001 Where 213 is the international code assigned to Algeria;016 is the code for Algiers Province, where the farms are located;P indicates the type of operator, with ‚ÄòP‚Äô denoting personal, ‚ÄòF‚Äô denoting family-owned, and ‚ÄòE‚Äô denoting external;001 indicates the rank of the operator within the specified type. The table references the existence of 18P, 32F, and 09E. The data collected in the field, as indicated inTable 1, cannot be directly utilized due to significant discrepancies in values for the same parameter; thus, we opted to work with averaged values for the cases studied. Table 1.Statistical data of farms. The key variables were identified based on their influence on the parameters involved in the production chains, established from the samples taken during this study. The mathematical modeling method unfolded as follows: The data are treated as mathematical functions of a single variable.A smoothing process is applied to the function to determine its shape across each interval of the chosen variable.Subsequently, a mathematical function that closely resembles the smoothed version is proposed.Using the smoothed curve data, certain data points deemed less realistic are corrected, allowing us to derive mathematical expressions that reflect the variations in the evolution curve of the studied parameters. InTable 2, in order to enhance the readability of the collected data, the following nomenclature is presented.The environmental management of farms considered in this study has not been the subject of mathematical modeling due to the current lack of certain data on the environmental aspect. It should be noted that the operations studied are managed by modest means and equipment. Regarding the supply of water and energy, the public authorities contribute financial aid to encourage operators to switch to green energy. In a future study, we will aim to present the environmental aspect of the exploitation of dairy cattle farms in Algeria.We are concerned with the efficiency of dairy cattle breeding activity. It should be noted that multiple factors determine this efficiency, in particular, the use of technology in dairy cattle breeding activity. On these farms, we recorded an average yield of 17 L/VL/Day (liters per dairy cow per day), close to that of the Maghreb countries, which is 20 L/VL/Day, but far from the milk yields recorded on European farms, which exceed 40 L/VL/Day [25]. On the other hand, the economic results observed show economic performance that can hardly be said to be weak. The production cost shows an archaic level of technical mastery; it depends on explanatory variables, such as food, which remains the most important item, with 56.89% of the total cost and 18.49%% of the labor costs. All these results explain the inefficiency of these dairy farms. Table 2.Nomenclature used to examine the case study.",
            "2.2. Statistical and Modeling Approach": "This study applied the neoclassical production theory to assess the relationships between inputs, production costs, and milk output in peri-urban dairy farms. Total production costs were calculated following FAO (2017) standards, including fixed costs (infrastructure, depreciation, taxes) and variable costs (feed, labor, veterinary care, utilities). The costs were standardized per liter of milk. 2.2.1. Production Function ModelingTo analyze the determinants of milk output and identify scale effects, we employed a modified Cobb‚ÄìDouglas production function. The explicit form used was:Q = AKŒ±LŒ≤FŒ≥(2)whereQ = milk production;K = capital (equipment and infrastructure);L = labor input;F = feed input;A, Œ±, Œ≤, and Œ≥ = parameters;The model was estimated using the log-linear transformation:ln(Q) = ln(A) + Œ±ln(K) + Œ≤ln(L) + Œ≥ln(F) + Œµ(3)Parameter estimation was performed using ordinary least squares (OLS). Coefficient significance was evaluated throught-tests, and model fit was assessed using R2, standard errors, and residual diagnostics. The Cobb‚ÄìDouglas form was selected due to its widespread use in agricultural production modeling and its ability to capture scale elasticity efficiently. 2.2.2. Cost Curve ModelingEmpirical cost functions were derived by fitting polynomial models to observed production costs across different herd sizes. This curve-fitting approach allowed the identification of the following:A critical herd size Nc;Cost asymptotes, such as the optimal cost level Cop;Profitability thresholds across size categories.Model selection was based on least-square error minimization and graphical inspection of fit quality.The mathematical treatment of the different parameters characterizing the production of raw milk by dairy cows was performed using a mathematical modeling method according to the following steps: First step: The real values of the data were entered. Second step: The mathematical curve was traced, indicating the variation in the parameter studied represented in type graphs. Third step: A mathematical correction was applied in order to find a mathematical expression in polynomial form, represented in type graphs.Figure 2shows the curve indicating the variation in the cost price per liter of raw milk, including all charges, depending on the number of elements processed. We see that the shape of this curve can be divided into two zones; the first zone concerns a group bringing together between one and ten cows. This set is characterized by a cost price, which increases almost linearly and progressively, depending on the number of elements processed. The second zone, concerning the second set, brings together between ten and thirty cows. We see that the variation in the cost price also increases linearly, but in a more pronounced manner than in the previous case. The mathematical modeling of the variation in the cost price per liter of raw milk, including all charges, as a function of the number of elements processed, is presented inFigure 3. We notice the existence of a critical numberùëÅùëêNcthat indicates the separation between the two zones. The intersection of the two curves is then made at this point, for a critical value corresponding toùëÅùëê=12Nc=12.Figure 4indicates a proposal for a law of variation in the TCF function as a function of the number of cows, according to the following expression:ùë¶=103(ùë•2+7ùë•)y=103(x2+7x)(4)Figure 2.Variation in total production costs as a function of the number of cows (N).Figure 3.Model depicting the variation in total production costs based on the number of cows (N).Figure 4.Variation in the total cost function (TCF) in relation to the number of cows (N).Figure 5shows the variation in the cost of raw milk production depending on the number of cows treated. We see that its appearance can be divided into three zones; the first zone concerns a group of one to four cows. For this set, we notice that the cost price is very high for a single element processed; it then falls following an inverse law, that is to say that the cost price is inversely proportional to the number of cows. The second zone relates to a group of four to eight cows. In this area, the cost price follows an almost linear increase to reach approximately the same values as in the previous case. The third zone concerns a set that brings together between eight and thirty elements. In this zone, the cost price decreases following a law inverse to the number of cows. The decrease in the curve reaches an asymptotic limit value; this is the optimal valueùê∂ùëúùëùCop, seeFigure 6.Figure 7shows a curve indicating the variation in the profitability of the cost price of raw milk production as a function of the number of cows treated. We see that it can be divided into three zones; the first zone concerns a group of one to six cows. For this set, we notice that the profitability has negative valuesùê∂ùëùCp, for1<ùëÅùëù<71<Np<7. The second zone concerns a set that brings together six to twelve elements. In this zone, the profitability is positive; it follows an almost linear increase to reach values that tend to keep constant valuesùê∂ùë†ùë°Cst, for a level of values of the number of elements satisfying the following inequality:10<ùëÅùë†ùë°<2010<Nst<20. The third zone concerns a group of twenty to thirty cows. In this zone, the cost price relative to the gainùê∂ùëîCgincreases following a linear law forùëÅùëî>20Ng>20. SeeFigure 8.Figure 9indicates the profitability corrected for the cost price of raw milk production represented by the variable y as a function of the number of cows treated, represented by the variable x according to the following expression:ùë¶=2ùë•‚àí20y=2x‚àí20(5)Figure 5.Variation in production costs as a function of the number of cows (N).Figure 6.Curve modeling the variation in production costs as a function of the number of cows (N).Figure 7.Variation in profit as a function of the number of cows (N).Figure 8.Curve representing the variation in profit as a function of the number of cows (N).Figure 9.Adjusted curve modeling profit variation as a function of the number of cows (N).",
            "2.2.1. Production Function Modeling": "To analyze the determinants of milk output and identify scale effects, we employed a modified Cobb‚ÄìDouglas production function. The explicit form used was:Q = AKŒ±LŒ≤FŒ≥(2)where Q = milk production;K = capital (equipment and infrastructure);L = labor input;F = feed input;A, Œ±, Œ≤, and Œ≥ = parameters; The model was estimated using the log-linear transformation:ln(Q) = ln(A) + Œ±ln(K) + Œ≤ln(L) + Œ≥ln(F) + Œµ(3) Parameter estimation was performed using ordinary least squares (OLS). Coefficient significance was evaluated throught-tests, and model fit was assessed using R2, standard errors, and residual diagnostics. The Cobb‚ÄìDouglas form was selected due to its widespread use in agricultural production modeling and its ability to capture scale elasticity efficiently.",
            "2.2.2. Cost Curve Modeling": "Empirical cost functions were derived by fitting polynomial models to observed production costs across different herd sizes. This curve-fitting approach allowed the identification of the following: A critical herd size Nc;Cost asymptotes, such as the optimal cost level Cop;Profitability thresholds across size categories. Model selection was based on least-square error minimization and graphical inspection of fit quality. The mathematical treatment of the different parameters characterizing the production of raw milk by dairy cows was performed using a mathematical modeling method according to the following steps: First step: The real values of the data were entered. Second step: The mathematical curve was traced, indicating the variation in the parameter studied represented in type graphs. Third step: A mathematical correction was applied in order to find a mathematical expression in polynomial form, represented in type graphs.Figure 2shows the curve indicating the variation in the cost price per liter of raw milk, including all charges, depending on the number of elements processed. We see that the shape of this curve can be divided into two zones; the first zone concerns a group bringing together between one and ten cows. This set is characterized by a cost price, which increases almost linearly and progressively, depending on the number of elements processed. The second zone, concerning the second set, brings together between ten and thirty cows. We see that the variation in the cost price also increases linearly, but in a more pronounced manner than in the previous case. The mathematical modeling of the variation in the cost price per liter of raw milk, including all charges, as a function of the number of elements processed, is presented inFigure 3. We notice the existence of a critical numberùëÅùëêNcthat indicates the separation between the two zones. The intersection of the two curves is then made at this point, for a critical value corresponding toùëÅùëê=12Nc=12.Figure 4indicates a proposal for a law of variation in the TCF function as a function of the number of cows, according to the following expression:ùë¶=103(ùë•2+7ùë•)y=103(x2+7x)(4) Figure 2.Variation in total production costs as a function of the number of cows (N). Figure 3.Model depicting the variation in total production costs based on the number of cows (N). Figure 4.Variation in the total cost function (TCF) in relation to the number of cows (N). Figure 5shows the variation in the cost of raw milk production depending on the number of cows treated. We see that its appearance can be divided into three zones; the first zone concerns a group of one to four cows. For this set, we notice that the cost price is very high for a single element processed; it then falls following an inverse law, that is to say that the cost price is inversely proportional to the number of cows. The second zone relates to a group of four to eight cows. In this area, the cost price follows an almost linear increase to reach approximately the same values as in the previous case. The third zone concerns a set that brings together between eight and thirty elements. In this zone, the cost price decreases following a law inverse to the number of cows. The decrease in the curve reaches an asymptotic limit value; this is the optimal valueùê∂ùëúùëùCop, seeFigure 6.Figure 7shows a curve indicating the variation in the profitability of the cost price of raw milk production as a function of the number of cows treated. We see that it can be divided into three zones; the first zone concerns a group of one to six cows. For this set, we notice that the profitability has negative valuesùê∂ùëùCp, for1<ùëÅùëù<71<Np<7. The second zone concerns a set that brings together six to twelve elements. In this zone, the profitability is positive; it follows an almost linear increase to reach values that tend to keep constant valuesùê∂ùë†ùë°Cst, for a level of values of the number of elements satisfying the following inequality:10<ùëÅùë†ùë°<2010<Nst<20. The third zone concerns a group of twenty to thirty cows. In this zone, the cost price relative to the gainùê∂ùëîCgincreases following a linear law forùëÅùëî>20Ng>20. SeeFigure 8.Figure 9indicates the profitability corrected for the cost price of raw milk production represented by the variable y as a function of the number of cows treated, represented by the variable x according to the following expression:ùë¶=2ùë•‚àí20y=2x‚àí20(5) Figure 5.Variation in production costs as a function of the number of cows (N). Figure 6.Curve modeling the variation in production costs as a function of the number of cows (N). Figure 7.Variation in profit as a function of the number of cows (N). Figure 8.Curve representing the variation in profit as a function of the number of cows (N). Figure 9.Adjusted curve modeling profit variation as a function of the number of cows (N).",
            "3. Results": "Farms and Production CostThe results inTable 3andTable 4show how production costs vary both by farming type and by herd size. Differences between farming systems suggest that management practices and resource use strongly influence overall expenses, while the cost patterns linked to herd size indicate where economies of scale may occur or where additional cows begin to generate higher marginal costs. Together, these findings highlight the importance of choosing an appropriate production system and herd level to maintain cost efficiency.Table 3.Data on production costs by type of farming.Table 4.Data on production costs by the number of cows.Figure 10depicts the distribution according to the type of farming. It is noteworthy that family-operated farms are the most prevalent, followed closely by individual farms. Despite this significant representation, it is important to emphasize that their profitability remains low. This observation should motivate stakeholders to explore more profitable management strategies. We believe that this management style should evolve towards some form of consolidation among farms situated in close proximity, which would facilitate shared management of forage cultivation and agricultural equipment. Additionally, experiences from France have illustrated that the yields achieved through modernization efforts have led to reduced production costs, making dairy farming more profitable. The modernization process has been a crucial factor behind the increased profitability of French dairy farms [25].Figure 10.Distribution by type of farming.Figure 11presents the distribution based on the age of the farmer. It is evident that the predominant age group includes those aged 20‚Äì40, closely followed by those aged 40‚Äì60, who comprise nearly half of the younger group. These data indicate that the aging workforce in dairy farming is becoming increasingly apparent. To tackle this challenge, public authorities must explore ways to enhance the attractiveness of this profession through vocational training and improved access to technical equipment.Figure 11.Distribution by age of the farmer.Although this study is mainly devoted to milk production in Algeria, we thought that providing information related to milk production in the countries of the Maghreb and the sub-Saharan zone would provide the elements necessary to conduct a comparative study. The various statistics come from local official sources and international organizations. Sixteen African countries located in West Africa were considered as a group in this analysis.Figure 12shows a graph of current milk consumption for the three countries of the Maghreb and sub-Saharan Africa. We note that Algeria consumes almost the same quantity as that of Morocco and sub-Saharan Africa combined. Tunisia has a lower consumption than Algeria and a higher consumption than the other countries cited.Figure 12.Actual consumption in Maghreb and sub-Saharan Africa.Figure 13shows the current milk consumption in percentage for the three countries of the Maghreb and sub-Saharan Africa. We note that Algeria consumes almost 37% of the global consumption. Tunisia has a consumption of 29%, Morocco has 22%, and sub-Saharan Africa has 11%.Figure 13.Actual consumption in percentage in Maghreb and sub-Saharan Africa.Figure 14shows milk production per liter and per capita from the year 1960 to the year 2025, as obtained from three authors. Ref. [13] covers the period 1970‚Äì1995, and [29] covers the period 2010‚Äì2024. This study covers the period 1960‚Äì2025. We note that the linear curves proposed by the mathematical approach used in this study present a certain similarity, despite the existence of differences in the growth rate between the three curves.Figure 14.Comparative linear curves produced by different authors [13,29].",
            "Farms and Production Cost": "The results inTable 3andTable 4show how production costs vary both by farming type and by herd size. Differences between farming systems suggest that management practices and resource use strongly influence overall expenses, while the cost patterns linked to herd size indicate where economies of scale may occur or where additional cows begin to generate higher marginal costs. Together, these findings highlight the importance of choosing an appropriate production system and herd level to maintain cost efficiency. Table 3.Data on production costs by type of farming. Table 4.Data on production costs by the number of cows. Figure 10depicts the distribution according to the type of farming. It is noteworthy that family-operated farms are the most prevalent, followed closely by individual farms. Despite this significant representation, it is important to emphasize that their profitability remains low. This observation should motivate stakeholders to explore more profitable management strategies. We believe that this management style should evolve towards some form of consolidation among farms situated in close proximity, which would facilitate shared management of forage cultivation and agricultural equipment. Additionally, experiences from France have illustrated that the yields achieved through modernization efforts have led to reduced production costs, making dairy farming more profitable. The modernization process has been a crucial factor behind the increased profitability of French dairy farms [25]. Figure 10.Distribution by type of farming. Figure 11presents the distribution based on the age of the farmer. It is evident that the predominant age group includes those aged 20‚Äì40, closely followed by those aged 40‚Äì60, who comprise nearly half of the younger group. These data indicate that the aging workforce in dairy farming is becoming increasingly apparent. To tackle this challenge, public authorities must explore ways to enhance the attractiveness of this profession through vocational training and improved access to technical equipment. Figure 11.Distribution by age of the farmer. Although this study is mainly devoted to milk production in Algeria, we thought that providing information related to milk production in the countries of the Maghreb and the sub-Saharan zone would provide the elements necessary to conduct a comparative study. The various statistics come from local official sources and international organizations. Sixteen African countries located in West Africa were considered as a group in this analysis.Figure 12shows a graph of current milk consumption for the three countries of the Maghreb and sub-Saharan Africa. We note that Algeria consumes almost the same quantity as that of Morocco and sub-Saharan Africa combined. Tunisia has a lower consumption than Algeria and a higher consumption than the other countries cited. Figure 12.Actual consumption in Maghreb and sub-Saharan Africa. Figure 13shows the current milk consumption in percentage for the three countries of the Maghreb and sub-Saharan Africa. We note that Algeria consumes almost 37% of the global consumption. Tunisia has a consumption of 29%, Morocco has 22%, and sub-Saharan Africa has 11%. Figure 13.Actual consumption in percentage in Maghreb and sub-Saharan Africa. Figure 14shows milk production per liter and per capita from the year 1960 to the year 2025, as obtained from three authors. Ref. [13] covers the period 1970‚Äì1995, and [29] covers the period 2010‚Äì2024. This study covers the period 1960‚Äì2025. We note that the linear curves proposed by the mathematical approach used in this study present a certain similarity, despite the existence of differences in the growth rate between the three curves. Figure 14.Comparative linear curves produced by different authors [13,29].",
            "4. Summary and Conclusions": "This study set out to construct a mathematical model that links the main factors influencing dairy farm management in the peri-urban zone of Algiers. The analysis revealed that production costs evolve in well-defined stages. Two major cost zones were identified, separated by a critical herd size of Nc= 12 cows. Costs are especially high at very small scales and then decline sharply as herd size increases, following an inverse relationship. This reduction gradually approaches an optimal cost level, Cop. A third zone, corresponding to herds of 8 to 30 cows, showed a similar inverse pattern, with costs stabilizing around Copbefore rising slightly at the upper end of this range. Profitability followed a comparable structure. Farms with fewer than six cows showed negative returns. Profitability became positive from about six to twelve cows and increased until it reached a relatively stable range between ten and twenty cows. Beyond twenty cows, profitability showed another upward tendency. Taken together, these results highlight the strong influence of herd size on both economic efficiency and the capacity of farms to remain viable. The diversity of the farms studied‚Äîreflected in their herd sizes, housing conditions, investment levels, labor availability, and land resources‚Äîexplains the notable variation in performance within the sample. Despite this heterogeneity, the overall trend is clear: farms operating with too few cows face structural difficulties, whereas those approaching medium scale achieve more stable costs and better margins. Modernizing production methods, improving infrastructure, and adopting digital monitoring tools would help producers benefit more fully from these scale effects. From an environmental standpoint, the results underline the importance of supporting farmers in adopting more efficient and cleaner technologies. Existing public incentives promoting renewable energy and environmentally responsible practices represent an essential step in this direction. Beyond the empirical findings, this study also demonstrates the usefulness of mathematical and statistical modeling‚Äîhere based on an enhanced form of the Cobb‚ÄìDouglas framework‚Äîin analyzing dairy production systems. This type of approach remains uncommon in national agricultural research, yet it provides valuable insights for identifying cost thresholds, optimal herd sizes, and key relationships between production variables. Finally, although this work focuses on small and medium farms typical of the peri-urban context, the methodology can be extended to larger industrial farms, which are becoming more prominent in the region and internationally. Future research will broaden the comparison to national and regional dairy systems in order to better situate the challenges and opportunities facing the Algerian milk sector."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2071-1050/17/24/10912",
        "scraped_at": "2025-12-05 23:49:59"
    },
    {
        "title": "PCB-Faster-RCNN: An Improved Object Detection Algorithm for PCB Surface Defects",
        "authors": "byZhige He,Yuezhou Wu,Yang LvandYuanqing He",
        "journal": "Appl. Sci.2025,15(24), 12881; https://doi.org/10.3390/app152412881 (registering¬†DOI) - 5 Dec 2025",
        "abstract": "As a fundamental and indispensable component of modern electronic devices, the printed circuit board (PCB) has a complex structure and highly integrated functions, with its manufacturing quality directly affecting the stability and reliability of electronic products. However, during large-scale automated PCB production, its surfaces are prone to various defects and imperfections due to uncontrollable factors, such as diverse manufacturing processes, stringent machining precision requirements, and complex production environments, which not only compromise product functionality but also pose potential safety hazards. At present, PCB defect detection in industry still predominantly relies on manual visual inspection, the efficiency and accuracy of which fall short of the automation and intelligence demands in modern electronics manufacturing. To address this issue, in this paper, we have made improvements based on the classical Faster-RCNN object detection framework. Firstly, ResNet-101 is employed to replace the conventional VGG-16 backbone, thereby enhancing the ability to perceive small objects and complex texture features. Then, we extract features from images by using deformable convolution in the backbone network to improve the model‚Äôs adaptive modeling capability for deformed objects and irregular defect regions. Finally, the Convolutional Block Attention Module is incorporated into the backbone, leveraging joint spatial and channel attention mechanisms to improve the effectiveness and discriminative power of feature representations. The experimental results demonstrate that the improved model achieves a 4.5% increase in mean average precision compared with the original Faster-RCNN. Moreover, the proposed method exhibits superior detection accuracy, robustness, and adaptability compared with mainstream object detection models, indicating strong potential for engineering applications and industrial deployment.",
        "keywords": ":printed circuit board; Faster-RCNN; ResNet-101; deformable convolution; convolutional block attention moduleprinted circuit board;Faster-RCNN;ResNet-101;deformable convolution;convolutional block attention module",
        "full_content": {
            "Abstract": "As a fundamental and indispensable component of modern electronic devices, the printed circuit board (PCB) has a complex structure and highly integrated functions, with its manufacturing quality directly affecting the stability and reliability of electronic products. However, during large-scale automated PCB production, its surfaces are prone to various defects and imperfections due to uncontrollable factors, such as diverse manufacturing processes, stringent machining precision requirements, and complex production environments, which not only compromise product functionality but also pose potential safety hazards. At present, PCB defect detection in industry still predominantly relies on manual visual inspection, the efficiency and accuracy of which fall short of the automation and intelligence demands in modern electronics manufacturing. To address this issue, in this paper, we have made improvements based on the classical Faster-RCNN object detection framework. Firstly, ResNet-101 is employed to replace the conventional VGG-16 backbone, thereby enhancing the ability to perceive small objects and complex texture features. Then, we extract features from images by using deformable convolution in the backbone network to improve the model‚Äôs adaptive modeling capability for deformed objects and irregular defect regions. Finally, the Convolutional Block Attention Module is incorporated into the backbone, leveraging joint spatial and channel attention mechanisms to improve the effectiveness and discriminative power of feature representations. The experimental results demonstrate that the improved model achieves a 4.5% increase in mean average precision compared with the original Faster-RCNN. Moreover, the proposed method exhibits superior detection accuracy, robustness, and adaptability compared with mainstream object detection models, indicating strong potential for engineering applications and industrial deployment. Keywords:printed circuit board; Faster-RCNN; ResNet-101; deformable convolution; convolutional block attention moduleprinted circuit board;Faster-RCNN;ResNet-101;deformable convolution;convolutional block attention module",
            "Share and Cite": "MDPI and ACS StyleHe, Z.;                     Wu, Y.;                     Lv, Y.;                     He, Y.    \n        PCB-Faster-RCNN: An Improved Object Detection Algorithm for PCB Surface Defects.Appl. Sci.2025,15, 12881.\n    https://doi.org/10.3390/app152412881AMA StyleHe Z,                                 Wu Y,                                 Lv Y,                                 He Y.        \n                PCB-Faster-RCNN: An Improved Object Detection Algorithm for PCB Surface Defects.Applied Sciences. 2025; 15(24):12881.\n        https://doi.org/10.3390/app152412881Chicago/Turabian StyleHe, Zhige,                                 Yuezhou Wu,                                 Yang Lv,                                 and Yuanqing He.        \n                2025. \"PCB-Faster-RCNN: An Improved Object Detection Algorithm for PCB Surface Defects\"Applied Sciences15, no. 24: 12881.\n        https://doi.org/10.3390/app152412881APA StyleHe, Z.,                                 Wu, Y.,                                 Lv, Y.,                                 & He, Y.        \n        \n        (2025). PCB-Faster-RCNN: An Improved Object Detection Algorithm for PCB Surface Defects.Applied Sciences,15(24), 12881.\n        https://doi.org/10.3390/app152412881 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further detailshere.",
            "Article Metrics": "YesCitationsNo citations were found for this article, but you may check onGoogle ScholarNoArticle metric data becomes available approximately 24 hours after publication online.",
            "Citations": "No citations were found for this article, but you may check onGoogle Scholar"
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2076-3417/15/24/12881",
        "scraped_at": "2025-12-05 23:50:06"
    },
    {
        "title": "Reinforcement Learning-Driven Evolutionary Stackelberg Game Model for Adaptive Breast Cancer Therapy",
        "authors": "byFatemeh Tavakoli,Davud Mohammadpur,Javad Salimi SartakhtiandMohammad Hossein Manshaei",
        "journal": "Math. Comput. Appl.2025,30(6), 134;https://doi.org/10.3390/mca30060134- 5 Dec 2025",
        "abstract": "In this paper, we present an integrative framework based on Evolutionary Stackelberg Game Theory to model the strategic interaction between a physician, acting as a rational leader, and a heterogeneous population of treatment-sensitive and treatment-resistant breast cancer cells. The model incorporates ecological competition, evolutionary adaptation, and spatial heterogeneity, enabling prediction of tumor progression under clinically relevant treatment protocols. Using tumor volume data obtained from breast cancer-bearing mice treated with Capecitabine and Gemcitabine, we estimated treatment and subject-specific parameters via the GEKKO optimization package in Python. Benchmarking against classical tumor growth models (Exponential, Logistic, and Gompertz) showed that while classical models capture monotonic growth, they fail to reproduce complex, non-monotonic behaviors such as treatment-induced regression, rebound, and phenotypic switching. The game-theoretic approach achieved superior alignment with experimental data across Maximum Tolerated Dose, Dose-Modulation Adaptive Therapy, and Intermittent Adaptive Therapy protocols. To enhance adaptability, we integrated reinforcement learning (RL) for both single-agent and combination chemotherapy. The RL agent learned dosing policies that maximized tumor regression while minimizing cumulative drug exposure and resistance, with combination therapy exploiting dose diversification to improve control without exceeding total dose budgets. Incorporating reaction diffusion equations allowed the model to capture spatial dispersal of sensitive (cooperative) and resistant (defector) phenotypes, revealing that spatially aware adaptive strategies more effectively suppress resistant clones than non-spatial approaches. These results demonstrate that evolutionarily informed, spatially explicit, and computationally optimized strategies can outperform conventional fixed-dose regimens in reducing resistance, lowering toxicity, and improving efficacy. This framework offers a biologically interpretable tool for guiding evolution-aware, patient-tailored cancer therapies toward improved long-term outcomes.",
        "keywords": ":breast cancer modeling; Stackelberg game theory; adaptive therapy; phenotypic dynamics; diffusion; treatment-sensitive cells; treatment-resistant cellsbreast cancer modeling;Stackelberg game theory;adaptive therapy;phenotypic dynamics;diffusion;treatment-sensitive cells;treatment-resistant cells",
        "full_content": {
            "Abstract": "In this paper, we present an integrative framework based on Evolutionary Stackelberg Game Theory to model the strategic interaction between a physician, acting as a rational leader, and a heterogeneous population of treatment-sensitive and treatment-resistant breast cancer cells. The model incorporates ecological competition, evolutionary adaptation, and spatial heterogeneity, enabling prediction of tumor progression under clinically relevant treatment protocols. Using tumor volume data obtained from breast cancer-bearing mice treated with Capecitabine and Gemcitabine, we estimated treatment and subject-specific parameters via the GEKKO optimization package in Python. Benchmarking against classical tumor growth models (Exponential, Logistic, and Gompertz) showed that while classical models capture monotonic growth, they fail to reproduce complex, non-monotonic behaviors such as treatment-induced regression, rebound, and phenotypic switching. The game-theoretic approach achieved superior alignment with experimental data across Maximum Tolerated Dose, Dose-Modulation Adaptive Therapy, and Intermittent Adaptive Therapy protocols. To enhance adaptability, we integrated reinforcement learning (RL) for both single-agent and combination chemotherapy. The RL agent learned dosing policies that maximized tumor regression while minimizing cumulative drug exposure and resistance, with combination therapy exploiting dose diversification to improve control without exceeding total dose budgets. Incorporating reaction diffusion equations allowed the model to capture spatial dispersal of sensitive (cooperative) and resistant (defector) phenotypes, revealing that spatially aware adaptive strategies more effectively suppress resistant clones than non-spatial approaches. These results demonstrate that evolutionarily informed, spatially explicit, and computationally optimized strategies can outperform conventional fixed-dose regimens in reducing resistance, lowering toxicity, and improving efficacy. This framework offers a biologically interpretable tool for guiding evolution-aware, patient-tailored cancer therapies toward improved long-term outcomes. Keywords:breast cancer modeling; Stackelberg game theory; adaptive therapy; phenotypic dynamics; diffusion; treatment-sensitive cells; treatment-resistant cellsbreast cancer modeling;Stackelberg game theory;adaptive therapy;phenotypic dynamics;diffusion;treatment-sensitive cells;treatment-resistant cells",
            "Share and Cite": "MDPI and ACS StyleTavakoli, F.;                     Mohammadpur, D.;                     Sartakhti, J.S.;                     Manshaei, M.H.    \n        Reinforcement Learning-Driven Evolutionary Stackelberg Game Model for Adaptive Breast Cancer Therapy.Math. Comput. Appl.2025,30, 134.\n    https://doi.org/10.3390/mca30060134AMA StyleTavakoli F,                                 Mohammadpur D,                                 Sartakhti JS,                                 Manshaei MH.        \n                Reinforcement Learning-Driven Evolutionary Stackelberg Game Model for Adaptive Breast Cancer Therapy.Mathematical and Computational Applications. 2025; 30(6):134.\n        https://doi.org/10.3390/mca30060134Chicago/Turabian StyleTavakoli, Fatemeh,                                 Davud Mohammadpur,                                 Javad Salimi Sartakhti,                                 and Mohammad Hossein Manshaei.        \n                2025. \"Reinforcement Learning-Driven Evolutionary Stackelberg Game Model for Adaptive Breast Cancer Therapy\"Mathematical and Computational Applications30, no. 6: 134.\n        https://doi.org/10.3390/mca30060134APA StyleTavakoli, F.,                                 Mohammadpur, D.,                                 Sartakhti, J. S.,                                 & Manshaei, M. H.        \n        \n        (2025). Reinforcement Learning-Driven Evolutionary Stackelberg Game Model for Adaptive Breast Cancer Therapy.Mathematical and Computational Applications,30(6), 134.\n        https://doi.org/10.3390/mca30060134",
            "Article Metrics": "YesCitationsNo citations were found for this article, but you may check onGoogle ScholarNoArticle metric data becomes available approximately 24 hours after publication online.",
            "Citations": "No citations were found for this article, but you may check onGoogle Scholar"
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2297-8747/30/6/134",
        "scraped_at": "2025-12-05 23:50:34"
    },
    {
        "title": "CLFF-NER: A Cross-Lingual Feature Fusion Model for Named Entity Recognition in the Traditional Chinese Festival Culture Domain",
        "authors": "byShenghe Yang,Kun He,Wei LiandYingying He",
        "journal": "Informatics2025,12(4), 136;https://doi.org/10.3390/informatics12040136- 5 Dec 2025",
        "abstract": "With the rapid development of information technology, there is an increasing demand for the digital preservation of traditional festival culture and the extraction of relevant knowledge. However, existing research on Named Entity Recognition (NER) for Chinese traditional festival culture lacks support from high-quality corpora and dedicated model methods. To address this gap, this study proposes a Named Entity Recognition model, CLFF-NER, which integrates multi-source heterogeneous information. The model operates as follows: first, Multilingual BERT is employed to obtain the contextual semantic representations of Chinese and English sentences. Subsequently, a Multiconvolutional Kernel Network (MKN) is used to extract the local structural features of entities. Then, a Transformer module is introduced to achieve cross-lingual, cross-attention fusion of Chinese and English semantics. Furthermore, a Graph Neural Network (GNN) is utilized to selectively supplement useful English information, thereby alleviating the interference caused by redundant information. Finally, a gating mechanism and Conditional Random Field (CRF) are combined to jointly optimize the recognition results. Experiments were conducted on the public Chinese Festival Culture Dataset (CTFCDataSet), and the model achieved 89.45%, 90.01%, and 89.73% in precision, recall, and F1 score, respectively‚Äîsignificantly outperforming a range of mainstream baseline models. Meanwhile, the model also demonstrated competitive performance on two other public datasets, Resume and Weibo, which verifies its strong cross-domain generalization ability.Keywords:English features;transformer;GNN;MKN",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Chinese traditional festivals are a crucial component of the long-standing historical and cultural heritage of the Chinese nation, boasting diverse forms and rich connotations. The formation of traditional festivals is not only a reflection of a nation‚Äôs historical development process but also a process of long-term accumulation and condensation of a country‚Äôs historical culture. The preservation of Chinese traditional festivals offers the following benefits: (1) it enables the intergenerational inheritance of these precious cultural heritages, allowing future generations to understand and recognize the historical and cultural roots of the Chinese nation; (2) certain festivals can drive the development of related industries such as tourism, catering, shopping, and entertainment, fostering a ‚Äúfestival economy‚Äù; (3) inheriting traditional festivals facilitates a better display of the profoundness and extensiveness of Chinese culture to the world, promotes exchanges and dialogs between different cultures, and enhances the international influence of Chinese culture. The application of digital technologies to protect Chinese traditional festival culture is a forward-looking approach that keeps pace with the times. Currently, the task of Chinese NER rarely involves traditional festivals. The purpose of this study is to attract more researchers to engage in this field and fill the gaps in relevant research. NER is an information extraction technology that holds significant advantages in extracting valuable information from text data. It serves as a fundamental task for downstream applications such as Natural Language Processing (NLP) and information extraction systems [1,2,3]. Text encoding is also a key technology, and a high-quality encoder plays a crucial role in improving model performance. Recently, the BERT pre-trained language model [4,5] has been widely applied in NER tasks. By conducting unsupervised pre-training on large-scale text corpora, BERT generates word embeddings by considering both left and right contextual information, capturing comprehensive semantic dependencies and contextual nuances of the text. For instance, in the sentence ‚ÄúApple Inc. released the new iPhone‚Äù, the representation of ‚ÄúApple‚Äù is influenced by both ‚ÄúInc.‚Äù and ‚ÄúiPhone‚Äù. Therefore, the model proposed in this study adopts the BERT pre-trained language model for sentence encoding. In recent years, various BERT-based deep learning and machine learning models have been proposed, such as BERT-BiLSTM [6], BERT-CRF [7], BERT-CNN [8], and BERT-BiLSTM-CRF [9]. However, these models often underperform on Chinese traditional festival entity recognition, as they struggle with long-tail, regional, or transliterated festival names, and generally lack mechanisms to incorporate external knowledge or cross-lingual information. Deep learning methods like Convolutional Neural Networks (CNNs) and Bidirectional Long Short-Term Memory (BiLSTM) networks are widely used in information extraction tasks. In the field of text processing, CNNs are mainly used to extract local information from sentences‚Äîsufficient local information usually contributes to improved model performance‚Äîwhile LSTM networks excel at capturing long-distance dependencies and are thus employed to extract contextual information from sentences. This study focuses on Chinese traditional festivals and trains the model using the public CTFCDataSet. The data of this dataset is sourced from the China National Cultural Resource Repository (www.minwang.com.cn/mzwhzyk/674771/index.html(accessed on November to December 2023)). Inspired by studies [10,11,12] that introduce multimodal data to enhance entity recognition performance, this paper explores the use of English data to investigate whether cross-lingual information can improve the recognition of domain-specific entities in Chinese. Specifically, since Chinese traditional festivals often involve long-tail, regional, transliterated, or compound names, relying solely on Chinese text may lead to missed or inaccurate recognition. Therefore, in this study, Chinese sentences are translated into English. English is used as auxiliary information to be fused with Chinese features, aiming to improve the model‚Äôs performance in recognizing culturally rich festival entities. This study proposes the CLFF-NER model, based on the Multilingual BERT pre-trained language model, which can encode over 100 languages, including Chinese and English. Since the dataset used in this study only contains Chinese sentences, corresponding English sentences were translated from the Chinese ones. Therefore, Multilingual BERT was adopted as the encoder to simultaneously encode Chinese sentences and their translated English counterparts in this paper. The translation service was provided via Baidu Cloud (https://cloud.baidu.com(accessed on 7 November 2024)). The contributions of this study are as follows: This study proposes the CLFF-NER model, an NER framework that integrates multi-source heterogeneous information. The model leverages English sentences as auxiliary information to enhance the performance of Chinese NER models. The model uses a Transformer module to fully fuse Chinese and English features, while a GNN module selectively integrates English features into Chinese features. To the best of our knowledge, this study is the first to utilize English features as auxiliary information to improve the performance of Chinese NER models. Experiments on the public Resume and Weibo datasets were conducted to verify the model‚Äôs generalization ability. The model achieved competitive results on both datasets, fully demonstrating its strong cross-domain generalization performance.",
            "2. Related Work": "In recent years, the versatility of NER has expanded to multiple domains, including medicine and finance [13,14,15,16,17,18,19,20,21]. In the medical domain, there are several existing datasets in this field, such as MIMIC-III, i2b2, NLM-2018, and BioCreative, each containing entities of varying quantities and types. This study focuses on Chinese traditional festivals; therefore, it employs the public CTFCDataSet. The data source of this dataset was specified in the previous section. The dataset used in this study includes six categories of entities, namely PER (Person), LOC (Location), DATE (Date), NATION (Ethnic Group), FESTIVAL (Festival), and ORG (Organization). For example, in the sentence ‚ÄúThe Naqu Qiangtang Qiaqing Horse Racing Festival is a shining pearl in the traditional festival culture of Tibet and enjoys a high reputation both inside and outside the region,‚Äù ‚ÄúNaqu Qiangtang Qiaqing‚Äù is labeled as the LOC entity type, and ‚ÄúHorse Racing Festival‚Äù is labeled as the FESTIVAL entity type. NER is one of the key technologies in information extraction, involving the detection and classification of words and phrases in text [22]. The understanding of a sentence‚Äôs meaning lies in grasping the key words within it, and these words are usually labeled as named entities of specific categories. Therefore, NER holds significant research significance and application value in NLP [23,24,25]. Probability-based models, including Hidden Markov Models (HMM) and CRFs, have been widely applied in various domains‚Äîuntil they were surpassed by neural networks in recent years. For instance, some papers introduced the LSTM-CRF model, which combines deep learning models and probability models [26,27,28]. The aim of this integration is to leverage the representation learning capability of neural networks and the structural loss of probability models. Existing NER models, including BERT-CRF, BERT-Softmax, BERT-BiLSTM-CRF, and BiLSTM-CRF, have achieved strong performance in general domains. However, they exhibit limitations in Chinese traditional festival recognition: low-frequency or regional festivals are often missed; transliterated or compound festival names are hard to identify; and none of these models incorporate external knowledge or cross-lingual information. These limitations motivate the development of CLFF-NER, which leverages multilingual features and heterogeneous information to improve the recognition of culturally rich festival terms, and its structure is illustrated inFigure 1. The CLFF-NER model uses CNNs to extract entity information of different lengths, and its structure is illustrated inFigure 2. This module utilizes four convolution kernels of different sizes to extract local information at four distinct scales. Researchers have proposed NER models tailored to different domains, in line with the unique characteristics of each field. In recent years, due to its excellent performance and high efficiency, the Transformer has been widely applied in various downstream tasks, such as image segmentation and text translation. As a deep learning-based method, the Transformer architecture comprises encoder and decoder modules [29,30]. However, only the encoder module is utilized in the proposed model, and its primary function is to fully fuse Chinese and English features. Through multi-layer feature interaction computations, the model can achieve sufficient fusion of these bilingual features, and the structure of this Transformer module is illustrated inFigure 3. GNNs are a class of deep neural networks that leverage graph properties to extract features for each node. In this study, graph properties are employed to selectively integrate English features into Chinese features, and the structure of the GNN module is presented inFigure 4. The proposed model operates as follows: First, Multilingual BERT is used to encode Chinese texts and their corresponding English translations. Second, the MKN module extracts multi-scale local information. Third, the Transformer and GNN modules handle the fusion of Chinese and English features‚Äîspecifically, the Transformer performs non-discriminative fusion, while the GNN module selectively integrates English features into Chinese features using attention feature weights. Subsequently, the features from these three modules are fused; after decoding via CRF, the final prediction results are obtained. Detailed descriptions of these three modules are provided inSection 3.",
            "3. Method": "The structure of the model proposed in this study is illustrated inFigure 1. Specifically, this model consists of three parts, namely the encoding layer, the feature extraction layer, and the decoding layer. The encoding layer employs the pre-trained BERT model as its encoder. The feature extraction layer comprises three modules: the MKN module, the Transformer module, and the GNN module. Finally, the decoding layer performs decoding to generate the final recognition results. Figure 1.CLFF-NER Structure Diagram. Figure 1.CLFF-NER Structure Diagram. The MKN module is designed to extract multi-scale local features, making full use of local information to enrich sentence representations; the Transformer module fuses English and Chinese features; and the GNN module selectively fuses English features into Chinese features. Given a Chinese sentenceùê∂={ùëê1,ùëê2,ùëê3,‚Ä¶,ùëên}C={c1,c2,c3,‚Ä¶,cn}, and its corresponding English sentenceùê∏={ùëí1,ùëí2,ùëí3,‚Ä¶,ùëíùëö}E={e1,e2,e3,‚Ä¶,em}, ci refers to the i-th character in the Chinese sentence, n represents the number of characters in the Chinese sentence, ej refers to the j-th character in the English sentence, and m represents the number of words in the English sentence. After BERT encoding, the corresponding embedding vectorsMcandMeare obtained, and the calculation formula is as follows:ùëÄc=ùê∏ùëõùëêùëúùëëùëñùëõùëî(ùê∂)Mc=Encoding(C)(1)ùëÄe=ùê∏ùëõùëêùëúùëëùëñùëõùëî(ùê∏)Me=Encoding(E)(2) Here,Mc‚àà B √ó n √ó 768 andMe‚àà B √ó m √ó 768, where B denotes the batch size. Detailed implementation details of each module will be provided in subsequent sections. 3.1. MKN ModuleThe MKN module is constructed using multiple CNNs with different convolution kernel sizes. The structure of this module is illustrated inFigure 2.Figure 2.Schematic diagram of the MKN structure.Figure 2.Schematic diagram of the MKN structure.By statistically analyzing the entity length distribution in the dataset, it was found that over 90% of entities have a length ranging from 1 to 7. This indicates that local information in sentences plays a crucial role; thus, this module is designed to extract more local information to enrich sentence representations. Specifically, the module utilizes CNNs with 4 convolutional kernels of sizei√ó 768 (wherei= {1, 3, 5, 7}) to extract local features. Its calculation formula is as follows:ùëÄ=ùëÅùëúùëüùëö(‚àë4ùëñ=1ùëÖùëíùêøùëà(ùê∂ùëúùëõùë£ùëñ√ó768(ùëÄùëê)))M=Norm(‚àëi=14ReLU(Convi√ó768(Mc)))(3)In the formula,Mc‚àà B √ó n √ó 768, whereReLUdenotes the activation function max andNormrepresents normalization. 3.2. Transformer ModuleThis module is improved based on the Transformer, where the self-attention mechanism of the Transformer is modified to a cross-attention mechanism. The structure of this module is illustrated inFigure 3. Specifically, the concatenated English and Chinese features serve as the key and value in the attention mechanism, while the Chinese features act as the query. Before feeding the features into the Transformer module, K and V undergo preliminary fusion via self-attention, and the fused results are then input into the Transformer module. The calculation formulas are as follows:ùùÜ=[ùëÄùëê;ùëÄùëí]œÅ=[Mc;Me](4)ùëòùëíùë¶=ùë£ùëéùëôùë¢ùëí=ùë†ùëíùëôùëì_ùê¥ùë°ùë°ùëíùëõùë°ùëñùëúùëõ(ùùÜ,ùùÜ,ùùÜ)=ùë†ùëúùëìùë°ùëöùëéùë•(ùùÜ√óùùÜùëáùëëùëò‚àí‚àí‚àö)key=value=self_Attention(œÅ,œÅ,œÅ)=softmax(œÅ√óœÅTdk)(5)In the formula, ‚Äò;‚Äô denotes the concatenation operation,ùùÜœÅ‚àà B √ó (n + m) √ó 768,key,value‚àà B √ó n √ó 768, In the formula,Tdenotes matrix transposition, anddkdenotes the number of attention heads.Figure 3.Schematic diagram of the Transformer module structure.Figure 3.Schematic diagram of the Transformer module structure.The encoding layer of this module consists of 6 layers. The first layer performs queriesùêê=ùë¥cQ=Mc, and for layers after the first, the input comes from the output of the previous layerùêê=ùëªQ=T.KandValways stand for key and value, respectively.Positional encoding is also incorporated into the model, whose purpose is to inject positional information into the input sequence, thereby enabling the model to perceive the position of each element in the sequence. The calculation formula for positional encoding is as follows:ùúã(ùëùùëúùë†,ùëñ)=ùë†ùëñùëõ(ùëùùëúùë†10,0002ùëñùëíùëöùëèùëíùëë_ùë†ùëñùëßùëí/)œÄ(pos,i)=sin(pos10,0002iembed_size)(6)ùúã(ùëùùëúùë†,ùëñ+1)=ùëêùëúùë†(ùëùùëúùë†10,0002ùëñùëíùëöùëèùëíùëë_ùë†ùëñùëßùëí/)œÄ(pos,i+1)=cos(pos10,0002iembed_size)(7)In the formula,posdenotes the actual positional information, andirepresents whether the position is even or odd. Specifically, whenposis even, the positional encoding is calculated using Equation (6); whenposis odd, the positional encoding is calculated using Equation (7).embed_sizedenotes the embedding dimension. The calculation formula for this module is as follows:ùëÑ=ùëÄùëê+ùúã(ùëùùëúùë†,ùëñ)Q=Mc+œÄ(pos,i)(8)‚Ñä=ùê∂ùëüùëúùë†ùë†ùê¥ùë°ùë°ùëíùëõùë°ùëñùëúùëõ(ùëÑ,ùêæ,ùëâ)=ùë†ùëúùëìùë°ùëöùëéùë•(ùëÑ√óùêæùëáùëëùëò‚àí‚àí‚àö)ùëâg=CrossAttention(Q,K,V)=softmax(Q√óKTdk)V(9)ùêª=ùëÅùëúùëüùëö(ùëÑ+‚Ñä)H=Norm(Q+g)(10)ùëì(ùêª)=ùê∑ùëüùëúùëùùëúùë¢ùë°(ùëÖùëíùêøùëà(ùêªùëä1+ùëè1))ùëä2+ùëè2f(H)=Dropout(ReLU(HW1+b1))W2+b2(11)ùëá=ùëÅùëúùëüùëö(ùëì(ùêª)+ùêª)T=Norm(f(H)+H)(12)In the formula,‚Ñäg‚àà B √ó n √ó 768,H‚àà B √ó n √ó 768, whereReLUdenotes the activation function max, andNormrepresents normalization.Wiandbiare trainable matrices. Since the Q and K,V have different sequence lengths, the Scaled Dot-Product Attention is adopted to allow each Q position to attend to all K,V positions.The function of this module is to fully fuse Chinese and English features using the attention mechanism; however, this process may introduce noisy information. Based on this consideration, a selective feature fusion module is proposed. This module, leveraging the attention mechanism, obtains weight scores of Chinese features relative to both Chinese and English features. Although selectively integrating English features into Chinese features reduces the introduction of noisy data, it also loses some effective information. Thus, this study employs both complete fusion and selective fusion of English features. 3.3. GNN ModuleThis study employs a GNN to selectively integrate English features into Chinese features. Compared with the Transformer module, this module reduces the introduction of noisy data, which can better improve the model‚Äôs performance. Its structure is illustrated inFigure 4. Specifically, inFigure 4,v,i,j,and k belong to Chinese nodes, while x and y are English nodes. The attention mechanism is used to obtain weight scoresùõºŒ±of Chinese features relative to both Chinese and English features, which are then used to construct the graph and the weights of the graph‚Äôs edges. The weight calculation method is as follows:Figure 4.GNN Structure Diagram.Figure 4.GNN Structure Diagram.ùõΩ=ùë†ùëúùëìùë°ùëöùëéùë•(ùùÜ√óùùÜùëáùëëùëò‚àí‚àí‚àö)Œ≤=softmax(œÅ√óœÅTdk)(13)ùõºùëñ,ùëó=ùëíùõΩùëñ,ùëó‚àëùê¥ùëò=1ùëíùõΩùëñ,ùëòŒ±i,j=eŒ≤i,j‚àëk=1AeŒ≤i,k(14)whereùùÜœÅrefers to the result of Equation (4),Œ≤‚àà (n + m) √ó (n + m),Œ±‚àà (n + m) √ó (n + m), anddkdenotes the number of attention heads. The results obtained through Equations (13) and (14) require further processing. Weight values less than 0 are set to 0, and the processing method is as follows:ùõºùëñ,ùëó={ùõºùëñ,ùëó,ùõºùëñ,ùëó>00,ùõºùëñ,ùëó‚â§0Œ±i,j=Œ±i,j,Œ±i,j>00,Œ±i,j‚â§0(15)In the formula,ùõºùëñ,ùëóŒ±i,jdenotes the attention score of thei-th character to thej-th character. Based on the above calculation results, the calculation method for the aggregation of graph nodes is as follows:ùõæùëñ=‚àëùëõùëò=1ùõºùëñ,ùëò√óùëÄùëêùëòùëÅùëê+1Œ≥i=‚àëk=1nŒ±i,k√óMckNc+1(16)In the formula,ùëÄùëêùëòMck‚àà 1 √ó 768 denotes the embedding vector of thek-th character in the Chinese sentence, andNcdenotes the number of nodes connected by an edge to nodei.The resultùõæùëñŒ≥icalculated by Equation (16) is the outcome of Chinese feature aggregation, which has not yet aggregated English features into Chinese features. To address this issue, English embeddings are aggregated intoùëîigiin a directed manner, and the formula is as follows:ùëîùëó=‚àëùëõ+ùëöùëò=ùëõ+1ùõºùëó,ùëò√óùëÄùëíùëò+ùõæùëóùëÅùëí+2,(ùëò‚â†ùëó,ùëó>ùëõ)gj=‚àëk=n+1n+mŒ±j,k√óMek+Œ≥jNe+2,(k‚â†j,j>n)(17)In the formula,ùëÄùëíùëòMek‚àà 1 √ó 768 denotes the embedding vector of thek-th character in the English sentence, andNedenotes the number of nodes connected by an edge to nodej. To avoid division by zero, a very small value is added toNe. Finally, the output of the GNN module can be denoted as G ‚àà n √ó 768,G=(ùëî1,ùëî2,ùëî3,‚Ä¶,ùëîn)G=(g1,g2,g3,‚Ä¶,gn). 3.4. Decode ModuleAppropriate feature fusion methods are a key technique for improving model performance. This work adopts residual addition to fuse different features, and the fused features are fed into a Conditional Random Field (CRF) for decoding. The calculation formula is as follows:ùúì1=ùúë1ùëÄ+(1‚àíùúë1)ùëáœà1=œÜ1M+(1‚àíœÜ1)T(18)ùúì2=ùúë2ùëÄ+(1‚àíùúë2)ùê∫œà2=œÜ2M+(1‚àíœÜ2)G(19)ùúê=ùêπùê∂(ùúë3ùúì1+(1‚àíùúë3)ùúì2)œÖ=FC(œÜ3œà1+(1‚àíœÜ3)œà2)(20)In the formula,ùúëiœÜidenotes a trainable matrix, andFCdenotes a fully connected layer.For a given observation sequenceX= {x1,x2,x3,‚Ä¶,xm} and its corresponding output label sequenceY= {y1,y2,y3,‚Ä¶,ym}, the results calculated by Equations (18) and (19) are input into the fully connected layer with the aim of mapping features to the label space. Finally, after CRF decoding, the probability scores for each label category are obtained. The calculation is as follows:ùë†ùëêùëúùëüùëíùë†(ùëã,ùëå)=‚àëùëõùëñ=1ùúêùëñ,ùë¶ùëñ+‚àëùëõ+1ùëñ=1ùê¥ùë¶ùëñ‚àí1,ùë¶ùëñscores(X,Y)=‚àëi=1nœÖi,yi+‚àëi=1n+1Ayi‚àí1,yi(21)In the formula,œÖœÖdenotes the output of the fully connected layer, and A denotes the transition matrix. The normalized calculation ofscores(X,Y) is as follows:ùëÉ(ùëå/ùëã)=1ùëç(ùëã)ùëíùë•ùëù(‚àëùëõùëñ=1ùë†ùëêùëúùëüùëíùë†(ùë¶ùëñ‚àí1,ùë¶ùëñ,ùëã))P(Y/X)=1Z(X)exp(‚àëi=1nscores(yi‚àí1,yi,X))(22)In the formula,Z(X) denotes the normalization factor.To address the class imbalance problem, the model uses a weighted cross-entropy loss function and assigns a weight to each label category. Additionally, it employs a maximum likelihood loss function to achieve better prediction performance. The calculation formulas for Equations (23) and (24) are as follows:ùëôùëúùë†ùë†1=‚àí1ùëÅ‚àëùëÅ1‚àëùê∂ùëê=1ùë§ùëêùë¶ùëñùëêùëôùëúùëî(ùë¶‚Ä≤ùëñùëê)loss1=‚àí1N‚àë1N‚àëc=1Cwcyiclog(yic‚Ä≤)(23)ùë§ùëê=ùëôùëúùëî(1+ùëÅùëêùëÅùëú)wc=log(1+NcNo)(24)ùëôùëúùë†ùë†2=‚àí‚àëùëõùëñ=1ùëôùëúùëîùëÉ(ùë¶ùëñ|ùë•ùëñ)loss2=‚àí‚àëi=1nlogP(yi|xi)(25)In the formula,Ndenotes the number of samples,c‚ààCdenotes a label category, yicdenotes the true label of the sequence,ùë¶‚Ä≤ùëñùëêyic‚Ä≤denotes the predicted label,wcdenotes the weight of categoryc,Ncdenotes the number of labels for categoryc, andNodenotes the total number of labels. The final model loss is calculated as the following equation:ùëôùëúùë†ùë†=ùúô√óùëôùëúùë†ùë†1+(1‚àíùúô)√óùëôùëúùë†ùë†2loss=œï√óloss1+(1‚àíœï)√óloss2(26)œïœïdenotes the trainable matrix.Finally, during the training phase, after decoding, the model obtains the sequence with the highest output probability.ùë¶‚àó=argmax(ùë†ùëêùëúùëüùëíùë†(ùëã,ùëå‚Ä≤))y*=argmax(scores(X,Y‚Ä≤))(27)",
            "3.1. MKN Module": "The MKN module is constructed using multiple CNNs with different convolution kernel sizes. The structure of this module is illustrated inFigure 2. Figure 2.Schematic diagram of the MKN structure. Figure 2.Schematic diagram of the MKN structure. By statistically analyzing the entity length distribution in the dataset, it was found that over 90% of entities have a length ranging from 1 to 7. This indicates that local information in sentences plays a crucial role; thus, this module is designed to extract more local information to enrich sentence representations. Specifically, the module utilizes CNNs with 4 convolutional kernels of sizei√ó 768 (wherei= {1, 3, 5, 7}) to extract local features. Its calculation formula is as follows:ùëÄ=ùëÅùëúùëüùëö(‚àë4ùëñ=1ùëÖùëíùêøùëà(ùê∂ùëúùëõùë£ùëñ√ó768(ùëÄùëê)))M=Norm(‚àëi=14ReLU(Convi√ó768(Mc)))(3) In the formula,Mc‚àà B √ó n √ó 768, whereReLUdenotes the activation function max andNormrepresents normalization.",
            "3.2. Transformer Module": "This module is improved based on the Transformer, where the self-attention mechanism of the Transformer is modified to a cross-attention mechanism. The structure of this module is illustrated inFigure 3. Specifically, the concatenated English and Chinese features serve as the key and value in the attention mechanism, while the Chinese features act as the query. Before feeding the features into the Transformer module, K and V undergo preliminary fusion via self-attention, and the fused results are then input into the Transformer module. The calculation formulas are as follows:ùùÜ=[ùëÄùëê;ùëÄùëí]œÅ=[Mc;Me](4)ùëòùëíùë¶=ùë£ùëéùëôùë¢ùëí=ùë†ùëíùëôùëì_ùê¥ùë°ùë°ùëíùëõùë°ùëñùëúùëõ(ùùÜ,ùùÜ,ùùÜ)=ùë†ùëúùëìùë°ùëöùëéùë•(ùùÜ√óùùÜùëáùëëùëò‚àí‚àí‚àö)key=value=self_Attention(œÅ,œÅ,œÅ)=softmax(œÅ√óœÅTdk)(5) In the formula, ‚Äò;‚Äô denotes the concatenation operation,ùùÜœÅ‚àà B √ó (n + m) √ó 768,key,value‚àà B √ó n √ó 768, In the formula,Tdenotes matrix transposition, anddkdenotes the number of attention heads. Figure 3.Schematic diagram of the Transformer module structure. Figure 3.Schematic diagram of the Transformer module structure. The encoding layer of this module consists of 6 layers. The first layer performs queriesùêê=ùë¥cQ=Mc, and for layers after the first, the input comes from the output of the previous layerùêê=ùëªQ=T.KandValways stand for key and value, respectively. Positional encoding is also incorporated into the model, whose purpose is to inject positional information into the input sequence, thereby enabling the model to perceive the position of each element in the sequence. The calculation formula for positional encoding is as follows:ùúã(ùëùùëúùë†,ùëñ)=ùë†ùëñùëõ(ùëùùëúùë†10,0002ùëñùëíùëöùëèùëíùëë_ùë†ùëñùëßùëí/)œÄ(pos,i)=sin(pos10,0002iembed_size)(6)ùúã(ùëùùëúùë†,ùëñ+1)=ùëêùëúùë†(ùëùùëúùë†10,0002ùëñùëíùëöùëèùëíùëë_ùë†ùëñùëßùëí/)œÄ(pos,i+1)=cos(pos10,0002iembed_size)(7) In the formula,posdenotes the actual positional information, andirepresents whether the position is even or odd. Specifically, whenposis even, the positional encoding is calculated using Equation (6); whenposis odd, the positional encoding is calculated using Equation (7).embed_sizedenotes the embedding dimension. The calculation formula for this module is as follows:ùëÑ=ùëÄùëê+ùúã(ùëùùëúùë†,ùëñ)Q=Mc+œÄ(pos,i)(8)‚Ñä=ùê∂ùëüùëúùë†ùë†ùê¥ùë°ùë°ùëíùëõùë°ùëñùëúùëõ(ùëÑ,ùêæ,ùëâ)=ùë†ùëúùëìùë°ùëöùëéùë•(ùëÑ√óùêæùëáùëëùëò‚àí‚àí‚àö)ùëâg=CrossAttention(Q,K,V)=softmax(Q√óKTdk)V(9)ùêª=ùëÅùëúùëüùëö(ùëÑ+‚Ñä)H=Norm(Q+g)(10)ùëì(ùêª)=ùê∑ùëüùëúùëùùëúùë¢ùë°(ùëÖùëíùêøùëà(ùêªùëä1+ùëè1))ùëä2+ùëè2f(H)=Dropout(ReLU(HW1+b1))W2+b2(11)ùëá=ùëÅùëúùëüùëö(ùëì(ùêª)+ùêª)T=Norm(f(H)+H)(12) In the formula,‚Ñäg‚àà B √ó n √ó 768,H‚àà B √ó n √ó 768, whereReLUdenotes the activation function max, andNormrepresents normalization.Wiandbiare trainable matrices. Since the Q and K,V have different sequence lengths, the Scaled Dot-Product Attention is adopted to allow each Q position to attend to all K,V positions. The function of this module is to fully fuse Chinese and English features using the attention mechanism; however, this process may introduce noisy information. Based on this consideration, a selective feature fusion module is proposed. This module, leveraging the attention mechanism, obtains weight scores of Chinese features relative to both Chinese and English features. Although selectively integrating English features into Chinese features reduces the introduction of noisy data, it also loses some effective information. Thus, this study employs both complete fusion and selective fusion of English features.",
            "3.3. GNN Module": "This study employs a GNN to selectively integrate English features into Chinese features. Compared with the Transformer module, this module reduces the introduction of noisy data, which can better improve the model‚Äôs performance. Its structure is illustrated inFigure 4. Specifically, inFigure 4,v,i,j,and k belong to Chinese nodes, while x and y are English nodes. The attention mechanism is used to obtain weight scoresùõºŒ±of Chinese features relative to both Chinese and English features, which are then used to construct the graph and the weights of the graph‚Äôs edges. The weight calculation method is as follows: Figure 4.GNN Structure Diagram. Figure 4.GNN Structure Diagram. ùõΩ=ùë†ùëúùëìùë°ùëöùëéùë•(ùùÜ√óùùÜùëáùëëùëò‚àí‚àí‚àö)Œ≤=softmax(œÅ√óœÅTdk)(13)ùõºùëñ,ùëó=ùëíùõΩùëñ,ùëó‚àëùê¥ùëò=1ùëíùõΩùëñ,ùëòŒ±i,j=eŒ≤i,j‚àëk=1AeŒ≤i,k(14)whereùùÜœÅrefers to the result of Equation (4),Œ≤‚àà (n + m) √ó (n + m),Œ±‚àà (n + m) √ó (n + m), anddkdenotes the number of attention heads. The results obtained through Equations (13) and (14) require further processing. Weight values less than 0 are set to 0, and the processing method is as follows:ùõºùëñ,ùëó={ùõºùëñ,ùëó,ùõºùëñ,ùëó>00,ùõºùëñ,ùëó‚â§0Œ±i,j=Œ±i,j,Œ±i,j>00,Œ±i,j‚â§0(15) In the formula,ùõºùëñ,ùëóŒ±i,jdenotes the attention score of thei-th character to thej-th character. Based on the above calculation results, the calculation method for the aggregation of graph nodes is as follows:ùõæùëñ=‚àëùëõùëò=1ùõºùëñ,ùëò√óùëÄùëêùëòùëÅùëê+1Œ≥i=‚àëk=1nŒ±i,k√óMckNc+1(16) In the formula,ùëÄùëêùëòMck‚àà 1 √ó 768 denotes the embedding vector of thek-th character in the Chinese sentence, andNcdenotes the number of nodes connected by an edge to nodei. The resultùõæùëñŒ≥icalculated by Equation (16) is the outcome of Chinese feature aggregation, which has not yet aggregated English features into Chinese features. To address this issue, English embeddings are aggregated intoùëîigiin a directed manner, and the formula is as follows:ùëîùëó=‚àëùëõ+ùëöùëò=ùëõ+1ùõºùëó,ùëò√óùëÄùëíùëò+ùõæùëóùëÅùëí+2,(ùëò‚â†ùëó,ùëó>ùëõ)gj=‚àëk=n+1n+mŒ±j,k√óMek+Œ≥jNe+2,(k‚â†j,j>n)(17) In the formula,ùëÄùëíùëòMek‚àà 1 √ó 768 denotes the embedding vector of thek-th character in the English sentence, andNedenotes the number of nodes connected by an edge to nodej. To avoid division by zero, a very small value is added toNe. Finally, the output of the GNN module can be denoted as G ‚àà n √ó 768,G=(ùëî1,ùëî2,ùëî3,‚Ä¶,ùëîn)G=(g1,g2,g3,‚Ä¶,gn).",
            "3.4. Decode Module": "Appropriate feature fusion methods are a key technique for improving model performance. This work adopts residual addition to fuse different features, and the fused features are fed into a Conditional Random Field (CRF) for decoding. The calculation formula is as follows:ùúì1=ùúë1ùëÄ+(1‚àíùúë1)ùëáœà1=œÜ1M+(1‚àíœÜ1)T(18)ùúì2=ùúë2ùëÄ+(1‚àíùúë2)ùê∫œà2=œÜ2M+(1‚àíœÜ2)G(19)ùúê=ùêπùê∂(ùúë3ùúì1+(1‚àíùúë3)ùúì2)œÖ=FC(œÜ3œà1+(1‚àíœÜ3)œà2)(20) In the formula,ùúëiœÜidenotes a trainable matrix, andFCdenotes a fully connected layer. For a given observation sequenceX= {x1,x2,x3,‚Ä¶,xm} and its corresponding output label sequenceY= {y1,y2,y3,‚Ä¶,ym}, the results calculated by Equations (18) and (19) are input into the fully connected layer with the aim of mapping features to the label space. Finally, after CRF decoding, the probability scores for each label category are obtained. The calculation is as follows:ùë†ùëêùëúùëüùëíùë†(ùëã,ùëå)=‚àëùëõùëñ=1ùúêùëñ,ùë¶ùëñ+‚àëùëõ+1ùëñ=1ùê¥ùë¶ùëñ‚àí1,ùë¶ùëñscores(X,Y)=‚àëi=1nœÖi,yi+‚àëi=1n+1Ayi‚àí1,yi(21) In the formula,œÖœÖdenotes the output of the fully connected layer, and A denotes the transition matrix. The normalized calculation ofscores(X,Y) is as follows:ùëÉ(ùëå/ùëã)=1ùëç(ùëã)ùëíùë•ùëù(‚àëùëõùëñ=1ùë†ùëêùëúùëüùëíùë†(ùë¶ùëñ‚àí1,ùë¶ùëñ,ùëã))P(Y/X)=1Z(X)exp(‚àëi=1nscores(yi‚àí1,yi,X))(22) In the formula,Z(X) denotes the normalization factor. To address the class imbalance problem, the model uses a weighted cross-entropy loss function and assigns a weight to each label category. Additionally, it employs a maximum likelihood loss function to achieve better prediction performance. The calculation formulas for Equations (23) and (24) are as follows:ùëôùëúùë†ùë†1=‚àí1ùëÅ‚àëùëÅ1‚àëùê∂ùëê=1ùë§ùëêùë¶ùëñùëêùëôùëúùëî(ùë¶‚Ä≤ùëñùëê)loss1=‚àí1N‚àë1N‚àëc=1Cwcyiclog(yic‚Ä≤)(23)ùë§ùëê=ùëôùëúùëî(1+ùëÅùëêùëÅùëú)wc=log(1+NcNo)(24)ùëôùëúùë†ùë†2=‚àí‚àëùëõùëñ=1ùëôùëúùëîùëÉ(ùë¶ùëñ|ùë•ùëñ)loss2=‚àí‚àëi=1nlogP(yi|xi)(25) In the formula,Ndenotes the number of samples,c‚ààCdenotes a label category, yicdenotes the true label of the sequence,ùë¶‚Ä≤ùëñùëêyic‚Ä≤denotes the predicted label,wcdenotes the weight of categoryc,Ncdenotes the number of labels for categoryc, andNodenotes the total number of labels. The final model loss is calculated as the following equation:ùëôùëúùë†ùë†=ùúô√óùëôùëúùë†ùë†1+(1‚àíùúô)√óùëôùëúùë†ùë†2loss=œï√óloss1+(1‚àíœï)√óloss2(26)œïœïdenotes the trainable matrix. Finally, during the training phase, after decoding, the model obtains the sequence with the highest output probability.ùë¶‚àó=argmax(ùë†ùëêùëúùëüùëíùë†(ùëã,ùëå‚Ä≤))y*=argmax(scores(X,Y‚Ä≤))(27)",
            "4. Experiments": "In this section, a series of experiments are conducted on the CTFCDataSet dataset to verify the performance of the proposed CLFF-NER method. 4.1. DatasetThe CTFCDataSet dataset consists of 4254 labeled data entries, containing six entity categories and 10,174 entities.Table 1presents the statistical information of various entity types in this dataset, andTable 2shows the statistics of the number of sentences in the train set and the test set.Table 1.Quantity Statistics of Various Entity Types in the CTFCDataSet Dataset.Table 2.Number of sentences in the train set and the test set in CTFCDataSet.Data entities are divided into six categories: PER, DATE, ORG, LOC, FESTIVAL, and NATION. In the dataset, data is stored in the form of key‚Äìvalue pairs, with each pair consisting of a text part and a label part. Specifically, the text part contains text about traditional Chinese festival culture, while the label part includes the start and end positions of each entity, along with its category and the entity itself. The labeling scheme used is the ‚ÄòBIOS‚Äô format, commonly used in NER tasks, where each character is labeled as B-X, I-X, S-X, or O. B-X indicates that the entity belongs to category X and is the beginning of the entity, I-X indicates that the character is part of an entity of category X and is inside the entity, and S-X denotes a single-character entity that belongs to category X, serving as both the start and end of the entity. The O label indicates that the character does not belong to any entity category.The dataset is divided into a training set and a testing set in a 9:1 ratio, and the validation set is separated from the training set at a 9:1 ratio using the train_test_split method in the code. 4.2. Evaluation MetricsThis paper uses three metrics:precision(P),recall(R), andF1 score(F1) to evaluate the performance of the CLFF-NER method. Higher metric values indicate a better performance of the corresponding method.Here, TP, TN, FP, and FN represent true positives, true negatives, false positives, and false negatives, respectively. The evaluation metrics P, R, and F1 can be calculated as follows:ùëÉ=ùëáùëÉ/(ùëáùëÉ+ùêπùëÉ)P=TP/(TP+FP)(28)ùëÖ=ùëáùëÉ/(ùëáùëÉ+ùêπùëÅ)R=TP/(TP+FN)(29)ùêπ1=2‚àóùëÉ‚àóùëÖ/(ùëÉ+ùëÖ)F1=2‚àóP‚àóR/(P+R)(30) 4.3. BaselinesIn the experiment, four typical NER models were selected as baselines to verify the performance of the proposed CLFF-NER model on the CTFCDataSet dataset.BERT-CRF [7]: A model widely used for NER tasks. Combines BERT‚Äôs powerful contextual representations with the sequence labeling capability of CRF.BERT-Softmax [4]: Simple structure with independent decoding, without relying on a CRF layer. Faster training and inference, easier to implement; suitable for scenarios with large datasets or less strict label sequence constraints.BERT-BiLSTM-CRF [9]: Adds a BiLSTM layer on top of BERT embeddings to further capture sequential contextual features, followed by a CRF layer for sequence decoding. Integrates deep semantic representations (BERT), long-distance dependency modeling (BiLSTM), and label dependency modeling (CRF), generally achieving higher accuracy and robustness than the previous two models.BiLSTM-CRF [31]: A pure deep learning approach using BiLSTM for contextual feature extraction and CRF for sequence decoding. Does not rely on pre-trained language models, has a clear structure, and can be trained stably on small to medium-sized datasets. It remains a strong baseline in traditional NER tasks. 4.4. Experimental EnvironmentIn the experiments, all models were run using Keras (version 2.12.0), transformers (version 2.2.2), and Python 3.8. All experiments were conducted on the same machine with the following specifications: operating system Ubuntu 20.04.3 LTS, 31 GB RAM, Intel(R) Xeon(R) CPU E5-2686 v4, and one 12 GB NVIDIA GeForce GTX TITAN X GPU. The experimental hyperparameter settings are shown inTable 3.Table 3.Hyperparameter Settings. 4.5. ResultBased on the setup of the experimental environment, a series of experiments were conducted on the CTFCDataSet dataset, and the experimental results were analyzed. The experimental results are shown inTable 4.Table 4.Comparison of Experimental Results with Baseline Models.Compared with baseline methods, the CLFF-NER method proposed in this study achieves the best performance on the Chinese Named Entity Recognition (NER) task using the CTFCDataSet dataset. Additionally, BERT-based methods generally perform better than those using BiLSTM models. In the stable state, all models attain stable performance in terms of the precision, recall, and F1 score metrics, respectively. Furthermore, the CLFF-NER method has advantages on the CTFCDataSet dataset and outperforms the baseline models significantly.The generalization ability of a model is a crucial factor in evaluating its quality. To verify the generalization ability of the model proposed in this study, tests were conducted on three public datasets. As shown inTable 5andTable 6.Table 5.Comparison on the Weibo dataset.Table 6.Comparison on the Resume dataset.To better understand the model‚Äôs recognition performance for each entity type, the P, R, and F1 score were calculated for each entity category. The results are presented inTable 7.Table 7.Recognition results for each type of entity.To verify the importance of each module, experimental validation was conducted. As shown inTable 8.Table 8.Ablation Experiment.The MKN module proposed in this paper is designed to capture more local information, and the statistic on entity length distribution have been calculated, as shown inFigure 5.Figure 5.The CTFCDataSet‚Äôs Distribution of Entity Lengths.",
            "4.1. Dataset": "The CTFCDataSet dataset consists of 4254 labeled data entries, containing six entity categories and 10,174 entities.Table 1presents the statistical information of various entity types in this dataset, andTable 2shows the statistics of the number of sentences in the train set and the test set. Table 1.Quantity Statistics of Various Entity Types in the CTFCDataSet Dataset. Table 2.Number of sentences in the train set and the test set in CTFCDataSet. Data entities are divided into six categories: PER, DATE, ORG, LOC, FESTIVAL, and NATION. In the dataset, data is stored in the form of key‚Äìvalue pairs, with each pair consisting of a text part and a label part. Specifically, the text part contains text about traditional Chinese festival culture, while the label part includes the start and end positions of each entity, along with its category and the entity itself. The labeling scheme used is the ‚ÄòBIOS‚Äô format, commonly used in NER tasks, where each character is labeled as B-X, I-X, S-X, or O. B-X indicates that the entity belongs to category X and is the beginning of the entity, I-X indicates that the character is part of an entity of category X and is inside the entity, and S-X denotes a single-character entity that belongs to category X, serving as both the start and end of the entity. The O label indicates that the character does not belong to any entity category. The dataset is divided into a training set and a testing set in a 9:1 ratio, and the validation set is separated from the training set at a 9:1 ratio using the train_test_split method in the code.",
            "4.2. Evaluation Metrics": "This paper uses three metrics:precision(P),recall(R), andF1 score(F1) to evaluate the performance of the CLFF-NER method. Higher metric values indicate a better performance of the corresponding method. Here, TP, TN, FP, and FN represent true positives, true negatives, false positives, and false negatives, respectively. The evaluation metrics P, R, and F1 can be calculated as follows:ùëÉ=ùëáùëÉ/(ùëáùëÉ+ùêπùëÉ)P=TP/(TP+FP)(28)ùëÖ=ùëáùëÉ/(ùëáùëÉ+ùêπùëÅ)R=TP/(TP+FN)(29)ùêπ1=2‚àóùëÉ‚àóùëÖ/(ùëÉ+ùëÖ)F1=2‚àóP‚àóR/(P+R)(30)",
            "4.3. Baselines": "In the experiment, four typical NER models were selected as baselines to verify the performance of the proposed CLFF-NER model on the CTFCDataSet dataset. BERT-CRF [7]: A model widely used for NER tasks. Combines BERT‚Äôs powerful contextual representations with the sequence labeling capability of CRF. BERT-Softmax [4]: Simple structure with independent decoding, without relying on a CRF layer. Faster training and inference, easier to implement; suitable for scenarios with large datasets or less strict label sequence constraints. BERT-BiLSTM-CRF [9]: Adds a BiLSTM layer on top of BERT embeddings to further capture sequential contextual features, followed by a CRF layer for sequence decoding. Integrates deep semantic representations (BERT), long-distance dependency modeling (BiLSTM), and label dependency modeling (CRF), generally achieving higher accuracy and robustness than the previous two models. BiLSTM-CRF [31]: A pure deep learning approach using BiLSTM for contextual feature extraction and CRF for sequence decoding. Does not rely on pre-trained language models, has a clear structure, and can be trained stably on small to medium-sized datasets. It remains a strong baseline in traditional NER tasks.",
            "4.4. Experimental Environment": "In the experiments, all models were run using Keras (version 2.12.0), transformers (version 2.2.2), and Python 3.8. All experiments were conducted on the same machine with the following specifications: operating system Ubuntu 20.04.3 LTS, 31 GB RAM, Intel(R) Xeon(R) CPU E5-2686 v4, and one 12 GB NVIDIA GeForce GTX TITAN X GPU. The experimental hyperparameter settings are shown inTable 3. Table 3.Hyperparameter Settings.",
            "4.5. Result": "Based on the setup of the experimental environment, a series of experiments were conducted on the CTFCDataSet dataset, and the experimental results were analyzed. The experimental results are shown inTable 4. Table 4.Comparison of Experimental Results with Baseline Models. Compared with baseline methods, the CLFF-NER method proposed in this study achieves the best performance on the Chinese Named Entity Recognition (NER) task using the CTFCDataSet dataset. Additionally, BERT-based methods generally perform better than those using BiLSTM models. In the stable state, all models attain stable performance in terms of the precision, recall, and F1 score metrics, respectively. Furthermore, the CLFF-NER method has advantages on the CTFCDataSet dataset and outperforms the baseline models significantly. The generalization ability of a model is a crucial factor in evaluating its quality. To verify the generalization ability of the model proposed in this study, tests were conducted on three public datasets. As shown inTable 5andTable 6. Table 5.Comparison on the Weibo dataset. Table 6.Comparison on the Resume dataset. To better understand the model‚Äôs recognition performance for each entity type, the P, R, and F1 score were calculated for each entity category. The results are presented inTable 7. Table 7.Recognition results for each type of entity. To verify the importance of each module, experimental validation was conducted. As shown inTable 8. Table 8.Ablation Experiment. The MKN module proposed in this paper is designed to capture more local information, and the statistic on entity length distribution have been calculated, as shown inFigure 5. Figure 5.The CTFCDataSet‚Äôs Distribution of Entity Lengths.",
            "5. Conclusions": "5.1. Result AnalysisTo verify whether the model performs well, experiments were conducted on three public datasets: CTFCDataSet, Resume, and Weibo. The experimental results indicate that the proposed model has good performance and generalization. A detailed analysis is as follows:Table 4shows that the proposed model achieves the best performance on the CTFCDataSet compared with the other four baseline models, indicating that the overall performance of the proposed model on the CTFCDataSet is good. Compared with BERT-CRF, the model‚Äôs P, R, and F1 values increase by 3.6%, 1.57%, and 2.60%, respectively; compared with BERT-Softmax, P, R, and F1 increase by 3.47%, 1.37%, and 2.44%, respectively; compared with BERT-LSTM-CRF, P and F1 increase by 0.89% and 0.32%, while R decreases by 0.27%; compared with BiLSTM-CRF, P, R, and F1 increase by 6.54%, 13.01%, and 9.89%, respectively. Compared with the four baseline models, the proposed model achieves higher F1 scores on the CTFCDataSet.Table 5shows that on the Resume dataset, the model proposed in this paper is highly competitive compared to other models, achieving the highest recall rate of 96.93% compared to several other models. Compared with the ISEA-NER model, the P value of our model decreases by 0.21%, the R value increases by 0.45%, and the F1 score increases by 0.11%; compared with KGNER, the F1 score increases by 0.04%; compared with HREB-CRF, the P, R, and F1 scores increase by 0.44%, 0.32%, and 0.37%, respectively; compared with GPAT-NER, the P and F1 values decrease by 0.41% and 0.04%, while the R value increases by 0.30%. Compared to the other four models, our model achieves performance that is either superior or comparable.Table 6shows that on the Weibo dataset, the model presented in this paper is highly competitive compared to the other models. It also achieves the highest recall rate of 72.84% among several models. Compared to the ISEA-NER model, this model improves the precision (P), recall (R), and F1 score by 1.47%, 2.34%, and 1.91%, respectively; compared to KGNER, the F1 score increases by 0.86%; compared to IFCG-NER, P, R, and F1 increase by 2.45%, 1.67%, and 2.06%, respectively; compared to MT-Learning, P and F1 decrease by 2.22% and 1.04%, while R rises by 0.14%. Compared with the other four models, the model in this paper also achieves performance that is either superior or comparable.Table 7provides the P, R, and F1 score for each entity category in the CTFCDataSet. The data reveals that all metrics for each entity category exceed 80%. The PER category achieves the highest P value, while the NATION category leads in R and F1 score. The ORG category exhibits the lowest values for P, R, and F1 score, which may be due to factors such as entity frequency or annotation accuracy. The NATION category achieves the highest F1 score at 95.45%, with P and R values of 93.75% and 97.22%, respectively. The PER category follows with an F1 score of 94.05%, and P and R values of 96.34% and 91.86%, respectively. Based on the data obtained from the experiments, the model demonstrates strong performance across all entity categories in the dataset.Table 8shows that removing any module leads to a decrease in the overall performance of the model. Therefore, each module contributes differently. The contribution of each module inTable 8is calculated using Formula (31):ModuleContribution=F1our‚àíF1ModuleF1ModuleModuleContribution=F1our‚àíF1ModuleF1Module(31)When measuring the importance of modules by relative improvement rate, the English module shows the most significant improvement, reaching 2.43%, followed by the GNN module at 2.06%, the MKN module at 1.92%, and the Transformer module at 1.00%. This indicates that English features and graph neural networks play a relatively crucial role in overall performance. By comparing the results after removing the GNN and Transformer modules, it is observed that the model‚Äôs performance improves when the Transformer is removed. According to the feature fusion mechanism of the GNN and Transformer modules, this phenomenon may be attributed to the Transformer module introducing excessive noise, whereas the GNN selectively integrates English features, which is conducive to enhancing model performance. When the English module is removed, the model experiences the largest performance drop, reaching 2.13%, indicating that the English data contains rich, beneficial information. Incorporating English data as an auxiliary input effectively improves the model‚Äôs performance. Furthermore, removing the MKN module results in a performance decline of 1.69%. As shown inFigure 5, approximately 90% of entities in the dataset have lengths between 1 and 7, suggesting that the rich local information captured by the MKN module contributes to the improvement of model performance.Figure 5shows the distribution intervals of entity lengths, where Q1 represents the lower quartile, Median represents the median, and Q3 represents the upper quartile. Q1 = 2 indicates that 25% of entity lengths fall between 1 and 2; Median = 3 indicates that 50% of entity lengths are less than or equal to 3; Q3 = 4 indicates that 75% of entity lengths are less than or equal to 4. The statistical results inFigure 5suggest that local information is crucial for improving model performance, which is why the MKN module is designed to capture more local information.To better understand the performance of the model, a line chart was used to depict the changes in loss during the training process. The trend of loss changes is shown inFigure 6.Figure 6.Training loss trend.The trend of training loss inFigure 6shows that the loss decreases rapidly at the beginning, and when the training reaches the 25th epoch, the training loss is close to zero, fully demonstrating that the model has a good convergence speed. To understand the stability of the model, the trends of accuracy, recall, and F1 score are plotted, as shown inFigure 7.Figure 7.Trends of precision, recall, and F1 score (CLFF-NER).From the curve trend inFigure 7, it can be seen that the P, R, and F1 values of the model in this paper begin to stabilize after the 35th epoch, indicating that the model has good stability. 5.2. Error AnalysisTo further evaluate the limitations of the proposed model, this section analyzes a representative misclassification involving a location entity. The case is shown inTable 9.Table 9.Case sentence.The golden annotation shows that ‚Äú‰∫ëÂçóÊ∞ëÊóèÊùë‚Äù is an O entity, its corresponding English is ‚ÄúYunnan Ethnic Village‚Äù. However, the model incorrectly predicts ‚Äú‰∫ëÂçóÊ∞ëÊóèÊùë‚Äù as LOC, while correctly identifying the long administrative entity. This reflects a typical false positive caused by geographic surface patterns. The phrase ‚Äú‰∫ëÂçóÊ∞ëÊóèÊùë‚Äù begins with the province name ‚Äú‰∫ëÂçó‚Äù, its corresponding English is ‚ÄúYunnan‚Äù, and ends with the location-like suffix ‚ÄúÊùë‚Äù, its corresponding English is ‚ÄúVillage ‚Äù, causing the model to over-generalize and treat it as a geographic region. The analysis and summary of label prediction errors for the given examples are shown inTable 10. Cross-lingual embeddings and CRF transition probabilities further strengthen these biases.Table 10.The analysis and summary of label prediction errors.This case demonstrates that the model performs well on standardized administrative locations but remains susceptible to semantic ambiguity between true geographic entities and cultural tourism venues, a common challenge in festival-related texts. 5.3. Summary and Future WorkThe CLFF-NER model proposed in this paper integrates multi-source heterogeneous features, including multilingual contextual information, local structural information, and selective English features. Local features are extracted through the MKN module, cross-lingual feature fusion is achieved via the Transformer module, and the GNN module reduces noise interference. On the CTFCDataSet, the CLFF-NER model achieved an accuracy of 89.45%, a recall of 90.01%, and an F1 score of 89.73%, significantly outperforming several classic NER baseline models. Ablation experiments verified the contribution of each module to performance improvement, with the introduction of the English information module contributing the most, increasing the F1 score by 2.43%. CLFF-NER not only performed excellently on the CTFCDataSet but was also tested for generalization on two public NER datasets, Resume and Weibo, achieving competitive scores. On the Resume dataset, the F1 score reached 96.44%, remaining ahead of or on par with multiple advanced models. On the Weibo dataset, the F1 score reached 72.76%, also demonstrating strong robustness and domain adaptability. These results indicate that the model has good cross-domain transferability and cross-linguistic fusion capability. At the same time, experiments have also verified that introducing English data into the model can improve its performance.The CLFF-NER model has already shown good performance and application potential in NER tasks within the field of traditional festival culture. In the future, exploration can focus on technical deepening, scenario expansion, and function upgrades to further unlock its value in cultural preservation and service areas. (1) Deep expansion of cultural digital protection: The current model has achieved the structured annotation of entities such as people, locations, and times in texts about traditional festivals. In the future, it will further integrate multimodal data (such as images, audio, and video materials) to construct a ‚Äútext‚Äìmultimodal‚Äù collaborative entity recognition framework, enabling accurate capture of non-text entities such as festival customs, clothing, and rituals. (2) Intelligent upgrade of cultural dissemination and tourism recommendation: In the future, a deeply personalized recommendation model will be built based on user characteristics to achieve precise matching of ‚Äúfestival cultural knowledge-tourism scenarios-user needs‚Äù. (3) Personalized and interactive exploration in education and popular science scenarios: Construct a layered content generation system for traditional cultural education, designing knowledge depth differences for different educational stages such as elementary, secondary, and adult education; develop interactive intelligent learning support systems, designing situational learning tasks based on entity recognition results to enhance learning interest and participation.",
            "5.1. Result Analysis": "To verify whether the model performs well, experiments were conducted on three public datasets: CTFCDataSet, Resume, and Weibo. The experimental results indicate that the proposed model has good performance and generalization. A detailed analysis is as follows: Table 4shows that the proposed model achieves the best performance on the CTFCDataSet compared with the other four baseline models, indicating that the overall performance of the proposed model on the CTFCDataSet is good. Compared with BERT-CRF, the model‚Äôs P, R, and F1 values increase by 3.6%, 1.57%, and 2.60%, respectively; compared with BERT-Softmax, P, R, and F1 increase by 3.47%, 1.37%, and 2.44%, respectively; compared with BERT-LSTM-CRF, P and F1 increase by 0.89% and 0.32%, while R decreases by 0.27%; compared with BiLSTM-CRF, P, R, and F1 increase by 6.54%, 13.01%, and 9.89%, respectively. Compared with the four baseline models, the proposed model achieves higher F1 scores on the CTFCDataSet. Table 5shows that on the Resume dataset, the model proposed in this paper is highly competitive compared to other models, achieving the highest recall rate of 96.93% compared to several other models. Compared with the ISEA-NER model, the P value of our model decreases by 0.21%, the R value increases by 0.45%, and the F1 score increases by 0.11%; compared with KGNER, the F1 score increases by 0.04%; compared with HREB-CRF, the P, R, and F1 scores increase by 0.44%, 0.32%, and 0.37%, respectively; compared with GPAT-NER, the P and F1 values decrease by 0.41% and 0.04%, while the R value increases by 0.30%. Compared to the other four models, our model achieves performance that is either superior or comparable. Table 6shows that on the Weibo dataset, the model presented in this paper is highly competitive compared to the other models. It also achieves the highest recall rate of 72.84% among several models. Compared to the ISEA-NER model, this model improves the precision (P), recall (R), and F1 score by 1.47%, 2.34%, and 1.91%, respectively; compared to KGNER, the F1 score increases by 0.86%; compared to IFCG-NER, P, R, and F1 increase by 2.45%, 1.67%, and 2.06%, respectively; compared to MT-Learning, P and F1 decrease by 2.22% and 1.04%, while R rises by 0.14%. Compared with the other four models, the model in this paper also achieves performance that is either superior or comparable. Table 7provides the P, R, and F1 score for each entity category in the CTFCDataSet. The data reveals that all metrics for each entity category exceed 80%. The PER category achieves the highest P value, while the NATION category leads in R and F1 score. The ORG category exhibits the lowest values for P, R, and F1 score, which may be due to factors such as entity frequency or annotation accuracy. The NATION category achieves the highest F1 score at 95.45%, with P and R values of 93.75% and 97.22%, respectively. The PER category follows with an F1 score of 94.05%, and P and R values of 96.34% and 91.86%, respectively. Based on the data obtained from the experiments, the model demonstrates strong performance across all entity categories in the dataset. Table 8shows that removing any module leads to a decrease in the overall performance of the model. Therefore, each module contributes differently. The contribution of each module inTable 8is calculated using Formula (31):ModuleContribution=F1our‚àíF1ModuleF1ModuleModuleContribution=F1our‚àíF1ModuleF1Module(31) When measuring the importance of modules by relative improvement rate, the English module shows the most significant improvement, reaching 2.43%, followed by the GNN module at 2.06%, the MKN module at 1.92%, and the Transformer module at 1.00%. This indicates that English features and graph neural networks play a relatively crucial role in overall performance. By comparing the results after removing the GNN and Transformer modules, it is observed that the model‚Äôs performance improves when the Transformer is removed. According to the feature fusion mechanism of the GNN and Transformer modules, this phenomenon may be attributed to the Transformer module introducing excessive noise, whereas the GNN selectively integrates English features, which is conducive to enhancing model performance. When the English module is removed, the model experiences the largest performance drop, reaching 2.13%, indicating that the English data contains rich, beneficial information. Incorporating English data as an auxiliary input effectively improves the model‚Äôs performance. Furthermore, removing the MKN module results in a performance decline of 1.69%. As shown inFigure 5, approximately 90% of entities in the dataset have lengths between 1 and 7, suggesting that the rich local information captured by the MKN module contributes to the improvement of model performance. Figure 5shows the distribution intervals of entity lengths, where Q1 represents the lower quartile, Median represents the median, and Q3 represents the upper quartile. Q1 = 2 indicates that 25% of entity lengths fall between 1 and 2; Median = 3 indicates that 50% of entity lengths are less than or equal to 3; Q3 = 4 indicates that 75% of entity lengths are less than or equal to 4. The statistical results inFigure 5suggest that local information is crucial for improving model performance, which is why the MKN module is designed to capture more local information. To better understand the performance of the model, a line chart was used to depict the changes in loss during the training process. The trend of loss changes is shown inFigure 6. Figure 6.Training loss trend. The trend of training loss inFigure 6shows that the loss decreases rapidly at the beginning, and when the training reaches the 25th epoch, the training loss is close to zero, fully demonstrating that the model has a good convergence speed. To understand the stability of the model, the trends of accuracy, recall, and F1 score are plotted, as shown inFigure 7. Figure 7.Trends of precision, recall, and F1 score (CLFF-NER). From the curve trend inFigure 7, it can be seen that the P, R, and F1 values of the model in this paper begin to stabilize after the 35th epoch, indicating that the model has good stability.",
            "5.2. Error Analysis": "To further evaluate the limitations of the proposed model, this section analyzes a representative misclassification involving a location entity. The case is shown inTable 9. Table 9.Case sentence. The golden annotation shows that ‚Äú‰∫ëÂçóÊ∞ëÊóèÊùë‚Äù is an O entity, its corresponding English is ‚ÄúYunnan Ethnic Village‚Äù. However, the model incorrectly predicts ‚Äú‰∫ëÂçóÊ∞ëÊóèÊùë‚Äù as LOC, while correctly identifying the long administrative entity. This reflects a typical false positive caused by geographic surface patterns. The phrase ‚Äú‰∫ëÂçóÊ∞ëÊóèÊùë‚Äù begins with the province name ‚Äú‰∫ëÂçó‚Äù, its corresponding English is ‚ÄúYunnan‚Äù, and ends with the location-like suffix ‚ÄúÊùë‚Äù, its corresponding English is ‚ÄúVillage ‚Äù, causing the model to over-generalize and treat it as a geographic region. The analysis and summary of label prediction errors for the given examples are shown inTable 10. Cross-lingual embeddings and CRF transition probabilities further strengthen these biases. Table 10.The analysis and summary of label prediction errors. This case demonstrates that the model performs well on standardized administrative locations but remains susceptible to semantic ambiguity between true geographic entities and cultural tourism venues, a common challenge in festival-related texts.",
            "5.3. Summary and Future Work": "The CLFF-NER model proposed in this paper integrates multi-source heterogeneous features, including multilingual contextual information, local structural information, and selective English features. Local features are extracted through the MKN module, cross-lingual feature fusion is achieved via the Transformer module, and the GNN module reduces noise interference. On the CTFCDataSet, the CLFF-NER model achieved an accuracy of 89.45%, a recall of 90.01%, and an F1 score of 89.73%, significantly outperforming several classic NER baseline models. Ablation experiments verified the contribution of each module to performance improvement, with the introduction of the English information module contributing the most, increasing the F1 score by 2.43%. CLFF-NER not only performed excellently on the CTFCDataSet but was also tested for generalization on two public NER datasets, Resume and Weibo, achieving competitive scores. On the Resume dataset, the F1 score reached 96.44%, remaining ahead of or on par with multiple advanced models. On the Weibo dataset, the F1 score reached 72.76%, also demonstrating strong robustness and domain adaptability. These results indicate that the model has good cross-domain transferability and cross-linguistic fusion capability. At the same time, experiments have also verified that introducing English data into the model can improve its performance. The CLFF-NER model has already shown good performance and application potential in NER tasks within the field of traditional festival culture. In the future, exploration can focus on technical deepening, scenario expansion, and function upgrades to further unlock its value in cultural preservation and service areas. (1) Deep expansion of cultural digital protection: The current model has achieved the structured annotation of entities such as people, locations, and times in texts about traditional festivals. In the future, it will further integrate multimodal data (such as images, audio, and video materials) to construct a ‚Äútext‚Äìmultimodal‚Äù collaborative entity recognition framework, enabling accurate capture of non-text entities such as festival customs, clothing, and rituals. (2) Intelligent upgrade of cultural dissemination and tourism recommendation: In the future, a deeply personalized recommendation model will be built based on user characteristics to achieve precise matching of ‚Äúfestival cultural knowledge-tourism scenarios-user needs‚Äù. (3) Personalized and interactive exploration in education and popular science scenarios: Construct a layered content generation system for traditional cultural education, designing knowledge depth differences for different educational stages such as elementary, secondary, and adult education; develop interactive intelligent learning support systems, designing situational learning tasks based on entity recognition results to enhance learning interest and participation."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2227-9709/12/4/136",
        "scraped_at": "2025-12-05 23:50:44"
    },
    {
        "title": "Sophimatics: A Two-Dimensional Temporal Cognitive Architecture for Paradox-Resilient Artificial Intelligence",
        "authors": "byGerardo IovaneandGiovanni Iovane",
        "journal": "Big Data Cogn. Comput.2025,9(12), 314;https://doi.org/10.3390/bdcc9120314- 5 Dec 2025",
        "abstract": "This work represents the natural continuation of the development of the cognitive architecture developed and named Sophimatics, organically integrating the spatio-temporal processing mechanisms of the Super Time Cognitive Neural Network (STCNN) with the advanced principles of Sophimatics. Sophimatics‚Äô goal is as challenging as it is fraught with obstacles, but its ultimate aim is to achieve a more humanized post-generative artificial intelligence, capable of understanding and analyzing context and evaluating the user‚Äôs purpose and intent, viewing time not only as a chronological sequence but also as an experiential continuum. The path to achieving this extremely ambitious goal has been made possible thanks to some previous work in which the philosophical thinking of interest in AI was first inherited as the inspiration for the aforementioned capabilities of the Sophimatic framework, then the issue of mapping concepts and philosophical thinking in Sophimatics‚Äô AI infrastructure was addressed, and finally a cognitive-inspired network such as STCNN was created. This work, on the other hand, addresses the challenge of how to endow the infrastructure with both chronological and experiential time and its powerful implications, such as the innate ability to resolve paradoxes, which generative AI does not have among its prerogatives precisely because of structural limitations. To reach these results, the model operates in the two-dimensional complex time domain ‚ÑÇ2, extending cognitive processing capabilities through the implementation of dual temporal operators that simultaneously manage the real temporal dimension, where past, present, and future are managed and the imaginary one, that considers memory, creativity, and imagination. The resulting architecture demonstrates superior capabilities in resolving informational paradoxes and integrating apparently contradictory cognitive states, maintaining computational coherence through adaptive Sophimatic mechanisms. In conclusion, this work introduces Phase 4 of the Sophimatic framework, enabling management of two-dimensional time within a novel cognitively inspired neural architecture grounded in philosophical concepts. It connects with existing research on temporal cognition, hybrid symbolic‚Äìconnectionist models, and ethical AI. The methodology translates philosophical insights into formal computational systems, culminating in a mathematical formalization that supports two-dimensional temporal reasoning and paradox resolution. Experimental results demonstrate efficiency, predictive accuracy, and computational feasibility, highlighting potential real-world applications, future research directions, and present limitations.",
        "keywords": ":context-aware AI; super time-cognitive neural networks; 2D complex time integration; philosophical AI; temporal cognition; neural-symbolic computing; Sophimatics framework; Sophimatics; education and philosophy of artificial intelligencecontext-aware AI;super time-cognitive neural networks;2D complex time integration;philosophical AI;temporal cognition;neural-symbolic computing;Sophimatics framework;Sophimatics;education and philosophy of artificial intelligence",
        "full_content": {
            "Abstract": "This work represents the natural continuation of the development of the cognitive architecture developed and named Sophimatics, organically integrating the spatio-temporal processing mechanisms of the Super Time Cognitive Neural Network (STCNN) with the advanced principles of Sophimatics. Sophimatics‚Äô goal is as challenging as it is fraught with obstacles, but its ultimate aim is to achieve a more humanized post-generative artificial intelligence, capable of understanding and analyzing context and evaluating the user‚Äôs purpose and intent, viewing time not only as a chronological sequence but also as an experiential continuum. The path to achieving this extremely ambitious goal has been made possible thanks to some previous work in which the philosophical thinking of interest in AI was first inherited as the inspiration for the aforementioned capabilities of the Sophimatic framework, then the issue of mapping concepts and philosophical thinking in Sophimatics‚Äô AI infrastructure was addressed, and finally a cognitive-inspired network such as STCNN was created. This work, on the other hand, addresses the challenge of how to endow the infrastructure with both chronological and experiential time and its powerful implications, such as the innate ability to resolve paradoxes, which generative AI does not have among its prerogatives precisely because of structural limitations. To reach these results, the model operates in the two-dimensional complex time domain ‚ÑÇ2, extending cognitive processing capabilities through the implementation of dual temporal operators that simultaneously manage the real temporal dimension, where past, present, and future are managed and the imaginary one, that considers memory, creativity, and imagination. The resulting architecture demonstrates superior capabilities in resolving informational paradoxes and integrating apparently contradictory cognitive states, maintaining computational coherence through adaptive Sophimatic mechanisms. In conclusion, this work introduces Phase 4 of the Sophimatic framework, enabling management of two-dimensional time within a novel cognitively inspired neural architecture grounded in philosophical concepts. It connects with existing research on temporal cognition, hybrid symbolic‚Äìconnectionist models, and ethical AI. The methodology translates philosophical insights into formal computational systems, culminating in a mathematical formalization that supports two-dimensional temporal reasoning and paradox resolution. Experimental results demonstrate efficiency, predictive accuracy, and computational feasibility, highlighting potential real-world applications, future research directions, and present limitations. Keywords:context-aware AI; super time-cognitive neural networks; 2D complex time integration; philosophical AI; temporal cognition; neural-symbolic computing; Sophimatics framework; Sophimatics; education and philosophy of artificial intelligencecontext-aware AI;super time-cognitive neural networks;2D complex time integration;philosophical AI;temporal cognition;neural-symbolic computing;Sophimatics framework;Sophimatics;education and philosophy of artificial intelligence",
            "Share and Cite": "MDPI and ACS StyleIovane, G.;                     Iovane, G.    \n        Sophimatics: A Two-Dimensional Temporal Cognitive Architecture for Paradox-Resilient Artificial Intelligence.Big Data Cogn. Comput.2025,9, 314.\n    https://doi.org/10.3390/bdcc9120314AMA StyleIovane G,                                 Iovane G.        \n                Sophimatics: A Two-Dimensional Temporal Cognitive Architecture for Paradox-Resilient Artificial Intelligence.Big Data and Cognitive Computing. 2025; 9(12):314.\n        https://doi.org/10.3390/bdcc9120314Chicago/Turabian StyleIovane, Gerardo,                                 and Giovanni Iovane.        \n                2025. \"Sophimatics: A Two-Dimensional Temporal Cognitive Architecture for Paradox-Resilient Artificial Intelligence\"Big Data and Cognitive Computing9, no. 12: 314.\n        https://doi.org/10.3390/bdcc9120314APA StyleIovane, G.,                                 & Iovane, G.        \n        \n        (2025). Sophimatics: A Two-Dimensional Temporal Cognitive Architecture for Paradox-Resilient Artificial Intelligence.Big Data and Cognitive Computing,9(12), 314.\n        https://doi.org/10.3390/bdcc9120314",
            "Article Metrics": "YesCitationsNo citations were found for this article, but you may check onGoogle ScholarNoArticle metric data becomes available approximately 24 hours after publication online.",
            "Citations": "No citations were found for this article, but you may check onGoogle Scholar"
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2504-2289/9/12/314",
        "scraped_at": "2025-12-05 23:50:52"
    },
    {
        "title": "Human Pose Intelligent Detection Algorithm Based on Spatiotemporal Hybrid Dilated Convolution Model",
        "authors": "byLili Zhang,Shenxi Dai,Lihuang SheandShuwei Huo",
        "journal": "Electronics2025,14(24), 4798;https://doi.org/10.3390/electronics14244798- 5 Dec 2025",
        "abstract": "Three-dimensional human pose estimation (3D HPE) refers to converting the input image or video into the coordinates of the keypoints of the 3D human body in the coordinate system. At present, the mainstream implementation scheme of a 3D HPE task is to take the 2D pose estimation result as the intermediate process and then return it to the 3D pose. The general difficulty of this scheme is how to effectively extract the features between 2D joint points and return them to 3D coordinates in a highly nonlinear 3D space. In this paper, we propose a new algorithm, called TSHDC, to solve the above dilemma by considering the temporal and spatial characteristics of human joint points. By introducing the self-attention mechanism and the temporal convolutional network (TCN) into the 3D HPE task, the model can use only 27 frames of temporal receptive field to make the model have fewer parameters and faster convergence speed when the accuracy is not much different from the SOTA-level algorithm (+6.8 mm). The TSHDC model is deployed on the embedded platform JetsonTX2, and by deploying TensorRT, the model inference speed can be greatly improved (13.7 times) with only a small loss of accuracy (5%). The comprehensive experimental results on representative benchmarks show that our method outperforms the state-of-the-art methods in quantitative and qualitative evaluation.",
        "keywords": ":3D human pose estimation; self-attention mechanism; time domain convolution network; jetsonTX2; tensorRT3D human pose estimation;self-attention mechanism;time domain convolution network;jetsonTX2;tensorRT",
        "full_content": {
            "Abstract": "Three-dimensional human pose estimation (3D HPE) refers to converting the input image or video into the coordinates of the keypoints of the 3D human body in the coordinate system. At present, the mainstream implementation scheme of a 3D HPE task is to take the 2D pose estimation result as the intermediate process and then return it to the 3D pose. The general difficulty of this scheme is how to effectively extract the features between 2D joint points and return them to 3D coordinates in a highly nonlinear 3D space. In this paper, we propose a new algorithm, called TSHDC, to solve the above dilemma by considering the temporal and spatial characteristics of human joint points. By introducing the self-attention mechanism and the temporal convolutional network (TCN) into the 3D HPE task, the model can use only 27 frames of temporal receptive field to make the model have fewer parameters and faster convergence speed when the accuracy is not much different from the SOTA-level algorithm (+6.8 mm). The TSHDC model is deployed on the embedded platform JetsonTX2, and by deploying TensorRT, the model inference speed can be greatly improved (13.7 times) with only a small loss of accuracy (5%). The comprehensive experimental results on representative benchmarks show that our method outperforms the state-of-the-art methods in quantitative and qualitative evaluation. Keywords:3D human pose estimation; self-attention mechanism; time domain convolution network; jetsonTX2; tensorRT3D human pose estimation;self-attention mechanism;time domain convolution network;jetsonTX2;tensorRT",
            "Share and Cite": "MDPI and ACS StyleZhang, L.;                     Dai, S.;                     She, L.;                     Huo, S.    \n        Human Pose Intelligent Detection Algorithm Based on Spatiotemporal Hybrid Dilated Convolution Model.Electronics2025,14, 4798.\n    https://doi.org/10.3390/electronics14244798AMA StyleZhang L,                                 Dai S,                                 She L,                                 Huo S.        \n                Human Pose Intelligent Detection Algorithm Based on Spatiotemporal Hybrid Dilated Convolution Model.Electronics. 2025; 14(24):4798.\n        https://doi.org/10.3390/electronics14244798Chicago/Turabian StyleZhang, Lili,                                 Shenxi Dai,                                 Lihuang She,                                 and Shuwei Huo.        \n                2025. \"Human Pose Intelligent Detection Algorithm Based on Spatiotemporal Hybrid Dilated Convolution Model\"Electronics14, no. 24: 4798.\n        https://doi.org/10.3390/electronics14244798APA StyleZhang, L.,                                 Dai, S.,                                 She, L.,                                 & Huo, S.        \n        \n        (2025). Human Pose Intelligent Detection Algorithm Based on Spatiotemporal Hybrid Dilated Convolution Model.Electronics,14(24), 4798.\n        https://doi.org/10.3390/electronics14244798 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further detailshere.",
            "Article Metrics": "YesCitationsNo citations were found for this article, but you may check onGoogle ScholarNoArticle metric data becomes available approximately 24 hours after publication online.",
            "Citations": "No citations were found for this article, but you may check onGoogle Scholar"
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2079-9292/14/24/4798",
        "scraped_at": "2025-12-05 23:50:59"
    },
    {
        "title": "An Empirical Evaluation of Low-Rank Adapted Vision‚ÄìLanguage Models for Radiology Image Captioning",
        "authors": "byMahmudul Hoque,Raisa Nusrat Chowdhury,Md Rakibul Hasan,Ojonugwa Oluwafemi Ejiga Peter,Fahmi KhalifaandMd Mahmudur Rahman",
        "journal": "Bioengineering2025,12(12), 1330;https://doi.org/10.3390/bioengineering12121330- 5 Dec 2025",
        "abstract": "Rapidly growing medical imaging volumes have increased radiologist workloads, creating demand for automated tools that support interpretation and reduce reporting delays. Vision-language models (VLMs) can generate clinically relevant captions to accelerate report drafting, yet their varying parameter scales require systematic evaluation for clinical utility. This study evaluated ten multimodal models fine-tuned on the Radiology Objects in Context version 2 (ROCOv2) dataset containing 116,635 images across eight modalities. We compared four Large VLMs (LVLMs) including LLaVA variants and IDEFICS-9B against four Small VLMs (SVLMs) including MoonDream2, Qwen variants, and SmolVLM, alongside two fully fine-tuned baseline architectures (VisionGPT2 and CNN-Transformer). Low-Rank Adaptation (LoRA), applied to fewer than 1% of selected model parameters, proved optimal among adaptation strategies, outperforming broader LoRA configurations. Models were assessed on relevance (semantic similarity) and factuality (concept-level correctness) metrics. Performance showed clear stratification: LVLMs (0.273 to 0.317 overall), SVLMs (0.188 to 0.279), and baselines (0.154 to 0.177). LLaVA-Mistral-7B achieved the highest performance with relevance and factuality scores of 0.516 and 0.118, respectively, substantially exceeding the VisionGPT2 baseline (0.325, 0.028). Among the SVLMs, MoonDream2 demonstrated competitive relevance (0.466), approaching the performance of some LVLMs despite its smaller size. To investigate performance enhancement strategies for underperforming SVLMs, we prepended predicted imaging modality labels at inference time, which yielded variable results. These findings provide quantitative benchmarks for VLM selection in medical imaging, demonstrating that while model scale influences performance, architectural design and targeted adaptation enable select compact models to achieve competitive results.",
        "keywords": ":vision-language models; medical image captioning; radiology report generation; Low-Rank Adaptation; ROCOv2 dataset; caption quality; parameter-efficient fine-tuningvision-language models;medical image captioning;radiology report generation;Low-Rank Adaptation;ROCOv2 dataset;caption quality;parameter-efficient fine-tuning",
        "full_content": {
            "Abstract": "Rapidly growing medical imaging volumes have increased radiologist workloads, creating demand for automated tools that support interpretation and reduce reporting delays. Vision-language models (VLMs) can generate clinically relevant captions to accelerate report drafting, yet their varying parameter scales require systematic evaluation for clinical utility. This study evaluated ten multimodal models fine-tuned on the Radiology Objects in Context version 2 (ROCOv2) dataset containing 116,635 images across eight modalities. We compared four Large VLMs (LVLMs) including LLaVA variants and IDEFICS-9B against four Small VLMs (SVLMs) including MoonDream2, Qwen variants, and SmolVLM, alongside two fully fine-tuned baseline architectures (VisionGPT2 and CNN-Transformer). Low-Rank Adaptation (LoRA), applied to fewer than 1% of selected model parameters, proved optimal among adaptation strategies, outperforming broader LoRA configurations. Models were assessed on relevance (semantic similarity) and factuality (concept-level correctness) metrics. Performance showed clear stratification: LVLMs (0.273 to 0.317 overall), SVLMs (0.188 to 0.279), and baselines (0.154 to 0.177). LLaVA-Mistral-7B achieved the highest performance with relevance and factuality scores of 0.516 and 0.118, respectively, substantially exceeding the VisionGPT2 baseline (0.325, 0.028). Among the SVLMs, MoonDream2 demonstrated competitive relevance (0.466), approaching the performance of some LVLMs despite its smaller size. To investigate performance enhancement strategies for underperforming SVLMs, we prepended predicted imaging modality labels at inference time, which yielded variable results. These findings provide quantitative benchmarks for VLM selection in medical imaging, demonstrating that while model scale influences performance, architectural design and targeted adaptation enable select compact models to achieve competitive results. Keywords:vision-language models; medical image captioning; radiology report generation; Low-Rank Adaptation; ROCOv2 dataset; caption quality; parameter-efficient fine-tuningvision-language models;medical image captioning;radiology report generation;Low-Rank Adaptation;ROCOv2 dataset;caption quality;parameter-efficient fine-tuning Graphical Abstract",
            "Share and Cite": "MDPI and ACS StyleHoque, M.;                     Chowdhury, R.N.;                     Hasan, M.R.;                     Peter, O.O.E.;                     Khalifa, F.;                     Rahman, M.M.    \n        An Empirical Evaluation of Low-Rank Adapted Vision‚ÄìLanguage Models for Radiology Image Captioning.Bioengineering2025,12, 1330.\n    https://doi.org/10.3390/bioengineering12121330AMA StyleHoque M,                                 Chowdhury RN,                                 Hasan MR,                                 Peter OOE,                                 Khalifa F,                                 Rahman MM.        \n                An Empirical Evaluation of Low-Rank Adapted Vision‚ÄìLanguage Models for Radiology Image Captioning.Bioengineering. 2025; 12(12):1330.\n        https://doi.org/10.3390/bioengineering12121330Chicago/Turabian StyleHoque, Mahmudul,                                 Raisa Nusrat Chowdhury,                                 Md Rakibul Hasan,                                 Ojonugwa Oluwafemi Ejiga Peter,                                 Fahmi Khalifa,                                 and Md Mahmudur Rahman.        \n                2025. \"An Empirical Evaluation of Low-Rank Adapted Vision‚ÄìLanguage Models for Radiology Image Captioning\"Bioengineering12, no. 12: 1330.\n        https://doi.org/10.3390/bioengineering12121330APA StyleHoque, M.,                                 Chowdhury, R. N.,                                 Hasan, M. R.,                                 Peter, O. O. E.,                                 Khalifa, F.,                                 & Rahman, M. M.        \n        \n        (2025). An Empirical Evaluation of Low-Rank Adapted Vision‚ÄìLanguage Models for Radiology Image Captioning.Bioengineering,12(12), 1330.\n        https://doi.org/10.3390/bioengineering12121330 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further detailshere.",
            "Article Metrics": "YesCitationsNo citations were found for this article, but you may check onGoogle ScholarNoArticle metric data becomes available approximately 24 hours after publication online.",
            "Citations": "No citations were found for this article, but you may check onGoogle Scholar"
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2306-5354/12/12/1330",
        "scraped_at": "2025-12-05 23:51:05"
    },
    {
        "title": "Integrated Quality Inspection and Production Run Optimization for Imperfect Production Systems with Zero-Inflated Non-Homogeneous Poisson Deterioration",
        "authors": "byChih-Chiang FangandMing-Nan Chen",
        "journal": "Mathematics2025,13(24), 3901;https://doi.org/10.3390/math13243901- 5 Dec 2025",
        "abstract": "This study develops an integrated quality inspection and production optimization framework for an imperfect production system, where system deterioration follows a zero-inflated non-homogeneous Poisson process (ZI-NHPP) characterized by a power-law intensity function. Parameters are estimated from historical data using the Expectation-Maximization (EM) algorithm, with a zero-inflation parameter œÄ modeling scenario where the system remains defect-free. Operating in either an in-control or out-of-control state, the system produces products with Weibull hazard rates, exhibiting higher failure rates in the out-of-control state. The proposed model integrates system status, defect rates, employee efficiency, and market demand to jointly optimize the number of conforming items inspected and the production run length, thereby minimizing total costs‚Äîincluding production, inspection, correction, inventory, and warranty expenses. Numerical analyses, supported by sensitivity studies, validate the effectiveness of this integrated approach in achieving cost-efficient quality control. This framework enhances quality assurance and production management, offering practical insights for manufacturing across diverse industries.Keywords:quality inspection;zero-inflated non-homogeneous Poisson process;negative binomial sampling;EM algorithm;Weibull hazard rate;imperfect production systemMSC:62F15; 62N02; 62N05; 62C10; 65C20",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "In modern manufacturing, achieving high-quality production while minimizing costs is critical, particularly in imperfect production systems designed for customized products under tight delivery schedules [1]. Imperfect production systems, which are prone to deterioration, produce both conforming and defective items, leading to increased costs from rework, warranty claims, and customer dissatisfaction [2,3,4]. Traditional models often rely on deterministic or simple stochastic assumptions, such as constant failure rates or Weibull distributions [1,5]. However, historical data may exhibit inflation, where systems occasionally remain defect-free due to robust design, necessitating advanced models like the Zero-Inflated Non-Homogeneous Poisson Process (ZI-NHPP) with a power-law intensity, estimated via the Expectation‚ÄìMaximization (EM) algorithm [6,7,8]. This study proposes an integrated framework to optimize both the production run length and the number of conforming items inspected to minimize the expected total cost, including production, inspection, correction, inventory holding, and warranty costs based on Weibull hazard rates [1]. By integrating system status, defect rates, employee efficiency, and market demands, the model leverages ZI-NHPP to enhance applicability in Industry 4.0 contexts [9]. The findings offer practical insights for cost-effective quality management in manufacturing.",
            "2. Literature Review": "The literature on quality management in imperfect production systems encompasses production‚Äìinventory optimization, maintenance policies, stochastic deterioration modeling, inspection planning, and reliability analysis. This review synthesizes key contributions from previous studies, highlighting gaps that the current research addresses by integrating Zero-Inflated Non-Homogeneous Poisson Process modeling with Expectation-Maximization estimation for parameter fitting using potentially inflated historical data. 2.1. Imperfect Production and Inventory ModelsImperfect production systems, characterized by defective outputs resulting from system deterioration, require integrated strategies to balance quality and cost effectively. Coordinated strategies that combine production lot sizing, quality control, and condition-based maintenance reduce downtime and costs [2]. Accounting for Type I and Type II inspection errors helps minimize warranty expenses in imperfect systems [10]. Reliability-aware and fuzzy optimization approaches further reduce holding costs and improve decisions under uncertainty [3]. EPQ models with probabilistic demand and collaborative approaches enhance supply chain efficiency through information sharing [11]. Sustainable production‚Äìinventory models incorporating quality-improvement investments and preservation technology yield long-term cost savings and environmental benefits [4]. Joint lot-sizing and condition-based maintenance under inspection errors demonstrate adaptability to degradation patterns [12]. Integrated supply chain modeling considering imperfect production, deterioration, and inspection errors underscores the need for coordinated quality control strategies [13].These studies offer robust frameworks for imperfect production systems [2,3,4,12], but they often assume simple deterioration models and do not address data inflation. The present study extends this work by employing ZI-NHPP with EM estimation to model heterogeneous degradation, thereby enhancing its applicability to imperfect production systems. 2.2. Maintenance and Inspection PoliciesMaintenance and inspection strategies are essential for managing deteriorating production systems. Joint optimization of control charts, production cycles, and maintenance schedules with stochastic shift sizes enhances defect detection via adaptive thresholds [14]. Multi-facility inspection and repair planning can be cost-minimized through mathematical programming [15]. Integrating production, preventive maintenance, and dynamic inspection with real-time monitoring reduces downtime in degrading systems [16].Stochastic processes, particularly NHPP, are widely used to model time-varying failure rates, with applications to anti-corrosion coating maintenance scheduling, distribution-system reliability evaluation, and condition-based maintenance with non-homogeneous degradation [6,7,8]. Bayesian methods further refine maintenance strategies under uncertainty, improving scheduling and warranty planning for hybrid failure modes and deterioration [17,18].These contributions highlight the efficacy of stochastic models and Bayesian approaches in maintenance [6,7,8,16,17,18]. However, few studies address zero-inflated data, which this study tackles using ZI-NHPP with EM estimation, improving deterioration modeling for production systems. 2.3. Quality Inspection PlanningEffective inspection planning ensures product quality in multi-stage manufacturing systems. Comprehensive reviews synthesize optimization strategies for part quality inspection, emphasizing inspection timing and extent for cost minimization [19,20]. Integrated models that co-optimize preventive maintenance and inspection in serial multi-stage systems use mixed-integer programming to control costs [21]. For imperfect production systems with Weibull deterioration, inspection plans that integrate system status, defective rates, and market demands can be optimized for quality and cost [1]. Supplier quality management shows that investment and incentive-aligned inspection improve sourced quality [22]. Multi-criteria algorithms support the selection of inspection alternatives considering quality, economic, and environmental factors [23]. Ontology-based expert systems automate construction inspection planning for Industry 4.0 [9]. In electronics, automatic optical inspection (AOI) technologies continue to advance defect detection [24]. Attribute-sampling plans for time-truncated life tests are applicable in reliability contexts [25]. Machine-learning-enabled risk-based inspection screening improves efficiency [26]. Under 100% inspection, optimizing process means with inspection errors further strengthens quality control parameters [27].These works advance inspection methodologies [1,9,19,20,21,22,23] but often rely on traditional deterioration models. This study builds on [1] by integrating ZI-NHPP, addressing inflated data with EM estimation for a robust negative-binomial sampling inspection (NBSI) plan. 2.4. Reliability Analysis and ModelingReliability modeling for complex systems has evolved to handle stochastic dependencies and uncertainty. Hybrid reliability analysis with adaptive Kriging improves uncertainty quantification with incomplete interval data [28]. Factor-analysis-based approaches model stochastic dependencies among components [29]. Degradation hidden Markov models with time-varying parameters improve reliability assessment for partially monitored systems [30]. Wiener-process-based re-prediction methods integrate monitoring and historical data for remaining useful life estimation in subsea systems [31]. Distribution families incorporating the exponentiated Weibull address v-shaped and bathtub-shaped failure rates [5]. Competing Weibull mixture models capture heterogeneous and censored data effectively [32]. Mahmood [33] argues that ZIP and ZINB regression models are superior to conventional control charts for the reliability analysis of high-yield processes characterized by zero-inflated defect data. These reliability studies [5,28,29,30,31,32] support the use of Weibull hazard rates in this study and highlight advanced stochastic modeling. However, they rarely address zero-inflated deterioration data, which this study tackles through ZI-NHPP with EM estimation, enhancing applicability to imperfect production systems in Industry 4.0 contexts. 2.5. Research Gaps and ContributionsAlthough the existing literature provides robust frameworks for managing imperfect production systems, several key gaps limit their applicability to real-world scenarios, particularly in imperfect production environments. First, studies on imperfect production and inventory models [2,3,4,10,11,12,13] often rely on deterministic or basic stochastic assumptions, without adequately addressing heterogeneity and zero-inflation in deterioration data‚Äîwhere systems may experience prolonged defect-free periods due to design robustness or operational variability‚Äîpotentially leading to overestimated risks and inefficient resource allocation.In maintenance and inspection policies [6,7,8,14,15,16,17,18], NHPP-based models are prevalent, but they rarely incorporate zero-inflation, leading to biased parameter estimates from datasets with excess zeros. Bayesian methods improve uncertainty handling, yet they typically overlook structural inflation in event counts.Quality inspection planning research [1,9,19,20,21,22,23,24,25,26,27] advances optimization techniques (e.g., mixed-integer programming, ontology-based systems) but usually assumes linear or Weibull deterioration without inflated zeros, which can undermine cost minimization in multi-stage production setups. Similarly, reliability modeling [5,28,29,30,31,32] excels in handling dependencies and censored data but seldom integrates zero-inflated processes for non-homogeneous degradation.Existing NHPP- and Weibull-based deterioration models generally assume a single failure-generating process and do not address zero-inflated defect patterns, which are common in high-yield manufacturing and may lead to biased intensity estimation. This study advances this literature by introducing a zero-inflated deterioration framework and linking it directly to joint inspection and production-run optimization in imperfect production systems. We model deterioration as a Zero-Inflated Non-Homogeneous Poisson Process (ZI-NHPP) with a power-law intensity function and estimate parameters using an EM-based procedure that properly accounts for inflation in historical defect data. Building on this foundation, we develop a Negative-Binomial Sampling Inspection (NBSI) strategy that determines the optimal number of conforming items inspected and jointly minimizes expected production, inspection, rework, holding, and warranty costs under state-dependent Weibull failure rates. By integrating deterioration dynamics with inspection scheduling and cycle planning‚Äîwhile incorporating defect susceptibility, learning effects, and demand conditions‚Äîthis study provides a unified decision framework that improves cost-efficiency and offers practical guidance for managing quality in imperfect production environments.",
            "2.1. Imperfect Production and Inventory Models": "Imperfect production systems, characterized by defective outputs resulting from system deterioration, require integrated strategies to balance quality and cost effectively. Coordinated strategies that combine production lot sizing, quality control, and condition-based maintenance reduce downtime and costs [2]. Accounting for Type I and Type II inspection errors helps minimize warranty expenses in imperfect systems [10]. Reliability-aware and fuzzy optimization approaches further reduce holding costs and improve decisions under uncertainty [3]. EPQ models with probabilistic demand and collaborative approaches enhance supply chain efficiency through information sharing [11]. Sustainable production‚Äìinventory models incorporating quality-improvement investments and preservation technology yield long-term cost savings and environmental benefits [4]. Joint lot-sizing and condition-based maintenance under inspection errors demonstrate adaptability to degradation patterns [12]. Integrated supply chain modeling considering imperfect production, deterioration, and inspection errors underscores the need for coordinated quality control strategies [13]. These studies offer robust frameworks for imperfect production systems [2,3,4,12], but they often assume simple deterioration models and do not address data inflation. The present study extends this work by employing ZI-NHPP with EM estimation to model heterogeneous degradation, thereby enhancing its applicability to imperfect production systems.",
            "2.2. Maintenance and Inspection Policies": "Maintenance and inspection strategies are essential for managing deteriorating production systems. Joint optimization of control charts, production cycles, and maintenance schedules with stochastic shift sizes enhances defect detection via adaptive thresholds [14]. Multi-facility inspection and repair planning can be cost-minimized through mathematical programming [15]. Integrating production, preventive maintenance, and dynamic inspection with real-time monitoring reduces downtime in degrading systems [16]. Stochastic processes, particularly NHPP, are widely used to model time-varying failure rates, with applications to anti-corrosion coating maintenance scheduling, distribution-system reliability evaluation, and condition-based maintenance with non-homogeneous degradation [6,7,8]. Bayesian methods further refine maintenance strategies under uncertainty, improving scheduling and warranty planning for hybrid failure modes and deterioration [17,18]. These contributions highlight the efficacy of stochastic models and Bayesian approaches in maintenance [6,7,8,16,17,18]. However, few studies address zero-inflated data, which this study tackles using ZI-NHPP with EM estimation, improving deterioration modeling for production systems.",
            "2.3. Quality Inspection Planning": "Effective inspection planning ensures product quality in multi-stage manufacturing systems. Comprehensive reviews synthesize optimization strategies for part quality inspection, emphasizing inspection timing and extent for cost minimization [19,20]. Integrated models that co-optimize preventive maintenance and inspection in serial multi-stage systems use mixed-integer programming to control costs [21]. For imperfect production systems with Weibull deterioration, inspection plans that integrate system status, defective rates, and market demands can be optimized for quality and cost [1]. Supplier quality management shows that investment and incentive-aligned inspection improve sourced quality [22]. Multi-criteria algorithms support the selection of inspection alternatives considering quality, economic, and environmental factors [23]. Ontology-based expert systems automate construction inspection planning for Industry 4.0 [9]. In electronics, automatic optical inspection (AOI) technologies continue to advance defect detection [24]. Attribute-sampling plans for time-truncated life tests are applicable in reliability contexts [25]. Machine-learning-enabled risk-based inspection screening improves efficiency [26]. Under 100% inspection, optimizing process means with inspection errors further strengthens quality control parameters [27]. These works advance inspection methodologies [1,9,19,20,21,22,23] but often rely on traditional deterioration models. This study builds on [1] by integrating ZI-NHPP, addressing inflated data with EM estimation for a robust negative-binomial sampling inspection (NBSI) plan.",
            "2.4. Reliability Analysis and Modeling": "Reliability modeling for complex systems has evolved to handle stochastic dependencies and uncertainty. Hybrid reliability analysis with adaptive Kriging improves uncertainty quantification with incomplete interval data [28]. Factor-analysis-based approaches model stochastic dependencies among components [29]. Degradation hidden Markov models with time-varying parameters improve reliability assessment for partially monitored systems [30]. Wiener-process-based re-prediction methods integrate monitoring and historical data for remaining useful life estimation in subsea systems [31]. Distribution families incorporating the exponentiated Weibull address v-shaped and bathtub-shaped failure rates [5]. Competing Weibull mixture models capture heterogeneous and censored data effectively [32]. Mahmood [33] argues that ZIP and ZINB regression models are superior to conventional control charts for the reliability analysis of high-yield processes characterized by zero-inflated defect data. These reliability studies [5,28,29,30,31,32] support the use of Weibull hazard rates in this study and highlight advanced stochastic modeling. However, they rarely address zero-inflated deterioration data, which this study tackles through ZI-NHPP with EM estimation, enhancing applicability to imperfect production systems in Industry 4.0 contexts.",
            "2.5. Research Gaps and Contributions": "Although the existing literature provides robust frameworks for managing imperfect production systems, several key gaps limit their applicability to real-world scenarios, particularly in imperfect production environments. First, studies on imperfect production and inventory models [2,3,4,10,11,12,13] often rely on deterministic or basic stochastic assumptions, without adequately addressing heterogeneity and zero-inflation in deterioration data‚Äîwhere systems may experience prolonged defect-free periods due to design robustness or operational variability‚Äîpotentially leading to overestimated risks and inefficient resource allocation. In maintenance and inspection policies [6,7,8,14,15,16,17,18], NHPP-based models are prevalent, but they rarely incorporate zero-inflation, leading to biased parameter estimates from datasets with excess zeros. Bayesian methods improve uncertainty handling, yet they typically overlook structural inflation in event counts. Quality inspection planning research [1,9,19,20,21,22,23,24,25,26,27] advances optimization techniques (e.g., mixed-integer programming, ontology-based systems) but usually assumes linear or Weibull deterioration without inflated zeros, which can undermine cost minimization in multi-stage production setups. Similarly, reliability modeling [5,28,29,30,31,32] excels in handling dependencies and censored data but seldom integrates zero-inflated processes for non-homogeneous degradation. Existing NHPP- and Weibull-based deterioration models generally assume a single failure-generating process and do not address zero-inflated defect patterns, which are common in high-yield manufacturing and may lead to biased intensity estimation. This study advances this literature by introducing a zero-inflated deterioration framework and linking it directly to joint inspection and production-run optimization in imperfect production systems. We model deterioration as a Zero-Inflated Non-Homogeneous Poisson Process (ZI-NHPP) with a power-law intensity function and estimate parameters using an EM-based procedure that properly accounts for inflation in historical defect data. Building on this foundation, we develop a Negative-Binomial Sampling Inspection (NBSI) strategy that determines the optimal number of conforming items inspected and jointly minimizes expected production, inspection, rework, holding, and warranty costs under state-dependent Weibull failure rates. By integrating deterioration dynamics with inspection scheduling and cycle planning‚Äîwhile incorporating defect susceptibility, learning effects, and demand conditions‚Äîthis study provides a unified decision framework that improves cost-efficiency and offers practical guidance for managing quality in imperfect production environments.",
            "3. Problem Description and Model Development": "3.1. Problem DescriptionIt is supposed that a manufacturer specializing in imperfect production systems for customized products faces the challenge of establishing an efficient production and inspection framework to handle imperfections in the manufacturing process. In such production configuration, the system must rapidly assemble components on demand to meet customer specifications, while minimizing costs and ensuring product quality. However, the production system is imperfect and subject to deterioration, which can cause it to shift from an in-control state (producing mostly conforming items with low defect rates) to an out-of-control state (generating more defective items, such as those with cracks, abnormal joints, or component malfunctions). This deterioration is modeled using a Zero-Inflated Non-Homogeneous Poisson Process, as historical data often shows inflated zeros (periods with no deterioration events), possibly due to robust periods in Industry 4.0-enabled systems. Therefore, the manufacturer faces several key challenges:System Deterioration and State Uncertainty: The production system deteriorates over time, with the rate influenced by factors such as machine wear, employee efficiency, and environmental conditions. The probability of being out-of-control increases with the production run lengthùëáùëü, leading to higher defect rates (ùê∑ùëü) and costs. Historical data may exhibit zero-inflation, where the system remains defect-free with probabilityùúã, complicating traditional modeling approaches.(1) Quality Inspection Trade-offs:Implementing a full inspection is time-consuming and costly, whereas sampling carries the risk of accepting defective lots or rejecting conforming ones, leading to correction (ùê∂ùëê) or warranty (ùê∂ùë§) expenses. Therefore, the manufacturer requires an optimal sampling plan to determine the number of items to inspect (ùëÅùëê) without disrupting production schedules.(2) Cost Minimization Under Multiple Factors:The total cost includes production (fixedùê∂ùëöand variable, based on learning), inspection (ùê∂ùëñandùê∂ùëù), inventory holding (ùê∂‚Ñé), and warranty based on product hazard rates. The employee learning rate (ùêøùëü) affects efficiency, while market demand (ùëÑùëë) and production rate (ùëÉùëü) influence inventory mismatches. Balancing these factors, while accounting for product deterioration, presents a complex challenge.(3) Parameter Estimation from Historical Data:Deterioration parameters (ùõº,ùõΩ,ùúã) must be estimated from potentially zero-inflated historical data, requiring advanced methods such as the Expectation-Maximization (EM) algorithm to handle zero-inflation.For better comprehension, the framework of the study is presented inFigure 1.Figure 1.Production System Operation with Inspection, Correction, and Warranty Framework.Figure 1illustrates the integrated operational framework for an imperfect production system, incorporating production, inspection, correction, and warranty processes to minimize total costs under zero-inflated non-homogeneous Poisson deterioration. This diagram provides a visual representation of the system‚Äôs workflow, highlighting key decision points, state transitions, and cost implications, while emphasizing the role of the ZI-NHPP model in estimating deterioration parameters from historical data.The framework begins at the top with ‚Äúproduction cost‚Äù considerations, which decrease exponentially with accumulated inspection experience and employee learning rate. Production initiates by setting the optimal production run lengthùëáùëü, during which the system may remain in-control or shift to out-of-control based on the ZI-NHPP intensity functionùúÜ(ùë°)=ùõΩùõºùõΩùë°ùõΩ‚àí1. Parameters such asùõºandùõΩare estimated via the EM algorithm from historical failure data of production systems and products, accounting for zero-inflationùúã.A central decision node checks if the system is in control. If it is in control, the product failure rate is set low (using Weibull parameters (ùõº1,ùõΩ1); if out-of-control, it is set high (ùõº2,ùõΩ2). This leads to ‚Äúproduct inspection‚Äù, where negative binomial sampling is employed to inspect items until the optimal number of conforming itemsùëÅùëêis reached. Items are classified as conforming or defective: defective items undergo correction, while conforming items proceed to product sales.The lower section addresses post-production stages. Conforming items contribute to ‚Äúinventory holding cost‚Äù estimation, based on mismatches between production output and demand. Sold products incur ‚Äúwarranty costs‚Äù, estimated using Weibull cumulative hazards over the warranty periodùëäùëù, differentiated by system state:(ùëäùëùùõº1)ùõΩ1for in-control (low failure risk) and(ùëäùëùùõº2)ùõΩ2for out-of-control (high failure risk). Historical data informs these estimations, ensuring alignment with real-world deterioration patterns.This framework underscores the interplay between quality inspection and production planning, optimizingùëÅùëêandùëáùëüto minimize the expected total costùê∏[ùëáùê∂(ùëÅùëê,ùëáùëü)], which includes production, inspection and correction, inventory holding, and warranty expenses. By integrating ZI-NHPP for robust deterioration modeling, the approach enhances cost-efficiency in production systems. 3.2. Development of the Mathematical ModelTo address these challenges, the manufacturer aims to develop a cost-minimizing inspection plan that optimizesùëÅùëê, minimizes the expected total cost (E[TC]), and adapts to mass customization needs. These notations or symbols used are summarized inTable 1.Table 1.Notations or Symbols.The expected total cost formula for the optimal inspection plan in imperfect production systems using a ZI-NHPP model incorporates various cost components. These costs reflect the integration of production, inspection, inventory management, and warranty considerations. Below is a detailed explanation of each cost component in the formula:(1) Production Cost:This component represents the total cost incurred during the production process and can be expressed as follows:ùê∂ùëùùëüùëúùëë=ùê∂ùëö+‚àëùëÑùëëùëñ=1ùê∂ùë¢ùëñln(ùêøùëü)ln(2)ùëí‚àíùêæùëêùëÅùëê.(1)whereùê∂ùëörepresents the fixed expenditure on production equipment during a period, reflecting the capital investment required to operate the manufacturing system. The variable cost is modeled as a summation fromùëñ=1toùëÑùëë(quantity demanded), whereùê∂ùë¢is the manufacturing cost of the first unit. The termùëñln(ùêøùëü)ln(2)accounts for the learning rate(ùêøùëü), a decreasing function that captures improvements in employee efficiency over time, derived from the learning curve effect. The exponential factorùëí‚àíùêæùëêùëÅùëêincorporates the contraction constant (ùêæùëê) and the optimal number of conforming items inspected (ùëÅùëê), reflecting reduced manufacturing costs as more conforming items are identified due to enhanced efficiency. This term ensures that production costs decrease with improved inspection outcomes.Generally, the study assumes a consistent learning rateùêøùëü, which may vary in practice due to worker turnover or skill disparities, potentially affecting cost estimates in dynamic production environments.ùêæùëêgoverns the pace of deterioration and its influence on production cost is analytically characterized in Lemma 1. A higherùêæùëêamplifies cost savings from inspection, but its value should be calibrated against empirical data to avoid over-optimism.Lemma 1.Monotonicity and Convexity of Production Cost.Statement:The production costùê∂ùëùùëüùëúùëë=ùê∂ùëö+‚àëùëÑùëëùëñ=1ùê∂ùë¢ùëñùëôùëõ(ùêøùëü)ùëôùëõ(2)ùëí‚àíùêæùëêùëÅùëêis strictly decreasing and convex inùëÅùëêforùêæùëê>0,ùêøùëü‚àà(0,1), andùëÑùëë‚â•1. Furthermore,ùê∂ùëùùëüùëúùëëis strictly increasing and convex inùêæùëê.Proof of Lemma 1.Please refer toAppendix A.The parameterùêæùëêgoverns the rate at which the system transitions to reduced productivity due to deterioration. Higherùêæùëêaccelerates performance degradation, driving costs upward, and its convexity implies that deterioration mitigation measures (e.g., preventive maintenance or higher sampling rates) become increasingly valuable when deterioration becomes severe. ‚ñ°(2) Inspection and Correction Cost:This costcoversthe expenses associated with inspecting the production system and correcting deviations, and it can be expressed as follows:ùê∂ùëñùëõùë†=ùê∂ùëñ+(ùê∂ùëê+ùê∂ùëùùëÅùëê1‚àíùê∑ùëü)(1‚àíùúã)(1‚àíexp(‚àí(ùëáùëüùõº)ùõΩ)).(2)whereùê∂ùëñrepresents the fixed cost of inspecting the system after each production run to assess its status [1]. The variable component includesùê∂ùëê(correction cost) andùê∂ùëùùëÅùëê1‚àíùê∑ùëü(product inspection cost per item, whereùëÅùëê1‚àíùê∑ùëüis the expected number of items inspected due to the defective rateùê∑ùëü). The probability of entering the out-of-control state isùëÉùëúùë¢ùë°(ùëáùëü)=(1‚àíùúã)(1‚àíexp(‚àíùõ¨(ùëáùëü))), in accordance with the ZI-NHPP deterioration model, whereùúãis the latent defect-free probability based on the Weibull deterioration model‚Äôs cumulative intensity. This probability indicates the likelihood of needing correction actions. In addition, please note thatùêπùëêis a quality proportion parameter, not a complementary probability toùê∑ùëü, and the two should not sum to 1.This component highlights a critical trade-off in imperfect production systems: full inspection is resource-intensive, whereas sampling risks accepting defective lots, incurringùê∂ùëêandùê∂ùë§(warranty) costs. The out-of-control probability, driven byùõºandùõΩ, is sensitive to production duration; longerùëáùëüincreases this probability, necessitating more frequent inspections. The defect rateùê∑ùëüfurther complicates this balance‚Äîhigherùê∑ùëüraises the expected number of items inspected, amplifying costs unless offset by robust design.Lemma 2.Expectation and Probability in Inspection and Correction Cost.Statement:The inspection and correction costùê∂ùëñùëõùë†=ùê∂ùëñ+(ùê∂ùëê+ùê∂ùëùùëÅùëê1‚àíùê∑ùëü)ùëÉùëúùë¢ùë°has expected number of inspectionsùê∏[ùëÅùëñ]=ùëÅùëê1‚àíùê∑ùëüunder negative binomial sampling, andùëÉùëúùë¢ùë°is the valid probability of needing correction, bounded in[0,1‚àíùúã].Proof of Lemma 2.Please refer toAppendix A. ‚ñ°(3) Inventory Holding Cost:This component quantifies the cost of holding inventory during the production cycle, and it can be expressed as follows:ùê∂‚Ñéùëúùëôùëë=ùê∂‚Ñé(ùëÉùëüùëáùëü‚àíùëÑùëë)22ùëÖùëë.(3)ùê∂‚Ñéis the inventory holding cost per unit time, applied to the squared difference between the total products produced (ùëÉùëüùëáùëü, whereùëÉùëüis the production rate) and the demand (ùëÑùëë), divided by twice the demand rate (ùëÖùëë). The quadratic term(ùëÉùëüùëáùëü‚àíùëÑùëë)2accounts for overproduction or underproduction relative to demand, amplifying costs when production exceeds or falls short ofùëÑùëë.In the study, where demand (ùëÑùëë) is often customized and variable, this cost component underscores the challenge of synchronizing production with customer orders under tight schedules. The quadratic nature ensures that small mismatches are tolerable, but large deviations (e.g., overproduction due to extendedùëáùëü) escalate costs rapidly. Sensitivity toùëÉùëüandùëáùëüis significant; increasingùëáùëüto meet demand may raise the related costs, creating a trade-off with quality costs.Lemma 3.Convexity of Inventory Holding Cost.Statement:The inventory holding costùê∂‚Ñéùëúùëôùëë=ùê∂‚Ñé‚éõ‚éù‚éú‚éú(ùëÉùëüùëáùëü‚àíùëÑùëë)22ùëÖùëë‚éû‚é†‚éü‚éüis convex inùëáùëüand minimized atùëáùëü=ùëÑùëë/ùëÉùëü(perfect production-demand match).Proof of Lemma 3.Please refer toAppendix A.This result highlights that inventory cost is driven not merely by time or production scale individually, but by the extent to which production output matches demand over a cycle. The convex structure implies that deviations from the balance pointùëáùëü=ùëÑùëë/ùëÉùëübecome increasingly costly, so small mismatches are inexpensive while large overproduction accumulates inventory rapidly. When combined with deterioration effects, longer production runs may reduce setup frequency but push the system further from the demand-matching point, reinforcing the trade-off between holding cost and quality-related risks. ‚ñ°(4) Warranty Cost:This cost accounts for warranty-related expenses due to product failures during the warranty periodùëäùëù.ùê∂ùë§ùëéùëü=ùê∂ùë§‚é°‚é£‚é¢(ùëäùëùùõº1)ùõΩ1exp(‚àí(ùëáùëüùõº)ùõΩ)+‚éõ‚éù‚éú‚éú‚éú(ùëÉùëüùëáùëü‚àíùëÅùëê1‚àíùê∑ùëü)‚éõ‚éù‚éú‚éú‚éúùêπùëê(ùëäùëùùõº1)ùõΩ1+(1‚àíùêπùëê)(ùëäùëùùõº2)ùõΩ2‚éû‚é†‚éü‚éü‚éü+ùëÅùëê1‚àíùê∑ùëü(ùëäùëùùõº2)ùõΩ2‚éû‚é†‚éü‚éü‚éü(1‚àíexp(‚àí(ùëáùëüùõº)ùõΩ))‚é§‚é¶‚é•.(4)ùê∂ùë§is the warranty cost per item. The warranty cost is divided into two parts: (i) In-Control Warranty Cost(ùëäùëùùõº1)ùõΩ1exp(‚àí(ùëáùëüùõº)ùõΩ)represents the expected failures of products produced in the in-control state, with hazard rate‚Ñé1(ùë°)=ùõΩ1ùõºùõΩ11ùë°ùõΩ1‚àí1. The termexp(‚àí(ùëáùëüùõº)ùõΩ)is the probability of the system remaining in-control. (ii) Out-of-Control Warranty Cost‚éõ‚éù‚éú‚éú(ùëÉùëüùëáùëü‚àíùëÅùëê1‚àíùê∑ùëü)‚éõ‚éù‚éú‚éúùêπùëê(ùëäùëùùõº1)ùõΩ1+(1‚àíùêπùëê)(ùëäùëùùõº2)ùõΩ2‚éû‚é†‚éü‚éü+ùëÅùëê1‚àíùê∑ùëü(ùëäùëùùõº2)ùõΩ2‚éû‚é†‚éü‚éü(1‚àíexp(‚àí(ùëáùëüùõº)ùõΩ))covers failures of unchecked, conforming, and reworked items from the out-of-control state, with hazard rate‚Ñé2(ùë°)=ùõΩ2ùõºùõΩ22ùë°ùõΩ2‚àí1(where‚Ñé2(ùë°)>‚Ñé1(ùë°)).ùêπùëêis the proportion of conforming items from the in-control state, and1‚àíexp(‚àí(ùëáùëüùõº)ùõΩ)is the out-of-control probability.Warranty costs are a major concern in production systems, where product customization increases failure risks if defects are undetected. The use of Weibull distributions (ùõº1,ùõΩ1for low hazard;ùõº2,ùõΩ2for high) reflects realistic failure patterns, withùõΩ2>ùõΩ1indicating accelerated degradation in out-of-control states. Inspection (ùëÅùëê) reduces warranty claims by reworking defective items, but the effectiveness depends onùêπùëê(conforming proportion) andùê∑ùëü. The zero-inflation parameterùúãalso plays a role; higherùúã(robust design) reduces baseline warranty costs, offering a strategic lever for manufacturers.Lemma 4.Expectation of Failures in Warranty Cost.Statement:The warranty costùê∂ùë§ùëéùëü=ùê∂ùë§ùê∏[ùëìùëéùëñùëôùë¢ùëüùëíùë†ùëñùëõ[0,ùëäùëù]]uses Weibull expected failures(ùëäùëù/ùõºùëó)ùõΩùëófor statesùëó=1(in-control, low hazard) andùëó=2(out-of-control, high hazard), weighted by system states, and is non-decreasing inùëÉùëúùë¢ùë°. (ùëÉùëúùë¢ùë°: The out-of-control probability,ùëÉùëñùëõ: The in-control probability).Proof of Lemma 4.Please refer toAppendix A.The warranty cost increases as the system becomes more likely to be out-of-control during production, because items produced in that regime fail at a much higher hazard rate. Thus, even when defects are not immediately observable, inadequate inspection increases long-run failure liability, linking inspection decisions to post-sale cost exposure. ‚ñ°(5) Total Expected Cost:According to the above mentioned, the total expected total cost is as follows:ùê∏[ùëáùê∂(ùëÅùëê,ùëáùëü)]=ùê∂ùëö+‚àëùëÑùëëùëñ=1ùê∂ùë¢ùëñln(ùêøùëü)ln(2)ùëí‚àíùêæùëêùëÅùëê+ùê∂ùëñ+(ùê∂ùëê+ùê∂ùëùùëÅùëê1‚àíùê∑ùëü)((1‚àíùúã)(1‚àíexp(‚àí(ùëáùëüùõº)ùõΩ)))+ùê∂‚Ñé(ùëÉùëüùëáùëü‚àíùëÑùëë)22ùëÖùëë+ùê∂ùë§‚é°‚é£‚é¢(ùëäùëùùõº1)ùõΩ1exp(‚àí(ùëáùëüùõº)ùõΩ)+‚éõ‚éù‚éú‚éú‚éú(ùëÉùëüùëáùëü‚àíùëÅùëê1‚àíùê∑ùëü)‚éõ‚éù‚éú‚éú‚éúùêπùëê(ùëäùëùùõº1)ùõΩ1+(1‚àíùêπùëê)(ùëäùëùùõº2)ùõΩ2‚éû‚é†‚éü‚éü‚éü+ùëÅùëê1‚àíùê∑ùëü(ùëäùëùùõº2)ùõΩ2‚éû‚é†‚éü‚éü‚éü(1‚àíexp(‚àí(ùëáùëüùõº)ùõΩ))‚é§‚é¶‚é•.(5)The total costùê∏[ùëáùê∂(ùëÅùëê,ùëáùëü)]integrates these components, reflecting a comprehensive approach to cost management in imperfect production systems. The ZI-NHPP framework addresses data inflation, a limitation in prior models (e.g., [1]), while the integrated optimization ofùëÅùëê(number of conforming items inspected) andùëáùëü(production run length) balances quality control and cost efficiency. The expanded discussion highlights parameter sensitivities (e.g.,ùëáùëü,Drùê∑ùëü,œÄùúã) and practical trade-offs, such as the interplay between inventory holding costs and production scheduling. For manufacturers, this suggests a dual strategy: invest in robust design to increaseœÄùúãand implement adaptive inspection and production planning to optimize bothNcùëÅùëêandTrùëáùëübased on defective rates and demand data. 3.3. ZI-NHPP Framework for Estimating the Parameters of a Production System‚Äôs DeteriorationTo accurately model the deterioration process in imperfect production systems, we adopt the ZI-NHPP framework. This model is particularly suited for historical datasets exhibiting excess zeros‚Äîperiods where no deterioration events occur‚Äîdue to inherent system robustness or Industry 4.0-enabled preventive measures (-[6,7]). Traditional NHPP assume a continuous intensity function but fail to account for structural zeros, leading to biased parameter estimates ([8]). The ZI-NHPP addresses this by incorporating a mixture distribution: with probabilityœÄùúã(the zero-inflation parameter), the system remains defect-free (no events), and with probability1‚àíœÄ1‚àíùúã, it follows an NHPP with a power-law intensity functionŒªt=Œ≤Œ±Œ≤tŒ≤‚àí1ùúÜ(ùë°)=ùõΩùõºùõΩùë°ùõΩ‚àí1(whereŒ±>0ùõº>0is the scale parameter andŒ≤>1ùõΩ>1indicates accelerating deterioration, reflecting wear-out in production systems).The cumulative intensity function is given byŒõt=tŒ±Œ≤Œõ(ùë°)=(ùë°ùõº)ùõΩ, and the probability of no events occurring up to timetùë°in the susceptible subpopulation isexp‚àíŒõtexp(‚àíŒõ(ùë°)). Therefore, the overall probability of zero events up to timeTrùëáùëü(the production run length) isœÄ+(1‚àíœÄ)exp‚àíTrŒ±Œ≤ùúã+(1‚àíùúã)exp(‚àí(ùëáùëüùõº)ùõΩ), while the probability of being out of control is1‚àíœÄ1‚àíexp‚àíTrŒ±Œ≤(1‚àíùúã)(1‚àíexp(‚àí(ùëáùëüùõº)ùõΩ)). This formulation captures the heterogeneous degradation in a production system, where deterioration events (e.g., a machine shifting to an out-of-control state) may be rare but increase over time due to factors such as machine wear, employee efficiency, or environmental conditions.ParametersœÄùúã,Œ±ùõº, andŒ≤ùõΩmust be estimated from historical data, which often includes multiple systems or runs with varying observation periods. We employ the Expectation-Maximization (EM) algorithm, an iterative method well-suited for handling latent variables in zero-inflated models. The EM algorithm treats the zero-inflation as arising from unobserved binary indicators: for each observationi=1,2,‚Ä¶,nùëñ=1,2,‚Ä¶,ùëõ, letZi=1ùëçùëñ=1if the system is susceptible (follows an NHPP) andZi=0ùëçùëñ=0if it is immune (experiencing zero events). The observed data consist of event countsYiùëåùëñover intervals0,Ti[0,ùëáùëñ], whereYi=0ùëåùëñ=0may originate from either subpopulation.Assumenùëõindependent systems, each observed over timeTiùëáùëñ. The likelihood for the ZI-NHPP is:L(Œ∏)=‚àèi=1n[œÄŒ¥Yi=0+(1‚àíœÄ)ŒõTiYiYi!exp‚àíŒõTi],ùêø(ùúÉ)=‚àèùëõùëñ=1[ùúãùõøùëåùëñ=0+(1‚àíùúã)[Œõ(ùëáùëñ)]ùëåùëñùëåùëñ!exp(‚àíŒõ(ùëáùëñ))],(6)whereŒ∏=(œÄ,Œ±,Œ≤)ùúÉ=(ùúã,ùõº,ùõΩ),Œ¥ùõøis the Dirac delta, andŒõTi=TiŒ±Œ≤Œõ(ùëáùëñ)=(ùëáùëñùõº)ùõΩ.Complete-Data Likelihood:Introduce latentZi‚àºBernoulli(1‚àíœÄ)ùëçùëñ‚àºBernoulli(1‚àíùúã). The complete data likelihood is:Lc(Œ∏)=‚àèi=1nœÄ1‚àíZi(1‚àíœÄ)Zi‚ãÖTiŒ±Œ≤YiYi!exp‚àíTiŒ±Œ≤Zi.(7)Taking the log as follows:lc(Œ∏)=‚àëi=1n1‚àíZiln(œÄ)+Ziln(1‚àíœÄ)+ZiYilnTiŒ±Œ≤‚àíln(Yi!)‚àíTiŒ±Œ≤.(8)EM Algorithm Steps:The EM algorithm iterates between E-step (computing expectedZi) and M-step (maximizinglc) until convergence (e.g., change inl<10‚àí6).E-step:Compute the responsibility (posterior expectation ofZi):Z^i(k)=EZi‚à£Yi,Œ∏(k)=1‚àíœÄ(k)Œõ(k)TiiYiYi!exp‚àíŒõ(k)TiœÄ(k)Œ¥Yi=0+1‚àíœÄ(k)Œõ(k)TiYiYi!exp‚àíŒõ(k)Ti.(9)ForYi=0, this simplifies toZ^i(k)=1‚àíœÄ(k)exp‚àíŒõ(k)TiœÄ(k)+1‚àíœÄ(k)exp‚àíŒõ(k)Ti; forYi>0,Z^i(k)=1.M-step:Update parameters by maximizing the expected complete log-likelihood:œÄ(k+1)=1‚àí1n‚àëi=1nZ^ik.(10)ForŒ±andŒ≤, solve the system for susceptible observations (Z^i>0):‚àëi:Z^i>0Z^i(k)YilnTiŒ±(k+1)Œ≤(k+1)‚àíTiŒ±(k+1)Œ≤(k+1)‚àílnYi!.(11)This requires numerical optimization methods (e.g., Newton-Raphson), as closed-form solutions are unavailable for the power-law NHPP. The reparameterization aligns with the Weibull forms used in the cost model (e.g., hazard rates), ensuring consistency.Figure 2presents the pseudocode for the EM algorithm used to estimate the parameters of a production system‚Äôs deterioration.Figure 2.Pseudo-code of the EM Algorithm for Estimating the Parameters of a Production System‚Äôs Deterioration.The above pseudo-code presented outlines an iterative EM algorithm designed to estimate the parametersŒ∏=(œÄ,Œ±,Œ≤)of a Zero-Inflated NonHomogeneous Poisson Process model, tailored to characterize the deterioration of production systems. This algorithm addresses the challenge of historical data exhibiting excess zeros-periods with no deterioration events-by integrating a zero-inflation parameterœÄwith a powerlaw intensity functionŒª(t)=Œ≤Œ±Œ≤tŒ≤‚àí1and cumulative intensityŒõ(t)=(t/Œ±)Œ≤. The algorithm begins by accepting input data as a list of tuples (Yi,Ti) fornsystems, whereYirepresents the number of deterioration events andTiis the observation time for systemi. Initial parameter valuesŒ∏0=œÄ0,Œ±0,Œ≤0, a maximum number of iterations (max_iter), and a convergence tolerance (tol) are also provided. The code first computesl(number of systems with events) andm=n‚àíl(number of zeroevent systems), concatenating event times from positive systems into all_times to determineK(total event count). IfK=0, an error is raised due to insufficient data. A precomputed statisticS, the sum oflogTi/tover event times, aids in parameter updates.The EM algorithm iterates up to max_iter times or until convergence. In the E-step, a listpof lengthnis initialized to store the responsibilityZ^i, the expected value of the latent indicatorZi(1 if susceptible, 0 if immune). For each systemi, the cumulative intensityŒõi=Ti/Œ±Œ≤and its exponentialexp‚àíŒõiare calculated. The responsibilityp[i‚àí1]is set to(1‚àíœÄ)exp‚àíŒõi/œÄ+(1‚àíœÄ)exp‚àíŒõiforYi=0(adjusted for zero division), 1 forYi>0(certainly susceptible), and 0 as a fallback. This step estimates the probability of each system belonging to the susceptible subpopulation. The weighted number of susceptible systemsWis then computed asl+m‚ãÖmean(pjforYj=0).In the M-step, the parameters are updated as follows:œÄnewis the proportion of immune systems, calculated asm‚ãÖ(1‚àímean(pjforYj=0)/n;Œ≤newis optimized numerically (e.g., via Newton-Raphson) to maximize the expected log-likelihood; andŒ±newis derived from the relationship between observed intensities andŒõi, approximated as(‚àë(TiŒ≤new‚ãÖp[i‚àí1]forYi>0)/K)1/Œ≤new. Convergence is checked by comparing the maximum absolute difference between old and new parameters against tol; if satisfied, the loop terminates, outputting the finalŒ∏=(œÄ,Œ±,Œ≤).Moreover, it should be noted that the estimates ofŒ±andŒ≤in the M-step do not have closed-form solutions due to the mixture structure of the ZI-NHPP likelihood. Therefore, we maximize the expected complete-data log-likelihood numerically using a Newton‚ÄìRaphson procedure with line search. If the Hessian becomes unstable, a quasi-Newton BFGS routine is used as a fallback. Initial values are obtained from standard NHPP estimation to ensure convergence. This guarantees monotonic likelihood improvement during iterations.This implementation is robust for production systems‚Äô deterioration data, handling per-system variability inTiandYi, and incorporates safeguards against edge cases (e.g., no events). It leverages the EM algorithm‚Äôs ability to manage latent variables, providing a practical tool for estimating parameters from zero-inflated datasets. Likewise, the estimation of the scale and shape parameters for product failures in both in-control and out-of-control states can be effectively implemented using the same algorithm in practice.The programs related to the algorithms and mathematical models are available in theSupplementary Materialsof the study.",
            "3.1. Problem Description": "It is supposed that a manufacturer specializing in imperfect production systems for customized products faces the challenge of establishing an efficient production and inspection framework to handle imperfections in the manufacturing process. In such production configuration, the system must rapidly assemble components on demand to meet customer specifications, while minimizing costs and ensuring product quality. However, the production system is imperfect and subject to deterioration, which can cause it to shift from an in-control state (producing mostly conforming items with low defect rates) to an out-of-control state (generating more defective items, such as those with cracks, abnormal joints, or component malfunctions). This deterioration is modeled using a Zero-Inflated Non-Homogeneous Poisson Process, as historical data often shows inflated zeros (periods with no deterioration events), possibly due to robust periods in Industry 4.0-enabled systems. Therefore, the manufacturer faces several key challenges: System Deterioration and State Uncertainty: The production system deteriorates over time, with the rate influenced by factors such as machine wear, employee efficiency, and environmental conditions. The probability of being out-of-control increases with the production run lengthùëáùëü, leading to higher defect rates (ùê∑ùëü) and costs. Historical data may exhibit zero-inflation, where the system remains defect-free with probabilityùúã, complicating traditional modeling approaches. (1) Quality Inspection Trade-offs:Implementing a full inspection is time-consuming and costly, whereas sampling carries the risk of accepting defective lots or rejecting conforming ones, leading to correction (ùê∂ùëê) or warranty (ùê∂ùë§) expenses. Therefore, the manufacturer requires an optimal sampling plan to determine the number of items to inspect (ùëÅùëê) without disrupting production schedules. (2) Cost Minimization Under Multiple Factors:The total cost includes production (fixedùê∂ùëöand variable, based on learning), inspection (ùê∂ùëñandùê∂ùëù), inventory holding (ùê∂‚Ñé), and warranty based on product hazard rates. The employee learning rate (ùêøùëü) affects efficiency, while market demand (ùëÑùëë) and production rate (ùëÉùëü) influence inventory mismatches. Balancing these factors, while accounting for product deterioration, presents a complex challenge. (3) Parameter Estimation from Historical Data:Deterioration parameters (ùõº,ùõΩ,ùúã) must be estimated from potentially zero-inflated historical data, requiring advanced methods such as the Expectation-Maximization (EM) algorithm to handle zero-inflation. For better comprehension, the framework of the study is presented inFigure 1. Figure 1.Production System Operation with Inspection, Correction, and Warranty Framework. Figure 1illustrates the integrated operational framework for an imperfect production system, incorporating production, inspection, correction, and warranty processes to minimize total costs under zero-inflated non-homogeneous Poisson deterioration. This diagram provides a visual representation of the system‚Äôs workflow, highlighting key decision points, state transitions, and cost implications, while emphasizing the role of the ZI-NHPP model in estimating deterioration parameters from historical data. The framework begins at the top with ‚Äúproduction cost‚Äù considerations, which decrease exponentially with accumulated inspection experience and employee learning rate. Production initiates by setting the optimal production run lengthùëáùëü, during which the system may remain in-control or shift to out-of-control based on the ZI-NHPP intensity functionùúÜ(ùë°)=ùõΩùõºùõΩùë°ùõΩ‚àí1. Parameters such asùõºandùõΩare estimated via the EM algorithm from historical failure data of production systems and products, accounting for zero-inflationùúã. A central decision node checks if the system is in control. If it is in control, the product failure rate is set low (using Weibull parameters (ùõº1,ùõΩ1); if out-of-control, it is set high (ùõº2,ùõΩ2). This leads to ‚Äúproduct inspection‚Äù, where negative binomial sampling is employed to inspect items until the optimal number of conforming itemsùëÅùëêis reached. Items are classified as conforming or defective: defective items undergo correction, while conforming items proceed to product sales. The lower section addresses post-production stages. Conforming items contribute to ‚Äúinventory holding cost‚Äù estimation, based on mismatches between production output and demand. Sold products incur ‚Äúwarranty costs‚Äù, estimated using Weibull cumulative hazards over the warranty periodùëäùëù, differentiated by system state:(ùëäùëùùõº1)ùõΩ1for in-control (low failure risk) and(ùëäùëùùõº2)ùõΩ2for out-of-control (high failure risk). Historical data informs these estimations, ensuring alignment with real-world deterioration patterns. This framework underscores the interplay between quality inspection and production planning, optimizingùëÅùëêandùëáùëüto minimize the expected total costùê∏[ùëáùê∂(ùëÅùëê,ùëáùëü)], which includes production, inspection and correction, inventory holding, and warranty expenses. By integrating ZI-NHPP for robust deterioration modeling, the approach enhances cost-efficiency in production systems.",
            "3.2. Development of the Mathematical Model": "To address these challenges, the manufacturer aims to develop a cost-minimizing inspection plan that optimizesùëÅùëê, minimizes the expected total cost (E[TC]), and adapts to mass customization needs. These notations or symbols used are summarized inTable 1. Table 1.Notations or Symbols. The expected total cost formula for the optimal inspection plan in imperfect production systems using a ZI-NHPP model incorporates various cost components. These costs reflect the integration of production, inspection, inventory management, and warranty considerations. Below is a detailed explanation of each cost component in the formula: (1) Production Cost:This component represents the total cost incurred during the production process and can be expressed as follows:ùê∂ùëùùëüùëúùëë=ùê∂ùëö+‚àëùëÑùëëùëñ=1ùê∂ùë¢ùëñln(ùêøùëü)ln(2)ùëí‚àíùêæùëêùëÅùëê.(1)whereùê∂ùëörepresents the fixed expenditure on production equipment during a period, reflecting the capital investment required to operate the manufacturing system. The variable cost is modeled as a summation fromùëñ=1toùëÑùëë(quantity demanded), whereùê∂ùë¢is the manufacturing cost of the first unit. The termùëñln(ùêøùëü)ln(2)accounts for the learning rate(ùêøùëü), a decreasing function that captures improvements in employee efficiency over time, derived from the learning curve effect. The exponential factorùëí‚àíùêæùëêùëÅùëêincorporates the contraction constant (ùêæùëê) and the optimal number of conforming items inspected (ùëÅùëê), reflecting reduced manufacturing costs as more conforming items are identified due to enhanced efficiency. This term ensures that production costs decrease with improved inspection outcomes. Generally, the study assumes a consistent learning rateùêøùëü, which may vary in practice due to worker turnover or skill disparities, potentially affecting cost estimates in dynamic production environments.ùêæùëêgoverns the pace of deterioration and its influence on production cost is analytically characterized in Lemma 1. A higherùêæùëêamplifies cost savings from inspection, but its value should be calibrated against empirical data to avoid over-optimism. Lemma 1.Monotonicity and Convexity of Production Cost.Statement:The production costùê∂ùëùùëüùëúùëë=ùê∂ùëö+‚àëùëÑùëëùëñ=1ùê∂ùë¢ùëñùëôùëõ(ùêøùëü)ùëôùëõ(2)ùëí‚àíùêæùëêùëÅùëêis strictly decreasing and convex inùëÅùëêforùêæùëê>0,ùêøùëü‚àà(0,1), andùëÑùëë‚â•1. Furthermore,ùê∂ùëùùëüùëúùëëis strictly increasing and convex inùêæùëê. Proof of Lemma 1.Please refer toAppendix A.The parameterùêæùëêgoverns the rate at which the system transitions to reduced productivity due to deterioration. Higherùêæùëêaccelerates performance degradation, driving costs upward, and its convexity implies that deterioration mitigation measures (e.g., preventive maintenance or higher sampling rates) become increasingly valuable when deterioration becomes severe. ‚ñ° (2) Inspection and Correction Cost:This costcoversthe expenses associated with inspecting the production system and correcting deviations, and it can be expressed as follows:ùê∂ùëñùëõùë†=ùê∂ùëñ+(ùê∂ùëê+ùê∂ùëùùëÅùëê1‚àíùê∑ùëü)(1‚àíùúã)(1‚àíexp(‚àí(ùëáùëüùõº)ùõΩ)).(2)whereùê∂ùëñrepresents the fixed cost of inspecting the system after each production run to assess its status [1]. The variable component includesùê∂ùëê(correction cost) andùê∂ùëùùëÅùëê1‚àíùê∑ùëü(product inspection cost per item, whereùëÅùëê1‚àíùê∑ùëüis the expected number of items inspected due to the defective rateùê∑ùëü). The probability of entering the out-of-control state isùëÉùëúùë¢ùë°(ùëáùëü)=(1‚àíùúã)(1‚àíexp(‚àíùõ¨(ùëáùëü))), in accordance with the ZI-NHPP deterioration model, whereùúãis the latent defect-free probability based on the Weibull deterioration model‚Äôs cumulative intensity. This probability indicates the likelihood of needing correction actions. In addition, please note thatùêπùëêis a quality proportion parameter, not a complementary probability toùê∑ùëü, and the two should not sum to 1. This component highlights a critical trade-off in imperfect production systems: full inspection is resource-intensive, whereas sampling risks accepting defective lots, incurringùê∂ùëêandùê∂ùë§(warranty) costs. The out-of-control probability, driven byùõºandùõΩ, is sensitive to production duration; longerùëáùëüincreases this probability, necessitating more frequent inspections. The defect rateùê∑ùëüfurther complicates this balance‚Äîhigherùê∑ùëüraises the expected number of items inspected, amplifying costs unless offset by robust design. Lemma 2.Expectation and Probability in Inspection and Correction Cost.Statement:The inspection and correction costùê∂ùëñùëõùë†=ùê∂ùëñ+(ùê∂ùëê+ùê∂ùëùùëÅùëê1‚àíùê∑ùëü)ùëÉùëúùë¢ùë°has expected number of inspectionsùê∏[ùëÅùëñ]=ùëÅùëê1‚àíùê∑ùëüunder negative binomial sampling, andùëÉùëúùë¢ùë°is the valid probability of needing correction, bounded in[0,1‚àíùúã]. Proof of Lemma 2.Please refer toAppendix A. ‚ñ° (3) Inventory Holding Cost:This component quantifies the cost of holding inventory during the production cycle, and it can be expressed as follows:ùê∂‚Ñéùëúùëôùëë=ùê∂‚Ñé(ùëÉùëüùëáùëü‚àíùëÑùëë)22ùëÖùëë.(3) ùê∂‚Ñéis the inventory holding cost per unit time, applied to the squared difference between the total products produced (ùëÉùëüùëáùëü, whereùëÉùëüis the production rate) and the demand (ùëÑùëë), divided by twice the demand rate (ùëÖùëë). The quadratic term(ùëÉùëüùëáùëü‚àíùëÑùëë)2accounts for overproduction or underproduction relative to demand, amplifying costs when production exceeds or falls short ofùëÑùëë. In the study, where demand (ùëÑùëë) is often customized and variable, this cost component underscores the challenge of synchronizing production with customer orders under tight schedules. The quadratic nature ensures that small mismatches are tolerable, but large deviations (e.g., overproduction due to extendedùëáùëü) escalate costs rapidly. Sensitivity toùëÉùëüandùëáùëüis significant; increasingùëáùëüto meet demand may raise the related costs, creating a trade-off with quality costs. Lemma 3.Convexity of Inventory Holding Cost.Statement:The inventory holding costùê∂‚Ñéùëúùëôùëë=ùê∂‚Ñé‚éõ‚éù‚éú‚éú(ùëÉùëüùëáùëü‚àíùëÑùëë)22ùëÖùëë‚éû‚é†‚éü‚éüis convex inùëáùëüand minimized atùëáùëü=ùëÑùëë/ùëÉùëü(perfect production-demand match). Proof of Lemma 3.Please refer toAppendix A.This result highlights that inventory cost is driven not merely by time or production scale individually, but by the extent to which production output matches demand over a cycle. The convex structure implies that deviations from the balance pointùëáùëü=ùëÑùëë/ùëÉùëübecome increasingly costly, so small mismatches are inexpensive while large overproduction accumulates inventory rapidly. When combined with deterioration effects, longer production runs may reduce setup frequency but push the system further from the demand-matching point, reinforcing the trade-off between holding cost and quality-related risks. ‚ñ° (4) Warranty Cost:This cost accounts for warranty-related expenses due to product failures during the warranty periodùëäùëù.ùê∂ùë§ùëéùëü=ùê∂ùë§‚é°‚é£‚é¢(ùëäùëùùõº1)ùõΩ1exp(‚àí(ùëáùëüùõº)ùõΩ)+‚éõ‚éù‚éú‚éú‚éú(ùëÉùëüùëáùëü‚àíùëÅùëê1‚àíùê∑ùëü)‚éõ‚éù‚éú‚éú‚éúùêπùëê(ùëäùëùùõº1)ùõΩ1+(1‚àíùêπùëê)(ùëäùëùùõº2)ùõΩ2‚éû‚é†‚éü‚éü‚éü+ùëÅùëê1‚àíùê∑ùëü(ùëäùëùùõº2)ùõΩ2‚éû‚é†‚éü‚éü‚éü(1‚àíexp(‚àí(ùëáùëüùõº)ùõΩ))‚é§‚é¶‚é•.(4) ùê∂ùë§is the warranty cost per item. The warranty cost is divided into two parts: (i) In-Control Warranty Cost(ùëäùëùùõº1)ùõΩ1exp(‚àí(ùëáùëüùõº)ùõΩ)represents the expected failures of products produced in the in-control state, with hazard rate‚Ñé1(ùë°)=ùõΩ1ùõºùõΩ11ùë°ùõΩ1‚àí1. The termexp(‚àí(ùëáùëüùõº)ùõΩ)is the probability of the system remaining in-control. (ii) Out-of-Control Warranty Cost‚éõ‚éù‚éú‚éú(ùëÉùëüùëáùëü‚àíùëÅùëê1‚àíùê∑ùëü)‚éõ‚éù‚éú‚éúùêπùëê(ùëäùëùùõº1)ùõΩ1+(1‚àíùêπùëê)(ùëäùëùùõº2)ùõΩ2‚éû‚é†‚éü‚éü+ùëÅùëê1‚àíùê∑ùëü(ùëäùëùùõº2)ùõΩ2‚éû‚é†‚éü‚éü(1‚àíexp(‚àí(ùëáùëüùõº)ùõΩ))covers failures of unchecked, conforming, and reworked items from the out-of-control state, with hazard rate‚Ñé2(ùë°)=ùõΩ2ùõºùõΩ22ùë°ùõΩ2‚àí1(where‚Ñé2(ùë°)>‚Ñé1(ùë°)).ùêπùëêis the proportion of conforming items from the in-control state, and1‚àíexp(‚àí(ùëáùëüùõº)ùõΩ)is the out-of-control probability. Warranty costs are a major concern in production systems, where product customization increases failure risks if defects are undetected. The use of Weibull distributions (ùõº1,ùõΩ1for low hazard;ùõº2,ùõΩ2for high) reflects realistic failure patterns, withùõΩ2>ùõΩ1indicating accelerated degradation in out-of-control states. Inspection (ùëÅùëê) reduces warranty claims by reworking defective items, but the effectiveness depends onùêπùëê(conforming proportion) andùê∑ùëü. The zero-inflation parameterùúãalso plays a role; higherùúã(robust design) reduces baseline warranty costs, offering a strategic lever for manufacturers. Lemma 4.Expectation of Failures in Warranty Cost.Statement:The warranty costùê∂ùë§ùëéùëü=ùê∂ùë§ùê∏[ùëìùëéùëñùëôùë¢ùëüùëíùë†ùëñùëõ[0,ùëäùëù]]uses Weibull expected failures(ùëäùëù/ùõºùëó)ùõΩùëófor statesùëó=1(in-control, low hazard) andùëó=2(out-of-control, high hazard), weighted by system states, and is non-decreasing inùëÉùëúùë¢ùë°. (ùëÉùëúùë¢ùë°: The out-of-control probability,ùëÉùëñùëõ: The in-control probability). Proof of Lemma 4.Please refer toAppendix A.The warranty cost increases as the system becomes more likely to be out-of-control during production, because items produced in that regime fail at a much higher hazard rate. Thus, even when defects are not immediately observable, inadequate inspection increases long-run failure liability, linking inspection decisions to post-sale cost exposure. ‚ñ° (5) Total Expected Cost:According to the above mentioned, the total expected total cost is as follows:ùê∏[ùëáùê∂(ùëÅùëê,ùëáùëü)]=ùê∂ùëö+‚àëùëÑùëëùëñ=1ùê∂ùë¢ùëñln(ùêøùëü)ln(2)ùëí‚àíùêæùëêùëÅùëê+ùê∂ùëñ+(ùê∂ùëê+ùê∂ùëùùëÅùëê1‚àíùê∑ùëü)((1‚àíùúã)(1‚àíexp(‚àí(ùëáùëüùõº)ùõΩ)))+ùê∂‚Ñé(ùëÉùëüùëáùëü‚àíùëÑùëë)22ùëÖùëë+ùê∂ùë§‚é°‚é£‚é¢(ùëäùëùùõº1)ùõΩ1exp(‚àí(ùëáùëüùõº)ùõΩ)+‚éõ‚éù‚éú‚éú‚éú(ùëÉùëüùëáùëü‚àíùëÅùëê1‚àíùê∑ùëü)‚éõ‚éù‚éú‚éú‚éúùêπùëê(ùëäùëùùõº1)ùõΩ1+(1‚àíùêπùëê)(ùëäùëùùõº2)ùõΩ2‚éû‚é†‚éü‚éü‚éü+ùëÅùëê1‚àíùê∑ùëü(ùëäùëùùõº2)ùõΩ2‚éû‚é†‚éü‚éü‚éü(1‚àíexp(‚àí(ùëáùëüùõº)ùõΩ))‚é§‚é¶‚é•.(5) The total costùê∏[ùëáùê∂(ùëÅùëê,ùëáùëü)]integrates these components, reflecting a comprehensive approach to cost management in imperfect production systems. The ZI-NHPP framework addresses data inflation, a limitation in prior models (e.g., [1]), while the integrated optimization ofùëÅùëê(number of conforming items inspected) andùëáùëü(production run length) balances quality control and cost efficiency. The expanded discussion highlights parameter sensitivities (e.g.,ùëáùëü,Drùê∑ùëü,œÄùúã) and practical trade-offs, such as the interplay between inventory holding costs and production scheduling. For manufacturers, this suggests a dual strategy: invest in robust design to increaseœÄùúãand implement adaptive inspection and production planning to optimize bothNcùëÅùëêandTrùëáùëübased on defective rates and demand data.",
            "3.3. ZI-NHPP Framework for Estimating the Parameters of a Production System‚Äôs Deterioration": "To accurately model the deterioration process in imperfect production systems, we adopt the ZI-NHPP framework. This model is particularly suited for historical datasets exhibiting excess zeros‚Äîperiods where no deterioration events occur‚Äîdue to inherent system robustness or Industry 4.0-enabled preventive measures (-[6,7]). Traditional NHPP assume a continuous intensity function but fail to account for structural zeros, leading to biased parameter estimates ([8]). The ZI-NHPP addresses this by incorporating a mixture distribution: with probabilityœÄùúã(the zero-inflation parameter), the system remains defect-free (no events), and with probability1‚àíœÄ1‚àíùúã, it follows an NHPP with a power-law intensity functionŒªt=Œ≤Œ±Œ≤tŒ≤‚àí1ùúÜ(ùë°)=ùõΩùõºùõΩùë°ùõΩ‚àí1(whereŒ±>0ùõº>0is the scale parameter andŒ≤>1ùõΩ>1indicates accelerating deterioration, reflecting wear-out in production systems). The cumulative intensity function is given byŒõt=tŒ±Œ≤Œõ(ùë°)=(ùë°ùõº)ùõΩ, and the probability of no events occurring up to timetùë°in the susceptible subpopulation isexp‚àíŒõtexp(‚àíŒõ(ùë°)). Therefore, the overall probability of zero events up to timeTrùëáùëü(the production run length) isœÄ+(1‚àíœÄ)exp‚àíTrŒ±Œ≤ùúã+(1‚àíùúã)exp(‚àí(ùëáùëüùõº)ùõΩ), while the probability of being out of control is1‚àíœÄ1‚àíexp‚àíTrŒ±Œ≤(1‚àíùúã)(1‚àíexp(‚àí(ùëáùëüùõº)ùõΩ)). This formulation captures the heterogeneous degradation in a production system, where deterioration events (e.g., a machine shifting to an out-of-control state) may be rare but increase over time due to factors such as machine wear, employee efficiency, or environmental conditions. ParametersœÄùúã,Œ±ùõº, andŒ≤ùõΩmust be estimated from historical data, which often includes multiple systems or runs with varying observation periods. We employ the Expectation-Maximization (EM) algorithm, an iterative method well-suited for handling latent variables in zero-inflated models. The EM algorithm treats the zero-inflation as arising from unobserved binary indicators: for each observationi=1,2,‚Ä¶,nùëñ=1,2,‚Ä¶,ùëõ, letZi=1ùëçùëñ=1if the system is susceptible (follows an NHPP) andZi=0ùëçùëñ=0if it is immune (experiencing zero events). The observed data consist of event countsYiùëåùëñover intervals0,Ti[0,ùëáùëñ], whereYi=0ùëåùëñ=0may originate from either subpopulation. Assumenùëõindependent systems, each observed over timeTiùëáùëñ. The likelihood for the ZI-NHPP is:L(Œ∏)=‚àèi=1n[œÄŒ¥Yi=0+(1‚àíœÄ)ŒõTiYiYi!exp‚àíŒõTi],ùêø(ùúÉ)=‚àèùëõùëñ=1[ùúãùõøùëåùëñ=0+(1‚àíùúã)[Œõ(ùëáùëñ)]ùëåùëñùëåùëñ!exp(‚àíŒõ(ùëáùëñ))],(6)whereŒ∏=(œÄ,Œ±,Œ≤)ùúÉ=(ùúã,ùõº,ùõΩ),Œ¥ùõøis the Dirac delta, andŒõTi=TiŒ±Œ≤Œõ(ùëáùëñ)=(ùëáùëñùõº)ùõΩ. Complete-Data Likelihood: Introduce latentZi‚àºBernoulli(1‚àíœÄ)ùëçùëñ‚àºBernoulli(1‚àíùúã). The complete data likelihood is:Lc(Œ∏)=‚àèi=1nœÄ1‚àíZi(1‚àíœÄ)Zi‚ãÖTiŒ±Œ≤YiYi!exp‚àíTiŒ±Œ≤Zi.(7) Taking the log as follows:lc(Œ∏)=‚àëi=1n1‚àíZiln(œÄ)+Ziln(1‚àíœÄ)+ZiYilnTiŒ±Œ≤‚àíln(Yi!)‚àíTiŒ±Œ≤.(8)EM Algorithm Steps: The EM algorithm iterates between E-step (computing expectedZi) and M-step (maximizinglc) until convergence (e.g., change inl<10‚àí6). E-step:Compute the responsibility (posterior expectation ofZi):Z^i(k)=EZi‚à£Yi,Œ∏(k)=1‚àíœÄ(k)Œõ(k)TiiYiYi!exp‚àíŒõ(k)TiœÄ(k)Œ¥Yi=0+1‚àíœÄ(k)Œõ(k)TiYiYi!exp‚àíŒõ(k)Ti.(9) ForYi=0, this simplifies toZ^i(k)=1‚àíœÄ(k)exp‚àíŒõ(k)TiœÄ(k)+1‚àíœÄ(k)exp‚àíŒõ(k)Ti; forYi>0,Z^i(k)=1. M-step:Update parameters by maximizing the expected complete log-likelihood:œÄ(k+1)=1‚àí1n‚àëi=1nZ^ik.(10) ForŒ±andŒ≤, solve the system for susceptible observations (Z^i>0):‚àëi:Z^i>0Z^i(k)YilnTiŒ±(k+1)Œ≤(k+1)‚àíTiŒ±(k+1)Œ≤(k+1)‚àílnYi!.(11) This requires numerical optimization methods (e.g., Newton-Raphson), as closed-form solutions are unavailable for the power-law NHPP. The reparameterization aligns with the Weibull forms used in the cost model (e.g., hazard rates), ensuring consistency.Figure 2presents the pseudocode for the EM algorithm used to estimate the parameters of a production system‚Äôs deterioration. Figure 2.Pseudo-code of the EM Algorithm for Estimating the Parameters of a Production System‚Äôs Deterioration. The above pseudo-code presented outlines an iterative EM algorithm designed to estimate the parametersŒ∏=(œÄ,Œ±,Œ≤)of a Zero-Inflated NonHomogeneous Poisson Process model, tailored to characterize the deterioration of production systems. This algorithm addresses the challenge of historical data exhibiting excess zeros-periods with no deterioration events-by integrating a zero-inflation parameterœÄwith a powerlaw intensity functionŒª(t)=Œ≤Œ±Œ≤tŒ≤‚àí1and cumulative intensityŒõ(t)=(t/Œ±)Œ≤. The algorithm begins by accepting input data as a list of tuples (Yi,Ti) fornsystems, whereYirepresents the number of deterioration events andTiis the observation time for systemi. Initial parameter valuesŒ∏0=œÄ0,Œ±0,Œ≤0, a maximum number of iterations (max_iter), and a convergence tolerance (tol) are also provided. The code first computesl(number of systems with events) andm=n‚àíl(number of zeroevent systems), concatenating event times from positive systems into all_times to determineK(total event count). IfK=0, an error is raised due to insufficient data. A precomputed statisticS, the sum oflogTi/tover event times, aids in parameter updates. The EM algorithm iterates up to max_iter times or until convergence. In the E-step, a listpof lengthnis initialized to store the responsibilityZ^i, the expected value of the latent indicatorZi(1 if susceptible, 0 if immune). For each systemi, the cumulative intensityŒõi=Ti/Œ±Œ≤and its exponentialexp‚àíŒõiare calculated. The responsibilityp[i‚àí1]is set to(1‚àíœÄ)exp‚àíŒõi/œÄ+(1‚àíœÄ)exp‚àíŒõiforYi=0(adjusted for zero division), 1 forYi>0(certainly susceptible), and 0 as a fallback. This step estimates the probability of each system belonging to the susceptible subpopulation. The weighted number of susceptible systemsWis then computed asl+m‚ãÖmean(pjforYj=0). In the M-step, the parameters are updated as follows:œÄnewis the proportion of immune systems, calculated asm‚ãÖ(1‚àímean(pjforYj=0)/n;Œ≤newis optimized numerically (e.g., via Newton-Raphson) to maximize the expected log-likelihood; andŒ±newis derived from the relationship between observed intensities andŒõi, approximated as(‚àë(TiŒ≤new‚ãÖp[i‚àí1]forYi>0)/K)1/Œ≤new. Convergence is checked by comparing the maximum absolute difference between old and new parameters against tol; if satisfied, the loop terminates, outputting the finalŒ∏=(œÄ,Œ±,Œ≤). Moreover, it should be noted that the estimates ofŒ±andŒ≤in the M-step do not have closed-form solutions due to the mixture structure of the ZI-NHPP likelihood. Therefore, we maximize the expected complete-data log-likelihood numerically using a Newton‚ÄìRaphson procedure with line search. If the Hessian becomes unstable, a quasi-Newton BFGS routine is used as a fallback. Initial values are obtained from standard NHPP estimation to ensure convergence. This guarantees monotonic likelihood improvement during iterations. This implementation is robust for production systems‚Äô deterioration data, handling per-system variability inTiandYi, and incorporates safeguards against edge cases (e.g., no events). It leverages the EM algorithm‚Äôs ability to manage latent variables, providing a practical tool for estimating parameters from zero-inflated datasets. Likewise, the estimation of the scale and shape parameters for product failures in both in-control and out-of-control states can be effectively implemented using the same algorithm in practice. The programs related to the algorithms and mathematical models are available in theSupplementary Materialsof the study.",
            "4. Application and Numerical Analysis": "4.1. Numerical ExampleAs a toy manufacturer specializing in electronic toys under an imperfect production system, we encounter persistent challenges in ensuring product quality while controlling costs amid system deterioration. Our production line for items such as interactive robots and remote-controlled vehicles often shifts from an in-control state to an out-of-control state, resulting in defects like faulty circuits and assembly errors. These issues increase rework, warranty claims, and inventory holding costs. Historical data from our operations reveal significant variability, including extended periods without deterioration events-possibly due to robust component designs or preventive maintenance‚Äîthat traditional models fail to capture accurately. To address this, we have implemented a Zero-Inflated Non-Homogeneous Poisson Process framework, employing the EM algorithm to estimate degradation parameters from our historical production logs.Table 2summarizes the key parameter values used in our case study, derived from empirical data and aligned with our electronic toy manufacturing context. For the production system‚Äôs degradation, the scale parameterŒ±=5.3and shape parameterŒ≤=2.0indicate a moderate degradation onset with accelerating failure rates, while the zero-inflation parameterœÄ=0.3suggests a 30% probability of defect-free runs, underscoring design robustness. The defect rate in the out-of-control state isDr=0.3, reflecting common issues like component wear. Inspection-related costs include system inspection atCi=125, correction atCc=100, and per-item inspection atCp=15. Product failure rates are modeled with Weibull parameters: in-control state (Œ±1=6.5,Œ≤1=1.1) for lower hazards, and out-of-control state (Œ±2=4.7,Œ≤2=3.6) for higher risks. Production parameters encompass a rate ofPr=800units, contraction constantKc=0.1, equipment expenditureCm=300, first-unit manufacturing costCu=10, employee learning rateLr=0.5, and demanded quantityQd=1200. Additional costs include inventory holding atCh=10.5per unit time and maintenance atCw=20per item, with a conforming proportion from incontrol stateFc=0.95and warranty termWp=2years.Table 2.Parameter Values for the Case.These parameters provide a realistic basis for optimizing our production run length (Tr) and the number of conforming items inspected (Nc), aiming to balance quality assurance with cost efficiency. By leveraging this model, we seek to develop actionable strategies that reduce overall expenses-spanning production, inspection, correction, inventory, and warranty-while adapting to market demands for customizable electronic toys.Using the EM algorithm on simulated historical data (n= 500), we estimatedœÄ‚âà0.3046,Œ±‚âà5.328, andŒ≤‚âà2.017, closely matching the true parameter values and demonstrating the algorithm‚Äôs reliability (seeFigure 3). These estimates inform our optimization of the production run length (Tr) and the number of conforming items inspected (Nc), aiming to balance quality assurance with cost efficiency. Numerical optimization of the expected total costE[TC]yields an optimalNc‚âà18.24andTr‚âà1.50months, resulting in a minimal expected total cost of approximately 902.73. SinceNcmust be an integer in practice, we round it down to 18, after which the expected total cost is 903.42. Note that the number of inspection items (Ni) can be deduced to be 26. Although a closed-form solution for minimizingE[TC]is not available, numerical methods demonstrate the convexity ofE[TC]with respect toNcandTr, as shown inFigure 4. This configuration minimizes costs by limiting production run lengths to reduce deterioration risk while inspecting a sufficient number of items to detect defects early.Figure 3.The Process of Applying the EM algorithm to Estimate the Parameters of the ZI-NHPP Model.Figure 4.Contour and 3D Plot of Total Expected Cost (E[TC]) under Various Production Run (Tr) Lengths and Numbers of Conforming Items for Inspection (Nc). 4.2. Sensitivity AnalysisFigure 5illustrates the sensitivity of the expected total costE[TC]and the optimal number of conforming items inspected (Nc) to variations in the warranty term (Wp). As expected, extendingWpincreasesE[TC]due to greater exposure to product failures under Weibull hazard rates, particularly in the out-of-control state where failure rates accelerate. This analysis quantifies the cost implications, revealing that a 1-year extension from the baselineWp= 2 years elevatesE[TC]by approximately 15‚Äì20%, depending on the adjustment inNc. Such insights enable manufacturers to evaluate the financial trade-offs of offering longer warranties, balancing customer satisfaction against increased warranty expenses.Figure 5.Impact of Optimal Number of Conforming Items Inspected (Nc) and Total Expected Cost (E[TC]) as Warranty Term (Wp) Changes.Notably, extendingWpalso induces non-monotonic and non-linear changes in the optimalNc. To mitigate the amplified warranty costs from prolonged coverage, the model adjusts inspection intensity; however, this adaptation is not straightforward. As shown inFigure 5,Ncpeaks at approximately 19 whenWp= 2.5 years, indicating a heightened need for sampling to detect and correct defects early, thereby reducing long-term failure risks. However, beyond or below this threshold‚Äîwhether increasing to 4 years or decreasing to 1 year‚ÄîNcdecreases, dropping to about 15 atWp= 4 years. This counterintuitive pattern arises because excessively long warranties shift the cost optimum toward shorter production runs and fewer inspections, prioritizing prevention through system robustness (e.g., higherœÄ) over extensive sampling, while shorter warranties lessen the incentive for aggressive inspection.Figure 6illustrates the sensitivity of the expected total costE[TC]and the optimal number of conforming items inspected (Nc) to variations in the scale (Œ±) and shape (Œ≤) parameters of the production system degradation, adjusted by ¬±30% from baseline values (Œ±= 5.3,Œ≤= 2.0). IncreasingŒ±to 6.89 (+30%) reducesE[TC]by approximately 20% to around 805, reflecting enhanced system reliability due to a slower degradation onset, which lowers out-of-control transitions and associated costs. Similarly, elevatingŒ≤to 2.6 (+30%) decreasesE[TC]by about 25%, as the accelerated but more predictable deterioration pattern allows for better preventive measures.Figure 6.Impact of Optimal Number of Conforming Items Inspected (Nc) and Total Expected Cost (E[TC]) as Scale & Shape Parameters (Œ±,Œ≤) Changes.Concurrently, these parameter shifts necessitate adjustments inNcto verify the in-control state. AtŒ±= 6.89,Ncrises to 23 (from baseline 18), requiring more inspections to confirm sustained reliability amid the extended scale. ForŒ≤= 2.6,Ncincreases to 26, as the sharper deterioration curve demands intensified sampling to detect shifts early. Reductions inŒ±andŒ≤lowerNcto 12 and 11, respectively, as the system is more prone to defects, shifting the focus from inspection to other cost mitigations. These non-linear responses highlight the model‚Äôs adaptability, guiding toy manufacturers to calibrate parameters for cost-efficient operations.Figure 7illustrates the sensitivity of the expected total costE[TC]and the optimal number of conforming items inspected (Nc) to variations in the employee learning rate (Lr), adjusted by ¬±30% from the baseline value (Lr= 0.5). IncreasingLrto 0.65 (+30%) improves production efficiency, thereby reducing production costs through accelerated learning curves; however, this requires more intensive inspection to maintain quality amid faster output, causingNcto rise to approximately 34. The offsetting effects of decreased production costs and increased inspection expenses result in minimal variation inE[TC], which remains stable around 900, fluctuating by only 2‚Äì3% across the range. While this cost-focused analysis indicates negligible changes inE[TC], it does not imply that increasingLrlacks benefits for manufacturers. Incorporating revenue perspectives reveals that improved learning rates enable higher production volumes and faster throughput, potentially generating additional product revenue‚Äîespecially in high-demand assemble-to-order segments such as electronic toys. Therefore, strategies like investing in employee training to enhanceLrremain advantageous overall, despite the balanced cost dynamics observed here.Figure 7.Impact of Optimal Number of Conforming Items Inspected (Nc) and Total Expected Cost (E[TC]) as Employee Learning Rate (Lr) Changes.Figure 8examines the sensitivity of the expected total costE[TC], the optimal number of conforming items inspected (Nc), and the optimal production run length (Tr) to variations in the production rate (Pr), adjusted by ¬±30% from the baseline value (Pr= 800). IncreasingPrto 1040 (+30%) markedly reducesE[TC]by approximately 25% to 718, as higher throughput lowers per-unit production costs and enhances efficiency, with effects more pronounced than those observed with employee learning rate (Lr) adjustments. This escalation also boostsNcto 23, necessitating more inspections to ensure quality amid accelerated output, while shorteningTrto 1.05 to mitigate deterioration risks during extended runs. Conversely, decreasingPrto 560 (‚àí30%) elevatesE[TC]by 15% to 1028, reducesNcto 18, and extendsTrto 2.1, reflecting diminished efficiency and prolonged cycles to meet demand. UnlikeLrincreases, which yield stableE[TC]due to offsetting costs,Prenhancements deliver clearer cost reductions alongsideTrcontraction.Figure 8.Impact of Optimal Number of Conforming Items Inspected (Nc), Total Expected Cost (E[TC]) and Optimal Production Run Length (Tr) as Production Rate (Pr) Changes.Figure 9presents a sensitivity analysis of the expected total cost,E[TC], the optimal number of conforming items inspected (Nc) during the current production run length (Tr), in response to variations in the defect rate (Dr), adjusted by ¬±30% from the baseline value (Dr= 0.3). The analysis reveals that changes inDr‚Äîincreasing to 0.39 (+30%) or decreasing it to 0.21 (‚àí30%) results in only minimal fluctuations inE[TC]. This modest impact can be attributed to the relatively low baseline defect rate, where defect occurrences are already limited, thereby reducing the overall cost sensitivity to further adjustments.Figure 9.Impact of Optimal Number of Conforming Items Inspected (Nc) and Total Expected Cost (E[TC]) as Defect Rate (Dr) Changes.Although the defect rateDraffects the expected number of nonconforming items, its impact on the total expected cost remains relatively modest. This is becauseDrinfluences primarily the inspection-, correction-, and warranty-related components, while production, deterioration, and holding costs constitute a substantial share of the total cost and are not directly driven byDr. Moreover, the optimal sampling effort adjusts endogenously asDrchanges: higher defect rates reduce the required number of conforming samples for detection, whereas lower defect rates require increased sampling to maintain diagnostic confidence. These offsetting behavioral responses contribute to the relatively small variation inE[TC]observed inFigure 9.However, the effect onNcis more pronounced, particularly at higherDrlevels. WhenDrincreases to 0.39,Ncdecreases to approximately 17, compared to the baseline value of 18. This reduction occurs because, with a higher defect rate, the expected number of conforming items per sample diminishes under the same inspection effort. Consequently, fewer conforming items need to be inspected to determine whether the production system is in control or out of control, thereby optimizing resource allocation. These findings suggest that, for our electronic toy manufacturing process, adjustments inDrprimarily influence the inspection strategy rather than the total cost, providing valuable insights for adaptive quality control in varying defect scenarios. Additionally,Table 3provides detailed information related toFigure 5,Figure 6,Figure 7,Figure 8andFigure 9, which are used to investigate the impacts of the relevant factors.Table 3.Impact ofWp,Œ±,Œ≤,Lr,Pr, andDronNc,Tr, andE[TC]. 4.3. Managerial InsightsThis subsection synthesizes the analytical findings from the numerical experiments and articulates their broader managerial implications. No new numerical results are introduced; instead, we integrate the patterns observed in previous analyses to clarify the strategic role of coordinated decisions regarding inspection intensity and production-run length. The focus is on how these decisions systematically shift in response to operational conditions, offering qualitative guidance on whether firms should prioritize inspection, shorten production cycles, or invest in reliability improvements. The summarized insights are presented as follows:(1)Warranty Period (Wp)Changes in the warranty duration affect both the total expected cost and the optimal inspection plan. Moderate extensions of the warranty period encourage the firm to increase inspection intensity, as detecting defects earlier becomes more economically valuable. However, when the warranty period becomes substantially long, continuously increasing inspection intensity becomes less cost-effective. In such cases, the preferred strategy shifts toward structural actions, such as shortening the production run or improving the inherent reliability of the system, rather than indefinitely expanding sampling efforts.Insight: Warranty policies reshape not only the cost structure but also the intended role of inspection. Moderate warranties emphasize detection, whereas extended warranties prioritize prevention and robustness.(2)Degradation Characteristics (Œ±,Œ≤): When the production system deteriorates more gradually or follows a predictable pattern, the economic necessity of confirming the in-control state increases. A healthier system incentivizes higher sampling intensity because verifying quality provides greater marginal benefits. Conversely, when deterioration accelerates or becomes more erratic, simply increasing inspection frequency becomes less effective; the optimal strategy shifts toward more frequent production cycles or equipment improvements.Insight: Reliable systems warrant more rigorous verification, whereas fragile systems require structural modifications instead of expanded inspections.(3)Employee Learning Rate (Lr): Improvements in learning rates enhance productivity and reduce production-related cost components; however, this does not directly lead to proportional reductions in total expected costs. This is because increased productivity typically necessitates higher sampling intensity to ensure that the greater output maintains the same level of quality assurance.Insight: Learning enhances throughput and operational efficiency but yields limited cost savings unless accompanied by appropriate inspection adjustments.(4)Production Rate (Pr): Higher production capacity reduces per-unit operational costs and enhances the firm‚Äôs ability to implement shorter production cycles, thereby mitigating deterioration risks. With increased capacity, inspection intensity tends to rise because verifying a larger output becomes more valuable. Conversely, at lower production rates, longer production cycles may be necessary to meet demand, increasing exposure to deterioration and making inspection a secondary rather than a primary control mechanism.Insight: Capacity expansion enhances the effectiveness of short-term, high-verification strategies, while capacity constraints shift the focus toward longer-term operations and more cautious inspection.(5)Defect Rate (Dr): Variations in defect probability have a limited impact on the total expected cost when the baseline defect rate is already moderate. However, they significantly influence inspection strategies: a higher likelihood of defects reduces the number of conforming samples required to infer the system state, as poor-quality signals become more apparent.Insight: Even when costs are insensitive to defect rates, inspection policies must adapt to the information patterns in the data, not merely to cost outcomes.(6)Integrated Managerial Implications: Across all parameter variations, a consistent pattern emerges. Production run length and inspection intensity are interconnected strategic levers rather than independent decisions. Policy shifts (e.g., warranty length, capacity, maintenance investment) alter the role of inspection‚Äîsometimes serving as the primary tool for verification, and other times acting as a secondary measure to prevention or production adjustments. Zero-inflation modeling prevents the overestimation of deterioration risks, thereby avoiding overly conservative inspection plans that traditional NHPP models might generate. The convex nature of the cost landscape indicates a stable region of near-optimal solutions, facilitating robust decision-making even when parameter estimates are imperfect.Overall Insight: The model not only determines the optimal sampling level and production interval but also offers a prescriptive framework that explains when firms should respond through inspection, operational redesign, or reliability investments.",
            "4.1. Numerical Example": "As a toy manufacturer specializing in electronic toys under an imperfect production system, we encounter persistent challenges in ensuring product quality while controlling costs amid system deterioration. Our production line for items such as interactive robots and remote-controlled vehicles often shifts from an in-control state to an out-of-control state, resulting in defects like faulty circuits and assembly errors. These issues increase rework, warranty claims, and inventory holding costs. Historical data from our operations reveal significant variability, including extended periods without deterioration events-possibly due to robust component designs or preventive maintenance‚Äîthat traditional models fail to capture accurately. To address this, we have implemented a Zero-Inflated Non-Homogeneous Poisson Process framework, employing the EM algorithm to estimate degradation parameters from our historical production logs. Table 2summarizes the key parameter values used in our case study, derived from empirical data and aligned with our electronic toy manufacturing context. For the production system‚Äôs degradation, the scale parameterŒ±=5.3and shape parameterŒ≤=2.0indicate a moderate degradation onset with accelerating failure rates, while the zero-inflation parameterœÄ=0.3suggests a 30% probability of defect-free runs, underscoring design robustness. The defect rate in the out-of-control state isDr=0.3, reflecting common issues like component wear. Inspection-related costs include system inspection atCi=125, correction atCc=100, and per-item inspection atCp=15. Product failure rates are modeled with Weibull parameters: in-control state (Œ±1=6.5,Œ≤1=1.1) for lower hazards, and out-of-control state (Œ±2=4.7,Œ≤2=3.6) for higher risks. Production parameters encompass a rate ofPr=800units, contraction constantKc=0.1, equipment expenditureCm=300, first-unit manufacturing costCu=10, employee learning rateLr=0.5, and demanded quantityQd=1200. Additional costs include inventory holding atCh=10.5per unit time and maintenance atCw=20per item, with a conforming proportion from incontrol stateFc=0.95and warranty termWp=2years. Table 2.Parameter Values for the Case. These parameters provide a realistic basis for optimizing our production run length (Tr) and the number of conforming items inspected (Nc), aiming to balance quality assurance with cost efficiency. By leveraging this model, we seek to develop actionable strategies that reduce overall expenses-spanning production, inspection, correction, inventory, and warranty-while adapting to market demands for customizable electronic toys. Using the EM algorithm on simulated historical data (n= 500), we estimatedœÄ‚âà0.3046,Œ±‚âà5.328, andŒ≤‚âà2.017, closely matching the true parameter values and demonstrating the algorithm‚Äôs reliability (seeFigure 3). These estimates inform our optimization of the production run length (Tr) and the number of conforming items inspected (Nc), aiming to balance quality assurance with cost efficiency. Numerical optimization of the expected total costE[TC]yields an optimalNc‚âà18.24andTr‚âà1.50months, resulting in a minimal expected total cost of approximately 902.73. SinceNcmust be an integer in practice, we round it down to 18, after which the expected total cost is 903.42. Note that the number of inspection items (Ni) can be deduced to be 26. Although a closed-form solution for minimizingE[TC]is not available, numerical methods demonstrate the convexity ofE[TC]with respect toNcandTr, as shown inFigure 4. This configuration minimizes costs by limiting production run lengths to reduce deterioration risk while inspecting a sufficient number of items to detect defects early. Figure 3.The Process of Applying the EM algorithm to Estimate the Parameters of the ZI-NHPP Model. Figure 4.Contour and 3D Plot of Total Expected Cost (E[TC]) under Various Production Run (Tr) Lengths and Numbers of Conforming Items for Inspection (Nc).",
            "4.2. Sensitivity Analysis": "Figure 5illustrates the sensitivity of the expected total costE[TC]and the optimal number of conforming items inspected (Nc) to variations in the warranty term (Wp). As expected, extendingWpincreasesE[TC]due to greater exposure to product failures under Weibull hazard rates, particularly in the out-of-control state where failure rates accelerate. This analysis quantifies the cost implications, revealing that a 1-year extension from the baselineWp= 2 years elevatesE[TC]by approximately 15‚Äì20%, depending on the adjustment inNc. Such insights enable manufacturers to evaluate the financial trade-offs of offering longer warranties, balancing customer satisfaction against increased warranty expenses. Figure 5.Impact of Optimal Number of Conforming Items Inspected (Nc) and Total Expected Cost (E[TC]) as Warranty Term (Wp) Changes. Notably, extendingWpalso induces non-monotonic and non-linear changes in the optimalNc. To mitigate the amplified warranty costs from prolonged coverage, the model adjusts inspection intensity; however, this adaptation is not straightforward. As shown inFigure 5,Ncpeaks at approximately 19 whenWp= 2.5 years, indicating a heightened need for sampling to detect and correct defects early, thereby reducing long-term failure risks. However, beyond or below this threshold‚Äîwhether increasing to 4 years or decreasing to 1 year‚ÄîNcdecreases, dropping to about 15 atWp= 4 years. This counterintuitive pattern arises because excessively long warranties shift the cost optimum toward shorter production runs and fewer inspections, prioritizing prevention through system robustness (e.g., higherœÄ) over extensive sampling, while shorter warranties lessen the incentive for aggressive inspection. Figure 6illustrates the sensitivity of the expected total costE[TC]and the optimal number of conforming items inspected (Nc) to variations in the scale (Œ±) and shape (Œ≤) parameters of the production system degradation, adjusted by ¬±30% from baseline values (Œ±= 5.3,Œ≤= 2.0). IncreasingŒ±to 6.89 (+30%) reducesE[TC]by approximately 20% to around 805, reflecting enhanced system reliability due to a slower degradation onset, which lowers out-of-control transitions and associated costs. Similarly, elevatingŒ≤to 2.6 (+30%) decreasesE[TC]by about 25%, as the accelerated but more predictable deterioration pattern allows for better preventive measures. Figure 6.Impact of Optimal Number of Conforming Items Inspected (Nc) and Total Expected Cost (E[TC]) as Scale & Shape Parameters (Œ±,Œ≤) Changes. Concurrently, these parameter shifts necessitate adjustments inNcto verify the in-control state. AtŒ±= 6.89,Ncrises to 23 (from baseline 18), requiring more inspections to confirm sustained reliability amid the extended scale. ForŒ≤= 2.6,Ncincreases to 26, as the sharper deterioration curve demands intensified sampling to detect shifts early. Reductions inŒ±andŒ≤lowerNcto 12 and 11, respectively, as the system is more prone to defects, shifting the focus from inspection to other cost mitigations. These non-linear responses highlight the model‚Äôs adaptability, guiding toy manufacturers to calibrate parameters for cost-efficient operations. Figure 7illustrates the sensitivity of the expected total costE[TC]and the optimal number of conforming items inspected (Nc) to variations in the employee learning rate (Lr), adjusted by ¬±30% from the baseline value (Lr= 0.5). IncreasingLrto 0.65 (+30%) improves production efficiency, thereby reducing production costs through accelerated learning curves; however, this requires more intensive inspection to maintain quality amid faster output, causingNcto rise to approximately 34. The offsetting effects of decreased production costs and increased inspection expenses result in minimal variation inE[TC], which remains stable around 900, fluctuating by only 2‚Äì3% across the range. While this cost-focused analysis indicates negligible changes inE[TC], it does not imply that increasingLrlacks benefits for manufacturers. Incorporating revenue perspectives reveals that improved learning rates enable higher production volumes and faster throughput, potentially generating additional product revenue‚Äîespecially in high-demand assemble-to-order segments such as electronic toys. Therefore, strategies like investing in employee training to enhanceLrremain advantageous overall, despite the balanced cost dynamics observed here. Figure 7.Impact of Optimal Number of Conforming Items Inspected (Nc) and Total Expected Cost (E[TC]) as Employee Learning Rate (Lr) Changes. Figure 8examines the sensitivity of the expected total costE[TC], the optimal number of conforming items inspected (Nc), and the optimal production run length (Tr) to variations in the production rate (Pr), adjusted by ¬±30% from the baseline value (Pr= 800). IncreasingPrto 1040 (+30%) markedly reducesE[TC]by approximately 25% to 718, as higher throughput lowers per-unit production costs and enhances efficiency, with effects more pronounced than those observed with employee learning rate (Lr) adjustments. This escalation also boostsNcto 23, necessitating more inspections to ensure quality amid accelerated output, while shorteningTrto 1.05 to mitigate deterioration risks during extended runs. Conversely, decreasingPrto 560 (‚àí30%) elevatesE[TC]by 15% to 1028, reducesNcto 18, and extendsTrto 2.1, reflecting diminished efficiency and prolonged cycles to meet demand. UnlikeLrincreases, which yield stableE[TC]due to offsetting costs,Prenhancements deliver clearer cost reductions alongsideTrcontraction. Figure 8.Impact of Optimal Number of Conforming Items Inspected (Nc), Total Expected Cost (E[TC]) and Optimal Production Run Length (Tr) as Production Rate (Pr) Changes. Figure 9presents a sensitivity analysis of the expected total cost,E[TC], the optimal number of conforming items inspected (Nc) during the current production run length (Tr), in response to variations in the defect rate (Dr), adjusted by ¬±30% from the baseline value (Dr= 0.3). The analysis reveals that changes inDr‚Äîincreasing to 0.39 (+30%) or decreasing it to 0.21 (‚àí30%) results in only minimal fluctuations inE[TC]. This modest impact can be attributed to the relatively low baseline defect rate, where defect occurrences are already limited, thereby reducing the overall cost sensitivity to further adjustments. Figure 9.Impact of Optimal Number of Conforming Items Inspected (Nc) and Total Expected Cost (E[TC]) as Defect Rate (Dr) Changes. Although the defect rateDraffects the expected number of nonconforming items, its impact on the total expected cost remains relatively modest. This is becauseDrinfluences primarily the inspection-, correction-, and warranty-related components, while production, deterioration, and holding costs constitute a substantial share of the total cost and are not directly driven byDr. Moreover, the optimal sampling effort adjusts endogenously asDrchanges: higher defect rates reduce the required number of conforming samples for detection, whereas lower defect rates require increased sampling to maintain diagnostic confidence. These offsetting behavioral responses contribute to the relatively small variation inE[TC]observed inFigure 9. However, the effect onNcis more pronounced, particularly at higherDrlevels. WhenDrincreases to 0.39,Ncdecreases to approximately 17, compared to the baseline value of 18. This reduction occurs because, with a higher defect rate, the expected number of conforming items per sample diminishes under the same inspection effort. Consequently, fewer conforming items need to be inspected to determine whether the production system is in control or out of control, thereby optimizing resource allocation. These findings suggest that, for our electronic toy manufacturing process, adjustments inDrprimarily influence the inspection strategy rather than the total cost, providing valuable insights for adaptive quality control in varying defect scenarios. Additionally,Table 3provides detailed information related toFigure 5,Figure 6,Figure 7,Figure 8andFigure 9, which are used to investigate the impacts of the relevant factors. Table 3.Impact ofWp,Œ±,Œ≤,Lr,Pr, andDronNc,Tr, andE[TC].",
            "4.3. Managerial Insights": "This subsection synthesizes the analytical findings from the numerical experiments and articulates their broader managerial implications. No new numerical results are introduced; instead, we integrate the patterns observed in previous analyses to clarify the strategic role of coordinated decisions regarding inspection intensity and production-run length. The focus is on how these decisions systematically shift in response to operational conditions, offering qualitative guidance on whether firms should prioritize inspection, shorten production cycles, or invest in reliability improvements. The summarized insights are presented as follows: (1)Warranty Period (Wp) Changes in the warranty duration affect both the total expected cost and the optimal inspection plan. Moderate extensions of the warranty period encourage the firm to increase inspection intensity, as detecting defects earlier becomes more economically valuable. However, when the warranty period becomes substantially long, continuously increasing inspection intensity becomes less cost-effective. In such cases, the preferred strategy shifts toward structural actions, such as shortening the production run or improving the inherent reliability of the system, rather than indefinitely expanding sampling efforts. Insight: Warranty policies reshape not only the cost structure but also the intended role of inspection. Moderate warranties emphasize detection, whereas extended warranties prioritize prevention and robustness. (2)Degradation Characteristics (Œ±,Œ≤): When the production system deteriorates more gradually or follows a predictable pattern, the economic necessity of confirming the in-control state increases. A healthier system incentivizes higher sampling intensity because verifying quality provides greater marginal benefits. Conversely, when deterioration accelerates or becomes more erratic, simply increasing inspection frequency becomes less effective; the optimal strategy shifts toward more frequent production cycles or equipment improvements. Insight: Reliable systems warrant more rigorous verification, whereas fragile systems require structural modifications instead of expanded inspections. (3)Employee Learning Rate (Lr): Improvements in learning rates enhance productivity and reduce production-related cost components; however, this does not directly lead to proportional reductions in total expected costs. This is because increased productivity typically necessitates higher sampling intensity to ensure that the greater output maintains the same level of quality assurance. Insight: Learning enhances throughput and operational efficiency but yields limited cost savings unless accompanied by appropriate inspection adjustments. (4)Production Rate (Pr): Higher production capacity reduces per-unit operational costs and enhances the firm‚Äôs ability to implement shorter production cycles, thereby mitigating deterioration risks. With increased capacity, inspection intensity tends to rise because verifying a larger output becomes more valuable. Conversely, at lower production rates, longer production cycles may be necessary to meet demand, increasing exposure to deterioration and making inspection a secondary rather than a primary control mechanism. Insight: Capacity expansion enhances the effectiveness of short-term, high-verification strategies, while capacity constraints shift the focus toward longer-term operations and more cautious inspection. (5)Defect Rate (Dr): Variations in defect probability have a limited impact on the total expected cost when the baseline defect rate is already moderate. However, they significantly influence inspection strategies: a higher likelihood of defects reduces the number of conforming samples required to infer the system state, as poor-quality signals become more apparent. Insight: Even when costs are insensitive to defect rates, inspection policies must adapt to the information patterns in the data, not merely to cost outcomes. (6)Integrated Managerial Implications: Across all parameter variations, a consistent pattern emerges. Production run length and inspection intensity are interconnected strategic levers rather than independent decisions. Policy shifts (e.g., warranty length, capacity, maintenance investment) alter the role of inspection‚Äîsometimes serving as the primary tool for verification, and other times acting as a secondary measure to prevention or production adjustments. Zero-inflation modeling prevents the overestimation of deterioration risks, thereby avoiding overly conservative inspection plans that traditional NHPP models might generate. The convex nature of the cost landscape indicates a stable region of near-optimal solutions, facilitating robust decision-making even when parameter estimates are imperfect. Overall Insight: The model not only determines the optimal sampling level and production interval but also offers a prescriptive framework that explains when firms should respond through inspection, operational redesign, or reliability investments.",
            "5. Conclusions": "This study presents a comprehensive framework for integrating quality inspection and production run optimization in imperfect production systems, where deterioration is modeled using a zero-inflated non-homogeneous Poisson process (ZI-NHPP) with a power-law intensity function. By addressing the limitations of traditional models that overlook zero inflation in historical data‚Äîsuch as prolonged defect-free periods due to system robustness or advanced preventive measures‚Äîthe proposed approach employs the Expectation-Maximization (EM) algorithm to accurately estimate key parameters, including the zero-inflation probability and degradation rates. This integration not only enhances the modeling of heterogeneous deterioration but also jointly optimizes the production run length and the number of conforming items inspected, minimizing the expected total cost encompassing production, inspection, correction, inventory holding, and warranty expenses. The framework‚Äôs emphasis on Weibull hazard rates for product failures, differentiated between in-control and out-of-control states, further refines cost predictions by accounting for accelerated failure risks under degraded conditions. Ultimately, this work bridges critical gaps in the literature, offering a robust tool for cost-efficient quality management in dynamic manufacturing environments. In the basic model development, the problem is framed around a manufacturer facing challenges in balancing the rapid assembly of customized products with imperfections arising from system deterioration. The production system operates in an in-control state, yielding mostly conforming items with low defect rates, but gradually shifts to an out-of-control state over time, increasing defects such as cracks or malfunctions. This transition is captured by the ZI-NHPP, which incorporates a zero-inflation parameter to model scenarios. Key trade-offs are highlighted, including the time and cost of full inspections versus the risks of sampling errors leading to unwarranted corrections or warranty claims. The model integrates multiple cost components: fixed and variable production costs influenced by employee learning rates, inspection and correction expenses, inventory holding costs tied to mismatches between production rates and market demand, and warranty costs based on product hazard rates. Parameter estimation from historical data is achieved via the EM algorithm, which handles zero-inflated observations by iteratively maximizing the likelihood, ensuring reliable fits for degradation parameters. The optimization seeks to determine the ideal production run length, which heightens out-of-control probabilities and defect rates if extended, and the number of conforming items to inspect, striking a balance to minimize overall expenses while incorporating factors like employee efficiency and demand variability. The flowchart illustrates this process, from system inspection under Weibull power-law probabilities to product sales and warranty estimation, underscoring the interconnected nature of production, quality control, and post-sale services. The application and numerical analysis demonstrate the framework‚Äôs practical utility through a case study of a toy manufacturer producing electronic items. Historical production log data, characterized by significant zero inflation due to periods without defects, are processed using the EM algorithm. This approach yields parameter estimates that closely align with underlying patterns, confirming the method‚Äôs reliability in handling zero-inflated data. Numerical optimization of the expected total cost function reveals a convex surface with respect to production run length and the number of conforming items inspected. This enables the identification of minima that effectively reduce deterioration risks while ensuring sufficient sampling to detect issues early. The resulting configuration limits extended runs to prevent out-of-control shifts and adjusts inspection intensity to maintain quality without incurring excessive costs. Sensitivity studies further validate the model‚Äôs robustness by examining responses to variations in key parameters. For example, extending the warranty period increases total costs due to prolonged exposure to product failures under Weibull hazard conditions, particularly in out-of-control states. This leads to non-monotonic adjustments in the optimal number of inspected items‚Äîinitially increasing to enhance early defect detection but decreasing beyond certain thresholds as the focus shifts toward system-level prevention. Changes in degradation parameters, such as scale and shape, indicate that improved system reliability (characterized by slower onset or more predictable degradation patterns) reduces costs and requires more inspections to confirm sustained performance. Conversely, reductions in reliability increase vulnerability and redirect efforts toward alternative mitigation strategies. Variations in the employee learning rate result in minimal cost fluctuations due to offsetting effects between improved production efficiency and increased inspection demands; however, they highlight broader benefits such as higher throughput and potential revenue gains in high-demand segments. Adjustments in production rate demonstrate clearer cost reductions with increases, accompanied by shorter production runs and intensified inspections to manage accelerated output, whereas decreases prolong production cycles and elevate expenses. Finally, changes in the defect rate have modest cost impacts given low baseline levels but more pronounced effects on inspection numbers, with higher defect rates reducing the number of conforming samples per inspection effort and prompting corresponding strategic shifts. From a managerial perspective, these insights underscore the value of adopting advanced stochastic models like ZI-NHPP in imperfect production environments to avoid overestimating risks from unaccounted zero-inflation, leading to more precise resource allocation and reduced unnecessary interventions. Manufacturers can utilize the integrated framework to dynamically adjust production and inspection strategies in response to market demands and employee capabilities, thereby enhancing cost efficiency without compromising quality. For instance, investing in training to improve learning rates not only stabilizes costs but also supports scalability in customized production. Warranty policies should be carefully calibrated, as extensions can encourage adaptive inspections but may prioritize preventive robustness over sampling in extreme cases, balancing customer trust with financial exposure. The proposed model provides a planning-level framework for linking deterioration, inspection, and production run length decisions. However, several assumptions limit its scope. First, deterioration is modeled as a two-regime process; systems exhibiting progressive multi-stage wear may require a more granular reliability model. Second, the ZI-NHPP parameters are calibrated from historical failure data and may not remain stable across different operating conditions, requiring periodic re-estimation. Third, cost parameters such as rework, inspection, and warranty costs are assumed to be stationary and do not reflect dynamic contract structures or learning-based cost reduction. Finally, the optimization is static-cycle-based rather than real-time adaptive; integrating state-dependent decision-making or reinforcement learning approaches would be a natural extension. Future research could extend this model by incorporating Bayesian analysis into parameter estimation, allowing the framework to be applied even in scenarios with limited or no historical data. This approach leverages prior distributions and updates posterior beliefs based on sparse observations, thereby enhancing robustness in data-scarce environments. Additionally, integrating uncertain demand factors, such as stochastic or probabilistic demand models, would broaden the practical applicability of the study to volatile market conditions where demand fluctuations impact inventory and production planning. By addressing these avenues, the framework can evolve to meet emerging challenges, ultimately contributing to more resilient and efficient production systems."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2227-7390/13/24/3901",
        "scraped_at": "2025-12-05 23:51:17"
    },
    {
        "title": "AI-Powered Prompt Engineering for Education 4.0: Transforming Digital Resources into Engaging Learning Experiences",
        "authors": "byPaulo Serraand√Çngela Oliveira",
        "journal": "Educ. Sci.2025,15(12), 1640;https://doi.org/10.3390/educsci15121640- 5 Dec 2025",
        "abstract": "The integration of Artificial Intelligence into educational environments is reshaping the way digital resources support teaching and learning, which reinforces the need to understand how prompting strategies can enhance engagement, autonomy, and personalisation. This study examines the pedagogical role of prompt engineering in the transformation of static digital materials into adaptive and interactive learning experiences aligned with the principles of Education 4.0. A systematic literature review was conducted between 2023 and 2025 following the PRISMA protocol, comprising a sample of 166 studies retrieved from the ACM Digital Library and Scopus databases. The search strategy employed the keywords ‚Äúartificial intelligence‚Äù OR ‚Äúintelligent tutoring systems‚Äù AND ‚Äúe-learning‚Äù OR ‚Äúdigital education‚Äù AND ‚Äúpersonalised learning‚Äù OR ‚Äúacademic performance‚Äù OR ‚Äústudent engagement‚Äù OR ‚Äúmotivation‚Äù OR ‚Äúethical issues‚Äù OR ‚Äústudent autonomy‚Äù OR ‚Äúlimitations of AI‚Äù. The analysis identified consistent improvements in academic performance, motivation, and student engagement, although persistent limitations remain related to technical integration, ethical risks, and limited pedagogical alignment. Building on these findings, the article proposes a structured prompt engineering methodology that integrates interdependent components including role definition, audience specification, feedback style, contextual framing, guided reasoning, operational rules, and output format. A practical illustration shows that embedding prompts into digital learning resources, exemplified through PDF-based exercises, enables AI agents to support personalised and adaptive study sessions. The study concludes that systematic prompt design can reposition educational resources as intelligent, transparent, and pedagogically rigorous systems for knowledge construction.",
        "keywords": ":artificial intelligence; prompt engineering; Education 4.0; digital educational resources; large language models; personalised learning; educational recommender systemsartificial intelligence;prompt engineering;Education 4.0;digital educational resources;large language models;personalised learning;educational recommender systems",
        "full_content": {
            "Abstract": "The integration of Artificial Intelligence into educational environments is reshaping the way digital resources support teaching and learning, which reinforces the need to understand how prompting strategies can enhance engagement, autonomy, and personalisation. This study examines the pedagogical role of prompt engineering in the transformation of static digital materials into adaptive and interactive learning experiences aligned with the principles of Education 4.0. A systematic literature review was conducted between 2023 and 2025 following the PRISMA protocol, comprising a sample of 166 studies retrieved from the ACM Digital Library and Scopus databases. The search strategy employed the keywords ‚Äúartificial intelligence‚Äù OR ‚Äúintelligent tutoring systems‚Äù AND ‚Äúe-learning‚Äù OR ‚Äúdigital education‚Äù AND ‚Äúpersonalised learning‚Äù OR ‚Äúacademic performance‚Äù OR ‚Äústudent engagement‚Äù OR ‚Äúmotivation‚Äù OR ‚Äúethical issues‚Äù OR ‚Äústudent autonomy‚Äù OR ‚Äúlimitations of AI‚Äù. The analysis identified consistent improvements in academic performance, motivation, and student engagement, although persistent limitations remain related to technical integration, ethical risks, and limited pedagogical alignment. Building on these findings, the article proposes a structured prompt engineering methodology that integrates interdependent components including role definition, audience specification, feedback style, contextual framing, guided reasoning, operational rules, and output format. A practical illustration shows that embedding prompts into digital learning resources, exemplified through PDF-based exercises, enables AI agents to support personalised and adaptive study sessions. The study concludes that systematic prompt design can reposition educational resources as intelligent, transparent, and pedagogically rigorous systems for knowledge construction. Keywords:artificial intelligence; prompt engineering; Education 4.0; digital educational resources; large language models; personalised learning; educational recommender systemsartificial intelligence;prompt engineering;Education 4.0;digital educational resources;large language models;personalised learning;educational recommender systems",
            "Share and Cite": "MDPI and ACS StyleSerra, P.;                     Oliveira, √Ç.    \n        AI-Powered Prompt Engineering for Education 4.0: Transforming Digital Resources into Engaging Learning Experiences.Educ. Sci.2025,15, 1640.\n    https://doi.org/10.3390/educsci15121640AMA StyleSerra P,                                 Oliveira √Ç.        \n                AI-Powered Prompt Engineering for Education 4.0: Transforming Digital Resources into Engaging Learning Experiences.Education Sciences. 2025; 15(12):1640.\n        https://doi.org/10.3390/educsci15121640Chicago/Turabian StyleSerra, Paulo,                                 and √Çngela Oliveira.        \n                2025. \"AI-Powered Prompt Engineering for Education 4.0: Transforming Digital Resources into Engaging Learning Experiences\"Education Sciences15, no. 12: 1640.\n        https://doi.org/10.3390/educsci15121640APA StyleSerra, P.,                                 & Oliveira, √Ç.        \n        \n        (2025). AI-Powered Prompt Engineering for Education 4.0: Transforming Digital Resources into Engaging Learning Experiences.Education Sciences,15(12), 1640.\n        https://doi.org/10.3390/educsci15121640 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further detailshere.",
            "Article Metrics": "YesCitationsNo citations were found for this article, but you may check onGoogle ScholarNoArticle metric data becomes available approximately 24 hours after publication online.",
            "Citations": "No citations were found for this article, but you may check onGoogle Scholar"
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2227-7102/15/12/1640",
        "scraped_at": "2025-12-05 23:51:24"
    },
    {
        "title": "Visual Translator: Bridging Students‚Äô Handwritten Solutions and Automatic Diagnosis of Students‚Äô Use of Number Lines to Represent Fractions",
        "authors": "byDake Zhang,Zhizhi Wang,Min LiandYuhan Tao",
        "journal": "Educ. Sci.2025,15(12), 1638;https://doi.org/10.3390/educsci15121638- 5 Dec 2025",
        "abstract": "The latest AI advancements have provided opportunities for developing automated scoring and diagnosis systems that interpret and evaluate students‚Äô written solutions and assist teachers‚Äô grading and evaluation, yet computer vision still represents a technical challenge in detecting and describing the numerical values and spatial locations of key elements in students‚Äô hand-written solutions to mathematics tasks. This study reports the development and evaluation of an AI-based platform, called Visual Translator (VT), that automatically detects and describes the key visual information which is essential to the next step of auto-grading and diagnosis. The VT was trained with a private dataset of students‚Äô handwritten solution images. Human-experts annotated the key elements in students‚Äô solution images to build ground truth. We evaluated the VT performance by comparing the fraction value identification accuracy and location detection accuracy between VT and available LLMs against human expert annotations. Results suggested that VT surpassed GPT and Grok in fraction value identification, and also outperformed Geimini, the only LLM that supports image segmentation, in location detection. This model serves as the first step to reach the ultimate goal for classifying problem-solving strategies and error types in students‚Äô handwritten solutions. Implications for computer vision research, auto-grading and diagnosis in K12 mathematics education are discussed.",
        "keywords": ":auto-diagnosis; fractions; number linesauto-diagnosis;fractions;number lines",
        "full_content": {
            "Abstract": "The latest AI advancements have provided opportunities for developing automated scoring and diagnosis systems that interpret and evaluate students‚Äô written solutions and assist teachers‚Äô grading and evaluation, yet computer vision still represents a technical challenge in detecting and describing the numerical values and spatial locations of key elements in students‚Äô hand-written solutions to mathematics tasks. This study reports the development and evaluation of an AI-based platform, called Visual Translator (VT), that automatically detects and describes the key visual information which is essential to the next step of auto-grading and diagnosis. The VT was trained with a private dataset of students‚Äô handwritten solution images. Human-experts annotated the key elements in students‚Äô solution images to build ground truth. We evaluated the VT performance by comparing the fraction value identification accuracy and location detection accuracy between VT and available LLMs against human expert annotations. Results suggested that VT surpassed GPT and Grok in fraction value identification, and also outperformed Geimini, the only LLM that supports image segmentation, in location detection. This model serves as the first step to reach the ultimate goal for classifying problem-solving strategies and error types in students‚Äô handwritten solutions. Implications for computer vision research, auto-grading and diagnosis in K12 mathematics education are discussed. Keywords:auto-diagnosis; fractions; number linesauto-diagnosis;fractions;number lines",
            "Share and Cite": "MDPI and ACS StyleZhang, D.;                     Wang, Z.;                     Li, M.;                     Tao, Y.    \n        Visual Translator: Bridging Students‚Äô Handwritten Solutions and Automatic Diagnosis of Students‚Äô Use of Number Lines to Represent Fractions.Educ. Sci.2025,15, 1638.\n    https://doi.org/10.3390/educsci15121638AMA StyleZhang D,                                 Wang Z,                                 Li M,                                 Tao Y.        \n                Visual Translator: Bridging Students‚Äô Handwritten Solutions and Automatic Diagnosis of Students‚Äô Use of Number Lines to Represent Fractions.Education Sciences. 2025; 15(12):1638.\n        https://doi.org/10.3390/educsci15121638Chicago/Turabian StyleZhang, Dake,                                 Zhizhi Wang,                                 Min Li,                                 and Yuhan Tao.        \n                2025. \"Visual Translator: Bridging Students‚Äô Handwritten Solutions and Automatic Diagnosis of Students‚Äô Use of Number Lines to Represent Fractions\"Education Sciences15, no. 12: 1638.\n        https://doi.org/10.3390/educsci15121638APA StyleZhang, D.,                                 Wang, Z.,                                 Li, M.,                                 & Tao, Y.        \n        \n        (2025). Visual Translator: Bridging Students‚Äô Handwritten Solutions and Automatic Diagnosis of Students‚Äô Use of Number Lines to Represent Fractions.Education Sciences,15(12), 1638.\n        https://doi.org/10.3390/educsci15121638 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further detailshere.",
            "Article Metrics": "YesCitationsNo citations were found for this article, but you may check onGoogle ScholarNoArticle metric data becomes available approximately 24 hours after publication online.",
            "Citations": "No citations were found for this article, but you may check onGoogle Scholar"
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2227-7102/15/12/1638",
        "scraped_at": "2025-12-05 23:51:31"
    },
    {
        "title": "RBF-Based Meshless Collocation Method for Time-Fractional Interface Problems with Highly Discontinuous Coefficients",
        "authors": "byFaisal Bilal,Muhammad Asif,Mehnaz ShakeelandIoan-Lucian Popa",
        "journal": "Math. Comput. Appl.2025,30(6), 133;https://doi.org/10.3390/mca30060133- 5 Dec 2025",
        "abstract": "Time-fractional interface problems arise in systems where interacting materials exhibit memory effects or anomalous diffusion. These models provide a more realistic description of physical processes than classical formulations and appear in heat conduction, fluid flow, porous media diffusion, and electromagnetic wave propagation. However, the presence of complex interfaces and the nonlocal nature of fractional derivatives makes their numerical treatment challenging. This article presents a numerical scheme that combines radial basis functions (RBFs) with the finite difference method (FDM) to solve time-fractional partial differential equations involving interfaces. The proposed approach applies to both linear and nonlinear models with constant or variable coefficients. Spatial derivatives are approximated using RBFs, while the Caputo definition is employed for the time-fractional term. First-order time derivatives are discretized using the FDM. Linear systems are solved via Gaussian elimination, and for nonlinear problems, two linearization strategies, a quasi-Newton method and a splitting technique, are implemented to improve efficiency and accuracy. The method‚Äôs performance is assessed using maximum absolute and root mean square errors across various grid resolutions. Numerical experiments demonstrate that the scheme effectively resolves sharp gradients and discontinuities while maintaining stability. Overall, the results confirm the robustness, accuracy, and broad applicability of the proposed technique.Keywords:fractional interface problems;Caputo derivative;radial basis functions;finite difference method;qausi-Newton linearization;innovative splitting technique",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Many physical phenomena occupy domains that are divided into subdomains, each with its own mathematical or physical properties. This leads to the appearance of interfaces, which exist in situations such as fluid flow through heterogeneous materials or structures designed from multiple layers. Propagation of waves through composite materials (a hyperbolic problem) and heat conduction in composite materials (a parabolic problem) are examples related to such interface phenomena. Traditional numerical techniques frequently have difficulty handling interface problems effectively because analytical solutions are rarely available. The main reason is that solutions near interfaces can be discontinuous or not sufficiently smooth, which reduces the accuracy and stability of traditional approaches like the FDM and the finite element method (FEM) [1]. In recent years, researchers in the mathematical and scientific communities have worked intensively to develop improved techniques for solving interface problems. These problems arise across a wide range of fields, including environmental science, biology, materials science, and electromagnetic wave theory [2,3]. Consequently, many numerical methods have been proposed. For example, Babu?ka applied the FDM to solve elliptic interface equations [4]. Aziz et al.  developed the Haar wavelet collocation method (HWCM) and meshfree approaches for elliptic and parabolic interface problems [5,6]. Asif and collaborators first applied the HWCM to hyperbolic interface problems and later extended the method to the telegraph equation with interfaces [7,8]. Similarly, Rana and co-authors used the HWCM to solve advection‚Äìdiffusion‚Äìreaction problems involving parabolic and elliptic equations [9]. Ahmad and Islam adopted localized meshless schemes to solve Stokes flow and elliptic interface models [10,11]. Asif and Bilal applied the HWCM to the telegraph equation with discontinuous coefficients [12]. Fractional calculus provides a powerful framework for modeling memory-dependent and anomalous transport phenomena. Over the past decades, fractional differential equations (FDEs) have proven highly effective in describing complex physical processes, including viscoelasticity, rheology, biological systems, dielectric polarization, and turbulent flows [13,14,15,16]. Their ability to capture nonlocal behavior and long-range temporal effects makes fractional operators particularly suitable for problems where classical integer-order models fail to represent the underlying dynamics accurately. As applications expanded, researchers developed numerous numerical methods for solving FDEs. For example, Zada et al. used the homotopy wavelet collocation method for fractional partial differential equations (FPDEs) [17]. Mehnaz et al.  developed a local meshfree method for space-dependent FPDEs [18]. Manzoor and Siraj used a meshless spectral technique to treat the time-fractional Korteweg‚Äìde Vries (KdV) equation [19]. Haifa and Carlo employed Chebyshev cardinal functions to approximate time-fractional FPDEs [20]. Rehma and Banan proposed a flexible numerical method for fractional elliptic interface models [21], and Wang et al. applied a space‚Äìtime FEM to time-fractional diffusion problems with interface conditions [22]. In this work, we focus on one-dimensional time-fractional interface problems where solutions may be discontinuous across interfaces. To solve these problems efficiently and accurately, we employ radial basis functions (RBFs) for spatial discretization in combination with the Caputo derivative for the temporal fractional operator. RBFs offer a meshfree framework and flexible approximation of spatial derivatives, which is particularly advantageous for handling irregular geometries and discontinuities at interfaces [23]. They have been successfully applied in diverse areas, including machine learning, fluid dynamics, biology, image processing, geostatistics, computer graphics, and finance [24]. Comprehensive treatments of RBF theory and practical implementation in MATLAB 7 are provided by Fasshauer [25] and extended by Fasshauer and McCourt [26], while theoretical studies by Franke and Schaback further established convergence and uniqueness properties [27]. In this work, we have focused on one-dimensional (1D) linear and nonlinear time-fractional interface problems, where the interface is fixed. The problem is represented in the following form:ùê∑ùúåùë°ùúë(ùë•,ùë°)=‚àá¬∑(ùúÇ‚àáùúë)+Œ¶,0<ùúå<1,ùë°‚â•ùë°0,inŒ©,DtœÅœÜ(x,t)=‚àá¬∑(Œ∑‚àáœÜ)+Œ¶,0<œÅ<1,t‚â•t0,inŒ©,(1)andùê∑ùúåùë°ùúë(ùë•,ùë°)=‚àá¬∑(ùúÇ‚àáùúë)+‚Ñ±(ùë•,ùë°,ùúë(ùë•,ùë°))+Œ¶,0<ùúå<1ùë°‚â•ùë°0,inŒ©.DtœÅœÜ(x,t)=‚àá¬∑(Œ∑‚àáœÜ)+F(x,t,œÜ(x,t))+Œ¶,0<œÅ<1t‚â•t0,inŒ©.(2) Letùúë(ùë•,ùë°)œÜ(x,t)be an unknown function, where‚Ñ±(ùë•,ùë°,ùúë1(ùë•,ùë°))F(x,t,œÜ1(x,t))represents a nonlinear term. The functionŒ¶(ùë•,ùë°)Œ¶(x,t)acts as a source term, andùúÇ(ùë•)Œ∑(x)is a smoothly varying coefficient. This formulation is considered forùë°‚â•ùë°0t‚â•t0, whereùë°0t0denotes the initial time, andùë•‚ààŒ©x‚ààŒ©, withŒ©‚äÇ‚ÑùŒ©‚äÇRfor one-dimensional problems. In this 1D case, the gradient operator reduces to‚àá=(‚àÇ‚àÇùë•)‚àá=‚àÇ‚àÇx. The domainŒ©Œ©is divided into two subdomains by a single interface located atùúÉŒ∏. Consequently, the functionsùúëœÜ,a, andŒ¶Œ¶are defined separately in each subdomain to reflect this partitioning.(ùúë,ùëé,Œ¶)={(ùúë‚àí,ùëé‚àí,Œ¶‚àí)(ùúë+,ùëé+,Œ¶+)inŒ©‚àí,inŒ©+.(œÜ,a,Œ¶)=(œÜ‚àí,a‚àí,Œ¶‚àí)inŒ©‚àí,(œÜ+,a+,Œ¶+)inŒ©+.(3)The boundary and initial conditions areùúë(ùë•,ùë°)ùúë(ùë•,0)ùúëùë°(ùë•,0)=ùúí,(ùë•,ùë°)‚àà‚àÇŒ©√ó(0,ùëáùëì],=ùúõ1,ùê±‚ààŒ©,=ùúõ2,ùê±‚ààŒ©.œÜ(x,t)=œá,(x,t)‚àà‚àÇŒ©√ó(0,Tf],œÜ(x,0)=œñ1,x‚ààŒ©,œÜt(x,0)=œñ2,x‚ààŒ©.(4)The subsequent interface conditions are imposed at the single interfaceùë•=ùúÉx=Œ∏:[ùúë]ùúÉ=ùúë1(ùúÉ,ùë°)‚àíùúë2(ùúÉ,ùë°)=ùúî(ùúÉ,ùë°),œÜŒ∏=œÜ1(Œ∏,t)‚àíœÜ2(Œ∏,t)=œâ(Œ∏,t),(5)[ùúÇùúëùë•]ùúÉ=ùúÇ+(ùúÉ,ùë°)ùúë1ùë•(ùúÉ,ùë°)‚àíùúÇ‚àí(ùúÉ,ùë°)ùúë2ùë•(ùúÉ,ùë°)=ùúà(ùúÉ,ùë°).Œ∑œÜxŒ∏=Œ∑+(Œ∏,t)œÜ1x(Œ∏,t)‚àíŒ∑‚àí(Œ∏,t)œÜ2x(Œ∏,t)=ŒΩ(Œ∏,t).(6) To simplify the analysis, this paper considers the spatial domainŒ©Œ©as the interval[0,1][0,1]for one-dimensional problems. Without loss of generality, the interface is placed at the pointùë•=ùúÉx=Œ∏, dividing the domain into two subdomains:Œ©‚àí=[0,ùúÉ]Œ©‚àí=[0,Œ∏]andŒ©+=[ùúÉ,1]Œ©+=[Œ∏,1]. To carry out the following analysis, we first introduce some essential definitions and notation related to fractional derivatives. Definition1.The Caputo fractional derivative of orderùúå>0œÅ>0for a functionùúë(ùë°)œÜ(t), which is absolutely continuous on the interval[0,ùëá][0,T], is defined asùê∑ùë°ùúåùê∂ùúë(ùë°)=1Œì(ùëö‚àíùúå)‚à´ùë°0ùúë(ùëö)(ùúÅ)(ùë°‚àíùúÅ)ùúå‚àíùëö+1ùëëùúÅ,DtœÅCœÜ(t)=1Œì(m‚àíœÅ)‚à´0tœÜ(m)(Œ∂)(t‚àíŒ∂)œÅ‚àím+1dŒ∂,(7)whereùëö=‚åàùúå‚åâm=‚åàœÅ‚åâis the smallest integer greater than or equal to œÅ, andŒì(¬∑)Œì(¬∑)denotes the Gamma function. This definition extends the concept of a derivative to non-integer (fractional) orders. This paper is organized like this: InSection 2, we look at radial basis functions. InSection 3, we provide the temporal approximation employing the Caputo derivative for time-fractional interface problems. InSection 4, we talk about spatial discretization. InSection 5, we will discuss stability analysis.Section 6is for numerical validation. Finally,Section 7gives some final thoughts and suggests some possible areas for future investigation.",
            "2. Radial Basis Functions": "If there is a single-valued functionùúì:[0,‚àû)‚Üí‚Ñùœà:[0,‚àû)‚ÜíRsuch thatùúô(ùê±)=ùúì(‚à•ùê±‚à•)œï(x)=œà(‚à•x‚à•), then a functionùúô:‚Ñùùëë‚Üí‚Ñùœï:Rd‚ÜíRis termed radial. Stated differently,ùúô(ùê±)œï(x)is solely dependent on the magnitude ofùê±xand not on its direction. An RBF is the functionùúì(ùëü)œà(r), whereùëü=‚à•ùê±‚à•r=‚à•x‚à•. It is defined with respect to the Euclidean norm forùëü‚â•0r‚â•0. A positive parameter, known as the shape parameter, is a component of every RBF. The choice of the shape parameter (shown here asùúñœµ), which has a significant impact on the accuracy of the approximation and the conditioning of the final system, is a crucial component of any RBF-based numerical technique. Higher values ofùúñœµcreate more peaked bases that are better conditioned but may not be as accurate. On the other hand, lower values create flatter basis functions that may be more accurate but may also lead to ill-conditioned matrices. For this reason, selecting the right form parameter is essential to striking a balance between precision and stability. Numerous research studies have addressed this problem. Cavoretto et al. [28] suggested employing univariate global optimization in conjunction with leave-one-out cross-validation to determine an idealùúñœµ, showing better results than conventional ad hoc selections. More recently, Cavoretto et al. [29] presented a Bayesian optimization strategy for choosing the form parameter in RBF methods, emphasizing its influence on robustness and accuracy. Based on these findings,ùúñœµis selected in the current work to guarantee a dependable and effective approximation. In this work, we employ the multiquadric radial basis function (MQ-RBF) because extensive analytical and numerical studies have shown that MQ offers higher approximation accuracy, smoother derivative reproduction, and better stability for PDE collocation compared with Gaussian, inverse multiquadric (IMQ), or thin-plate spline (TPS) kernels. MQ-RBF is one of the most popular RBFs and was first used in the interpolation method by Hardy [30]. Interpolation by using polynomials and trigonometric functions is popular but deficient in some aspects. The MQ interpolation scheme was unnoticed until 1979, but the method received much attention when Richard Franked concluded that the MQ RBF interpolation is the best among various interpolation methods to solve the scattered data interpolation problem [31]. The popularity of the method increased further when Edward Kansa [32] solved PDEs through the MQ-RBF method. An advantage of MQ-RBF is its low computational cost and high accuracy even for small shape parameters, making it especially effective for stable derivative evaluation in interface problems. The following is the mathematical form of MQ-RBFs:ùúô(ùëü)=1+(ùúñùëü)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö,œï(r)=1+(œµr)2,where r =||ùë•‚àíùë•ùëñ||||x‚àíxi||, andxis the center point [33]. RBFs are typically used for the approximation defined on scattered data of the formùúë(ùë•,ùë°)=‚àëùëñ=1ùí©ùúÜùëñùúô(||ùë•‚àíùë•ùëñ||),œÜ(x,t)=‚àëi=1NŒªiœï(||x‚àíxi||),(8)whereùúë(ùë•,ùë°)œÜ(x,t)is an approximated function whose value depends upon the sum ofùí©Nradial basis functions. Each basis function depends upon different weightsùúÜùëñŒªiand centersùë•ùëñxi. The weight can be calculated by simplifying the linear equations. Letùúëùëñ=ùúë(ùë•ùëñ)œÜi=œÜ(xi); the weightùúÜùëñŒªican be solved by‚é°‚é£‚é¢‚é¢‚é¢ùúô1,1‚ãÆùúôùêπ,1‚Ä¶‚ãÆ‚Ä¶ùúô1,ùêπ‚ãÆùúôùêπ,ùêπ‚é§‚é¶‚é•‚é•‚é•‚é°‚é£‚é¢‚é¢ùõº1‚ãÆùõºùëí‚é§‚é¶‚é•‚é•=‚é°‚é£‚é¢‚é¢‚é¢ùõΩ1‚ãÆùõΩùëö‚é§‚é¶‚é•‚é•‚é•œï1,1‚Ä¶œï1,F‚ãÆ‚ãÆ‚ãÆœïF,1‚Ä¶œïF,FŒ±1‚ãÆŒ±e=Œ≤1‚ãÆŒ≤m(9) Advantages and Disadvantages of the Meshless Collocation Method (MCM)The MCM with RBFs has been extensively applied to compute numerical solutions for a broad spectrum of ODEs and PDEs. This method offers several significant advantages:The method is efficient, capable of delivering accurate results using only a limited number of nodes.The method is well-suited for PDE problems involving complicated and non-uniform domains.The method maintains manageable computational requirements as the dimensionality increases.There is no need to define nodal or element connectivity.Unlike many other numerical techniques, this method does not require numerical integration.However, like any numerical method, the MCM also has some drawbacks. One limitation of applying the MCM with RBFs is the tendency for the discretized system matrices to exhibit ill-conditioning. To address this issue, regularization techniques such as truncated singular value decomposition are often employed to improve the stability of the solution.",
            "Advantages and Disadvantages of the Meshless Collocation Method (MCM)": "The MCM with RBFs has been extensively applied to compute numerical solutions for a broad spectrum of ODEs and PDEs. This method offers several significant advantages: The method is efficient, capable of delivering accurate results using only a limited number of nodes.The method is well-suited for PDE problems involving complicated and non-uniform domains.The method maintains manageable computational requirements as the dimensionality increases.There is no need to define nodal or element connectivity.Unlike many other numerical techniques, this method does not require numerical integration. However, like any numerical method, the MCM also has some drawbacks. One limitation of applying the MCM with RBFs is the tendency for the discretized system matrices to exhibit ill-conditioning. To address this issue, regularization techniques such as truncated singular value decomposition are often employed to improve the stability of the solution.",
            "3. Temporal Approximation Using Caputo Derivative": "This section aims to derive the Caputo fractional derivative forùúå‚àà(0,1)œÅ‚àà(0,1), which is defined by‚àÇùúåùúë‚àÇùë°ùúå=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉ1Œì(1‚àíùúå)‚à´0ùë°ùúë‚Ä≤(ùë°)(ùë°‚àíùúÅ)‚àíùúåùëëùúÅ,‚àÇùúë‚àÇùë°,0<ùúå<1.ùúå=1.‚àÇœÅœÜ‚àÇtœÅ=1Œì1‚àíœÅ‚à´0tœÜ‚Ä≤(t)t‚àíŒ∂‚àíœÅdŒ∂,0<œÅ<1.‚àÇœÜ‚àÇt,œÅ=1.Differentiating atùë°=ùë°ùëö+1t=tm+1, we obtain‚àÇùúåùúë(ùúë,ùë°ùëö+1)‚àÇùë°ùúå=1Œì(1‚àíùúå)‚à´0ùë°ùëö+1ùúë‚Ä≤(ùúÅ)(ùë°ùëö+1‚àíùúÅ)‚àíùúåùëëùúÅ,‚àÇœÅœÜ(œÜ,tm+1)‚àÇtœÅ=1Œì(1‚àíœÅ)‚à´0tm+1œÜ‚Ä≤(Œ∂)tm+1‚àíŒ∂‚àíœÅdŒ∂,‚áí‚àÇùúåùúë)‚àÇùë°ùúå=1Œì(1‚àíùúå)‚àëùëò=0ùëö‚à´ùë°ùëòùë°ùëò+1ùúë‚Ä≤(ùë°)(ùë°ùëö+1‚àíùúÅ)‚àíùúåùëëùúÅ‚áí‚àÇœÅœÜ)‚àÇtœÅ=1Œì(1‚àíœÅ)‚àëk=0m‚à´tktk+1œÜ‚Ä≤(t)tm+1‚àíŒ∂‚àíœÅdŒ∂Now, we apply the FDM to approximate‚àÇùëö+1ùúë‚àÇùë°ùëö+1‚àÇm+1œÜ‚àÇtm+1, as follows:‚àÇùúë(ùúë,ùúÅùëò+1)‚àÇùúÅ=ùúëùëò+1‚àíùúëùëòùëëùë°+ùëú(ùëëùë°).‚àÇœÜ(œÜ,Œ∂k+1)‚àÇŒ∂=œÜk+1‚àíœÜkdt+o(dt).Then‚àÇùúåùúë(ùúë,ùë°ùëö+1)‚àÇùë°ùúå‚âà1Œì(1‚àíùúå)‚àëùëò=0ùëö‚à´ùë°ùëòùë°ùëò+1ùúëùëò+1‚àíùúëùëòùëëùë°(ùë°ùëö+1‚àíùúÅ)‚àíùúåùëëùúÅ,‚àÇœÅœÜ(œÜ,tm+1)‚àÇtœÅ‚âà1Œì(1‚àíœÅ)‚àëk=0m‚à´tktk+1œÜk+1‚àíœÜkdttm+1‚àíŒ∂‚àíœÅdŒ∂,‚àÇùúåùúë‚àÇùë°ùúå=ùëé0‚àëùëò=0ùëöùëèùëò(ùë¢ùëö‚àíùëò+1‚àíùúëùëö‚àíùëò),‚àÇœÅœÜ‚àÇtœÅ=a0‚àëk=0mbkum‚àík+1‚àíœÜm‚àík,whereùëèùëò=(ùëò+1)1‚àíùúå‚àíùëò1‚àíùúåbk=(k+1)1‚àíœÅ‚àík1‚àíœÅ,ùëò=0,1,‚Ä¶,ùëök=0,1,‚Ä¶,mandùëé0=ùëëùë°‚àíùúåŒì(2‚àíùúå)a0=dt‚àíœÅŒì(2‚àíœÅ). Finally, we can express it in a more refined form as follows:‚àÇùúåùúë‚àÇùë°ùúå=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùëé0(ùúëùëö+1‚àíùúëùëö)+ùëé0‚àëùëò=1ùëöùëèùëò(ùúëùëö‚àíùëò+1‚àíùúëùëö‚àíùëò),ùëé0(ùúë1‚àíùúë0),ùëõ‚â•1,ùëö=0.‚àÇœÅœÜ‚àÇtœÅ=a0œÜm+1‚àíœÜm+a0‚àëk=1mbkœÜm‚àík+1‚àíœÜm‚àík,n‚â•1,a0œÜ1‚àíœÜ0,m=0.(10)",
            "4. Meshless Collocation Approach for Single Interface Model": "The interval[0,1][0,1]is split into two subintervals:Œ©+=[0,ùúÉ]Œ©+=[0,Œ∏]andŒ©‚àí=[ùúÉ,1]Œ©‚àí=[Œ∏,1]. The first subintervalŒ©+Œ©+is discretized using nodes such that0=ùë•1<ùë•2<‚ãØ<ùë•‚Ñ≥1+1=ùõº0=x1<x2<‚ãØ<xM1+1=Œ±. Similarly, the second subintervalŒ©‚àíŒ©‚àíis discretized asùë•‚Ñ≥1+1=ùõº<ùë•‚Ñ≥1+2<‚ãØ<ùë•ùí©=1xM1+1=Œ±<xM1+2<‚ãØ<xN=1, whereùí©=‚Ñ≥1+‚Ñ≥2+1N=M1+M2+1represents the total number of nodes, and‚Ñ≥1M1and‚Ñ≥2M2denote the number of divisions inŒ©+Œ©+andŒ©‚àíŒ©‚àí, respectively. The estimation of the functionùúë1(ùë•,ùë°)œÜ1(x,t)on the intervalŒ©+Œ©+using the multiquadric MQ-RBF is expressed as follows. On the intervalùêº=[0,ùõº)I=[0,Œ±), we approximateùúë1(ùë•,ùë°)œÜ1(x,t)by employing MQ-RBFs in the following manner:ùúë1(ùë•,ùë°)=‚àëùëñ=1‚Ñ≥+1ùúÜùëñùúô(‚à•ùë•‚àíùë•ùëñ‚à•),ùë•‚ààŒ©+,œÜ1(x,t)=‚àëi=1M+1Œªiœï(‚à•x‚àíxi‚à•),x‚ààŒ©+,(11)whereŒ©+=[0,ùúÉ)Œ©+=[0,Œ∏), andùúôœïis the MQ radial basis function. Differentiating Equation (11) with respect tox, we obtainùúë1ùë•(ùë•,ùë°)=‚àëùëñ=1‚Ñ≥+1ùúÜùëñùëëùúôùëëùë•(‚à•ùë•‚àíùë•ùëñ‚à•),ùë•‚ààŒ©+,œÜ1x(x,t)=‚àëi=1M+1Œªidœïdx(‚à•x‚àíxi‚à•),x‚ààŒ©+,(12)and the second derivative is given byùúë1ùë•ùë•(ùë•,ùë°)=‚àëùëñ=1‚Ñ≥+1ùúÜùëñùëë2ùúôùëëùë•2(‚à•ùë•‚àíùë•ùëñ‚à•),ùë•‚ààŒ©+.œÜ1xx(x,t)=‚àëi=1M+1Œªid2œïdx2(‚à•x‚àíxi‚à•),x‚ààŒ©+.(13) Similarly, the unknown functionùúë2(ùë•,ùë°)œÜ2(x,t)is approximated over the intervalŒ©‚àí=[ùúÉ,1]Œ©‚àí=[Œ∏,1]asùúë2(ùë•,ùë°)=‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùúô(‚à•ùë•‚àíùë•ùëñ‚à•),ùë•‚ààŒ©‚àí.œÜ2(x,t)=‚àëi=M+1NŒ≥iœï(‚à•x‚àíxi‚à•),x‚ààŒ©‚àí.(14)Differentiating Equation (14) with respect toxyieldsùúë2ùë•(ùë•,ùë°)=‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùëëùúôùëëùë•(‚à•ùë•‚àíùë•ùëñ‚à•),ùë•‚ààŒ©‚àí,œÜ2x(x,t)=‚àëi=M+1NŒ≥idœïdx(‚à•x‚àíxi‚à•),x‚ààŒ©‚àí,(15)and the second derivative becomesùúë2ùë•ùë•(ùë•,ùë°)=‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùëë2ùúôùëëùë•2(‚à•ùë•‚àíùë•ùëñ‚à•),ùë•‚ààŒ©‚àí.œÜ2xx(x,t)=‚àëi=M+1NŒ≥id2œïdx2(‚à•x‚àíxi‚à•),x‚ààŒ©‚àí.(16)Approximating the boundary conditions atùë•=0x=0andùë•=1x=1using Equations (11) and (14), we obtain‚àëùëñ=1‚Ñ≥+1ùúÜùëñùúô(‚à•0‚àíùë•ùëñ‚à•)=Œò1(ùë°),‚àëi=1M+1Œªiœï(‚à•0‚àíxi‚à•)=Œò1(t),(17)‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùúô(‚à•1‚àíùë•ùëñ‚à•)=Œò2(ùë°),‚àëi=M+1NŒ≥iœï(‚à•1‚àíxi‚à•)=Œò2(t),(18)whereŒò1(ùë°)=ùúë(0,ùë°)Œò1(t)=œÜ(0,t)andŒò2(ùë°)=ùúë(1,ùë°)Œò2(t)=œÜ(1,t). The meshless approximations for the interface conditions are given by‚àëùëñ=1‚Ñ≥+1ùúÜùëñùúô(‚à•ùë•‚àíùë•ùëñ‚à•)‚àí‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùúô(‚à•ùë•‚àíùë•ùëñ‚à•)=ùëû1(ùë°),‚àëi=1M+1Œªiœï(‚à•x‚àíxi‚à•)‚àí‚àëi=M+1NŒ≥iœï(‚à•x‚àíxi‚à•)=q1(t),(19)ùúÇ+‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=1‚Ñ≥+1ùúÜùëñùëëùúôùëëùë•(‚à•ùë•‚àíùë•ùëñ‚à•)‚éû‚é†‚éü‚éü‚éü‚àíùúÇ‚àí‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùëëùúôùëëùë•(‚à•ùë•‚àíùë•ùëñ‚à•)‚éû‚é†‚éü‚éü‚éü=ùëû2(ùë°).Œ∑+‚àëi=1M+1Œªidœïdx(‚à•x‚àíxi‚à•)‚àíŒ∑‚àí‚àëi=M+1NŒ≥idœïdx(‚à•x‚àíxi‚à•)=q2(t).(20)We will solve the above system for both linear and nonlinear cases separately. 4.1. Linear CaseFor the linear time-fractional interface problems described in Equation (1), we obtainùëé0‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=1‚Ñ≥+1ùúÜùëñùúô(‚à•ùë•‚àíùë•ùëñ‚à•)‚àíùúë1(ùë•,ùë°0)+‚àëùëò=0ùëöùëèùëò(ùúëùëö‚àíùëò+1‚àíùúëùëö‚àíùëò)‚éû‚é†‚éü‚éü‚éü=ùúÇ+ùë•(ùë•)‚àëùëñ=1‚Ñ≥+1ùúÜùëñùëëùúôùëëùë•(‚à•ùë•‚àíùë•ùëñ‚à•)+ùúÇ+(ùë•)‚àëùëñ=1‚Ñ≥+1ùúÜùëñùëë2ùúôùëëùë•2(‚à•ùë•‚àíùë•ùëñ‚à•)+ùëì1(ùë•,ùë°),ùë•‚ààŒ©+.a0‚àëi=1M+1Œªiœï(‚à•x‚àíxi‚à•)‚àíœÜ1(x,t0)+‚àëk=0mbk(œÜm‚àík+1‚àíœÜm‚àík)=Œ∑x+(x)‚àëi=1M+1Œªidœïdx(‚à•x‚àíxi‚à•)+Œ∑+(x)‚àëi=1M+1Œªid2œïdx2(‚à•x‚àíxi‚à•)+f1(x,t),x‚ààŒ©+.(21)andùëé0‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùúô(‚à•ùë•‚àíùë•ùëñ‚à•)‚àíùúë2(ùë•,ùë°0)+‚àëùëò=0ùëöùëèùëò(ùúëùëö‚àíùëò+1‚àíùúëùëö‚àíùëò)‚éû‚é†‚éü‚éü‚éü=ùúÇ‚àíùë•(ùë•)‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùëëùúôùëëùë•(‚à•ùë•‚àíùë•ùëñ‚à•)+ùúÇ‚àí(ùë•)‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùëë2ùúôùëëùë•2(‚à•ùë•‚àíùë•ùëñ‚à•)+ùëì2(ùë•,ùë°),ùë•‚ààŒ©‚àí.a0‚àëi=M+1NŒ≥iœï(‚à•x‚àíxi‚à•)‚àíœÜ2(x,t0)+‚àëk=0mbk(œÜm‚àík+1‚àíœÜm‚àík)=Œ∑x‚àí(x)‚àëi=M+1NŒ≥idœïdx(‚à•x‚àíxi‚à•)+Œ∑‚àí(x)‚àëi=M+1NŒ≥id2œïdx2(‚à•x‚àíxi‚à•)+f2(x,t),x‚ààŒ©‚àí.(22)Now, discretizing the following equation, we haveùëé0‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=1‚Ñ≥+1ùúÜùëñùúô(‚à•ùë•ùëó‚àíùë•ùëñ‚à•)‚éû‚é†‚éü‚éü‚éü‚àíùúÇ+ùë•(ùë•)‚àëùëñ=1‚Ñ≥+1ùúÜùëñùëëùúôùëëùë•(‚à•ùë•ùëó‚àíùë•ùëñ‚à•)‚àíùúÇ+(ùë•)‚àëùëñ=1‚Ñ≥+1ùúÜùëñùëë2ùúôùëëùë•2(‚à•ùë•ùëó‚àíùë•ùëñ‚à•)=ùúë1(ùë•ùëó,ùë°0)‚àíùëé0‚àëùëò=0ùëöùëèùëò(ùúëùëö‚àíùëò+1‚àíùúëùëö‚àíùëò)+ùëì1(ùë•,ùë°),ùë•‚ààŒ©+.a0‚àëi=1M+1Œªiœï(‚à•xj‚àíxi‚à•)‚àíŒ∑x+(x)‚àëi=1M+1Œªidœïdx(‚à•xj‚àíxi‚à•)‚àíŒ∑+(x)‚àëi=1M+1Œªid2œïdx2(‚à•xj‚àíxi‚à•)=œÜ1(xj,t0)‚àía0‚àëk=0mbk(œÜm‚àík+1‚àíœÜm‚àík)+f1(x,t),x‚ààŒ©+.(23)andùëé0‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùúô(‚à•ùë•ùëó‚àíùë•ùëñ‚à•)‚éû‚é†‚éü‚éü‚éü‚àíùúÇ‚àíùë•(ùë•)‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùëëùúôùëëùë•(‚à•ùë•ùëó‚àíùë•ùëñ‚à•)‚àíùúÇ‚àí(ùë•)‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùëë2ùúôùëëùë•2(‚à•ùë•ùëó‚àíùë•ùëñ‚à•)=ùúë2(ùë•ùëó,ùë°0)‚àí‚àëùëò=0ùëöùëèùëò(ùúëùëö‚àíùëò+1‚àíùúëùëö‚àíùëò)+ùëì2(ùë•,ùë°),ùë•‚ààŒ©‚àí.a0‚àëi=M+1NŒ≥iœï(‚à•xj‚àíxi‚à•)‚àíŒ∑x‚àí(x)‚àëi=M+1NŒ≥idœïdx(‚à•xj‚àíxi‚à•)‚àíŒ∑‚àí(x)‚àëi=M+1NŒ≥id2œïdx2(‚à•xj‚àíxi‚à•)=œÜ2(xj,t0)‚àí‚àëk=0mbk(œÜm‚àík+1‚àíœÜm‚àík)+f2(x,t),x‚ààŒ©‚àí.(24)A linear system consisting ofùí©+1N+1unknowns is formulated using Equations (19), (20), (23), and (24). In this system, the functionsùúÜùëñ(ùúè)Œªi(œÑ)forùëñ=1,2,‚Ä¶,‚Ñ≥+1i=1,2,‚Ä¶,M+1andùõæùëñ(ùúè)Œ≥i(œÑ)forùëñ=‚Ñ≥+1,‚Ñ≥+2,‚Ä¶,ùí©i=M+1,M+2,‚Ä¶,Nappear as key components in the expression. 4.2. Nonlinear Case4.2.1. Quasi-Newton Linearization TechniqueThe first step in addressing the nonlinear case involves applying the quasi-Newton linearization formula to Equation (2).(ùúàùúá)ùëö+1=ùúàùëöùúáùëö+1+ùúàùëö+1ùúáùëö‚àíùúàùëöùúáùëö+ùí™(Œîùöù2).(ŒΩŒº)m+1=ŒΩmŒºm+1+ŒΩm+1Œºm‚àíŒΩmŒºm+O(Œît2).(25)The theoretical foundation and rigorous proof of the proposed approach are presented in [34].An analogous method can be adopted to handle nonlinear time-fractional interface problems.4.2.2. Innovative Splitting TechniqueThe governing equation under consideration is a nonlinear time-fractional partial differential equation:ùê∑ùúåùöùùúë(ùö°,ùöù)‚àí‚àá¬∑(ùúÇ‚àáùúë)=‚Ñ±(ùö°,ùöù,ùúë(ùö°,ùöù)),0<ùúå<1,ùöù>0,ùö°‚ààŒ©,DtœÅœÜ(x,t)‚àí‚àá¬∑(Œ∑‚àáœÜ)=F(x,t,œÜ(x,t)),0<œÅ<1,t>0,x‚ààŒ©,(26)whereùê∑ùúåùöùDtœÅdenotes the fractional derivative of orderùúåœÅ, and‚Ñ±Fis a nonlinear function of the solutionùúëœÜ. To handle the nonlinearity, we reformulate the source term. The nonlinear operatorùí©(ùúë)N(œÜ)is defined asùí©(ùúë)=‚Ñ±(ùö°,ùöù,ùúë)ùúë,N(œÜ)=F(x,t,œÜ)œÜ,(27)provided thatùúë‚â†0œÜ‚â†0in the domain. This allows the original equation to be rewritten asùê∑ùúåùöùùúë(ùö°,ùöù)‚àí‚àá¬∑(ùúÇ‚àáùúë)+ùí©(ùúë)ùúë=ùëì(ùö°,ùöù).DtœÅœÜ(x,t)‚àí‚àá¬∑(Œ∑‚àáœÜ)+N(œÜ)œÜ=f(x,t).(28)In this form,ùí©(ùúë)ùúëN(œÜ)œÜconstitutes the nonlinear term.To ensure the numerical solution remains positive, thereby guaranteeing thatùí©(ùúë)N(œÜ)is well-defined, we apply the transformationùúë(ùö°,ùöù)=ùë£(ùö°,ùöù)+ùëê,œÜ(x,t)=v(x,t)+c,(29)wherecis a sufficiently large positive constant chosen such thatùúë>0œÜ>0throughout the computational domainŒ©Œ©. Substituting this into the equation and noting that‚àáùúë=‚àáùë£‚àáœÜ=‚àávandùê∑ùúåùöùùúë=ùê∑ùúåùöùùë£DtœÅœÜ=DtœÅv, the transformed equation forvisùê∑ùúåùöùùë£(ùö°,ùöù)‚àí‚àá¬∑(ùúÇ‚àáùë£)+ùí©(ùë£+ùëê)(ùë£+ùëê)=ùëì(ùö°,ùöù).DtœÅv(x,t)‚àí‚àá¬∑(Œ∑‚àáv)+N(v+c)(v+c)=f(x,t).(30)The corresponding Dirichlet boundary condition becomesùë£(ùö°,ùöù)=ùúá(ùö°,ùöù),ùö°‚àà‚àÇŒ©,v(x,t)=Œº(x,t),x‚àà‚àÇŒ©,(31)whereùúá(ùö°,ùöù)Œº(x,t)is the prescribed boundary data. For the sake of notational simplicity in the subsequent linearization process, we will continue to useùúëœÜwith the understanding that it now represents the positively shifted solutionùë£+ùëêv+c. Linearization ProcessThe nonlinear termùí©(ùúë)ùúëN(œÜ)œÜis linearized using a relaxation splitting technique, splitting the term asùí©(ùúë)ùúë=ùúîùí©(ùúë)ùúë+(1‚àíùúî)ùí©(ùúë)ùúë,N(œÜ)œÜ=œâN(œÜ)œÜ+(1‚àíœâ)N(œÜ)œÜ,(32)whereùúî‚àà[0,1]œâ‚àà[0,1]is a relaxation parameter. In an iterative solution process (denoting the current iteration asùëò+1k+1and the previous known iteration ask), we treat part of the term explicitly. Specifically, the term multiplied byùúîœâis evaluated at the previous iterationùúëùëòœÜkand moved to the right-hand side as a source term. The term multiplied by(1‚àíùúî)(1‚àíœâ)is kept on the left-hand side and evaluated at the current unknownùúëùëò+1œÜk+1. This yields the following linearized equation for the next iterative solutionùúëùëò+1œÜk+1:ùê∑ùúåùöùùúëùëò+1(ùö°,ùöù)‚àí‚àá¬∑(ùúÇ‚àáùúëùëò+1)+(1‚àíùúî)ùí©(ùúëùëò+1)ùúëùëò+1=ùëì(ùö°,ùöù)+ùúîùí©(ùúëùëò)ùúëùëò,ùö°‚ààŒ©.DtœÅœÜk+1(x,t)‚àí‚àá¬∑(Œ∑‚àáœÜk+1)+(1‚àíœâ)N(œÜk+1)œÜk+1=f(x,t)+œâN(œÜk)œÜk,x‚ààŒ©.(33)The relaxation parameterùúîœâplays a critical role in controlling how much the previous iteration contributes to the current update, thereby ensuring a balanced and stable convergence process. In our numerical experiments, its value was selected empirically. We tested several candidate values ofùúîœâand evaluated their corresponding errors, as reported in Table 7, and then fixed the value that provided the best balance between convergence rate and stability for the considered problem configurations. More advanced strategies, such as adaptive line search techniques, may further improve efficiency [35,36].A similar scheme can also be extended to nonlinear time-fractional interface problems.",
            "4.1. Linear Case": "For the linear time-fractional interface problems described in Equation (1), we obtainùëé0‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=1‚Ñ≥+1ùúÜùëñùúô(‚à•ùë•‚àíùë•ùëñ‚à•)‚àíùúë1(ùë•,ùë°0)+‚àëùëò=0ùëöùëèùëò(ùúëùëö‚àíùëò+1‚àíùúëùëö‚àíùëò)‚éû‚é†‚éü‚éü‚éü=ùúÇ+ùë•(ùë•)‚àëùëñ=1‚Ñ≥+1ùúÜùëñùëëùúôùëëùë•(‚à•ùë•‚àíùë•ùëñ‚à•)+ùúÇ+(ùë•)‚àëùëñ=1‚Ñ≥+1ùúÜùëñùëë2ùúôùëëùë•2(‚à•ùë•‚àíùë•ùëñ‚à•)+ùëì1(ùë•,ùë°),ùë•‚ààŒ©+.a0‚àëi=1M+1Œªiœï(‚à•x‚àíxi‚à•)‚àíœÜ1(x,t0)+‚àëk=0mbk(œÜm‚àík+1‚àíœÜm‚àík)=Œ∑x+(x)‚àëi=1M+1Œªidœïdx(‚à•x‚àíxi‚à•)+Œ∑+(x)‚àëi=1M+1Œªid2œïdx2(‚à•x‚àíxi‚à•)+f1(x,t),x‚ààŒ©+.(21)andùëé0‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùúô(‚à•ùë•‚àíùë•ùëñ‚à•)‚àíùúë2(ùë•,ùë°0)+‚àëùëò=0ùëöùëèùëò(ùúëùëö‚àíùëò+1‚àíùúëùëö‚àíùëò)‚éû‚é†‚éü‚éü‚éü=ùúÇ‚àíùë•(ùë•)‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùëëùúôùëëùë•(‚à•ùë•‚àíùë•ùëñ‚à•)+ùúÇ‚àí(ùë•)‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùëë2ùúôùëëùë•2(‚à•ùë•‚àíùë•ùëñ‚à•)+ùëì2(ùë•,ùë°),ùë•‚ààŒ©‚àí.a0‚àëi=M+1NŒ≥iœï(‚à•x‚àíxi‚à•)‚àíœÜ2(x,t0)+‚àëk=0mbk(œÜm‚àík+1‚àíœÜm‚àík)=Œ∑x‚àí(x)‚àëi=M+1NŒ≥idœïdx(‚à•x‚àíxi‚à•)+Œ∑‚àí(x)‚àëi=M+1NŒ≥id2œïdx2(‚à•x‚àíxi‚à•)+f2(x,t),x‚ààŒ©‚àí.(22)Now, discretizing the following equation, we haveùëé0‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=1‚Ñ≥+1ùúÜùëñùúô(‚à•ùë•ùëó‚àíùë•ùëñ‚à•)‚éû‚é†‚éü‚éü‚éü‚àíùúÇ+ùë•(ùë•)‚àëùëñ=1‚Ñ≥+1ùúÜùëñùëëùúôùëëùë•(‚à•ùë•ùëó‚àíùë•ùëñ‚à•)‚àíùúÇ+(ùë•)‚àëùëñ=1‚Ñ≥+1ùúÜùëñùëë2ùúôùëëùë•2(‚à•ùë•ùëó‚àíùë•ùëñ‚à•)=ùúë1(ùë•ùëó,ùë°0)‚àíùëé0‚àëùëò=0ùëöùëèùëò(ùúëùëö‚àíùëò+1‚àíùúëùëö‚àíùëò)+ùëì1(ùë•,ùë°),ùë•‚ààŒ©+.a0‚àëi=1M+1Œªiœï(‚à•xj‚àíxi‚à•)‚àíŒ∑x+(x)‚àëi=1M+1Œªidœïdx(‚à•xj‚àíxi‚à•)‚àíŒ∑+(x)‚àëi=1M+1Œªid2œïdx2(‚à•xj‚àíxi‚à•)=œÜ1(xj,t0)‚àía0‚àëk=0mbk(œÜm‚àík+1‚àíœÜm‚àík)+f1(x,t),x‚ààŒ©+.(23)andùëé0‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùúô(‚à•ùë•ùëó‚àíùë•ùëñ‚à•)‚éû‚é†‚éü‚éü‚éü‚àíùúÇ‚àíùë•(ùë•)‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùëëùúôùëëùë•(‚à•ùë•ùëó‚àíùë•ùëñ‚à•)‚àíùúÇ‚àí(ùë•)‚àëùëñ=‚Ñ≥+1ùí©ùõæùëñùëë2ùúôùëëùë•2(‚à•ùë•ùëó‚àíùë•ùëñ‚à•)=ùúë2(ùë•ùëó,ùë°0)‚àí‚àëùëò=0ùëöùëèùëò(ùúëùëö‚àíùëò+1‚àíùúëùëö‚àíùëò)+ùëì2(ùë•,ùë°),ùë•‚ààŒ©‚àí.a0‚àëi=M+1NŒ≥iœï(‚à•xj‚àíxi‚à•)‚àíŒ∑x‚àí(x)‚àëi=M+1NŒ≥idœïdx(‚à•xj‚àíxi‚à•)‚àíŒ∑‚àí(x)‚àëi=M+1NŒ≥id2œïdx2(‚à•xj‚àíxi‚à•)=œÜ2(xj,t0)‚àí‚àëk=0mbk(œÜm‚àík+1‚àíœÜm‚àík)+f2(x,t),x‚ààŒ©‚àí.(24)A linear system consisting ofùí©+1N+1unknowns is formulated using Equations (19), (20), (23), and (24). In this system, the functionsùúÜùëñ(ùúè)Œªi(œÑ)forùëñ=1,2,‚Ä¶,‚Ñ≥+1i=1,2,‚Ä¶,M+1andùõæùëñ(ùúè)Œ≥i(œÑ)forùëñ=‚Ñ≥+1,‚Ñ≥+2,‚Ä¶,ùí©i=M+1,M+2,‚Ä¶,Nappear as key components in the expression.",
            "4.2. Nonlinear Case": "4.2.1. Quasi-Newton Linearization TechniqueThe first step in addressing the nonlinear case involves applying the quasi-Newton linearization formula to Equation (2).(ùúàùúá)ùëö+1=ùúàùëöùúáùëö+1+ùúàùëö+1ùúáùëö‚àíùúàùëöùúáùëö+ùí™(Œîùöù2).(ŒΩŒº)m+1=ŒΩmŒºm+1+ŒΩm+1Œºm‚àíŒΩmŒºm+O(Œît2).(25)The theoretical foundation and rigorous proof of the proposed approach are presented in [34].An analogous method can be adopted to handle nonlinear time-fractional interface problems. 4.2.2. Innovative Splitting TechniqueThe governing equation under consideration is a nonlinear time-fractional partial differential equation:ùê∑ùúåùöùùúë(ùö°,ùöù)‚àí‚àá¬∑(ùúÇ‚àáùúë)=‚Ñ±(ùö°,ùöù,ùúë(ùö°,ùöù)),0<ùúå<1,ùöù>0,ùö°‚ààŒ©,DtœÅœÜ(x,t)‚àí‚àá¬∑(Œ∑‚àáœÜ)=F(x,t,œÜ(x,t)),0<œÅ<1,t>0,x‚ààŒ©,(26)whereùê∑ùúåùöùDtœÅdenotes the fractional derivative of orderùúåœÅ, and‚Ñ±Fis a nonlinear function of the solutionùúëœÜ. To handle the nonlinearity, we reformulate the source term. The nonlinear operatorùí©(ùúë)N(œÜ)is defined asùí©(ùúë)=‚Ñ±(ùö°,ùöù,ùúë)ùúë,N(œÜ)=F(x,t,œÜ)œÜ,(27)provided thatùúë‚â†0œÜ‚â†0in the domain. This allows the original equation to be rewritten asùê∑ùúåùöùùúë(ùö°,ùöù)‚àí‚àá¬∑(ùúÇ‚àáùúë)+ùí©(ùúë)ùúë=ùëì(ùö°,ùöù).DtœÅœÜ(x,t)‚àí‚àá¬∑(Œ∑‚àáœÜ)+N(œÜ)œÜ=f(x,t).(28)In this form,ùí©(ùúë)ùúëN(œÜ)œÜconstitutes the nonlinear term.To ensure the numerical solution remains positive, thereby guaranteeing thatùí©(ùúë)N(œÜ)is well-defined, we apply the transformationùúë(ùö°,ùöù)=ùë£(ùö°,ùöù)+ùëê,œÜ(x,t)=v(x,t)+c,(29)wherecis a sufficiently large positive constant chosen such thatùúë>0œÜ>0throughout the computational domainŒ©Œ©. Substituting this into the equation and noting that‚àáùúë=‚àáùë£‚àáœÜ=‚àávandùê∑ùúåùöùùúë=ùê∑ùúåùöùùë£DtœÅœÜ=DtœÅv, the transformed equation forvisùê∑ùúåùöùùë£(ùö°,ùöù)‚àí‚àá¬∑(ùúÇ‚àáùë£)+ùí©(ùë£+ùëê)(ùë£+ùëê)=ùëì(ùö°,ùöù).DtœÅv(x,t)‚àí‚àá¬∑(Œ∑‚àáv)+N(v+c)(v+c)=f(x,t).(30)The corresponding Dirichlet boundary condition becomesùë£(ùö°,ùöù)=ùúá(ùö°,ùöù),ùö°‚àà‚àÇŒ©,v(x,t)=Œº(x,t),x‚àà‚àÇŒ©,(31)whereùúá(ùö°,ùöù)Œº(x,t)is the prescribed boundary data. For the sake of notational simplicity in the subsequent linearization process, we will continue to useùúëœÜwith the understanding that it now represents the positively shifted solutionùë£+ùëêv+c.",
            "4.2.1. Quasi-Newton Linearization Technique": "The first step in addressing the nonlinear case involves applying the quasi-Newton linearization formula to Equation (2).(ùúàùúá)ùëö+1=ùúàùëöùúáùëö+1+ùúàùëö+1ùúáùëö‚àíùúàùëöùúáùëö+ùí™(Œîùöù2).(ŒΩŒº)m+1=ŒΩmŒºm+1+ŒΩm+1Œºm‚àíŒΩmŒºm+O(Œît2).(25)The theoretical foundation and rigorous proof of the proposed approach are presented in [34]. An analogous method can be adopted to handle nonlinear time-fractional interface problems.",
            "4.2.2. Innovative Splitting Technique": "The governing equation under consideration is a nonlinear time-fractional partial differential equation:ùê∑ùúåùöùùúë(ùö°,ùöù)‚àí‚àá¬∑(ùúÇ‚àáùúë)=‚Ñ±(ùö°,ùöù,ùúë(ùö°,ùöù)),0<ùúå<1,ùöù>0,ùö°‚ààŒ©,DtœÅœÜ(x,t)‚àí‚àá¬∑(Œ∑‚àáœÜ)=F(x,t,œÜ(x,t)),0<œÅ<1,t>0,x‚ààŒ©,(26)whereùê∑ùúåùöùDtœÅdenotes the fractional derivative of orderùúåœÅ, and‚Ñ±Fis a nonlinear function of the solutionùúëœÜ. To handle the nonlinearity, we reformulate the source term. The nonlinear operatorùí©(ùúë)N(œÜ)is defined asùí©(ùúë)=‚Ñ±(ùö°,ùöù,ùúë)ùúë,N(œÜ)=F(x,t,œÜ)œÜ,(27)provided thatùúë‚â†0œÜ‚â†0in the domain. This allows the original equation to be rewritten asùê∑ùúåùöùùúë(ùö°,ùöù)‚àí‚àá¬∑(ùúÇ‚àáùúë)+ùí©(ùúë)ùúë=ùëì(ùö°,ùöù).DtœÅœÜ(x,t)‚àí‚àá¬∑(Œ∑‚àáœÜ)+N(œÜ)œÜ=f(x,t).(28)In this form,ùí©(ùúë)ùúëN(œÜ)œÜconstitutes the nonlinear term. To ensure the numerical solution remains positive, thereby guaranteeing thatùí©(ùúë)N(œÜ)is well-defined, we apply the transformationùúë(ùö°,ùöù)=ùë£(ùö°,ùöù)+ùëê,œÜ(x,t)=v(x,t)+c,(29)wherecis a sufficiently large positive constant chosen such thatùúë>0œÜ>0throughout the computational domainŒ©Œ©. Substituting this into the equation and noting that‚àáùúë=‚àáùë£‚àáœÜ=‚àávandùê∑ùúåùöùùúë=ùê∑ùúåùöùùë£DtœÅœÜ=DtœÅv, the transformed equation forvisùê∑ùúåùöùùë£(ùö°,ùöù)‚àí‚àá¬∑(ùúÇ‚àáùë£)+ùí©(ùë£+ùëê)(ùë£+ùëê)=ùëì(ùö°,ùöù).DtœÅv(x,t)‚àí‚àá¬∑(Œ∑‚àáv)+N(v+c)(v+c)=f(x,t).(30)The corresponding Dirichlet boundary condition becomesùë£(ùö°,ùöù)=ùúá(ùö°,ùöù),ùö°‚àà‚àÇŒ©,v(x,t)=Œº(x,t),x‚àà‚àÇŒ©,(31)whereùúá(ùö°,ùöù)Œº(x,t)is the prescribed boundary data. For the sake of notational simplicity in the subsequent linearization process, we will continue to useùúëœÜwith the understanding that it now represents the positively shifted solutionùë£+ùëêv+c.",
            "Linearization Process": "The nonlinear termùí©(ùúë)ùúëN(œÜ)œÜis linearized using a relaxation splitting technique, splitting the term asùí©(ùúë)ùúë=ùúîùí©(ùúë)ùúë+(1‚àíùúî)ùí©(ùúë)ùúë,N(œÜ)œÜ=œâN(œÜ)œÜ+(1‚àíœâ)N(œÜ)œÜ,(32)whereùúî‚àà[0,1]œâ‚àà[0,1]is a relaxation parameter. In an iterative solution process (denoting the current iteration asùëò+1k+1and the previous known iteration ask), we treat part of the term explicitly. Specifically, the term multiplied byùúîœâis evaluated at the previous iterationùúëùëòœÜkand moved to the right-hand side as a source term. The term multiplied by(1‚àíùúî)(1‚àíœâ)is kept on the left-hand side and evaluated at the current unknownùúëùëò+1œÜk+1. This yields the following linearized equation for the next iterative solutionùúëùëò+1œÜk+1:ùê∑ùúåùöùùúëùëò+1(ùö°,ùöù)‚àí‚àá¬∑(ùúÇ‚àáùúëùëò+1)+(1‚àíùúî)ùí©(ùúëùëò+1)ùúëùëò+1=ùëì(ùö°,ùöù)+ùúîùí©(ùúëùëò)ùúëùëò,ùö°‚ààŒ©.DtœÅœÜk+1(x,t)‚àí‚àá¬∑(Œ∑‚àáœÜk+1)+(1‚àíœâ)N(œÜk+1)œÜk+1=f(x,t)+œâN(œÜk)œÜk,x‚ààŒ©.(33)The relaxation parameterùúîœâplays a critical role in controlling how much the previous iteration contributes to the current update, thereby ensuring a balanced and stable convergence process. In our numerical experiments, its value was selected empirically. We tested several candidate values ofùúîœâand evaluated their corresponding errors, as reported in Table 7, and then fixed the value that provided the best balance between convergence rate and stability for the considered problem configurations. More advanced strategies, such as adaptive line search techniques, may further improve efficiency [35,36]. A similar scheme can also be extended to nonlinear time-fractional interface problems.",
            "5. Stability Analysis": "We analyze the stability of the established MQ-RBFs following the basic definition described in LeVeque‚Äôs work on numerical schemes [37]. Definition2.In the numerical solution of the interface problem, the resulting systemùêóùëé=ùëèXa=bis considered stable if the matrixùêóXhas a bounded inverse:‚à•ùêó‚àí1‚à•‚â§ùëô0,‚à•X‚àí1‚à•‚â§l0,whereùëô0l0is a constant. The stability trend of the scheme is shown inFigure 1.Figure 1.The magnitudes of the 2-norm and spectral radius ofùêó‚àí1X‚àí1for the proposed approach atŒîùë°=0.5Œît=0.5, withùë°=1t=1, are presented for Examples 1, 2, 5, and 6.",
            "6. Numerical Validation": "To assess the performance of the proposed MQ-RBF, numerical solutions for time-fractional interface problems are obtained, and their accuracy is determined via the following error norms:ùëÄùê¥ùê∏ùë†=||ùúëùë°ùëüùë¢ùëí‚àíùúëùëéùëùùëùùëüùëúùë•||‚àû‚âàùëöùëéùë•(|ùúëùë°ùëüùë¢ùëíùëó‚àíùúëùëéùëùùëùùëüùëúùë•ùëó|),MAEs=||œÜtrue‚àíœÜapprox||‚àû‚âàmax(|œÜjtrue‚àíœÜjapprox|),(34)ùëÖùëÄùëÜùê∏ùë†‚âà‚éõ‚éù‚éú‚éú‚éú1ùëÄ‚àëùëó=1ùëÄ(ùúëùë°ùëüùë¢ùëíùëó‚àíùúëùëéùëùùëùùëüùëúùë•ùëó)2‚éû‚é†‚éü‚éü‚éü‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àíÓÑ¥‚é∑ÓÑ≥ÓÑ≥.RMSEs‚âà1M‚àëj=1M(œÜjtrue‚àíœÜjapprox)2.(35) Example1.As the first example, linear time-fractional interface Problem (1) is considered assumingùúÇ+(ùë•)=1Œ∑+(x)=1andùúÇ‚àí(ùë•)=2Œ∑‚àí(x)=2. Exact solutions and source functions are given byùúë(ùë•,ùë°)=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùúë1(ùë•,ùë°)=ùë•8ùëí‚àíùë°,ùúë2(ùë•,ùë°)=12(ùë•8+1256)ùëí‚àíùë°,0‚â§ùë•‚â§12,12‚â§ùë•‚â§1,œÜ(x,t)=œÜ1(x,t)=x8e‚àít,0‚â§x‚â§12,œÜ2(x,t)=12x8+1256e‚àít,12‚â§x‚â§1,(36)andùëì(ùë•,ùë°)=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùëì1(ùë•,ùë°)=‚àí56ùë•6ùëí‚àíùë°‚àíùë°1‚àíùúå‚Ñ≥1,2‚àíùúå(‚àíùë°)ùë•8,ùëì2(ùë•,ùë°)=‚àí12ùë°1‚àíùúå‚Ñ≥1,2‚àíùúå(‚àíùë°)(ùë•8+1256)‚àí56ùë•6ùëí‚àíùë°ùëí‚àíùë°,0‚â§ùë•‚â§12,12‚â§ùë•‚â§1.f(x,t)=f1(x,t)=‚àí56x6e‚àít‚àít1‚àíœÅM1,2‚àíœÅ(‚àít)x8,0‚â§x‚â§12,f2(x,t)=‚àí12t1‚àíœÅM1,2‚àíœÅ(‚àít)(x8+1256)‚àí56x6e‚àíte‚àít,12‚â§x‚â§1.(37)where‚Ñ≥ùëé,ùëè(¬∑)Ma,b(¬∑)is the Mittag‚ÄìLeffler function of two parameters defined as‚Ñ≥ùëé,ùëè(ùë°)=‚àëùëò=0‚àûùë°ùëòŒì(ùëéùëò+ùëè),‚àÄùë°‚àà‚ÑÇ,ùëé,ùëè‚àà‚Ñù+Ma,b(t)=‚àëk=0‚àûtkŒì(ak+b),‚àÄt‚ààC,a,b‚ààR+and‚Ñ≥1,1(ùë°)=exp(ùë°),‚àÄùë°‚àà‚ÑÇ.M1,1(t)=exp(t),‚àÄt‚ààC. For this linear 1D time-fractional interface problem, the true solution calculates the interface, boundary, and initial conditions. The MQ-RBF method is applied to approximate the solution, and the corresponding numerical results are summarized inTable 1. The tests conducted for different fractional ordersùúåœÅand collocation pointsùí©Ndemonstrate, through MAE and RMSE metrics, that the technique achieves high precision. Asùí©Nincreases, the errors decrease consistently. For example, whenùí©=24N=24, the MAEs are6.1504√ó10‚àí46.1504√ó10‚àí4forùúå=0.2œÅ=0.2and1.2150√ó10‚àí41.2150√ó10‚àí4forùúå=1œÅ=1. These results indicate that the accuracy improves asùúåœÅapproaches the classical case of integer order. The method remains stable and convergent in all the values tested inùúåœÅ, although smaller fractional orders tend to produce slightly larger errors due to the stronger memory effects inherent in fractional models. To further illustrate the behavior of the solution, a surface plot 3D forùúå=0.5œÅ=0.5is provided inFigure 2. Additionally,Figure 3compares the solution profiles for several values ofùúåœÅ, providing further insight into how fractional order influences system dynamics. Figure 2.(a) Approximate versus (b) exact solution for Example 1 is presented for distinct values ofxandtwithùúå=0.5œÅ=0.5. Figure 3.Graphical representation of MAEs for different values ofùúåœÅfor Example 1. Table 1.Error analysis for different values ofùúåœÅandNwithùúñ=2œµ=2,ùëëùë°=0.5dt=0.5, andùë°=1t=1for Example 1 using Caputo derivative. Example2.As the second example, linear time-fractional interface Problem (1) is considered assumingùúÇ+(ùë•)=ùëíùë•ùëù(‚àí10(ùë•‚àí0.5)4ùë•4)Œ∑+(x)=exp(‚àí10(x‚àí0.5)4x4)andùúÇ‚àí(ùë•)=3Œ∑‚àí(x)=3. Exact solutions and source functions are given byùúë(ùë•,ùë°)=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùúë1(ùë•,ùë°)=sin5ùúãùë•ùëí‚àíùë°,ùúë2(ùë•,ùë°)=(2(ùë•‚àí12)7+1)ùëí‚àíùë°,0‚â§ùë•‚â§12,12‚â§ùë•‚â§1,œÜ(x,t)=œÜ1(x,t)=sin5œÄxe‚àít,0‚â§x‚â§12,œÜ2(x,t)=(2(x‚àí12)7+1)e‚àít,12‚â§x‚â§1,(38)andùëì(ùë•,ùë°)=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùëì1(ùë•,ùë°)=‚àíùë°1‚àíùúå‚Ñ≥1,2‚àíùúå(‚àíùë°)sin(5ùúãùë•)+15ùúãùëí‚àíùë°‚àí10ùë•4(ùë•‚àí12)4cos(5ùúãùë•)(40ùë•3(ùë•‚àí12)4+40ùë•4(ùë°‚àí12)3),ùëì2(ùë•,ùë°)=‚àíùë°1‚àíùúå‚Ñ≥1,2‚àíùúå(‚àíùë°)(2(ùë•‚àí12)7+1)‚àí252exp(‚àíùë°)(ùë•‚àí12)5,0‚â§ùë•‚â§12,12‚â§ùë•‚â§1.f(x,t)=f1(x,t)=‚àít1‚àíœÅM1,2‚àíœÅ(‚àít)sin(5œÄx)+15œÄe‚àít‚àí10x4(x‚àí12)4cos(5œÄx)(40x3(x‚àí12)4+40x4(t‚àí12)3),0‚â§x‚â§12,f2(x,t)=‚àít1‚àíœÅM1,2‚àíœÅ(‚àít)2(x‚àí12)7+1‚àí252exp(‚àít)(x‚àí12)5,12‚â§x‚â§1.(39)where‚Ñ≥ùëé,ùëè(¬∑)Ma,b(¬∑)is the Mittag‚ÄìLeffler function of two parameters defined as‚Ñ≥ùëé,ùëè(ùë°)=‚àëùëò=0‚àûùë°ùëòŒì(ùëéùëò+ùëè),‚àÄùë°‚àà‚ÑÇ,ùëé,ùëè‚àà‚Ñù+Ma,b(t)=‚àëk=0‚àûtkŒì(ak+b),‚àÄt‚ààC,a,b‚ààR+and‚Ñ≥1,1(ùë°)=exp(ùë°),‚àÄùë°‚àà‚ÑÇ.M1,1(t)=exp(t),‚àÄt‚ààC. In this linear 1D time-fractional interface problem, the exact solution is utilized to define the interface, boundary, and initial conditions. The problem is then solved using the MQ-RBF method.Table 2reports the error analysis for Example 2, where the Caputo fractional derivative is evaluated atùë°=1t=1with a fixed time step ofùëëùë°=0.5dt=0.5. The results are presented for several fractional ordersùúåœÅand grid sizesùí©N, including MAEs and RMSEs. As expected, errors decrease as the grid is refined, demonstrating the precision and convergence of the proposed numerical approach. To further illustrate how fractional orderùúåœÅinfluences the solution,Figure 4shows the behavior of the method for a range ofùúåœÅvalues. These visualizations provide a clearer understanding of the system response under different fractional dynamics. In addition, a 3D plot of the solution forùúå=0.5œÅ=0.5is presented inFigure 5, highlighting the qualitative behavior of the solution in the domain. Figure 4.Graphical representation of MAEs for different vales ofùúåœÅfor Example 2. Figure 5.Comparison of (a) approximate and (b) exact solutions for Example 2 is presented for various values ofxandtwithùúå=0.5œÅ=0.5. Table 2.Error analysis for different values ofùúåœÅandùí©Nwithùúñ=2.2œµ=2.2,ùëëùë°=0.5dt=0.5, andùë°=1t=1for Example 2 using Caputo derivative. Example3.As the third example, linear time-fractional interface Problem (1) is considered assumingùúÇ+(ùë•)=1Œ∑+(x)=1andùúÇ‚àí(ùë•)=2Œ∑‚àí(x)=2. Exact solutions and source functions are given byùúë(ùë•,ùë°)=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùúë1(ùë•,ùë°)=ùë•4ùëí‚àíùë°,ùúë2(ùë•,ùë°)=12(ùë•4+116)ùëí‚àíùë°,0‚â§ùë•‚â§12,12‚â§ùë•‚â§1,œÜ(x,t)=œÜ1(x,t)=x4e‚àít,0‚â§x‚â§12,œÜ2(x,t)=12x4+116e‚àít,12‚â§x‚â§1,(40)andùëì(ùë•,ùë°)=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùëì1(ùë•,ùë°)=‚àí12ùë•2ùëí‚àíùë°‚àíùë°1‚àíùúå‚Ñ≥1,2‚àíùúå(‚àíùë°)ùë•4,ùëì2(ùë•,ùë°)=‚àí12ùë°1‚àíùúå‚Ñ≥1,2‚àíùúå(‚àíùë°)(ùë•4+116)‚àí12ùë•2ùëí‚àíùë°ùëí‚àíùë°,0‚â§ùë•‚â§12,12‚â§ùë•‚â§1.f(x,t)=f1(x,t)=‚àí12x2e‚àít‚àít1‚àíœÅM1,2‚àíœÅ(‚àít)x4,0‚â§x‚â§12,f2(x,t)=‚àí12t1‚àíœÅM1,2‚àíœÅ(‚àít)(x4+116)‚àí12x2e‚àíte‚àít,12‚â§x‚â§1.(41)where‚Ñ≥ùëé,ùëè(¬∑)Ma,b(¬∑)is the Mittag‚ÄìLeffler function of two parameters defined as‚Ñ≥ùëé,ùëè(ùë°)=‚àëùëò=0‚àûùë°ùëòŒì(ùëéùëò+ùëè),‚àÄùë°‚àà‚ÑÇ,ùëé,ùëè‚àà‚Ñù+Ma,b(t)=‚àëk=0‚àûtkŒì(ak+b),‚àÄt‚ààC,a,b‚ààR+and‚Ñ≥1,1(ùë°)=exp(ùë°),‚àÄùë°‚àà‚ÑÇ.M1,1(t)=exp(t),‚àÄt‚ààC. This linear 1D time-fractional interface problem is formulated using the exact solution to define the interface, boundary, and initial conditions and is subsequently solved using the MQ-RBF method.Table 3summarizes the error analysis for Example 3, where the Caputo fractional derivative is evaluated atùë°=1t=1. The table reports results for several fractional ordersùúåœÅand grid resolutionsùí©N, including both MAE and RMSE values. As the grid is refined, both error measures consistently decrease, confirming the accuracy and convergence of the proposed numerical method. Furthermore,Figure 6presents 2D plots for different values ofùúåœÅ, along with their corresponding error profiles, providing additional insight into the influence of the fractional order on the solution behavior. Figure 6.Exact versus approximate solutions for Example 3 along with absolute error at different values ofùúåœÅ,ùí©=20,ùë°=1N=20,t=1. Table 3.Error analysis for different values ofùúåœÅandùí©Nwithùúñ=1œµ=1,ùëëùë°=0.1dt=0.1andùë°=1t=1for Example 3 using Caputo derivative. Example4.As the fourth example, we present the 1D time-fractional interface Problem1with its initial and boundary conditions, for which no exact solution is available:ùúë1(0,ùë°)ùúë2(1,ùë°)ùúë1(ùë•,0)ùúë2(ùë•,0)=0,=257512ùëí‚àíùë°,=0,=ùë•8,(ùë•,ùë°)‚àà‚àÇŒ©√ó(0,ùëáùëì],(ùë•,ùë°)‚àà‚àÇŒ©√ó(0,ùëáùëì],ùë•‚ààŒ©,ùë•‚ààŒ©.œÜ1(0,t)=0,(x,t)‚àà‚àÇŒ©√ó(0,Tf],œÜ2(1,t)=257512e‚àít,(x,t)‚àà‚àÇŒ©√ó(0,Tf],œÜ1(x,0)=0,x‚ààŒ©,œÜ2(x,0)=x8,x‚ààŒ©.(42)We use the residual error to assess the effectiveness of the suggested MQ-RBF scheme because there is no exact solution for this problem. The residual errors for various values of œÅ are reported inTable 4, demonstrating that the technique consistently produces modest residuals even for coarse discretizations. Plotting the numerical solution and associated residual error forùúå=0.5œÅ=0.5andùúå=0.8œÅ=0.8(seeFigure 7) allows us to further evaluate the accuracy and validate the stability and dependability of the suggested method.Figure 7.Numerical solution and residual error graph forùí©=10N=10andùúñ=1œµ=1for Example 4.Table 4.Residual errors for various values ofùúåœÅwithùúñ=1œµ=1, using MQ-RBFs atùëëùóç=0.5dt=0.5andùóç=1t=1for Example 4. Example5.As the fifth example, nonlinear time-fractional interface Problem (1) is considered assumingùúÇ+(ùë•)=1Œ∑+(x)=1andùúÇ‚àí(ùë•)=2Œ∑‚àí(x)=2. Exact solutions and source functions are given byùúë(ùë•,ùë°)=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùúë1(ùë•,ùë°)=ùë•8ùëí‚àíùë°,ùúë2(ùë•,ùë°)=12(ùë•8+1256)ùëí‚àíùë°,0‚â§ùë•‚â§12,12‚â§ùë•‚â§1.œÜ(x,t)=œÜ1(x,t)=x8e‚àít,0‚â§x‚â§12,œÜ2(x,t)=12x8+1256e‚àít,12‚â§x‚â§1.(43)andùëì(ùë•,ùë°)=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùëì1(ùë•,ùë°)=‚àí2ùë•16ùëí‚àí2ùë°‚àí56ùë•6ùëí‚àíùë°‚àíùë°1‚àíùúå‚Ñ≥1,2‚àíùúå(‚àíùë°)ùë•8,ùëì2(ùë•,ùë°)=‚àí12ùë°1‚àíùúå‚Ñ≥1,2‚àíùúå(‚àíùë°)(ùë•8+1256)‚àí(ùë•8+1256)2ùëí‚àí2ùë°‚àí56ùë•6ùëí‚àíùë°ùëí‚àíùë°,0‚â§ùë•‚â§12,12‚â§ùë•‚â§1,f(x,t)=f1(x,t)=‚àí2x16e‚àí2t‚àí56x6e‚àít‚àít1‚àíœÅM1,2‚àíœÅ(‚àít)x8,0‚â§x‚â§12,f2(x,t)=‚àí12t1‚àíœÅM1,2‚àíœÅ(‚àít)(x8+1256)‚àíx8+12562e‚àí2t‚àí56x6e‚àíte‚àít,12‚â§x‚â§1,(44)where‚Ñ≥ùëé,ùëè(¬∑)Ma,b(¬∑)is the Mittag‚ÄìLeffler function of two parameters defined as‚Ñ≥ùëé,ùëè(ùë°)=‚àëùëò=0‚àûùë°ùëòŒì(ùëéùëò+ùëè),‚àÄùë°‚àà‚ÑÇ,ùëé,ùëè‚àà‚Ñù+Ma,b(t)=‚àëk=0‚àûtkŒì(ak+b),‚àÄt‚ààC,a,b‚ààR+and‚Ñ≥1,1(ùë°)=exp(ùë°),‚àÄùë°‚àà‚ÑÇ.M1,1(t)=exp(t),‚àÄt‚ààC.To formulate the computational problem, the exact analytical solution is used to define the initial, boundary, and interface conditions. The MQ-RBF method is then applied to the nonlinear 1D time-fractional interface model, using both quasi-Newton linearization and a new splitting technique to address the nonlinear terms.Table 5andTable 6show that for various fractional orders œÅ and discretization levelsùí©N, the MAEs and RMSEs consistently decrease asùí©Nincreases, confirming the accuracy and convergence of the scheme. Although the method performs well for all values of œÅ, smaller fractional orders yield slightly higher errors due to stronger memory effects. A comparison of the two linearization strategies indicates that the proposed splitting technique is more accurate and converges faster than the quasi-Newton method. This improvement results from its better handling of nonlinearities while maintaining numerical stability. A more direct comparison can be made by selecting the cases with the largest error reduction. Atùí©=16N=16andùúå=0.9œÅ=0.9, the innovative splitting technique yields an MAE of6.3176√ó10‚àí046.3176√ó10‚àí04, which is approximately 38% lower than the MAE1.0152√ó10‚àí031.0152√ó10‚àí03from the quasi-Newton method. Similarly, forùí©=16N=16andùúå=0.8œÅ=0.8, the splitting method achieves an MAE of6.6953√ó10‚àí046.6953√ó10‚àí04, showing an improvement of about 25% compared to the quasi-Newton MAE8.9016√ó10‚àí048.9016√ó10‚àí04. We also computed the errors for different values of the parameter œâ for the splitting method, and the results help in identifying the optimal choice of œâ fromTable 7.Figure 8provides a two-dimensional error comparison of the methods, and additional figures (e.g.,Figure 9) show the impact of varyingùí©Nfor a fixed œÅ.Figure 8.Graphical comparison of MAEs for different vales ofùúåœÅfor Example 5.Figure 9.Estimated solutions of Example 5 using MQ-RBFs for various numbers of GPs atùë°=1t=1andùúå=0.7œÅ=0.7.Table 5.Error analysis for different values ofùúåœÅandùí©Nwithùúñ=2œµ=2,ùëëùë°=0.5dt=0.5, andùë°=1t=1for Example 5 using Caputo derivative and qausi-Newton linearization technique.Table 6.Error analysis for different values ofùúåœÅandùí©Nwithùúî=1œâ=1,ùúñ=2œµ=2,ùëëùë°=0.5dt=0.5, andùë°=1t=1for Example 5 using Caputo derivative and innovative splitting technique technique.Table 7.Error analysis for various values of the relaxation parameterùúîœâat fixedùëÅ=20N=20andùúñ=1œµ=1using the Caputo-based splitting technique, withùëëùóç=0.5dt=0.5andùóç=1t=1for test Problem 5. Example6.As the sixth example, nonlinear time-fractional interface Problem (2) is considered assumingùúÇ+(ùë•)=ùëí(‚àí10(ùë•‚àí0.5)4ùë•4)Œ∑+(x)=e(‚àí10(x‚àí0.5)4x4)andùúÇ‚àí(ùë•)=3Œ∑‚àí(x)=3. Exact solutions and source functions are given byùúë(ùë•,ùë°)=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùúë1(ùë•,ùë°)=sin5ùúãùë•ùëí‚àíùë°,ùúë2(ùë•,ùë°)=(2(ùë•‚àí12)7+1)ùëí‚àíùë°,0‚â§ùë•‚â§12,12‚â§ùë•‚â§1.œÜ(x,t)=œÜ1(x,t)=sin5œÄxe‚àít,0‚â§x‚â§12,œÜ2(x,t)=(2(x‚àí12)7+1)e‚àít,12‚â§x‚â§1.(45)andùëì(ùë•,ùë°)=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùëì1(ùë•,ùë°)=‚àíùë°1‚àíùúå‚Ñ≥1,2‚àíùúå(‚àíùë°)sin(5ùúãùë•)+15ùúãùëí‚àíùë°‚àí10ùë•4(ùë•‚àí12)4cos(5ùúãùë•)(40ùë•3(ùë•‚àí12)4+40ùë•4(ùë°‚àí12)3)‚àí(sin5ùúãùë•ùëí‚àíùë°)2‚àí2sin25ùúãùë•ùëí‚àí2ùë°,ùëì2(ùë•,ùë°)=‚àíùë°1‚àíùúå‚Ñ≥1,2‚àíùúå(‚àíùë°)(2(ùë•‚àí12)7+1)‚àí252ùëí(‚àíùë°)(ùë•‚àí12)5‚àí2(2(ùë•‚àí12)7+1)2ùëí‚àí2ùë°,0‚â§ùë•‚â§12,12‚â§ùë•‚â§1,f(x,t)=f1(x,t)=‚àít1‚àíœÅM1,2‚àíœÅ(‚àít)sin(5œÄx)+15œÄe‚àít‚àí10x4(x‚àí12)4cos(5œÄx)(40x3(x‚àí12)4+40x4(t‚àí12)3)‚àísin5œÄxe‚àít2‚àí2sin25œÄxe‚àí2t,0‚â§x‚â§12,f2(x,t)=‚àít1‚àíœÅM1,2‚àíœÅ(‚àít)2(x‚àí12)7+1‚àí252e(‚àít)(x‚àí12)5‚àí22x‚àí127+12e‚àí2t,12‚â§x‚â§1,(46)where‚Ñ≥ùëé,ùëè(¬∑)Ma,b(¬∑)is the Mittag‚ÄìLeffler function of two parameters defined as‚Ñ≥ùëé,ùëè(ùë°)=‚àëùëò=0‚àûùë°ùëòŒì(ùëéùëò+ùëè),‚àÄùë°‚àà‚ÑÇ,ùëé,ùëè‚àà‚Ñù+Ma,b(t)=‚àëk=0‚àûtkŒì(ak+b),‚àÄt‚ààC,a,b‚ààR+and‚Ñ≥1,1(ùë°)=exp(ùë°),‚àÄùë°‚àà‚ÑÇ.M1,1(t)=exp(t),‚àÄt‚ààC. To accurately define the interface, boundary, and initial conditions for this nonlinear 1D time-fractional interface problem, the exact analytical solution is employed as a reference. The MQ-RBFs, enhanced by a novel splitting approach to efficiently manage the nonlinearity, is utilized to obtain the numerical solution. Computational experiments, detailed inTable 8, explore the performance of the method across various fractional ordersùúåœÅand spatial resolutionsùí©N. The accuracy of the approach is quantitatively assessed using mean MAEs and RMSEs, both of which consistently decline as the grid becomes finer, indicating strong convergence behavior. While the method remains stable and reliable for all considered values ofùúåœÅ, lower fractional orders tend to yield higher errors, reflecting the intensified memory effects inherent to the fractional derivative. Overall, the results affirm the precision and robustness of the MQ-RBFs in addressing nonlinear time-fractional interface problems. InFigure 10we have plotted 2D plots for different values ofùúåœÅ, along with the corresponding errors. Figure 10.Exact versus estimate solution for Example 6 along with absolute error at different values ofùúåœÅ,ùí©=24,ùë°=1N=24,t=1. Table 8.Error analysis for different values ofùúåœÅandùí©Nwithùúî=1œâ=1,ùúñ=2œµ=2,ùëëùë°=0.5dt=0.5andùë°=1t=1for Example 6 using Caputo derivative and innovative splitting technique.",
            "7. Conclusions": "This study presents a new numerical method called MQ-RBF, developed to solve both linear and nonlinear time-fractional interface problems involving constant and variable coefficients. The method uses the Caputo definition of fractional derivatives, where the first-order time derivatives are approximated using a finite difference scheme. To effectively manage nonlinearity, an innovative splitting technique and the quasi-Newton linearization method are implemented. The innovative splitting technique produced more accurate and stable results than quasi-Newton linearization, particularly in capturing the dynamic behavior of the solution. This enhancement is due to the splitting technique‚Äôs superior capability in handling nonlinear interactions while preserving numerical accuracy. A key advantage of the MQ-RBF method is that it achieves high accuracy using only a small number of grid points, making it computationally efficient. The effectiveness of the MQ-RBF method is demonstrated through several benchmark test cases, which include both constant and variable coefficient problems. The results show that the method delivers highly accurate solutions, as measured by the MAEs and RMSEs with different fractional-order values (ùúåœÅ). Visualizations in 2D and 3D further illustrate how the solution evolves over time and with varyingùúåœÅ. In general, the proposed MQ-RBF method is shown to be accurate and efficient. Future work may explore extending it to two and three-dimensional time-fractional interface problems, including cases with single or multiple interfaces."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2297-8747/30/6/133",
        "scraped_at": "2025-12-05 23:51:42"
    },
    {
        "title": "The Energy Potential of Woody Vine Shoots Depending on the Training System, Cultivar, and Colour of the Fruit",
        "authors": "byRichard Danko,Radek Sotol√°≈ô,Mojmir Baro≈à,Magdalena Kap≈Çan,Kamila E. KlimekandGrzegorz Maj",
        "journal": "Agriculture2025,15(24), 2524;https://doi.org/10.3390/agriculture15242524- 5 Dec 2025",
        "abstract": "The aim of this study was to assess the energy potential of woody grapevine (Vitis viniferaL.) shoots depending on the cultivation system, cultivar, and fruit colour. Field studies were conducted in 2024 at the Mendel University Vineyard in Lednice (Czech Republic) on Chardonnay, Merlot, Riesling, and Zweigelt cultivars, cultivated using the Guyot and Cordon systems. The cultivar analysis covered both the amount of biomass produced during pruning and its energy and emission properties. Laboratory tests of the energy potential of the biomass obtained were carried out at the University of Life Sciences in Lublin. The results showed that the varietal factor significantly influenced the biomass parameters‚ÄîChardonnay was characterised by the highest total plant weight (773.57 g), while Zweigelt (8.60 pcs.) had the highest number of shoots with the lowest unit weight (74.82 g). The Cordon system generated significantly higher biomass yields and more favourable combustion properties compared to Guyot. Differences in fruit colour indicate that, among the studied cultivars, white-berried varieties produce heavier shoots, whereas red varieties produce a greater number of shoots. The analysis of gas emissions showed a significant influence of the cultivar and training system, with the highest CO, CO2, and NOx emissions recorded for the Zweigelt cultivar. The results emphasise that an integrated approach, taking into account both genotypic factors, training systems and phenotypic characteristics of the vines, is crucial for optimising the use of wine biomass as an energy source in the context of a circular economy.",
        "keywords": ":grapevine; biomass; energy potential; guyot; cordon;Vitis viniferavarieties; exhaust emissions; circular economygrapevine;biomass;energy potential;guyot;cordon;Vitis viniferavarieties;exhaust emissions;circular economy",
        "full_content": {
            "Abstract": "The aim of this study was to assess the energy potential of woody grapevine (Vitis viniferaL.) shoots depending on the cultivation system, cultivar, and fruit colour. Field studies were conducted in 2024 at the Mendel University Vineyard in Lednice (Czech Republic) on Chardonnay, Merlot, Riesling, and Zweigelt cultivars, cultivated using the Guyot and Cordon systems. The cultivar analysis covered both the amount of biomass produced during pruning and its energy and emission properties. Laboratory tests of the energy potential of the biomass obtained were carried out at the University of Life Sciences in Lublin. The results showed that the varietal factor significantly influenced the biomass parameters‚ÄîChardonnay was characterised by the highest total plant weight (773.57 g), while Zweigelt (8.60 pcs.) had the highest number of shoots with the lowest unit weight (74.82 g). The Cordon system generated significantly higher biomass yields and more favourable combustion properties compared to Guyot. Differences in fruit colour indicate that, among the studied cultivars, white-berried varieties produce heavier shoots, whereas red varieties produce a greater number of shoots. The analysis of gas emissions showed a significant influence of the cultivar and training system, with the highest CO, CO2, and NOx emissions recorded for the Zweigelt cultivar. The results emphasise that an integrated approach, taking into account both genotypic factors, training systems and phenotypic characteristics of the vines, is crucial for optimising the use of wine biomass as an energy source in the context of a circular economy. Keywords:grapevine; biomass; energy potential; guyot; cordon;Vitis viniferavarieties; exhaust emissions; circular economygrapevine;biomass;energy potential;guyot;cordon;Vitis viniferavarieties;exhaust emissions;circular economy",
            "Share and Cite": "MDPI and ACS StyleDanko, R.;                     Sotol√°≈ô, R.;                     Baro≈à, M.;                     Kap≈Çan, M.;                     Klimek, K.E.;                     Maj, G.    \n        The Energy Potential of Woody Vine Shoots Depending on the Training System, Cultivar, and Colour of the Fruit.Agriculture2025,15, 2524.\n    https://doi.org/10.3390/agriculture15242524AMA StyleDanko R,                                 Sotol√°≈ô R,                                 Baro≈à M,                                 Kap≈Çan M,                                 Klimek KE,                                 Maj G.        \n                The Energy Potential of Woody Vine Shoots Depending on the Training System, Cultivar, and Colour of the Fruit.Agriculture. 2025; 15(24):2524.\n        https://doi.org/10.3390/agriculture15242524Chicago/Turabian StyleDanko, Richard,                                 Radek Sotol√°≈ô,                                 Mojmir Baro≈à,                                 Magdalena Kap≈Çan,                                 Kamila E. Klimek,                                 and Grzegorz Maj.        \n                2025. \"The Energy Potential of Woody Vine Shoots Depending on the Training System, Cultivar, and Colour of the Fruit\"Agriculture15, no. 24: 2524.\n        https://doi.org/10.3390/agriculture15242524APA StyleDanko, R.,                                 Sotol√°≈ô, R.,                                 Baro≈à, M.,                                 Kap≈Çan, M.,                                 Klimek, K. E.,                                 & Maj, G.        \n        \n        (2025). The Energy Potential of Woody Vine Shoots Depending on the Training System, Cultivar, and Colour of the Fruit.Agriculture,15(24), 2524.\n        https://doi.org/10.3390/agriculture15242524 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further detailshere.",
            "Article Metrics": "YesCitationsNo citations were found for this article, but you may check onGoogle ScholarNoArticle metric data becomes available approximately 24 hours after publication online.",
            "Citations": "No citations were found for this article, but you may check onGoogle Scholar"
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2077-0472/15/24/2524",
        "scraped_at": "2025-12-05 23:51:49"
    },
    {
        "title": "A Single-Neuron-per-Class Readout for Image-Encoded Sensor Time Series",
        "authors": "byDavid Bernal-CasasandJaime Gallego",
        "journal": "Mathematics2025,13(24), 3893;https://doi.org/10.3390/math13243893- 5 Dec 2025",
        "abstract": "We introduce an ultra-compact, single-neuron-per-class end-to-end readout for binary classification of noisy, image-encoded sensor time series. The approach compares a linear single-unit perceptron (E2E-MLP-1) with a resonate-and-fire (RAF) neuron (E2E-RAF-1), which merges feature selection and decision-making in a single block. Beyond empirical evaluation, we provide a mathematical analysis of the RAF readout: starting from its subthreshold ordinary differential equation, we derive the transfer functionùêª(jùúî)H(jœâ), characterize the frequency response, and relate the output signal-to-noise ratio (SNR) to|ùêª(jùúî)|2|H(jœâ)|2and the noise power spectral densityùëÜùëõ(ùúî)‚àùùúîùõºSn(œâ)‚àùœâŒ±(brown, pink, and blue noise). We present a stable discrete-time implementation compatible with surrogate gradient training and discuss the associated stability constraints. As a case study, we classify walk-in-place (WIP) in a virtual reality (VR) environment, a vision-based motion encoding (72 √ó 56 grayscale) derived from 3D trajectories, comprising 44,084 samples from 15 participants. On clean data, both single-neuron-per-class models approach ceiling accuracy. At the same time, under colored noise, the RAF readout yields consistent gains (typically +5‚Äì8% absolute accuracy at medium/high perturbations), indicative of intrinsic band-selective filtering induced by resonance. With ‚àº8 k parameters and sub-2 ms inference on commodity graphical processing units (GPUs), the RAF readout provides a mathematically grounded, robust, and efficient alternative for stochastic signal processing across domains, with virtual reality locomotion used here as an illustrative validation.Keywords:end-to-end learning;single-neuron-per-class readout;neuromorphic computing;image-encoded time series;neural networks;spiking neural networks;resonate-and-fire (RAF) neuron;noisy environmentsMSC:68T07; 68U10; 62M10",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "End-to-end learning often relies on multi-layer architectures that separate input preprocessing, representation learning, and decision-making [1]. While effective, this separation, typical of models such as the multi-layer perceptron (MLP) [2], can result in substantial parameter counts and limited insight into where robustness emerges. We study a minimal alternative: a single-neuron-per-class readout that operates directly on high-dimensional, image-encoded sensor time series, integrating feature selection and classification into a single block. We instantiate this idea with two models: a linear single-unit perceptron (E2E-MLP-1) and a single-unit resonate-and-fire (E2E-RAF-1) neuron [3], a neuromorphic spiking model whose subthreshold dynamics implements a tunable, band-selective filter [4]. We develop a mathematical analysis of the RAF transfer function and its noise-shaping properties under colored noise, and we provide a stable discrete-time implementation suitable for training with surrogate gradients [5]. To illustrate the generality and performance of this approach, we present a case study that classifies walk-in-place (WIP) movements within a virtual reality (VR) environment. This classification is based on visual-style encodings of human motion data, a common technique for transforming sensor time series data for classification [6]. This validation is not the focus of the contribution but serves as a practical example showing that (i) single-neuron-per-class readouts can achieve near-ceiling accuracy on clean inputs, and (ii) the RAF dynamics confer robustness advantages in colored-noise regimes (pink, brown, and blue noise) that mimic real-world sensing artifacts [7,8]. The contribution is methodological and demonstrates that a single-neuron-per-class readout can effectively discriminate image-encoded time series while avoiding deep, layered models. The experimental section is thus positioned as a direct application example to illustrate the behavior and efficiency of the proposed readouts in a controlled setting. The paper is structured as follows:Section 2reviews related work.Section 3details the proposed architecture and experimental setup.Section 4describes the mathematical analysis of the RAF Readout.Section 5illustrates the generality and performance of the proposed approach on vision-style encodings of human motion data.Section 6presents the results.Section 7discusses findings, andSection 8concludes the paper.",
            "2. Background and Related Work": "This section presents a comprehensive review of the theoretical foundations that underpin our innovative approach of utilizing a single neuron per class for readout. We will explore conventional neural architectures, delve into neuromorphic spiking models, and analyze the noise characteristics present in sensor data, highlighting their crucial roles in the effectiveness of our methodology. 2.1. Conventional Neural ArchitecturesArtificial neural networks (ANNs) form the basis of modern machine learning, with multi-layer perceptrons (MLPs) serving as a foundational architecture that showcases the power of layered processing to unlock intricate patterns and insights in vast datasets [9]. MLPs perform hierarchical feature extraction through layers of computational units, applying a weighted sum followed by a non-linear activation function (e.g., ReLU) [2]. Their capacity to model complex functions is supported by universal approximation theorems [10]. However, their layered structure can be parameter-heavy, posing challenges for deployment in resource-constrained or embedded settings, which motivates the exploration of more compact models. 2.2. Neuromorphic Computing and Spiking ModelsSpiking neural networks (SNNs) offer an alternative paradigm inspired by neurobiology, leveraging event-driven processing that can lead to greater energy efficiency and robustness for dynamic signals [11]. The resonate-and-fire (RAF) neuron is a significant model governed by a second-order ordinary differential equation (ODE):ùëë2ùëâ(ùë°)ùëëùë°2+2ùúâùëëùëâ(ùë°)ùëëùë°+ùúî20ùëâ(ùë°)=ùêºùëñùëõ(ùë°)d2V(t)dt2+2ŒædV(t)dt+œâ02V(t)=Iin(t), whereVis the neuron membrane potential,ùúâŒæis the damping factor,ùúî0œâ0is the angular frequency, andùêºùëñùëõ(ùë°)Iin(t)is the input current [3]. This dynamic model exhibits intrinsic frequency selectivity, making it particularly suitable for processing time-varying inputs. Despite the non-differentiable nature of spike generation, such models can be trained effectively within deep learning frameworks using surrogate gradients to approximate the derivative of the spiking mechanism [5]. 2.3. Noise Models in SensingSensor data in real-world applications is rarely corrupted solely by simple white noise. Instead, noise often exhibits temporal correlations, leading to power spectral densities that follow a power-law distribution,ùëÜùëõ(ùúî)‚àùùúîùõºSn(œâ)‚àùœâŒ±[7]. Common examples include brown noise (ùõº=‚àí2Œ±=‚àí2or‚àùùúî‚àí2‚àùœâ‚àí2), which models processes like random walks or sensor drift; pink noise (ùõº=‚àí1Œ±=‚àí1or‚àùùúî‚àí1‚àùœâ‚àí1), prevalent in many natural systems, and blue noise (ùõº=1Œ±=1or‚àùùúî‚àùœâ), representing high-frequency jitter [12].Testing the performance of WIP in VR with colored noise is essential for several reasons:Realistic environmental conditions.Colored noise (such as pink, brown, or other non-white noise spectra) more accurately simulates real-world environments. Evaluating WIP performance under these conditions ensures that the system functions reliably in typical user scenarios.Assessing robustness and reliability.Background noise can interfere with motion tracking and user perception. Testing under colored noise helps assess how well the system maintains accurate, consistent locomotion, ensuring users do not experience disorientation or motion sickness from tracking errors exacerbated by noisy environments.Optimizing system performance.By analyzing how colored noise affects WIP performance, developers can refine motion-detection algorithms, improve noise-filtering techniques, and enhance overall system robustness. This leads to more seamless and immersive VR experiences.User experience and comfort.Ensuring that walk-in-place methods perform well under various auditory conditions helps prevent user frustration or discomfort, making VR systems more accessible and enjoyable across diverse environments.Robust classifiers benefit from frequency-selective mechanisms that can align with signal-dominated frequency bands while attenuating off-band perturbations, a property central to our investigation of the RAF neuron. 2.4. Frequency-Awareness and Compact ArchitecturesWhile deep learning has trended towards increasingly complex architectures, parallel efforts in ‚ÄúTinyML‚Äù focus on reducing computational overhead via model pruning, quantization, and efficient architectures such as MobileNet and SqueezeNet [13]. However, these lightweight CNNs still rely on layered hierarchies, as in foundational architectures such as LeNet-5 [14]. Simultaneously, recent research has highlighted the importance of spectral biases in neural networks, exploring frequency-domain learning and generative adversarial networks (GANs) with frequency-awareness to handle structural anomalies and noise [15,16].In this landscape, our single-neuron readouts are intentionally minimal. Unlike deep compact models, the proposed E2E-MLP-1 provides a flat frequency response, whereas the RAF neuron implements a tunable, band-selective filter through its second-order dynamics. This distinction anticipates our findings under colored noise, where spectral shaping is central. We position the proposed readouts against lightweight CNNs by reporting parameter/latency trade-offs and by discussing when hierarchical features (convolutions) are strictly necessary versus when a bio-inspired, band-selective readout suffices for robust classification.",
            "2.1. Conventional Neural Architectures": "Artificial neural networks (ANNs) form the basis of modern machine learning, with multi-layer perceptrons (MLPs) serving as a foundational architecture that showcases the power of layered processing to unlock intricate patterns and insights in vast datasets [9]. MLPs perform hierarchical feature extraction through layers of computational units, applying a weighted sum followed by a non-linear activation function (e.g., ReLU) [2]. Their capacity to model complex functions is supported by universal approximation theorems [10]. However, their layered structure can be parameter-heavy, posing challenges for deployment in resource-constrained or embedded settings, which motivates the exploration of more compact models.",
            "2.2. Neuromorphic Computing and Spiking Models": "Spiking neural networks (SNNs) offer an alternative paradigm inspired by neurobiology, leveraging event-driven processing that can lead to greater energy efficiency and robustness for dynamic signals [11]. The resonate-and-fire (RAF) neuron is a significant model governed by a second-order ordinary differential equation (ODE):ùëë2ùëâ(ùë°)ùëëùë°2+2ùúâùëëùëâ(ùë°)ùëëùë°+ùúî20ùëâ(ùë°)=ùêºùëñùëõ(ùë°)d2V(t)dt2+2ŒædV(t)dt+œâ02V(t)=Iin(t), whereVis the neuron membrane potential,ùúâŒæis the damping factor,ùúî0œâ0is the angular frequency, andùêºùëñùëõ(ùë°)Iin(t)is the input current [3]. This dynamic model exhibits intrinsic frequency selectivity, making it particularly suitable for processing time-varying inputs. Despite the non-differentiable nature of spike generation, such models can be trained effectively within deep learning frameworks using surrogate gradients to approximate the derivative of the spiking mechanism [5].",
            "2.3. Noise Models in Sensing": "Sensor data in real-world applications is rarely corrupted solely by simple white noise. Instead, noise often exhibits temporal correlations, leading to power spectral densities that follow a power-law distribution,ùëÜùëõ(ùúî)‚àùùúîùõºSn(œâ)‚àùœâŒ±[7]. Common examples include brown noise (ùõº=‚àí2Œ±=‚àí2or‚àùùúî‚àí2‚àùœâ‚àí2), which models processes like random walks or sensor drift; pink noise (ùõº=‚àí1Œ±=‚àí1or‚àùùúî‚àí1‚àùœâ‚àí1), prevalent in many natural systems, and blue noise (ùõº=1Œ±=1or‚àùùúî‚àùœâ), representing high-frequency jitter [12]. Testing the performance of WIP in VR with colored noise is essential for several reasons: Realistic environmental conditions.Colored noise (such as pink, brown, or other non-white noise spectra) more accurately simulates real-world environments. Evaluating WIP performance under these conditions ensures that the system functions reliably in typical user scenarios.Assessing robustness and reliability.Background noise can interfere with motion tracking and user perception. Testing under colored noise helps assess how well the system maintains accurate, consistent locomotion, ensuring users do not experience disorientation or motion sickness from tracking errors exacerbated by noisy environments.Optimizing system performance.By analyzing how colored noise affects WIP performance, developers can refine motion-detection algorithms, improve noise-filtering techniques, and enhance overall system robustness. This leads to more seamless and immersive VR experiences.User experience and comfort.Ensuring that walk-in-place methods perform well under various auditory conditions helps prevent user frustration or discomfort, making VR systems more accessible and enjoyable across diverse environments. Robust classifiers benefit from frequency-selective mechanisms that can align with signal-dominated frequency bands while attenuating off-band perturbations, a property central to our investigation of the RAF neuron.",
            "2.4. Frequency-Awareness and Compact Architectures": "While deep learning has trended towards increasingly complex architectures, parallel efforts in ‚ÄúTinyML‚Äù focus on reducing computational overhead via model pruning, quantization, and efficient architectures such as MobileNet and SqueezeNet [13]. However, these lightweight CNNs still rely on layered hierarchies, as in foundational architectures such as LeNet-5 [14]. Simultaneously, recent research has highlighted the importance of spectral biases in neural networks, exploring frequency-domain learning and generative adversarial networks (GANs) with frequency-awareness to handle structural anomalies and noise [15,16]. In this landscape, our single-neuron readouts are intentionally minimal. Unlike deep compact models, the proposed E2E-MLP-1 provides a flat frequency response, whereas the RAF neuron implements a tunable, band-selective filter through its second-order dynamics. This distinction anticipates our findings under colored noise, where spectral shaping is central. We position the proposed readouts against lightweight CNNs by reporting parameter/latency trade-offs and by discussing when hierarchical features (convolutions) are strictly necessary versus when a bio-inspired, band-selective readout suffices for robust classification.",
            "3. Methods: A Single-Neuron-per-Class End-to-End Readout": "We propose a method for classifying 2D grayscale images (in our case study, 72 √ó 56, corresponding to 4032 pixels) that encode short windows of sensor time series. The core of the method is a single-neuron-per-class readout that integrates feature selection and classification. 3.1. Single-Unit ReadoutsE2E-MLP-1 (Linear + threshold).Given a flattened input vectorùê±‚àà‚Ñù4032x‚ààR4032, this model computes a logitùëß=ùê∞‚ä§ùê±+ùëèz=w‚ä§x+b. The final binary decision is obtained by applying a threshold, implicitly handled by the logistic loss function during training.E2E-RAF-1 (Resonate-and-fire).This model is a single Resonate-and-Fire (RAF) neuron [3] whose subthreshold membrane potentialVobeys the second-order ordinary differential equation:ùëë2ùëâ(ùë°)ùëëùë°2+2ùúâùëëùëâ(ùë°)ùëëùë°+ùúî20ùëâ(ùë°)=ùêºin(ùë°),d2V(t)dt2+2ŒædV(t)dt+œâ02V(t)=Iin(t),(1)with a learnable damping ratioùúâ>0Œæ>0, angular frequencyùúî0>0œâ0>0, and an input driveùêºin(ùë°)Iin(t)proportional to the weighted input. Spikes are generated whenVcrosses a learnable thresholdùëâthVth. The final decision is based on the spike count or firing rate over a discrete simulation window. All model parameters, including the neuron‚Äôs intrinsic parameters(ùúâ,ùúî0,ùëâth)(Œæ,œâ0,Vth)and the input weightsùê∞wthat determineùêºinIin, are trained end-to-end using surrogate gradients to handle the non-differentiable spiking event [5]. 3.2. Input HandlingInput images are converted to one-dimensional vectors, with each pixel‚Äôs intensity scaled to[0,1][0,1]. This normalization process ensures that the values are standardized, facilitating more effective analysis and processing in various computational tasks. For the E2E-RAF-1 neuron, these analog intensity values are converted into spike trains. We used Poisson-distributed spike trains where the firing rate of each input neuron is proportional to the corresponding pixel intensity [17]. 3.3. Training ProtocolModels were trained using the Adam optimizer [18] with a binary cross-entropy loss function on the logits (BCEWithLogitsLoss). We used a batch size of 512 and trained for up to 150 epochs, with an early stopping mechanism that terminated training if the validation loss did not improve for 20 consecutive epochs. The initial learning rate was set to10‚àí310‚àí3and was reduced by a factor of 0.9 if the validation loss plateaued (with a minimum learning rate of10‚àí1410‚àí14). To ensure robust results, all experiments were repeated with at least 5 different random seeds. Models were implemented in PyTorch 1.12.1 and trained on an NVIDIA Quadro RTX 6000 GPU. 3.4. Robustness Stress TestsTo rigorously evaluate model robustness, we conducted stress tests by adding colored noise to the clean test set inputs. We used three types of noise with distinct spectral properties:Brown Noise:Power spectral density (PSD) proportional toùúî‚àí2œâ‚àí2;Pink Noise:PSD proportional toùúî‚àí1œâ‚àí1;Blue Noise:PSD proportional toùúîœâ.The noise was generated in the frequency domain by shaping Gaussian white noise to the target PSD and then transforming it to the input domain via an inverse Fast Fourier Transform (FFT). We systematically varied the noise intensity by sweeping the noise standard deviation from 0 to4.04.0in steps of0.50.5. Final performance metrics are reported as the mean and standard deviation across multiple seeds. To assess statistical significance between model predictions under noise, we use McNemar‚Äôs test, which is appropriate for comparing paired categorical data.",
            "3.1. Single-Unit Readouts": "E2E-MLP-1 (Linear + threshold).Given a flattened input vectorùê±‚àà‚Ñù4032x‚ààR4032, this model computes a logitùëß=ùê∞‚ä§ùê±+ùëèz=w‚ä§x+b. The final binary decision is obtained by applying a threshold, implicitly handled by the logistic loss function during training.E2E-RAF-1 (Resonate-and-fire).This model is a single Resonate-and-Fire (RAF) neuron [3] whose subthreshold membrane potentialVobeys the second-order ordinary differential equation:ùëë2ùëâ(ùë°)ùëëùë°2+2ùúâùëëùëâ(ùë°)ùëëùë°+ùúî20ùëâ(ùë°)=ùêºin(ùë°),d2V(t)dt2+2ŒædV(t)dt+œâ02V(t)=Iin(t),(1)with a learnable damping ratioùúâ>0Œæ>0, angular frequencyùúî0>0œâ0>0, and an input driveùêºin(ùë°)Iin(t)proportional to the weighted input. Spikes are generated whenVcrosses a learnable thresholdùëâthVth. The final decision is based on the spike count or firing rate over a discrete simulation window. All model parameters, including the neuron‚Äôs intrinsic parameters(ùúâ,ùúî0,ùëâth)(Œæ,œâ0,Vth)and the input weightsùê∞wthat determineùêºinIin, are trained end-to-end using surrogate gradients to handle the non-differentiable spiking event [5].",
            "3.2. Input Handling": "Input images are converted to one-dimensional vectors, with each pixel‚Äôs intensity scaled to[0,1][0,1]. This normalization process ensures that the values are standardized, facilitating more effective analysis and processing in various computational tasks. For the E2E-RAF-1 neuron, these analog intensity values are converted into spike trains. We used Poisson-distributed spike trains where the firing rate of each input neuron is proportional to the corresponding pixel intensity [17].",
            "3.3. Training Protocol": "Models were trained using the Adam optimizer [18] with a binary cross-entropy loss function on the logits (BCEWithLogitsLoss). We used a batch size of 512 and trained for up to 150 epochs, with an early stopping mechanism that terminated training if the validation loss did not improve for 20 consecutive epochs. The initial learning rate was set to10‚àí310‚àí3and was reduced by a factor of 0.9 if the validation loss plateaued (with a minimum learning rate of10‚àí1410‚àí14). To ensure robust results, all experiments were repeated with at least 5 different random seeds. Models were implemented in PyTorch 1.12.1 and trained on an NVIDIA Quadro RTX 6000 GPU.",
            "3.4. Robustness Stress Tests": "To rigorously evaluate model robustness, we conducted stress tests by adding colored noise to the clean test set inputs. We used three types of noise with distinct spectral properties: Brown Noise:Power spectral density (PSD) proportional toùúî‚àí2œâ‚àí2;Pink Noise:PSD proportional toùúî‚àí1œâ‚àí1;Blue Noise:PSD proportional toùúîœâ. The noise was generated in the frequency domain by shaping Gaussian white noise to the target PSD and then transforming it to the input domain via an inverse Fast Fourier Transform (FFT). We systematically varied the noise intensity by sweeping the noise standard deviation from 0 to4.04.0in steps of0.50.5. Final performance metrics are reported as the mean and standard deviation across multiple seeds. To assess statistical significance between model predictions under noise, we use McNemar‚Äôs test, which is appropriate for comparing paired categorical data.",
            "4. Mathematical Analysis of the RAF Readout": "In this section, we delve into the continuous-time representation of the subthreshold resonate-and-fire (RAF) dynamics, modeling it as a linear time-invariant (LTI) second-order system. By analyzing its transfer function and frequency response, we uncover the conditions under which the system exhibits resonance and how this influences signal processing capabilities. We further explore how the resonant properties can be harnessed to improve the signal-to-noise ratio (SNR) in noisy environments, particularly by tuning system parameters to favor task-relevant frequencies. Finally, practical considerations for discretizing the dynamics for training and inference are discussed, alongside alternative implementations and the transition from subthreshold filtering to classification. 4.1. Continuous-Time Dynamics and Transfer FunctionWe consider the subthreshold RAF dynamics as a linear time-invariant (LTI) second-order system driven by an input currentùêºin(ùë°)Iin(t):ùëë2ùëâ(ùë°)ùëëùë°2+2ùúâùëëùëâ(ùë°)ùëëùë°+ùúî20ùëâ(ùë°)=ùêºin(ùë°),d2V(t)dt2+2ŒædV(t)dt+œâ02V(t)=Iin(t),(2)with damping ratioùúâ>0Œæ>0and angular frequencyùúî0>0œâ0>0. In the frequency domain (ùë†=jùúîs=jœâ), the transfer function from inputIto membrane potentialVisùêª(jùúî)=ùëâ(jùúî)ùêº(jùúî)=1‚àíùúî2+j2ùúâùúî+ùúî20.H(jœâ)=V(jœâ)I(jœâ)=1‚àíœâ2+j2Œæœâ+œâ02.(3)Thus, the power gain is|ùêª(jùúî)|2=1(ùúî20‚àíùúî2)2+(2ùúâùúî)2.|H(jœâ)|2=1(œâ02‚àíœâ2)2+(2Œæœâ)2.(4)The resonance peak corresponds to the maximum of|ùêª(ùëóùúî)|2|H(jœâ)|2with respect toùúîœâ. Since the denominator determines the magnitude, the peak occurs at the frequency where the denominator is minimizedùê∑(ùúî)=(ùúî20‚àíùúî2)2+(2ùúâùúî)2.D(œâ)=(œâ02‚àíœâ2)2+(2Œæœâ)2.To findùúî‚òÖœâ‚òÖ, we set the derivative ofùê∑(ùúî)D(œâ)with respect toùúîœâto zeroùëëùê∑ùëëùúî=0.dDdœâ=0.We obtain the following equationùúî[‚àí4(ùúî20‚àíùúî2)+8ùúâ2]=0.œâ‚àí4(œâ02‚àíœâ2)+8Œæ2=0.This yields two solutionsOn the one hand, the trivial solution, i.e.,ùúî‚òÖ=0œâ‚òÖ=0.On the other hand, the non-trivial solution where the bracketed term is zero‚àí4(ùúî20‚àíùúî2)+8ùúâ2=0.‚àí4(œâ02‚àíœâ2)+8Œæ2=0.Solving forùúîœâ, we obtain the resonance frequencyùúî‚òÖ=ùúî01‚àí2ùúâ2‚àí‚àí‚àí‚àí‚àí‚àí‚àö.œâ‚òÖ=œâ01‚àí2Œæ2.For the resonance peak to exist at a real frequencyùúî‚òÖœâ‚òÖ, the expression under the square root must be positive1‚àí2ùúâ2>0‚áíùúâ<12‚àí‚àí‚àö.1‚àí2Œæ2>0‚áíŒæ<12.In summary, the power gain exhibits a resonance with a peak atùúî‚òÖ=ùúî01‚àí2ùúâ2‚àí‚àí‚àí‚àí‚àí‚àí‚àöœâ‚òÖ=œâ01‚àí2Œæ2, considering thatùúâ<1/2‚àí‚àí‚àöŒæ<1/2. 4.2. Noise Shaping and Output Signal-to-Noise Ratio (SNR)Let the signal component have a PSD denoted byùëÜùë†(ùúî)Ss(œâ)and the additive noise PSD beùëÜùëõ(ùúî)=ùëêùúîùõºSn(œâ)=cœâŒ±, withùõº‚àà{‚àí2,‚àí1,1}Œ±‚àà{‚àí2,‚àí1,1}modeling brown (ùõº=‚àí2Œ±=‚àí2), pink (ùõº=‚àí1Œ±=‚àí1), and blue (ùõº=1Œ±=1) noise, respectively, up to a proportionality constantc.The output SNR induced by the subthreshold dynamics isSNRout=‚à´‚àû0|ùêª(jùúî)|2ùëÜùë†(ùúî)ùëëùúî‚à´‚àû0|ùêª(jùúî)|2ùëÜùëõ(ùúî)ùëëùúî.SNRout=‚à´0‚àû|H(jœâ)|2Ss(œâ)dœâ‚à´0‚àû|H(jœâ)|2Sn(œâ)dœâ.(5)Assume thatùëÜùë†(ùúî)Ss(œâ)is band-limited and peaks aroundùúîùë†œâsandùëÜùëõ(ùúî)=ùëêùúîùõºSn(œâ)=cœâŒ±forùõº‚àà{‚àí2,‚àí1,1}Œ±‚àà{‚àí2,‚àí1,1}. Considering thatùúâ<1/2‚àí‚àí‚àöŒæ<1/2, there exists a valueùúî0œâ0in a neighborhood ofùúîùë†œâsthat maximizesSNRoutSNRoutin Equation (5). In this way, approximatingùëÜùë†Sslocally by its value atùúîùë†œâsand using Laplace‚Äôs method yields‚à´|ùêª|2ùëÜùë†(ùúî)ùëëùúî‚âàùëÜùë†(ùúîùë†)‚à´|ùêª|2ùëëùúî‚à´|H|2Ss(œâ)dœâ‚âàSs(œâs)‚à´|H|2dœâ, which is maximized whenùúîùë†=ùúî‚òÖœâs=œâ‚òÖ, and, therefore,ùúî0=ùúîùë†/1‚àí2ùúâ2‚àí‚àí‚àí‚àí‚àí‚àí‚àöœâ0=œâs/1‚àí2Œæ2. The noise term‚à´|ùêª|2ùúîùõºùëëùúî‚à´|H|2œâŒ±dœâdecreases as the peak narrows and as the peak moves toward frequencies whereùúîùõºœâŒ±is smaller. Forùõº=‚àí2Œ±=‚àí2(brown), noise power is higher at lowùúîœâ; forùõº=‚àí1Œ±=‚àí1(pink), it still favors low frequencies but less steeply; forùõº=+1Œ±=+1(blue), it increases withùúîœâ. Tuningùúî0œâ0near the signal bandùúîùë†œâsreduces the overlap with regions of larger noise power, and the relative advantage over a flat filter grows.In essence, the RAF acts as a frequency selector by tuning(ùúâ,ùúî0)(Œæ,œâ0)so that|ùêª(jùúî)|2|H(jœâ)|2concentrates around the dominant signal frequency; the denominator in Equation (5) is suppressed whenùëÜùëõ(ùúî)Sn(œâ)puts relatively more noise outside that band. 4.3. Discrete-Time Implementation for Training and InferenceFor learning and deployment, we used a first-order state-space discretization, in particular, the explicit Euler method. We also implemented the semi-implicit Euler and the Runge-Kutta 4 method, but the differences in results were not statistically significant. Using the explicit Euler method, we may defineùë•1(ùë°)=ùëâ(ùë°)x1(t)=V(t)andùë•2(ùë°)=ùëâÀô(ùë°)x2(t)=VÀô(t). From Equation (2) follows:ùë•Àô1=ùë•2,ùë•Àô2=‚àí2ùúâùë•2‚àíùúî20ùë•1+ùêºin(ùë°).xÀô1=x2,xÀô2=‚àí2Œæx2‚àíœâ02x1+Iin(t).(6)Now, using a sampling stepŒîùë°Œît, we obtain the next discrete equations:ùë•ùëò+11=ùë•ùëò1+Œîùë°ùë•ùëò2,x1k+1=x1k+Œîtx2k,(7)ùë•ùëò+12=ùë•ùëò2+Œîùë°(‚àí2ùúâùë•ùëò2‚àíùúî20ùë•ùëò1+ùêºùëòin).x2k+1=x2k+Œît‚àí2Œæx2k‚àíœâ02x1k+Iink.(8)To ensure stability, we selectŒîùë°Œîtsuch thatŒîùë°ùúî0‚â™1Œîtœâ0‚â™1, which is a criterion for properly sampling the harmonic oscillator. We also utilize the same time grid as the spike window. The spike readout uses a hard thresholdùëâthVth:ifùë•ùëò+11‚â•ùëâththenemitspike.ifx1k+1‚â•Vththenemitspike.During training we use a standard surrogate-gradientùúéùõΩ(ùëß)=11+exp(‚àíùõΩùëß)œÉŒ≤(z)=11+exp(‚àíŒ≤z)around the threshold with slopeùõΩŒ≤. 4.4. Alternative IIR (Bilinear) FormThe Equation (2) also admits an IIR realization via the bilinear transformùë†=2Œîùë°1‚àíùëß‚àí11+ùëß‚àí1s=2Œît1‚àíz‚àí11+z‚àí1, yielding:ùêª(ùëß)=ùëè0+ùëè1ùëß‚àí1+ùëè2ùëß‚àí21+ùëé1ùëß‚àí1+ùëé2ùëß‚àí2,H(z)=b0+b1z‚àí1+b2z‚àí21+a1z‚àí1+a2z‚àí2,with coefficients obtained by substitutingsin Equation (3) and algebraic normalization. We preferred Equations (7) and (8) for transparency and compatibility with backpropagation through time. 4.5. From Subthreshold Filtering to DecisionsLet the spike count in aT-step window beùëÅùëá=‚àëùëáùëò=1ùüè[ùë•ùëò1‚â•ùëâth]NT=‚àëk=1T1[x1k‚â•Vth]. The classifier output is either the firing rateùëü=ùëÅùëá/ùëár=NT/Tor a post-threshold logistic score. Under mild regularity,rincreases with the band-power of the filtered signal, thus implementing a band-selective energy detector. This explains the improved robustness under colored noise.",
            "4.1. Continuous-Time Dynamics and Transfer Function": "We consider the subthreshold RAF dynamics as a linear time-invariant (LTI) second-order system driven by an input currentùêºin(ùë°)Iin(t):ùëë2ùëâ(ùë°)ùëëùë°2+2ùúâùëëùëâ(ùë°)ùëëùë°+ùúî20ùëâ(ùë°)=ùêºin(ùë°),d2V(t)dt2+2ŒædV(t)dt+œâ02V(t)=Iin(t),(2)with damping ratioùúâ>0Œæ>0and angular frequencyùúî0>0œâ0>0. In the frequency domain (ùë†=jùúîs=jœâ), the transfer function from inputIto membrane potentialVisùêª(jùúî)=ùëâ(jùúî)ùêº(jùúî)=1‚àíùúî2+j2ùúâùúî+ùúî20.H(jœâ)=V(jœâ)I(jœâ)=1‚àíœâ2+j2Œæœâ+œâ02.(3) Thus, the power gain is|ùêª(jùúî)|2=1(ùúî20‚àíùúî2)2+(2ùúâùúî)2.|H(jœâ)|2=1(œâ02‚àíœâ2)2+(2Œæœâ)2.(4) The resonance peak corresponds to the maximum of|ùêª(ùëóùúî)|2|H(jœâ)|2with respect toùúîœâ. Since the denominator determines the magnitude, the peak occurs at the frequency where the denominator is minimizedùê∑(ùúî)=(ùúî20‚àíùúî2)2+(2ùúâùúî)2.D(œâ)=(œâ02‚àíœâ2)2+(2Œæœâ)2. To findùúî‚òÖœâ‚òÖ, we set the derivative ofùê∑(ùúî)D(œâ)with respect toùúîœâto zeroùëëùê∑ùëëùúî=0.dDdœâ=0. We obtain the following equationùúî[‚àí4(ùúî20‚àíùúî2)+8ùúâ2]=0.œâ‚àí4(œâ02‚àíœâ2)+8Œæ2=0. This yields two solutions On the one hand, the trivial solution, i.e.,ùúî‚òÖ=0œâ‚òÖ=0.On the other hand, the non-trivial solution where the bracketed term is zero‚àí4(ùúî20‚àíùúî2)+8ùúâ2=0.‚àí4(œâ02‚àíœâ2)+8Œæ2=0.Solving forùúîœâ, we obtain the resonance frequencyùúî‚òÖ=ùúî01‚àí2ùúâ2‚àí‚àí‚àí‚àí‚àí‚àí‚àö.œâ‚òÖ=œâ01‚àí2Œæ2. For the resonance peak to exist at a real frequencyùúî‚òÖœâ‚òÖ, the expression under the square root must be positive1‚àí2ùúâ2>0‚áíùúâ<12‚àí‚àí‚àö.1‚àí2Œæ2>0‚áíŒæ<12. In summary, the power gain exhibits a resonance with a peak atùúî‚òÖ=ùúî01‚àí2ùúâ2‚àí‚àí‚àí‚àí‚àí‚àí‚àöœâ‚òÖ=œâ01‚àí2Œæ2, considering thatùúâ<1/2‚àí‚àí‚àöŒæ<1/2.",
            "4.2. Noise Shaping and Output Signal-to-Noise Ratio (SNR)": "Let the signal component have a PSD denoted byùëÜùë†(ùúî)Ss(œâ)and the additive noise PSD beùëÜùëõ(ùúî)=ùëêùúîùõºSn(œâ)=cœâŒ±, withùõº‚àà{‚àí2,‚àí1,1}Œ±‚àà{‚àí2,‚àí1,1}modeling brown (ùõº=‚àí2Œ±=‚àí2), pink (ùõº=‚àí1Œ±=‚àí1), and blue (ùõº=1Œ±=1) noise, respectively, up to a proportionality constantc. The output SNR induced by the subthreshold dynamics isSNRout=‚à´‚àû0|ùêª(jùúî)|2ùëÜùë†(ùúî)ùëëùúî‚à´‚àû0|ùêª(jùúî)|2ùëÜùëõ(ùúî)ùëëùúî.SNRout=‚à´0‚àû|H(jœâ)|2Ss(œâ)dœâ‚à´0‚àû|H(jœâ)|2Sn(œâ)dœâ.(5) Assume thatùëÜùë†(ùúî)Ss(œâ)is band-limited and peaks aroundùúîùë†œâsandùëÜùëõ(ùúî)=ùëêùúîùõºSn(œâ)=cœâŒ±forùõº‚àà{‚àí2,‚àí1,1}Œ±‚àà{‚àí2,‚àí1,1}. Considering thatùúâ<1/2‚àí‚àí‚àöŒæ<1/2, there exists a valueùúî0œâ0in a neighborhood ofùúîùë†œâsthat maximizesSNRoutSNRoutin Equation (5). In this way, approximatingùëÜùë†Sslocally by its value atùúîùë†œâsand using Laplace‚Äôs method yields‚à´|ùêª|2ùëÜùë†(ùúî)ùëëùúî‚âàùëÜùë†(ùúîùë†)‚à´|ùêª|2ùëëùúî‚à´|H|2Ss(œâ)dœâ‚âàSs(œâs)‚à´|H|2dœâ, which is maximized whenùúîùë†=ùúî‚òÖœâs=œâ‚òÖ, and, therefore,ùúî0=ùúîùë†/1‚àí2ùúâ2‚àí‚àí‚àí‚àí‚àí‚àí‚àöœâ0=œâs/1‚àí2Œæ2. The noise term‚à´|ùêª|2ùúîùõºùëëùúî‚à´|H|2œâŒ±dœâdecreases as the peak narrows and as the peak moves toward frequencies whereùúîùõºœâŒ±is smaller. Forùõº=‚àí2Œ±=‚àí2(brown), noise power is higher at lowùúîœâ; forùõº=‚àí1Œ±=‚àí1(pink), it still favors low frequencies but less steeply; forùõº=+1Œ±=+1(blue), it increases withùúîœâ. Tuningùúî0œâ0near the signal bandùúîùë†œâsreduces the overlap with regions of larger noise power, and the relative advantage over a flat filter grows. In essence, the RAF acts as a frequency selector by tuning(ùúâ,ùúî0)(Œæ,œâ0)so that|ùêª(jùúî)|2|H(jœâ)|2concentrates around the dominant signal frequency; the denominator in Equation (5) is suppressed whenùëÜùëõ(ùúî)Sn(œâ)puts relatively more noise outside that band.",
            "4.3. Discrete-Time Implementation for Training and Inference": "For learning and deployment, we used a first-order state-space discretization, in particular, the explicit Euler method. We also implemented the semi-implicit Euler and the Runge-Kutta 4 method, but the differences in results were not statistically significant. Using the explicit Euler method, we may defineùë•1(ùë°)=ùëâ(ùë°)x1(t)=V(t)andùë•2(ùë°)=ùëâÀô(ùë°)x2(t)=VÀô(t). From Equation (2) follows:ùë•Àô1=ùë•2,ùë•Àô2=‚àí2ùúâùë•2‚àíùúî20ùë•1+ùêºin(ùë°).xÀô1=x2,xÀô2=‚àí2Œæx2‚àíœâ02x1+Iin(t).(6) Now, using a sampling stepŒîùë°Œît, we obtain the next discrete equations:ùë•ùëò+11=ùë•ùëò1+Œîùë°ùë•ùëò2,x1k+1=x1k+Œîtx2k,(7)ùë•ùëò+12=ùë•ùëò2+Œîùë°(‚àí2ùúâùë•ùëò2‚àíùúî20ùë•ùëò1+ùêºùëòin).x2k+1=x2k+Œît‚àí2Œæx2k‚àíœâ02x1k+Iink.(8) To ensure stability, we selectŒîùë°Œîtsuch thatŒîùë°ùúî0‚â™1Œîtœâ0‚â™1, which is a criterion for properly sampling the harmonic oscillator. We also utilize the same time grid as the spike window. The spike readout uses a hard thresholdùëâthVth:ifùë•ùëò+11‚â•ùëâththenemitspike.ifx1k+1‚â•Vththenemitspike. During training we use a standard surrogate-gradientùúéùõΩ(ùëß)=11+exp(‚àíùõΩùëß)œÉŒ≤(z)=11+exp(‚àíŒ≤z)around the threshold with slopeùõΩŒ≤.",
            "4.4. Alternative IIR (Bilinear) Form": "The Equation (2) also admits an IIR realization via the bilinear transformùë†=2Œîùë°1‚àíùëß‚àí11+ùëß‚àí1s=2Œît1‚àíz‚àí11+z‚àí1, yielding:ùêª(ùëß)=ùëè0+ùëè1ùëß‚àí1+ùëè2ùëß‚àí21+ùëé1ùëß‚àí1+ùëé2ùëß‚àí2,H(z)=b0+b1z‚àí1+b2z‚àí21+a1z‚àí1+a2z‚àí2,with coefficients obtained by substitutingsin Equation (3) and algebraic normalization. We preferred Equations (7) and (8) for transparency and compatibility with backpropagation through time.",
            "4.5. From Subthreshold Filtering to Decisions": "Let the spike count in aT-step window beùëÅùëá=‚àëùëáùëò=1ùüè[ùë•ùëò1‚â•ùëâth]NT=‚àëk=1T1[x1k‚â•Vth]. The classifier output is either the firing rateùëü=ùëÅùëá/ùëár=NT/Tor a post-threshold logistic score. Under mild regularity,rincreases with the band-power of the filtered signal, thus implementing a band-selective energy detector. This explains the improved robustness under colored noise.",
            "5. Case Study: Vision-Based Motion Encodings": "This section provides a direct application example to illustrate the behavior of the proposed readouts on a realistic binary task. 3D motion trajectories from a head-mounted display and hand controllers are projected onto two planes and accumulated over a short temporal buffer to yield 72 √ó 56 grayscale images (4032 pixels). The dataset comprises 44,084 samples from 15 participants, with subject-independent splits to avoid data leakage. 5.1. Acquisition and Image EncodingData were recorded using a consumer-grade virtual reality headset (Meta Quest 3) with inside-out tracking at a sampling rate of 90 Hz [19]. Each image sample encodes approximately 1.1 s of motion via a temporal buffer ofùëÄ=100M=100frames. The 3D coordinates are first projected onto the frontal (XY) and depth (YZ) planes. Unlike approaches that aim to reconstruct the whole 3D geometry from such frontal views [20], here we leverage these projections solely for efficient classification. These projected coordinates are then normalized to the range[0,1][0,1]:ùë•ÃÇ(ùëñ)=ùë•(ùëñ)‚àíùë•minùë•max‚àíùë•min,ùë¶ÃÇ(ùëñ)=ùë¶(ùëñ)‚àíùë¶minùë¶max‚àíùë¶min.x^(i)=x(i)‚àíxminxmax‚àíxmin,y^(i)=y(i)‚àíyminymax‚àíymin.(9)We map raw 3D head/hands coordinates to lightweight 2D motion encodings. Normalized coordinates are scaled to image indices (ùëä√óùêª√óùê∑W√óH√óD), and clamped. Per frame, we build two binary views‚Äîfrontal (XY) and depth-integrated (YZ/XZ)‚Äîand concatenate them; trajectories are emphasized via max-pool aggregation over the lastMframes. Unlike complex foreground segmentation techniques used in uncontrolled environments [21], this lightweight projection suffices for the controlled VR tracking setup while minimizing computational load. Labeled PNGs (e.g., ‚ÄúWIP‚Äù/‚Äúnon-WIP‚Äù) form the dataset, using an 80/20 split to assess overall generalization. A temporal buffer accumulates these normalized traces to highlight movement patterns over the 100-frame window, thereby enhancing the visual representation of motion dynamics.The final image is created by concatenating the processed frontal and depth planes.Figure 1provides examples of the resulting encoded images for the two motion classes, highlighting the 3D sensor position (Head, Left Hand and Right Hand).Figure 1.Examples of 2D image encodings from the validation case study. (a) A sample from Class 1 (periodic motion), showing structured, repetitive traces from the head and hand sensors. (b) A sample from Class 2 (aperiodic motion), showing more irregular patterns. Each 72 √ó 56 pixel image combines normalized frontal (XY) and depth (YZ) plane projections accumulated over a 100-frame window. 5.2. Train/Test Splits and Statistical AnalysisWe use an 80/20 split for training and testing, stratified across participants to ensure a subject-independent evaluation. The training set consists of 35,267 images, and the test set contains 8817 images. All experimental results report the mean and standard deviation (mean ¬± std) across at least five independent runs with different random seeds. For the noise robustness comparisons, we employ McNemar‚Äôs paired test to assess the statistical significance of the differences in predictions made by the two models.",
            "5.1. Acquisition and Image Encoding": "Data were recorded using a consumer-grade virtual reality headset (Meta Quest 3) with inside-out tracking at a sampling rate of 90 Hz [19]. Each image sample encodes approximately 1.1 s of motion via a temporal buffer ofùëÄ=100M=100frames. The 3D coordinates are first projected onto the frontal (XY) and depth (YZ) planes. Unlike approaches that aim to reconstruct the whole 3D geometry from such frontal views [20], here we leverage these projections solely for efficient classification. These projected coordinates are then normalized to the range[0,1][0,1]:ùë•ÃÇ(ùëñ)=ùë•(ùëñ)‚àíùë•minùë•max‚àíùë•min,ùë¶ÃÇ(ùëñ)=ùë¶(ùëñ)‚àíùë¶minùë¶max‚àíùë¶min.x^(i)=x(i)‚àíxminxmax‚àíxmin,y^(i)=y(i)‚àíyminymax‚àíymin.(9) We map raw 3D head/hands coordinates to lightweight 2D motion encodings. Normalized coordinates are scaled to image indices (ùëä√óùêª√óùê∑W√óH√óD), and clamped. Per frame, we build two binary views‚Äîfrontal (XY) and depth-integrated (YZ/XZ)‚Äîand concatenate them; trajectories are emphasized via max-pool aggregation over the lastMframes. Unlike complex foreground segmentation techniques used in uncontrolled environments [21], this lightweight projection suffices for the controlled VR tracking setup while minimizing computational load. Labeled PNGs (e.g., ‚ÄúWIP‚Äù/‚Äúnon-WIP‚Äù) form the dataset, using an 80/20 split to assess overall generalization. A temporal buffer accumulates these normalized traces to highlight movement patterns over the 100-frame window, thereby enhancing the visual representation of motion dynamics. The final image is created by concatenating the processed frontal and depth planes.Figure 1provides examples of the resulting encoded images for the two motion classes, highlighting the 3D sensor position (Head, Left Hand and Right Hand). Figure 1.Examples of 2D image encodings from the validation case study. (a) A sample from Class 1 (periodic motion), showing structured, repetitive traces from the head and hand sensors. (b) A sample from Class 2 (aperiodic motion), showing more irregular patterns. Each 72 √ó 56 pixel image combines normalized frontal (XY) and depth (YZ) plane projections accumulated over a 100-frame window.",
            "5.2. Train/Test Splits and Statistical Analysis": "We use an 80/20 split for training and testing, stratified across participants to ensure a subject-independent evaluation. The training set consists of 35,267 images, and the test set contains 8817 images. All experimental results report the mean and standard deviation (mean ¬± std) across at least five independent runs with different random seeds. For the noise robustness comparisons, we employ McNemar‚Äôs paired test to assess the statistical significance of the differences in predictions made by the two models.",
            "6. Results": "This section presents the performance of the proposed readouts. We first establish a baseline performance on clean data and then evaluate its robustness under systematic noise stress tests, interpreting the findings within the context of our case study. 6.1. Clean-Data Performance and Computational EfficiencyOn the clean validation dataset, both single-neuron-per-class readouts achieve near-ceiling classification accuracy:E2E-MLP-1 Accuracy:99.91% ¬± 0.3%.E2E-RAF-1 Accuracy:99.68% ¬± 0.4%.These results indicate that a single-neuron-per-class readout can solve this task with remarkably few parameters. Reported CNN-based systems for related VR motion detection tasks typically reach accuracies in the 90‚Äì95% range, such as 94% reported by [22] for treadmill-based locomotion and 92% by [23] for real-time walking-in-place detection. In contrast, our single-neuron-per-class models achieve near-ceiling accuracy on the dataset while using fewer parameters. Specifically, for E2E-MLP-1, the calculation is as follows:2(classes)√ó4032(weights)+2(classes)√ó1(bias)=80662(classes)√ó4032(weights)+2(classes)√ó1(bias)=8066parameters. For E2E-RAF-1, the calculation is:2(classes)√ó4032(weights)+2(classes)√ó3(damping,omega,threshold)=80702(classes)√ó4032(weights)+2(classes)√ó3(damping,omega,threshold)=8070parameters. This is significantly fewer parameters than the tens of thousands typically found in CNNs. The models are also highly efficient. Inference latency on a desktop GPU (NVIDIA Quadro RTX 6000, NVIDIA Corporation, Santa Clara, CA, USA) averaged 0.5 ms for E2E-MLP-1 and 1.2 ms for E2E-RAF-1, making the approach suitable for real-time applications. 6.2. Robustness to Colored NoiseUnder additive brown (‚àùùúî‚àí2‚àùœâ‚àí2), pink (‚àùùúî‚àí1‚àùœâ‚àí1), and blue (‚àùùúî‚àùœâ) noise, the E2E-RAF-1 model consistently outperforms the linear readout. As shown inTable 1andFigure 2,Figure 3andFigure 4, the RAF-based model achieves a consistent accuracy advantage at medium-to-high perturbation levels. For example,Underbrown noiseat STDEV = 3.0, the performance gap is +2.2% (77.0.% vs. 74.8%).Underpink noiseat STDEV = 3.0, the RAF model leads by +1.5% absolute accuracy (64.3% vs. 62.8%).Underblue noise, the advantage is most pronounced, reaching +4.7% at STDEV = 2.0 (92.4% vs. 87.7%).Table 1.Testing Accuracy (%) for E2E-MLP-1 and E2E-RAF-1 under different noise types and intensities. Values are mean ¬± STDEV across 5 seeds.Table 1.Testing Accuracy (%) for E2E-MLP-1 and E2E-RAF-1 under different noise types and intensities. Values are mean ¬± STDEV across 5 seeds.Noise TypeSTDEVE2E-MLP-1E2E-RAF-1DifferenceBrown (‚àùùúî‚àí2‚àùœâ‚àí2)0.099.91 ¬± 0.399.68 ¬± 0.4‚àí0.231.096.1 ¬± 0.495.7 ¬± 0.5‚àí0.42.084.8 ¬± 0.685.8 ¬± 0.7+1.03.074.8 ¬± 0.677.0 ¬± 0.7+2.24.068.9 ¬± 0.671.6 ¬± 0.7+2.7Pink (‚àùùúî‚àí1‚àùœâ‚àí1)0.099.91 ¬± 0.399.68 ¬± 0.4‚àí0.231.085.8 ¬± 0.585.5 ¬± 0.6‚àí0.32.069.9 ¬± 0.770.7 ¬± 0.8+0.83.062.8 ¬± 0.764.3 ¬± 0.8+1.54.059.5 ¬± 0.761.6 ¬± 0.8+2.1Blue (‚àùùúî‚àùœâ)0.099.91 ¬± 0.399.68 ¬± 0.4‚àí0.231.098.8 ¬± 0.598.7 ¬± 0.6‚àí0.12.087.7 ¬± 0.792.4 ¬± 0.8+4.73.078.6 ¬± 0.886.1 ¬± 0.9+7.54.072.2 ¬± 0.880.9 ¬± 0.9+8.7Figure 2.Testing accuracy (%) with varying standard deviations of added brown noise. This comparison highlights the performance differences between the E2E-MLP-1 (shown by the yellow line) and the E2E-RAF-1 (represented by the orange line) under the influence of additive brown noise (‚àùùúî‚àí2‚àùœâ‚àí2).Figure 2.Testing accuracy (%) with varying standard deviations of added brown noise. This comparison highlights the performance differences between the E2E-MLP-1 (shown by the yellow line) and the E2E-RAF-1 (represented by the orange line) under the influence of additive brown noise (‚àùùúî‚àí2‚àùœâ‚àí2).Figure 3.Testing accuracy (%) with varying standard deviations of added pink noise. This comparison highlights the performance differences between the E2E-MLP-1 (shown by the yellow line) and the E2E-RAF-1 (represented by the orange line) under the influence of additive pink noise (‚àùùúî‚àí1‚àùœâ‚àí1).Figure 3.Testing accuracy (%) with varying standard deviations of added pink noise. This comparison highlights the performance differences between the E2E-MLP-1 (shown by the yellow line) and the E2E-RAF-1 (represented by the orange line) under the influence of additive pink noise (‚àùùúî‚àí1‚àùœâ‚àí1).Figure 4.Testing accuracy (%) with varying standard deviations of added blue noise. This comparison highlights the performance differences between the E2E-MLP-1 (shown by the yellow line) and the E2E-RAF-1 (represented by the orange line) under the influence of additive blue noise (‚àùùúî‚àùœâ).Figure 4.Testing accuracy (%) with varying standard deviations of added blue noise. This comparison highlights the performance differences between the E2E-MLP-1 (shown by the yellow line) and the E2E-RAF-1 (represented by the orange line) under the influence of additive blue noise (‚àùùúî‚àùœâ).Paired McNemar tests on the predictions confirm that these differences are statistically significant across all seeds (ùëù<0.01p<0.01). This trend of superior robustness aligns with the frequency-selective filtering properties predicted by the mathematical analysis inSection 4, where the RAF neuron‚Äôs resonant dynamics act as an intrinsic filter against off-band noise.These findings are consistent with this application example with the frequency-selective behavior predicted by the RAF transfer function analysis inSection 4, where resonance concentrates energy around task-relevant bands and attenuates off-band perturbations. 6.3. Interpretation Within the Motion Encoding Case StudyIn the context of our validation case study, where the positive class represents periodic in-place stepping patterns and the negative class represents other hand/head motions, the single-neuron-per-class models achieve high performance. Both models effectively learn to distinguish these complex motion patterns from their 2D image representations. Crucially, under the simulated sensor noise, the E2E-RAF-1 model degrades more gracefully. This suggests that, for a practical motion classification application, the RAF-based readout would provide more reliable performance in real-world conditions, where sensor data is inevitably corrupted by noise, and would maintain usable accuracy at higher noise levels than the linear baseline. 6.4. Comparison with a Lightweight CNN BaselineTo contextualize the proposed readouts against a compact deep architecture, we trained a lightweight CNN (8 layers with early blocks of ‚àº100 feature maps, ReLU activations, and max-pooling, followed by a single fully connected classifier) on the same subject-independent splits and with the same optimizer/scheduler settings. As summarized inTable 2, the CNN attains 99.93% accuracy on clean data, which is comparable to the single-neuron-per-class readouts (99.91% for E2E-MLP-1; 99.68% for E2E-RAF-1). We therefore emphasize the architectural simplicity of single-neuron-per-class readouts for image-encoded time series in this case-study setting.Table 2.Clean-data accuracy in the case-study setting.",
            "6.1. Clean-Data Performance and Computational Efficiency": "On the clean validation dataset, both single-neuron-per-class readouts achieve near-ceiling classification accuracy: E2E-MLP-1 Accuracy:99.91% ¬± 0.3%.E2E-RAF-1 Accuracy:99.68% ¬± 0.4%. These results indicate that a single-neuron-per-class readout can solve this task with remarkably few parameters. Reported CNN-based systems for related VR motion detection tasks typically reach accuracies in the 90‚Äì95% range, such as 94% reported by [22] for treadmill-based locomotion and 92% by [23] for real-time walking-in-place detection. In contrast, our single-neuron-per-class models achieve near-ceiling accuracy on the dataset while using fewer parameters. Specifically, for E2E-MLP-1, the calculation is as follows:2(classes)√ó4032(weights)+2(classes)√ó1(bias)=80662(classes)√ó4032(weights)+2(classes)√ó1(bias)=8066parameters. For E2E-RAF-1, the calculation is:2(classes)√ó4032(weights)+2(classes)√ó3(damping,omega,threshold)=80702(classes)√ó4032(weights)+2(classes)√ó3(damping,omega,threshold)=8070parameters. This is significantly fewer parameters than the tens of thousands typically found in CNNs. The models are also highly efficient. Inference latency on a desktop GPU (NVIDIA Quadro RTX 6000, NVIDIA Corporation, Santa Clara, CA, USA) averaged 0.5 ms for E2E-MLP-1 and 1.2 ms for E2E-RAF-1, making the approach suitable for real-time applications.",
            "6.2. Robustness to Colored Noise": "Under additive brown (‚àùùúî‚àí2‚àùœâ‚àí2), pink (‚àùùúî‚àí1‚àùœâ‚àí1), and blue (‚àùùúî‚àùœâ) noise, the E2E-RAF-1 model consistently outperforms the linear readout. As shown inTable 1andFigure 2,Figure 3andFigure 4, the RAF-based model achieves a consistent accuracy advantage at medium-to-high perturbation levels. For example, Underbrown noiseat STDEV = 3.0, the performance gap is +2.2% (77.0.% vs. 74.8%).Underpink noiseat STDEV = 3.0, the RAF model leads by +1.5% absolute accuracy (64.3% vs. 62.8%).Underblue noise, the advantage is most pronounced, reaching +4.7% at STDEV = 2.0 (92.4% vs. 87.7%). Table 1.Testing Accuracy (%) for E2E-MLP-1 and E2E-RAF-1 under different noise types and intensities. Values are mean ¬± STDEV across 5 seeds. Table 1.Testing Accuracy (%) for E2E-MLP-1 and E2E-RAF-1 under different noise types and intensities. Values are mean ¬± STDEV across 5 seeds.Noise TypeSTDEVE2E-MLP-1E2E-RAF-1DifferenceBrown (‚àùùúî‚àí2‚àùœâ‚àí2)0.099.91 ¬± 0.399.68 ¬± 0.4‚àí0.231.096.1 ¬± 0.495.7 ¬± 0.5‚àí0.42.084.8 ¬± 0.685.8 ¬± 0.7+1.03.074.8 ¬± 0.677.0 ¬± 0.7+2.24.068.9 ¬± 0.671.6 ¬± 0.7+2.7Pink (‚àùùúî‚àí1‚àùœâ‚àí1)0.099.91 ¬± 0.399.68 ¬± 0.4‚àí0.231.085.8 ¬± 0.585.5 ¬± 0.6‚àí0.32.069.9 ¬± 0.770.7 ¬± 0.8+0.83.062.8 ¬± 0.764.3 ¬± 0.8+1.54.059.5 ¬± 0.761.6 ¬± 0.8+2.1Blue (‚àùùúî‚àùœâ)0.099.91 ¬± 0.399.68 ¬± 0.4‚àí0.231.098.8 ¬± 0.598.7 ¬± 0.6‚àí0.12.087.7 ¬± 0.792.4 ¬± 0.8+4.73.078.6 ¬± 0.886.1 ¬± 0.9+7.54.072.2 ¬± 0.880.9 ¬± 0.9+8.7 Figure 2.Testing accuracy (%) with varying standard deviations of added brown noise. This comparison highlights the performance differences between the E2E-MLP-1 (shown by the yellow line) and the E2E-RAF-1 (represented by the orange line) under the influence of additive brown noise (‚àùùúî‚àí2‚àùœâ‚àí2). Figure 2.Testing accuracy (%) with varying standard deviations of added brown noise. This comparison highlights the performance differences between the E2E-MLP-1 (shown by the yellow line) and the E2E-RAF-1 (represented by the orange line) under the influence of additive brown noise (‚àùùúî‚àí2‚àùœâ‚àí2). Figure 3.Testing accuracy (%) with varying standard deviations of added pink noise. This comparison highlights the performance differences between the E2E-MLP-1 (shown by the yellow line) and the E2E-RAF-1 (represented by the orange line) under the influence of additive pink noise (‚àùùúî‚àí1‚àùœâ‚àí1). Figure 3.Testing accuracy (%) with varying standard deviations of added pink noise. This comparison highlights the performance differences between the E2E-MLP-1 (shown by the yellow line) and the E2E-RAF-1 (represented by the orange line) under the influence of additive pink noise (‚àùùúî‚àí1‚àùœâ‚àí1). Figure 4.Testing accuracy (%) with varying standard deviations of added blue noise. This comparison highlights the performance differences between the E2E-MLP-1 (shown by the yellow line) and the E2E-RAF-1 (represented by the orange line) under the influence of additive blue noise (‚àùùúî‚àùœâ). Figure 4.Testing accuracy (%) with varying standard deviations of added blue noise. This comparison highlights the performance differences between the E2E-MLP-1 (shown by the yellow line) and the E2E-RAF-1 (represented by the orange line) under the influence of additive blue noise (‚àùùúî‚àùœâ). Paired McNemar tests on the predictions confirm that these differences are statistically significant across all seeds (ùëù<0.01p<0.01). This trend of superior robustness aligns with the frequency-selective filtering properties predicted by the mathematical analysis inSection 4, where the RAF neuron‚Äôs resonant dynamics act as an intrinsic filter against off-band noise. These findings are consistent with this application example with the frequency-selective behavior predicted by the RAF transfer function analysis inSection 4, where resonance concentrates energy around task-relevant bands and attenuates off-band perturbations.",
            "6.3. Interpretation Within the Motion Encoding Case Study": "In the context of our validation case study, where the positive class represents periodic in-place stepping patterns and the negative class represents other hand/head motions, the single-neuron-per-class models achieve high performance. Both models effectively learn to distinguish these complex motion patterns from their 2D image representations. Crucially, under the simulated sensor noise, the E2E-RAF-1 model degrades more gracefully. This suggests that, for a practical motion classification application, the RAF-based readout would provide more reliable performance in real-world conditions, where sensor data is inevitably corrupted by noise, and would maintain usable accuracy at higher noise levels than the linear baseline.",
            "6.4. Comparison with a Lightweight CNN Baseline": "To contextualize the proposed readouts against a compact deep architecture, we trained a lightweight CNN (8 layers with early blocks of ‚àº100 feature maps, ReLU activations, and max-pooling, followed by a single fully connected classifier) on the same subject-independent splits and with the same optimizer/scheduler settings. As summarized inTable 2, the CNN attains 99.93% accuracy on clean data, which is comparable to the single-neuron-per-class readouts (99.91% for E2E-MLP-1; 99.68% for E2E-RAF-1). We therefore emphasize the architectural simplicity of single-neuron-per-class readouts for image-encoded time series in this case-study setting. Table 2.Clean-data accuracy in the case-study setting.",
            "7. Discussion": "Our primary contribution is both architectural and mathematical. We demonstrate that a single-neuron-per-class readout can effectively integrate feature selection and decision-making, challenging the necessity of deep, layered structures for certain high-dimensional classification tasks. The key insight is that the Resonate-and-Fire (RAF) neuron‚Äôs dynamics provide a principled mechanism for robustness under colored noise. Through its tunable resonance and bandwidth, controlled by the parameters(ùúâ,ùúî0)(Œæ,œâ0), the neuron acts as an intrinsic, adaptive filter. The motion-encoding case study demonstrates that this minimal design achieves high accuracy with orders of magnitude fewer parameters than deep models typically used for similar representations. 7.1. Robustness in Complex Signal EnvironmentsWhile our analysis focuses on specific noise colors (ùõº‚àà{‚àí2,‚àí1,1}Œ±‚àà{‚àí2,‚àí1,1}), real-world conditions often involve mixed noise types (e.g., a superposition of white sensor thermal noise and pink environmental drift). In such cases, the RAF‚Äôs selective advantage depends on the spectral dominance within the signal bandùúîùë†œâs. For broadband (non-band-limited) signals spanning the entire frequency spectrum, the benefits of the RAF‚Äôs band-pass filtering diminish relative to frequency-agnostic readouts. However, for the class of oscillating sensor signals typical of human motion (walking, gesturing), the band-selective property remains a critical asset for separating signal from complex background noise. 7.2. LimitationsThe capacity of a single unit may be insufficient for tasks that fundamentally require extracting hierarchical features, such as recognizing complex objects in natural images. While we reported parameter counts and inference latency, a full analysis of Floating Point Operations (FLOPs) and energy consumption profiling on embedded devices remains as future work. Furthermore, our robustness evaluation focused on continuous colored noise; the study of its resilience to other types of perturbations, such as signal bursts, data dropouts, or spatial occlusions in the input images, also merits further investigation. 7.3. OutlookThe architectural simplicity and inherent robustness of the single-neuron-per-class readout open several promising avenues for future research. Extending the concept to small ensembles of resonant units or developing hybrid linear+RAF readouts could preserve computational compactness while improving expressive power. More advanced strategies, such as adaptive resonance, in which the neuron‚Äôs parameters adjust in real time to the statistics of the input signal, could further enhance performance. Critically, the mathematical analysis presented inSection 4offers a blueprint not just for understanding the model, but for actively tuning the neuron‚Äôs parameters(ùúâ,ùúî0,ùëâth)(Œæ,œâ0,Vth)to align with the known spectral properties of a given task, bridging the gap between theory and practical application.",
            "7.1. Robustness in Complex Signal Environments": "While our analysis focuses on specific noise colors (ùõº‚àà{‚àí2,‚àí1,1}Œ±‚àà{‚àí2,‚àí1,1}), real-world conditions often involve mixed noise types (e.g., a superposition of white sensor thermal noise and pink environmental drift). In such cases, the RAF‚Äôs selective advantage depends on the spectral dominance within the signal bandùúîùë†œâs. For broadband (non-band-limited) signals spanning the entire frequency spectrum, the benefits of the RAF‚Äôs band-pass filtering diminish relative to frequency-agnostic readouts. However, for the class of oscillating sensor signals typical of human motion (walking, gesturing), the band-selective property remains a critical asset for separating signal from complex background noise.",
            "7.2. Limitations": "The capacity of a single unit may be insufficient for tasks that fundamentally require extracting hierarchical features, such as recognizing complex objects in natural images. While we reported parameter counts and inference latency, a full analysis of Floating Point Operations (FLOPs) and energy consumption profiling on embedded devices remains as future work. Furthermore, our robustness evaluation focused on continuous colored noise; the study of its resilience to other types of perturbations, such as signal bursts, data dropouts, or spatial occlusions in the input images, also merits further investigation.",
            "7.3. Outlook": "The architectural simplicity and inherent robustness of the single-neuron-per-class readout open several promising avenues for future research. Extending the concept to small ensembles of resonant units or developing hybrid linear+RAF readouts could preserve computational compactness while improving expressive power. More advanced strategies, such as adaptive resonance, in which the neuron‚Äôs parameters adjust in real time to the statistics of the input signal, could further enhance performance. Critically, the mathematical analysis presented inSection 4offers a blueprint not just for understanding the model, but for actively tuning the neuron‚Äôs parameters(ùúâ,ùúî0,ùëâth)(Œæ,œâ0,Vth)to align with the known spectral properties of a given task, bridging the gap between theory and practical application.",
            "8. Conclusions": "We presented a minimal end-to-end classifier that collapses feature selection, representation, and decision-making into a single-neuron-per-class. Validated on a challenging motion classification benchmark using image-encoded sensor time series, both a linear perceptron and a resonant Resonate-and-Fire (RAF) variant achieved near-ceiling accuracy (over 99.6%) on clean data. Crucially, the RAF readout showed clear, statistically significant advantages under colored noise, with performance gains of 5‚Äì8% at higher noise levels. With approximately 8000 parameters and millisecond-level inference latencies, this work demonstrates that single-neuron-per-class readouts are a promising architectural alternative for robust, low-latency signal processing. Overall, the paper should be read as a direct application example that evidences the feasibility and efficiency of single-neuron-per-class readouts for image-encoded time series in this setting."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2227-7390/13/24/3893",
        "scraped_at": "2025-12-05 23:51:57"
    },
    {
        "title": "Nondestructive Identification of Eggshell Cracks Using Hyperspectral Imaging Combined with Attention-Enhanced 3D-CNN",
        "authors": "byHao Li,Aoyun Zheng,Chaoxian Liu,Jun Huang,Yong Ma,Huanjun HuandYou Du",
        "journal": "Foods2025,14(24), 4183;https://doi.org/10.3390/foods14244183- 5 Dec 2025",
        "abstract": "Eggshell cracks are a critical factor affecting egg quality and food safety, with traditional detection methods often struggling to detect fine cracks, especially under multi-colored shells and complex backgrounds. To address this issue, we propose a non-destructive detection approach based on an enhanced three-dimensional convolutional neural network (3D-CNN), named 3D-CrackNet, integrated with hyperspectral imaging (HSI) for high-precision identification and localization of eggshell cracks. Operating within the 1000‚Äì2500 nm spectral range, the proposed framework employs spectral preprocessing and optimal band selection to improve discriminative feature representation. A residual learning module is incorporated to mitigate gradient degradation during deep joint spectral-spatial feature extraction, while a parameter-free SimAM attention mechanism adaptively enhances crack-related regions and suppresses background interference. This architecture enables the network to effectively capture both fine-grained spatial textures and contiguous spectral patterns associated with cracks. Experiments on a self-constructed dataset of 400 egg samples show that 3D-CrackNet achieves an F1-score of 75.49% and an Intersection over Union (IoU) of 60.62%, significantly outperforming conventional 1D-CNN and 2D-CNN models. These findings validate that 3D-CrackNet offers a robust, non-destructive, and efficient solution for accurately detecting and localizing subtle eggshell cracks, demonstrating strong potential for intelligent online egg quality grading and micro-defect monitoring in industrial applications.Keywords:hyperspectral imaging;3D convolutional neural network;attention mechanism;eggshell crack;residual modules",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Eggshell cracks significantly compromise both the visual quality and structural integrity of eggs, leading to moisture loss, nutrient degradation, and increased risk of microbial contamination (e.g.,Salmonella). Such defects not only shorten shelf life and reduce commercial value, but may also cause food safety incidents [1,2]. Current eggshell crack detection methods mainly include manual inspection, acoustic techniques, and machine vision. However, manual inspection is prone to subjective bias and fatigue, resulting in low efficiency and low speed [3]; acoustic detection is susceptible to interference from industrial vibrations and noise, and it may cause secondary damage [4]; although machine vision offers high speed, its ability to detect invisible or micro-scale cracks, especially under varying shell colors, dirt contamination, and changing lighting conditions [5]. Hyperspectral imaging (HSI) technology captures sequential spectral reflectance at each pixel, enabling simultaneous analysis of the composition and structural characteristics of the target material. This ‚Äújoint spectral-spatial feature extraction‚Äù characteristic makes HSI highly sensitive to micro-cracks on eggshells. In crack regions, stress concentration and microstructural damage alter the local optical properties (reflection, absorption, and transmission), resulting in shifts in reflectance at specific wavelengths or changes in spectral shape. Consequently, even cracks that are nearly invisible under visible light can be detected in the near-infrared or short-wave infrared region [6]. Previous studies have shown that HSI outperforms conventional RGB imaging in agricultural product defect detection, offering significant advantages in both accuracy and robustness [7,8]. For instance, Huang et al. (2021) integrated hyperspectral spectral features with texture features in nectarine quality assessment [9]. Using a Least Squares Support Vector Machine model, they achieved simultaneous identification of external defects and prediction of soluble solids content [9]. Xu et al. (2023) integrated HSI in the range of 866.4‚Äì1701 nm with an attention-based convolutional neural network to classify corn seed defects, reaching over 90% accuracy with the optimal model [10]. In the field of non-destructive detection of eggshell cracks, Ahmed et al. systematically reviewed recent advances in optical sensing technologies for the egg industry and emphasized the potential of combining HSI with deep learning to improve crack detection accuracy and enable automated grading [11]. Yao et al. proposed a method that captures images at different wavelengths via HSI to detect cracked eggshells; however, crack tips that are too thin remain difficult to detect due to minimal brightness contrast with surrounding areas [12]. Han et al. employed visible‚Äìnear-infrared spectroscopy for egg sorting and crack detection, finding that eggshell color differences significantly affect spectral data acquisition, with the high reflectance of white shells causing spectral information loss and reduced detection accuracy [13]. Chen et al. integrated HSI with band selection and deep learning algorithms to develop the HEDIT system, capable of real-time operation on production lines [14]. So et al. utilized visible‚Äìnear-infrared surface HSI to capture multi-dimensional spectral data from both the eggshell surface and interior. By combining multivariate analysis with pattern recognition algorithms, they realized non-destructive and high-precision microcrack detection, providing crucial technical support for replacing manual inspection and enhancing automated grading [15]. Although HSI combined with machine learning or shallow neural networks can achieve object-level localization of eggshell cracks, such coarse detection remains inadequate for industrial grading applications. Accurate quality control requires not only identifying the presence of cracks but also evaluating their severity, spatial extent, and distinction from surface artifacts (e.g., blood spots and calcification lines). These objectives can only be fulfilled through pixel-level segmentation, which enables precise delineation of crack boundaries and regions, and provides morphological details that serve as quantitative evidence for grading and safety evaluation‚Äîinformation that object-level localization cannot deliver. Despite these advances, several key challenges remain. Variations in shell color and complex backgrounds often lead to feature confusion and missed detections. Shallow models relying on hand-crafted features struggle to integrate joint spectral-spatial feature extraction and to represent the subtle characteristics of microcracks. Furthermore, the high-dimensional and nonlinear nature of hyperspectral data frequently results in information loss in shallow or single-task models, thereby constraining detection sensitivity and overall accuracy. With the advancement of deep learning, three-dimensional convolutional neural networks (3D-CNNs) have shown advantages in hyperspectral image analysis as they can simultaneously process spatial and spectral information. 3D-CNNs enhance the detection of minute defects while preserving spectral continuity and spatial texture features. Li et al. systematically reviewed 3D-CNN-based hyperspectral image classification approaches, emphasizing their advantage in spectral‚Äìspatial feature extraction [16]. Compared with one-dimensional or two-dimensional CNNs, 3D-CNNs significantly reduce information loss and perform excellently in detecting subtle defects such as plant disease identification [17]. Zhang et al. incorporated 3D-CNN and deformable convolution in agricultural product crack detection, further optimizing spectral feature processing to suit the detection of subtle defects such as produce cracks [18]. However, in eggshell crack detection, the standard 3D-CNN still faces limitations: complex background noise, shell color variation, and gradient vanishing or feature degradation as network depth increases, all of which reduce detection accuracy. To address these issues, this paper proposes a non-destructive eggshell crack detection method that integrates HSI with an improved 3D-CNN to fully exploit joint spectral‚Äìspatial features, thereby addressing the limitations of conventional methods in microcrack recognition. Specifically, within the 3D-CNN framework, we incorporate a lightweight SimAM attention mechanism to enhance the response to key spectral‚Äìspatial crack regions while suppressing background noise. Additionally, residual modules are adopted to replace standard convolution layers, mitigating gradient attenuation and feature degradation in deeper networks and thus balancing detection accuracy with computational efficiency. Experimental results under varied shell colors and complex background conditions confirm that the proposed method achieves high-precision crack detection and accurate localization, offering a feasible technical pathway for online egg quality grading and intelligent inspection.",
            "2. Materials": "2.1. Sample PreparationA total of 400 fresh eggs were procured from multiple farmers‚Äô markets and large chain supermarkets in Wuhan, Hubei Province, China, to encompass a diverse range of varieties, shell colors, and production batches, thereby enhancing the dataset‚Äôs representativeness. To ensure model robustness, the dataset was constructed to include eggs exhibiting various surface contaminants. After collection, the samples were acclimatized at room temperature (25 ¬± 2 ¬∞C; relative humidity: 60 ¬± 5%) for 2 h to eliminate the effects of transportation and environmental fluctuations. To simulate realistic damage scenarios, a specialized crack-induction device was designed. Specifically, a customized slider with a mass of 150 g was used on a smooth inclined plane (length: 24 cm). The release height was adjusted between 10 cm and 15 cm to avoid excessive loading while ensuring consistent crack initiation. This produced impact velocities of approximately 1.40‚Äì1.71 m/s, corresponding to impact energies of 0.15‚Äì0.22 J. To ensure reproducibility across samples, post-impact inspection was performed according to predefined criteria; eggs exhibiting shell crushing or insufficient crack formation were excluded from subsequent analysis. By adjusting the mass and height, various crack types were induced, including visible fractures, star-shaped radial cracks, and fine linear cracks. Representative samples are presented inFigure 1. Specifically, the top row depicts cracks with prominent morphological features that are readily distinguishable, whereas the bottom row illustrates ultra-subtle hairline micro-cracks characterized by minimal contrast and marginal visibility to the unaided eye. This methodology ensured the generation of a diverse and realistic crack dataset, closely mimicking damage incurred during actual transport and handling.Figure 1.Representative eggs of different varieties and corresponding cracks. 2.2. Hyperspectral Data AcquisitionAs shown inFigure 2, the HSI system used in this experiment consisted primarily of a hyperspectral camera, halogen light sources, a motorized translation stage, and a control computer. The hyperspectral imaging system used in this study was a Specim SWIR camera (Spectral Imaging Ltd., Oulu, Finland) equipped with a Stirling-cooled MCT detector. The system covers a spectral range of 1000‚Äì2500 nm with a spectral sampling interval of 5.6 nm and a spectral resolution of 10 nm, yielding 273 spectral bands. The spatial dimension consists of 384 pixels with a pixel size of 24 Œºm √ó 24 Œºm. An F/2.0 fore objective lens was used for image acquisition. During data collection, the exposure time was set to 3.00 ms, and the frame rate was maintained at 50.00 fps to ensure high-quality spectral image reconstruction. The illumination system consisted of two sets of halogen lamps symmetrically positioned at approximately 45¬∞ angles on either side of the sample to provide uniform and stable lighting to the imaging area. The egg samples were placed at the center of the motorized stage, which was covered with black velvet to minimize background reflection interference. In this experimental setup, eggs were manually positioned to ensure that the cracked regions were directly exposed to the camera‚Äôs field of view. Given that the spectral reflectance of the curved eggshell varies between direct and oblique angles, this orientation was necessary to capture consistent and intrinsic spectral signatures for model training. While this study utilizes a static acquisition protocol, in industrial scenarios, this requirement would be naturally met by roller conveyor systems that rotate the egg to present the entire surface to the sensor. Driven by a stepper motor, the stage moved linearly at a constant speed, while the camera operated in push-broom mode to sequentially capture spatial and spectral information line-by-line, ultimately producing a three-dimensional hyperspectral data cube (Hypercube) that encompassed the entire crack region.Figure 2.Schematic diagram of the hyperspectral image acquisition system.In the current laboratory configuration, a single scanning pass requires approximately 10 s to ensure high spectral resolution. However, it is important to note that the system‚Äôs push-broom design enables high spatial flexibility and batch processing: multiple eggs can be arranged in a linear or matrix array within the field of view for simultaneous imaging. Consequently, the scan duration is amortized across all samples, significantly reducing the effective acquisition time per egg. While the current speed is limited by the precision settings of the research-grade translation stage, the imaging sensor supports significantly higher frame rates, indicating that the system is adaptable to continuous high-speed scanning on industrial roller conveyors.Considering the presence of dark current noise and non-uniform illumination in the hyperspectral system, all raw data were subjected to black‚Äìwhite calibration to obtain relative reflectance [19]. To correct for dark current noise and illumination non-uniformity, standard black‚Äìwhite reference calibration method was applied in this study, and the calculation formula is expressed as follows:I=I0‚àíDW‚àíDI=I0‚àíDW‚àíD(1)whereIIdenotes the calibrated spectral reflectance;I0I0represents the reflected intensity of the hyperspectral image;WWcorresponds to the reflected intensity of the full white calibration image (white reference), andDDrepresents the reflected intensity from the full black calibration image (dark reference). 2.3. Egg Crack AnnotationGiven the complex and irregular morphology of eggshell cracks, pixel-level manual annotation was performed to ensure accurate ground-truth labels (Figure 3). The annotation process was carried out using ENVI 5.3 software (ITT Visual Information Solutions, Inc., in Boulder, CO, USA), which features a Region of Interest (ROI) tool that supports pixel-level operations. Specifically, a false-color composite image of each hyperspectral cube was first generated. Using the Region of Interest (ROI) tool, annotators manually traced crack boundaries pixel by pixel. Crack regions were assigned a value of 1, and non-crack regions (including intact shell and background) were assigned 0, producing a binary mask for each sample. Each calibrated hypercube was thus paired with a corresponding mask, forming the image-label pairs for subsequent model training and evaluation.Figure 3.Illustration of the data annotation process: (a) original false-color image; (b) pixel-wise annotation via the ROI tool, where the green area denotes cracks; (c) the resulting binary mask, in which cracks are represented in white.",
            "2.1. Sample Preparation": "A total of 400 fresh eggs were procured from multiple farmers‚Äô markets and large chain supermarkets in Wuhan, Hubei Province, China, to encompass a diverse range of varieties, shell colors, and production batches, thereby enhancing the dataset‚Äôs representativeness. To ensure model robustness, the dataset was constructed to include eggs exhibiting various surface contaminants. After collection, the samples were acclimatized at room temperature (25 ¬± 2 ¬∞C; relative humidity: 60 ¬± 5%) for 2 h to eliminate the effects of transportation and environmental fluctuations. To simulate realistic damage scenarios, a specialized crack-induction device was designed. Specifically, a customized slider with a mass of 150 g was used on a smooth inclined plane (length: 24 cm). The release height was adjusted between 10 cm and 15 cm to avoid excessive loading while ensuring consistent crack initiation. This produced impact velocities of approximately 1.40‚Äì1.71 m/s, corresponding to impact energies of 0.15‚Äì0.22 J. To ensure reproducibility across samples, post-impact inspection was performed according to predefined criteria; eggs exhibiting shell crushing or insufficient crack formation were excluded from subsequent analysis. By adjusting the mass and height, various crack types were induced, including visible fractures, star-shaped radial cracks, and fine linear cracks. Representative samples are presented inFigure 1. Specifically, the top row depicts cracks with prominent morphological features that are readily distinguishable, whereas the bottom row illustrates ultra-subtle hairline micro-cracks characterized by minimal contrast and marginal visibility to the unaided eye. This methodology ensured the generation of a diverse and realistic crack dataset, closely mimicking damage incurred during actual transport and handling. Figure 1.Representative eggs of different varieties and corresponding cracks.",
            "2.2. Hyperspectral Data Acquisition": "As shown inFigure 2, the HSI system used in this experiment consisted primarily of a hyperspectral camera, halogen light sources, a motorized translation stage, and a control computer. The hyperspectral imaging system used in this study was a Specim SWIR camera (Spectral Imaging Ltd., Oulu, Finland) equipped with a Stirling-cooled MCT detector. The system covers a spectral range of 1000‚Äì2500 nm with a spectral sampling interval of 5.6 nm and a spectral resolution of 10 nm, yielding 273 spectral bands. The spatial dimension consists of 384 pixels with a pixel size of 24 Œºm √ó 24 Œºm. An F/2.0 fore objective lens was used for image acquisition. During data collection, the exposure time was set to 3.00 ms, and the frame rate was maintained at 50.00 fps to ensure high-quality spectral image reconstruction. The illumination system consisted of two sets of halogen lamps symmetrically positioned at approximately 45¬∞ angles on either side of the sample to provide uniform and stable lighting to the imaging area. The egg samples were placed at the center of the motorized stage, which was covered with black velvet to minimize background reflection interference. In this experimental setup, eggs were manually positioned to ensure that the cracked regions were directly exposed to the camera‚Äôs field of view. Given that the spectral reflectance of the curved eggshell varies between direct and oblique angles, this orientation was necessary to capture consistent and intrinsic spectral signatures for model training. While this study utilizes a static acquisition protocol, in industrial scenarios, this requirement would be naturally met by roller conveyor systems that rotate the egg to present the entire surface to the sensor. Driven by a stepper motor, the stage moved linearly at a constant speed, while the camera operated in push-broom mode to sequentially capture spatial and spectral information line-by-line, ultimately producing a three-dimensional hyperspectral data cube (Hypercube) that encompassed the entire crack region. Figure 2.Schematic diagram of the hyperspectral image acquisition system. In the current laboratory configuration, a single scanning pass requires approximately 10 s to ensure high spectral resolution. However, it is important to note that the system‚Äôs push-broom design enables high spatial flexibility and batch processing: multiple eggs can be arranged in a linear or matrix array within the field of view for simultaneous imaging. Consequently, the scan duration is amortized across all samples, significantly reducing the effective acquisition time per egg. While the current speed is limited by the precision settings of the research-grade translation stage, the imaging sensor supports significantly higher frame rates, indicating that the system is adaptable to continuous high-speed scanning on industrial roller conveyors. Considering the presence of dark current noise and non-uniform illumination in the hyperspectral system, all raw data were subjected to black‚Äìwhite calibration to obtain relative reflectance [19]. To correct for dark current noise and illumination non-uniformity, standard black‚Äìwhite reference calibration method was applied in this study, and the calculation formula is expressed as follows:I=I0‚àíDW‚àíDI=I0‚àíDW‚àíD(1)whereIIdenotes the calibrated spectral reflectance;I0I0represents the reflected intensity of the hyperspectral image;WWcorresponds to the reflected intensity of the full white calibration image (white reference), andDDrepresents the reflected intensity from the full black calibration image (dark reference).",
            "2.3. Egg Crack Annotation": "Given the complex and irregular morphology of eggshell cracks, pixel-level manual annotation was performed to ensure accurate ground-truth labels (Figure 3). The annotation process was carried out using ENVI 5.3 software (ITT Visual Information Solutions, Inc., in Boulder, CO, USA), which features a Region of Interest (ROI) tool that supports pixel-level operations. Specifically, a false-color composite image of each hyperspectral cube was first generated. Using the Region of Interest (ROI) tool, annotators manually traced crack boundaries pixel by pixel. Crack regions were assigned a value of 1, and non-crack regions (including intact shell and background) were assigned 0, producing a binary mask for each sample. Each calibrated hypercube was thus paired with a corresponding mask, forming the image-label pairs for subsequent model training and evaluation. Figure 3.Illustration of the data annotation process: (a) original false-color image; (b) pixel-wise annotation via the ROI tool, where the green area denotes cracks; (c) the resulting binary mask, in which cracks are represented in white.",
            "3. Methodology": "3.1. Selection of Hyperspectral Feature BandsHyperspectral data are characterized by high dimensionality, strong inter-band correlation, and significant information redundancy. Effective band selection is crucial to mitigate the ‚Äúcurse of dimensionality,‚Äù reduce computational cost, and improve model generalization. This study employed three complementary band selection methods which apply distinct constraints from the perspectives of variable importance, collinearity minimization, and global optimization, respectively. This multi-strategy approach ensures the selected feature bands are both informative and non-redundant. Three band selection algorithms‚ÄîCARS, SPA, and RF‚Äîwere evaluated. Based on a preliminary assessment of their performance, the CARS algorithm was chosen for final model development due to its superior effectiveness in identifying the most informative spectral bands. A detailed quantitative comparison of the three methods is presented inSection 4.2to further justify the selection.(1)Competitive Adaptive Reweighted Sampling (CARS)CARS simulates the ‚Äúsurvival of the fittest‚Äù process by iteratively eliminating redundant variables during the selection procedure. The core principle involves using Partial Least Squares Regression (PLSR) to evaluate the importance of each spectral band, while employing an exponential decay mechanism to control the number of retained variables [20]. The weighting coefficient for each band is calculated as:wj=|Œ≤j|‚àëpi=1|Œ≤i|wj=Œ≤j‚àëi=1p|Œ≤i|(2)whereŒ≤jŒ≤jrepresents the regression coefficient of the j-th band in the current PLSR model, and p denotes the total number of bands. These weights are used for weighted sampling, retaining the most important variables. Finally, CARS retains the subset of bands that minimizes the cross-validation error, thereby enhancing prediction accuracy and robustness.(2)Successive Projections Algorithm (SPA)SPA is a forward-selection algorithm designed to minimize multicollinearity among variables by leveraging vector projection operations. It starts by selecting the spectral variable (band) with the largest norm. In each subsequent step, the algorithm projects all remaining bands onto the orthogonal complement of the subspace spanned by the already-selected bands [21]. The band with the maximum projection value (i.e., the largest amount of novel information) is then added to the feature set. The projection operation is defined as:Pxj=xj‚àíXs(XTsXs)‚àí1XTsxjPxj=xj‚àíXs(XsTXs)‚àí1XsTxj(3)wherexjxjis the vector of thejj-th remaining band,XsXsis the matrix of already-selected bands, andPxjPxjis the projection residual. SPA iterates until a predefined number of bands are selected, effectively constructing a set of variables with minimal redundancy.(3)Random Frog (RF)The RF algorithm is a global search method inspired by reversible-jump Markov Chain Monte Carlo (MCMC), which effectively explores the feature space to avoid local optima [22]. It works by iteratively generating and evaluating candidate band subsets. The basic procedure involves generating a new subset S‚Ä≤ through random operations such as replacing, adding, or removing spectral bands. The subset is accepted based on a performance metric. The acceptance probability is given by:P(accept)=min(1,exp(‚àíE(S‚Ä≤)‚àíE(S)T))Paccept=min1,exp‚àíES‚Ä≤‚àíEST(4)where E(S) and E(S‚Ä≤) represent the cross-validation errors of models built on the current subset and the candidate subset, respectively.TTis the temperature parameter that decreases over iterations, controlling the exploration-exploitation trade-off. After numerous iterations, the frequency of each band appearing in high-performing subsets is recorded and used as the final selection criterion. This approach can identify bands with limited individual contribution but significant synergistic effect when combined, thereby enhancing the model‚Äôs ability to capture complex spectral characteristics. 3.2. Proposed 3D-CrackNet ModelConventional 3D-CNNs can simultaneously extract spatial‚Äìspectral features from hyperspectral images (HSI) but face two primary limitations: (1) gradient vanishing and network degradation with increasing depth, which hinders the learning of complex features [23]; and (2) insufficient feature discrimination, as standard convolutions treat all features equally, making it difficult to distinguish subtle cracks from complex backgrounds [24]. To overcome these challenges, we propose 3D-CrackNet, a deep learning framework that integrates residual learning and a parameter-free attention mechanism to enhance feature representation and segmentation accuracy for eggshell cracks.3.2.1. Overall ArchitectureAs illustrated inFigure 4, 3D-CrackNet adopts a U-Net-like encoder‚Äìdecoder architecture tailored for pixel-wise hyperspectral image segmentation. The encoder comprises three sequential feature extraction units, each containing a 3D residual block (3D-ResBlock), a SimAM attention module, and a 3D max-pooling layer. This design enables progressive abstraction of the input hyperspectral cube, transforming high-resolution shallow features into low-resolution deep semantic representations. The decoder gradually restores spatial details through upsampling and skip connections, effectively recovering crack boundaries. The network culminates in a sigmoid activation function that outputs a probability map with the same spatial dimensions as the input. This map is subsequently thresholded to produce the final binary crack segmentation, enabling end-to-end training and optimization.Figure 4.Overall architecture of the proposed 3D-CrackNet model.3.2.2. Three-Dimensional Residual BlockIn hyperspectral egg crack detection tasks, fine cracks often appear as weak and spatially‚Äìspectrally discrete anomaly signals that are prone to being diminished or lost during deep network training. To address this issue, the proposed 3D-CrackNet incorporates a three-dimensional convolutional residual block (3D Residual Block), as shown inFigure 5, to jointly model local features in both the spatial and spectral dimensions while employing a residual mechanism to preserve crack-specific features.Figure 5.Structure of the 3D residual block (3D-ResBlock).The block consists of two sequential 3 √ó 3 √ó 3 3D convolutional layers, each followed by Batch Normalization (BN) and a ReLU activation, which enhance the network‚Äôs nonlinear representational ability and stabilize the training process. Since egg surface cracks manifest as subtle spectral anomalies and localized texture perturbations in hyperspectral imagery, 3D convolution aggregates contextual neighborhood information in the spatial domain while capturing spectral reflectance variations across different bands in the spectral domain. To mitigate gradient vanishing and information loss in deeper networks, a residual connection is incorporated. Here, the input feature x is added directly to the output of the convolutional main branch via an identity mapping or a downsampling pathway F(x). The final output is formulated as:y=œÉ(BN(Conv3D(BN(Conv3D(x)))))+‚Ñ±(x)y=œÉBN(Conv3D(BN(Conv3D(x))))+F(x)(5)where œÉ denotes the ReLU activation function, which is applied to the output of the preceding batch normalization layer. This structure not only ensures stable training in deep networks but also effectively enhances the fidelity of fine cracks within multi-scale features, enabling more accurate discrimination between intact eggshell regions and those containing cracks. In summary, the 3D Residual Block, by combining spatial‚Äìspectral joint modeling with residual enhancement, improves the robust extraction of crack features and provides more discriminative high-level semantic features for subsequent modeling.3.2.3. SimAM Attention ModuleWithin the 3D-CrackNet framework, 3D Residual Block operates synergistically with the SimAM attention module. The 3D-ResBlock extracts joint spatial‚Äìspectral features of eggshell cracks through multi-scale convolution while maintaining training stability. The SimAM module then refines these features via fine-grained, voxel-level weighting, which amplifies crack-related responses and suppresses background interference. This complementary mechanism significantly enhances the precision and robustness of hyperspectral crack segmentation. Unlike conventional one-dimensional spectral attention or two-dimensional spatial attention, SimAM assigns a unique three-dimensional weight to each voxel in the input feature map. This capability enables the simultaneous modeling of discriminative information across both spatial dimensions (H, W) and the spectral dimension (D), allowing the model to highlight abnormal spectral reflectance in crack regions while reinforcing spatial continuity in their vicinity. A distinctive advantage of SimAM is its parameter-free design: attention weights are derived directly from a closed-form solution to an energy function, without introducing additional learnable parameters. This results in a lightweight structure with high computational efficiency and ease of integration into various network architectures. The design is inspired by visual neuroscience, where salient features (e.g., abnormal spectral signatures or surface textures) naturally attract attention in biological visual systems, while redundant background information is suppressed. Given an input feature map of size H √ó W √ó D, the SimAM module constructs an energy function for each voxel to evaluate its separability from other voxels within the same channel, defined as:E(xi)=(yg‚àíg*)2+1N‚àí1‚àëi=1N‚àí1(yo‚àíx*i)2E(xi)=(yg‚àíg*)2+1N‚àí1‚àëi=1N‚àí1(yo‚àíxi*)2(6)whereygygdenotes the response of the target voxel,g*g*andx*ixi*represent the channel mean and other voxel values within the same channel, respectively, andNNis the total number of voxels per channel. This energy function determines the saliency of a voxel by quantifying the difference between the target feature and the surrounding background features. Subsequently, the energy values are normalized using a Sigmoid function to produce a voxel-wise weight matrix, which is applied element-wise to the input feature map to yield the attention-weighted output:X‚Ä≤=œÉ(E(X))‚äôXX‚Ä≤=œÉ(E(X))‚äôX(7)where‚äô‚äôdenotes element-wise multiplication, andœÉœÉrepresents the Sigmoid function. Compared with one-dimensional or two-dimensional attention mechanisms, SimAM (Figure 6) offers several advantages: (1) Three-dimensional weight assignment, enabling the model to simultaneously account for joint spectral‚Äìspatial discriminative characteristics; (2) No additional learnable parameters‚Äîanalytical (closed-form) computation ensures a lightweight and efficient design; (3) Simple architecture, making it straightforward to integrate into existing networks. Leveraging this module, 3D-CrackNet can effectively highlight the abnormal spatial‚Äìspectral signatures of egg crack regions, thereby improving detection sensitivity and discriminative capability.Figure 6.SimAM attention structure. 3.3. Experimental Setup and Evaluation MetricsThe experiments were implemented in PyTorch 2.1.0 with CUDA 11.8 acceleration on a workstation featuring an NVIDIA GeForce RTX 3090 GPU and an Intel Core i7-13700KF CPU. The training procedure spanned 100 epochs with a batch size of 32, utilizing the AdamW optimizer at an initial learning rate of 0.0001. A Cosine Annealing Warm Restarts scheduler was used to adjust the learning rate dynamically, thereby balancing training efficiency and final performance. To mitigate the class imbalance issue in crack segmentation, the Focal Dice Loss was chosen as the criterion for model optimization.A total of 400 hyperspectral egg samples were collected and randomly divided into training (320 samples), validation (40 samples), and testing (40 samples) subsets at a ratio of 8:1:1. To mitigate the risk of overfitting, regularization was integrated through the training process and model architecture. An L2 weight decay term (Œª = 1 √ó 10‚àí4) was applied within the AdamW optimizer to penalize large weights. Furthermore, the inclusion of batch normalization layers in the 3D-CNN architecture helped stabilize training and act as an implicit regularizer. Most importantly, an early stopping strategy was adopted, which halted training if the validation loss did not improve for 15 consecutive epochs, thereby preventing the model from over-optimizing on the training data. These strategies collectively enhanced the model‚Äôs ability to generalize to unseen samples.Since crack pixels occupy only a small proportion of the overall image, conventional overall accuracy metrics cannot adequately evaluate crack recognition performance. Therefore, a multi-dimensional evaluation strategy was adopted in this study. Based on the confusion matrix, four standard metrics‚ÄîIntersection over Union (IoU), Precision, Recall, and F1-Score‚Äîwere computed to comprehensively assess segmentation performance. In addition, the model‚Äôs training time was recorded to evaluate computational efficiency in practical applications.IOU=TPTP+FP+FNIOU=TPTP+FP+FN(8)Precision=TPTP+FPPrecision=TPTP+FP(9)Recall=TPTP+FNRecall=TPTP+FN(10)F1-score=2TP2TP+FP+FNF1-score=2TP2TP+FP+FN(11)Here, TP denotes the number of positive samples correctly predicted by the model; FP denotes the number of negative samples incorrectly predicted as positive; and FN denotes the number of positive samples incorrectly predicted as negative.",
            "3.1. Selection of Hyperspectral Feature Bands": "Hyperspectral data are characterized by high dimensionality, strong inter-band correlation, and significant information redundancy. Effective band selection is crucial to mitigate the ‚Äúcurse of dimensionality,‚Äù reduce computational cost, and improve model generalization. This study employed three complementary band selection methods which apply distinct constraints from the perspectives of variable importance, collinearity minimization, and global optimization, respectively. This multi-strategy approach ensures the selected feature bands are both informative and non-redundant. Three band selection algorithms‚ÄîCARS, SPA, and RF‚Äîwere evaluated. Based on a preliminary assessment of their performance, the CARS algorithm was chosen for final model development due to its superior effectiveness in identifying the most informative spectral bands. A detailed quantitative comparison of the three methods is presented inSection 4.2to further justify the selection. (1)Competitive Adaptive Reweighted Sampling (CARS) CARS simulates the ‚Äúsurvival of the fittest‚Äù process by iteratively eliminating redundant variables during the selection procedure. The core principle involves using Partial Least Squares Regression (PLSR) to evaluate the importance of each spectral band, while employing an exponential decay mechanism to control the number of retained variables [20]. The weighting coefficient for each band is calculated as:wj=|Œ≤j|‚àëpi=1|Œ≤i|wj=Œ≤j‚àëi=1p|Œ≤i|(2)whereŒ≤jŒ≤jrepresents the regression coefficient of the j-th band in the current PLSR model, and p denotes the total number of bands. These weights are used for weighted sampling, retaining the most important variables. Finally, CARS retains the subset of bands that minimizes the cross-validation error, thereby enhancing prediction accuracy and robustness. (2)Successive Projections Algorithm (SPA) SPA is a forward-selection algorithm designed to minimize multicollinearity among variables by leveraging vector projection operations. It starts by selecting the spectral variable (band) with the largest norm. In each subsequent step, the algorithm projects all remaining bands onto the orthogonal complement of the subspace spanned by the already-selected bands [21]. The band with the maximum projection value (i.e., the largest amount of novel information) is then added to the feature set. The projection operation is defined as:Pxj=xj‚àíXs(XTsXs)‚àí1XTsxjPxj=xj‚àíXs(XsTXs)‚àí1XsTxj(3)wherexjxjis the vector of thejj-th remaining band,XsXsis the matrix of already-selected bands, andPxjPxjis the projection residual. SPA iterates until a predefined number of bands are selected, effectively constructing a set of variables with minimal redundancy. (3)Random Frog (RF) The RF algorithm is a global search method inspired by reversible-jump Markov Chain Monte Carlo (MCMC), which effectively explores the feature space to avoid local optima [22]. It works by iteratively generating and evaluating candidate band subsets. The basic procedure involves generating a new subset S‚Ä≤ through random operations such as replacing, adding, or removing spectral bands. The subset is accepted based on a performance metric. The acceptance probability is given by:P(accept)=min(1,exp(‚àíE(S‚Ä≤)‚àíE(S)T))Paccept=min1,exp‚àíES‚Ä≤‚àíEST(4)where E(S) and E(S‚Ä≤) represent the cross-validation errors of models built on the current subset and the candidate subset, respectively.TTis the temperature parameter that decreases over iterations, controlling the exploration-exploitation trade-off. After numerous iterations, the frequency of each band appearing in high-performing subsets is recorded and used as the final selection criterion. This approach can identify bands with limited individual contribution but significant synergistic effect when combined, thereby enhancing the model‚Äôs ability to capture complex spectral characteristics.",
            "3.2. Proposed 3D-CrackNet Model": "Conventional 3D-CNNs can simultaneously extract spatial‚Äìspectral features from hyperspectral images (HSI) but face two primary limitations: (1) gradient vanishing and network degradation with increasing depth, which hinders the learning of complex features [23]; and (2) insufficient feature discrimination, as standard convolutions treat all features equally, making it difficult to distinguish subtle cracks from complex backgrounds [24]. To overcome these challenges, we propose 3D-CrackNet, a deep learning framework that integrates residual learning and a parameter-free attention mechanism to enhance feature representation and segmentation accuracy for eggshell cracks. 3.2.1. Overall ArchitectureAs illustrated inFigure 4, 3D-CrackNet adopts a U-Net-like encoder‚Äìdecoder architecture tailored for pixel-wise hyperspectral image segmentation. The encoder comprises three sequential feature extraction units, each containing a 3D residual block (3D-ResBlock), a SimAM attention module, and a 3D max-pooling layer. This design enables progressive abstraction of the input hyperspectral cube, transforming high-resolution shallow features into low-resolution deep semantic representations. The decoder gradually restores spatial details through upsampling and skip connections, effectively recovering crack boundaries. The network culminates in a sigmoid activation function that outputs a probability map with the same spatial dimensions as the input. This map is subsequently thresholded to produce the final binary crack segmentation, enabling end-to-end training and optimization.Figure 4.Overall architecture of the proposed 3D-CrackNet model. 3.2.2. Three-Dimensional Residual BlockIn hyperspectral egg crack detection tasks, fine cracks often appear as weak and spatially‚Äìspectrally discrete anomaly signals that are prone to being diminished or lost during deep network training. To address this issue, the proposed 3D-CrackNet incorporates a three-dimensional convolutional residual block (3D Residual Block), as shown inFigure 5, to jointly model local features in both the spatial and spectral dimensions while employing a residual mechanism to preserve crack-specific features.Figure 5.Structure of the 3D residual block (3D-ResBlock).The block consists of two sequential 3 √ó 3 √ó 3 3D convolutional layers, each followed by Batch Normalization (BN) and a ReLU activation, which enhance the network‚Äôs nonlinear representational ability and stabilize the training process. Since egg surface cracks manifest as subtle spectral anomalies and localized texture perturbations in hyperspectral imagery, 3D convolution aggregates contextual neighborhood information in the spatial domain while capturing spectral reflectance variations across different bands in the spectral domain. To mitigate gradient vanishing and information loss in deeper networks, a residual connection is incorporated. Here, the input feature x is added directly to the output of the convolutional main branch via an identity mapping or a downsampling pathway F(x). The final output is formulated as:y=œÉ(BN(Conv3D(BN(Conv3D(x)))))+‚Ñ±(x)y=œÉBN(Conv3D(BN(Conv3D(x))))+F(x)(5)where œÉ denotes the ReLU activation function, which is applied to the output of the preceding batch normalization layer. This structure not only ensures stable training in deep networks but also effectively enhances the fidelity of fine cracks within multi-scale features, enabling more accurate discrimination between intact eggshell regions and those containing cracks. In summary, the 3D Residual Block, by combining spatial‚Äìspectral joint modeling with residual enhancement, improves the robust extraction of crack features and provides more discriminative high-level semantic features for subsequent modeling. 3.2.3. SimAM Attention ModuleWithin the 3D-CrackNet framework, 3D Residual Block operates synergistically with the SimAM attention module. The 3D-ResBlock extracts joint spatial‚Äìspectral features of eggshell cracks through multi-scale convolution while maintaining training stability. The SimAM module then refines these features via fine-grained, voxel-level weighting, which amplifies crack-related responses and suppresses background interference. This complementary mechanism significantly enhances the precision and robustness of hyperspectral crack segmentation. Unlike conventional one-dimensional spectral attention or two-dimensional spatial attention, SimAM assigns a unique three-dimensional weight to each voxel in the input feature map. This capability enables the simultaneous modeling of discriminative information across both spatial dimensions (H, W) and the spectral dimension (D), allowing the model to highlight abnormal spectral reflectance in crack regions while reinforcing spatial continuity in their vicinity. A distinctive advantage of SimAM is its parameter-free design: attention weights are derived directly from a closed-form solution to an energy function, without introducing additional learnable parameters. This results in a lightweight structure with high computational efficiency and ease of integration into various network architectures. The design is inspired by visual neuroscience, where salient features (e.g., abnormal spectral signatures or surface textures) naturally attract attention in biological visual systems, while redundant background information is suppressed. Given an input feature map of size H √ó W √ó D, the SimAM module constructs an energy function for each voxel to evaluate its separability from other voxels within the same channel, defined as:E(xi)=(yg‚àíg*)2+1N‚àí1‚àëi=1N‚àí1(yo‚àíx*i)2E(xi)=(yg‚àíg*)2+1N‚àí1‚àëi=1N‚àí1(yo‚àíxi*)2(6)whereygygdenotes the response of the target voxel,g*g*andx*ixi*represent the channel mean and other voxel values within the same channel, respectively, andNNis the total number of voxels per channel. This energy function determines the saliency of a voxel by quantifying the difference between the target feature and the surrounding background features. Subsequently, the energy values are normalized using a Sigmoid function to produce a voxel-wise weight matrix, which is applied element-wise to the input feature map to yield the attention-weighted output:X‚Ä≤=œÉ(E(X))‚äôXX‚Ä≤=œÉ(E(X))‚äôX(7)where‚äô‚äôdenotes element-wise multiplication, andœÉœÉrepresents the Sigmoid function. Compared with one-dimensional or two-dimensional attention mechanisms, SimAM (Figure 6) offers several advantages: (1) Three-dimensional weight assignment, enabling the model to simultaneously account for joint spectral‚Äìspatial discriminative characteristics; (2) No additional learnable parameters‚Äîanalytical (closed-form) computation ensures a lightweight and efficient design; (3) Simple architecture, making it straightforward to integrate into existing networks. Leveraging this module, 3D-CrackNet can effectively highlight the abnormal spatial‚Äìspectral signatures of egg crack regions, thereby improving detection sensitivity and discriminative capability.Figure 6.SimAM attention structure.",
            "3.2.1. Overall Architecture": "As illustrated inFigure 4, 3D-CrackNet adopts a U-Net-like encoder‚Äìdecoder architecture tailored for pixel-wise hyperspectral image segmentation. The encoder comprises three sequential feature extraction units, each containing a 3D residual block (3D-ResBlock), a SimAM attention module, and a 3D max-pooling layer. This design enables progressive abstraction of the input hyperspectral cube, transforming high-resolution shallow features into low-resolution deep semantic representations. The decoder gradually restores spatial details through upsampling and skip connections, effectively recovering crack boundaries. The network culminates in a sigmoid activation function that outputs a probability map with the same spatial dimensions as the input. This map is subsequently thresholded to produce the final binary crack segmentation, enabling end-to-end training and optimization. Figure 4.Overall architecture of the proposed 3D-CrackNet model.",
            "3.2.2. Three-Dimensional Residual Block": "In hyperspectral egg crack detection tasks, fine cracks often appear as weak and spatially‚Äìspectrally discrete anomaly signals that are prone to being diminished or lost during deep network training. To address this issue, the proposed 3D-CrackNet incorporates a three-dimensional convolutional residual block (3D Residual Block), as shown inFigure 5, to jointly model local features in both the spatial and spectral dimensions while employing a residual mechanism to preserve crack-specific features. Figure 5.Structure of the 3D residual block (3D-ResBlock). The block consists of two sequential 3 √ó 3 √ó 3 3D convolutional layers, each followed by Batch Normalization (BN) and a ReLU activation, which enhance the network‚Äôs nonlinear representational ability and stabilize the training process. Since egg surface cracks manifest as subtle spectral anomalies and localized texture perturbations in hyperspectral imagery, 3D convolution aggregates contextual neighborhood information in the spatial domain while capturing spectral reflectance variations across different bands in the spectral domain. To mitigate gradient vanishing and information loss in deeper networks, a residual connection is incorporated. Here, the input feature x is added directly to the output of the convolutional main branch via an identity mapping or a downsampling pathway F(x). The final output is formulated as:y=œÉ(BN(Conv3D(BN(Conv3D(x)))))+‚Ñ±(x)y=œÉBN(Conv3D(BN(Conv3D(x))))+F(x)(5)where œÉ denotes the ReLU activation function, which is applied to the output of the preceding batch normalization layer. This structure not only ensures stable training in deep networks but also effectively enhances the fidelity of fine cracks within multi-scale features, enabling more accurate discrimination between intact eggshell regions and those containing cracks. In summary, the 3D Residual Block, by combining spatial‚Äìspectral joint modeling with residual enhancement, improves the robust extraction of crack features and provides more discriminative high-level semantic features for subsequent modeling.",
            "3.2.3. SimAM Attention Module": "Within the 3D-CrackNet framework, 3D Residual Block operates synergistically with the SimAM attention module. The 3D-ResBlock extracts joint spatial‚Äìspectral features of eggshell cracks through multi-scale convolution while maintaining training stability. The SimAM module then refines these features via fine-grained, voxel-level weighting, which amplifies crack-related responses and suppresses background interference. This complementary mechanism significantly enhances the precision and robustness of hyperspectral crack segmentation. Unlike conventional one-dimensional spectral attention or two-dimensional spatial attention, SimAM assigns a unique three-dimensional weight to each voxel in the input feature map. This capability enables the simultaneous modeling of discriminative information across both spatial dimensions (H, W) and the spectral dimension (D), allowing the model to highlight abnormal spectral reflectance in crack regions while reinforcing spatial continuity in their vicinity. A distinctive advantage of SimAM is its parameter-free design: attention weights are derived directly from a closed-form solution to an energy function, without introducing additional learnable parameters. This results in a lightweight structure with high computational efficiency and ease of integration into various network architectures. The design is inspired by visual neuroscience, where salient features (e.g., abnormal spectral signatures or surface textures) naturally attract attention in biological visual systems, while redundant background information is suppressed. Given an input feature map of size H √ó W √ó D, the SimAM module constructs an energy function for each voxel to evaluate its separability from other voxels within the same channel, defined as:E(xi)=(yg‚àíg*)2+1N‚àí1‚àëi=1N‚àí1(yo‚àíx*i)2E(xi)=(yg‚àíg*)2+1N‚àí1‚àëi=1N‚àí1(yo‚àíxi*)2(6)whereygygdenotes the response of the target voxel,g*g*andx*ixi*represent the channel mean and other voxel values within the same channel, respectively, andNNis the total number of voxels per channel. This energy function determines the saliency of a voxel by quantifying the difference between the target feature and the surrounding background features. Subsequently, the energy values are normalized using a Sigmoid function to produce a voxel-wise weight matrix, which is applied element-wise to the input feature map to yield the attention-weighted output:X‚Ä≤=œÉ(E(X))‚äôXX‚Ä≤=œÉ(E(X))‚äôX(7)where‚äô‚äôdenotes element-wise multiplication, andœÉœÉrepresents the Sigmoid function. Compared with one-dimensional or two-dimensional attention mechanisms, SimAM (Figure 6) offers several advantages: (1) Three-dimensional weight assignment, enabling the model to simultaneously account for joint spectral‚Äìspatial discriminative characteristics; (2) No additional learnable parameters‚Äîanalytical (closed-form) computation ensures a lightweight and efficient design; (3) Simple architecture, making it straightforward to integrate into existing networks. Leveraging this module, 3D-CrackNet can effectively highlight the abnormal spatial‚Äìspectral signatures of egg crack regions, thereby improving detection sensitivity and discriminative capability. Figure 6.SimAM attention structure.",
            "3.3. Experimental Setup and Evaluation Metrics": "The experiments were implemented in PyTorch 2.1.0 with CUDA 11.8 acceleration on a workstation featuring an NVIDIA GeForce RTX 3090 GPU and an Intel Core i7-13700KF CPU. The training procedure spanned 100 epochs with a batch size of 32, utilizing the AdamW optimizer at an initial learning rate of 0.0001. A Cosine Annealing Warm Restarts scheduler was used to adjust the learning rate dynamically, thereby balancing training efficiency and final performance. To mitigate the class imbalance issue in crack segmentation, the Focal Dice Loss was chosen as the criterion for model optimization. A total of 400 hyperspectral egg samples were collected and randomly divided into training (320 samples), validation (40 samples), and testing (40 samples) subsets at a ratio of 8:1:1. To mitigate the risk of overfitting, regularization was integrated through the training process and model architecture. An L2 weight decay term (Œª = 1 √ó 10‚àí4) was applied within the AdamW optimizer to penalize large weights. Furthermore, the inclusion of batch normalization layers in the 3D-CNN architecture helped stabilize training and act as an implicit regularizer. Most importantly, an early stopping strategy was adopted, which halted training if the validation loss did not improve for 15 consecutive epochs, thereby preventing the model from over-optimizing on the training data. These strategies collectively enhanced the model‚Äôs ability to generalize to unseen samples. Since crack pixels occupy only a small proportion of the overall image, conventional overall accuracy metrics cannot adequately evaluate crack recognition performance. Therefore, a multi-dimensional evaluation strategy was adopted in this study. Based on the confusion matrix, four standard metrics‚ÄîIntersection over Union (IoU), Precision, Recall, and F1-Score‚Äîwere computed to comprehensively assess segmentation performance. In addition, the model‚Äôs training time was recorded to evaluate computational efficiency in practical applications.IOU=TPTP+FP+FNIOU=TPTP+FP+FN(8)Precision=TPTP+FPPrecision=TPTP+FP(9)Recall=TPTP+FNRecall=TPTP+FN(10)F1-score=2TP2TP+FP+FNF1-score=2TP2TP+FP+FN(11) Here, TP denotes the number of positive samples correctly predicted by the model; FP denotes the number of negative samples incorrectly predicted as positive; and FN denotes the number of positive samples incorrectly predicted as negative.",
            "4. Experimental Results and Analysis": "4.1. Spectral Feature Analysis of Intact and Cracked TargetsA systematic analysis of hyperspectral reflectance data from cracked and intact eggshell regions was conducted.Figure 7a illustrates the raw spectral curves for the two sample types in the 1000‚Äì2500 nm wavelength range. As observed, the raw data exhibit baseline drift and scattering effects, leading to considerable intra-class variation. To highlight the overall trend, the mean spectra for each class were calculated, as shown inFigure 7b. The spectral shapes demonstrate high consistency with significant overlap, with only slight variations in reflectance intensity. Specifically, cracked eggshells generally exhibit lower reflectance than intact eggshells, yet the overall spectral pattern remains consistent across all bands. To enhance spectral quality and accentuate subtle differences, the raw spectral data were pre-processed using the Savitzky‚ÄìGolay (SG) smoothing algorithm, combined with Multiplicative Scatter Correction (MSC) [25,26]. The results of this preprocessing are shown inFigure 7c, where the spectral curves are more compact and concentrated. This indicates that the preprocessing method effectively reduces baseline drift, scattering effects, and other non-chemical interferences. As a result, the mean spectral curve inFigure 7d is smoother and more standardized.Figure 7.Spectral profiles of intact and cracked eggshells: (a) original spectra; (b) mean original spectra; (c) spectra after SG + MSC preprocessing; (d) mean spectra after SG + MSC preprocessing.From a biochemical perspective, spectral characteristics at different wavelengths are closely tied to the primary chemical constituents of the eggshell and its underlying structures. Considering that the average eggshell thickness is approximately 0.30‚Äì0.40 mm, the incident near-infrared light (1000‚Äì2500 nm) exhibits sufficient penetration depth to pass through the calcified layer and interact with the shell membrane and outer albumen [27,28]. Consequently, the recorded spectra represent a composite signal of the shell and subsurface components. Previous studies have demonstrated that the absorption valleys near 1450 nm and 1940 nm correspond to the first and second overtone water absorption bands [29,30], which are detectable due to the presence of bound moisture in the underlying organic membrane and albumen. Although the calcified shell itself contains limited water, micro-cracks may increase the optical contribution of the exposed membrane, thereby influencing reflectance in these bands. Furthermore, the region between 2100 and 2300 nm has been linked to combination absorptions of C=O and N‚ÄìH functional groups associated with the protein-rich eggshell membrane [31]. Micro-cracks can alter the scattering path and partially expose this membrane material, resulting in subtle spectral differences within this range. Additionally, near-infrared scattering features related to the crystalline CaCO3structure may also be affected by microstructural disruption of the shell.Despite the preprocessing, the spectral curves of both classes still exhibit significant overlap across most wavelengths, indicating weak separability. Despite the preprocessing, the spectral curves of both classes still exhibit significant overlap across most wavelengths, indicating weak separability. The main challenge in hyperspectral crack detection lies in the high spectral redundancy across the broad 1000‚Äì2500 nm wavelength range, where adjacent bands are strongly correlated, with many contributing minimally to biochemical discrimination. This redundancy in-creases the computational complexity of data processing and model training and can obscure subtle spectral variations caused by cracks. 4.2. Feature Band Extraction and Performance Analysis for Crack DetectionGiven the spectral redundancy and weak separability identified inSection 4.1, three feature band selection methods‚ÄîCARS, SPA, and RF‚Äîwere applied to reduce dimensionality, mitigate multicollinearity, and enhance crack detection performance.(1)Feature Band Selection Using the CARS AlgorithmFor the CARS algorithm, the number of iterations was set to 50, and the sampling ratio was set to 0.8. As illustrated inFigure 8, the feature selection process significantly reduces the number of wavelengths. InFigure 8a, the initial count of approximately 280 wavelengths rapidly decreases as iterations progress, indicating the efficient removal of redundant and irrelevant features.Figure 8b shows that the RMSECV, calculated via Monte Carlo cross-validation (MCCV), decreases steadily until reaching a minimum at the 30th iteration, suggesting optimal predictive performance at this point. After the 30th iteration, the RMSECV slightly increases as some critical features are inadvertently discarded. The regression coefficients inFigure 8c confirm that the 30th iteration marks the optimal selection, resulting in the identification of 15 key wavelengths.Figure 8.Feature wavelengths selection using the CARS method: (a) number of selected feature wavelengths; (b) RMSECV values across iterations; (c) regression coefficient paths, where each colored curve represents the change in the regression coefficient of a specific wavelength variable across sampling iterations; the blue vertical dashed line indicates the optimal number of sampling runs selected based on the minimal RMSECV.(2)Feature Band Selection Using the SPA AlgorithmFor the SPA algorithm, the maximum number of wavelengths was set to 20. As shown inFigure 9a, the RMSE value decreases sharply at first as more wavelengths are added, then plateaus, indicating the elimination of redundant information. For the SG + MSC preprocessed spectral data, 13 key wavelengths were selected, achieving the lowest RMSE and optimal predictive accuracy, as shown inFigure 9b. These 13 wavelengths cover multiple crucial regions of the spectrum, providing a comprehensive representation of the samples‚Äô spectral characteristics.Figure 9.Feature wavelength selection using the SPA method: (a) RMSE versus number of wavelengths; (b) distribution of selected wavelengths.(3)Feature Band Selection Using the RF AlgorithmThe RF algorithm obtains the selection probability distribution of each wavelength variable through multiple iterations of sampling (Figure 10). The horizontal axis represents the full-spectrum wavelength index, while the vertical axis denotes the probability of a variable being selected. To identify highly important bands, a selection probability threshold of 0.2 was set, with variables exceeding this threshold considered to be strongly correlated with the modeling objective. The RF method ultimately selected 23 key wavelengths, providing an alternative feature selection approach.Figure 10.Feature wavelength selection using the RF method: probability distribution of selected wavelengths.(4)Method Comparison and Performance EvaluationTo evaluate the effectiveness of the three feature selection methods, the selected wavelengths from CARS, SPA, and RF were used as model inputs and compared with the full-band input. The results, summarized inTable 1, demonstrate that the CARS method outperforms all other methods across all evaluation metrics. Specifically, the IoU and F1 scores for CARS reached 59.99% and 74.99%, respectively‚Äîan improvement of approximately 8.2% and 6.8% over the full-band input‚Äîwhile reducing training time to 35.5 min. Despite a reduction of over 90% in data dimensionality, CARS maintained or even improved segmentation performance, effectively eliminating noise and redundancy while preserving the most discriminative features. In contrast, while the SPA method offers a training efficiency advantage, its accuracy metrics are slightly lower than those of CARS. The RF method performed well in terms of Precision but had a low Recall, resulting in overall performance inferior to both CARS and SPA. The full-band input method achieved the lowest scores across all metrics and incurred significantly higher training costs. In conclusion, similar to the spectral feature analysis inSection 4.1, the results from this feature band selection process demonstrate that the CARS method strikes the optimal balance between accuracy and efficiency. Therefore, the feature bands selected by CARS were adopted as model inputs for all subsequent experiments.Table 1.Performance comparison of different feature band selection methods and full-band input. 4.3. Ablation ExperimentTo evaluate the individual contributions of the core components‚Äî3D-ResBlock and the SimAM attention mechanism‚Äîin the proposed 3D-CrackNet model, a comprehensive ablation study was conducted. A standard 3D-CNN served as the baseline. The performance impact of each component was assessed by progressively integrating them into the baseline model. All models were trained and tested under identical conditions using the 15 feature bands selected by the CARS method. To statistically validate the performance improvements, a paired t-test was performed on the evaluation metrics (IoU, Precision, Recall, F1-score) obtained from five independent runs of each model configuration. The results are summarized inTable 2.Table 2.Ablation experiment results and performance comparison.The baseline 3D-CNN model achieved IoU, Precision, Recall, and F1-score values of 52.08%, 71.09%, 66.07%, and 68.49%, respectively, serving as a reference for subsequent performance improvements. With the integration of the SimAM attention mechanism, the IoU increased to 53.95%, and Precision rose to 72.70%. Statistical analysis confirmed that the improvement in Precision was statistically significant (p< 0.05), indicating that this mechanism effectively enhances feature representation and spatial dependency modeling, thereby improving feature extraction performance. When the ResBlock module was incorporated, the IoU improved to 56.77%, and the F1-score increased to 72.42%. Both of these improvements were found to be statistically significant (p< 0.05), demonstrating its positive effect in strengthening deep feature propagation and mitigating gradient vanishing. By integrating both SimAM and ResBlock to form the complete 3D-CrackNet model, overall performance reached its peak: IoU, Precision, Recall, and F1-score achieved 60.62%, 75.09%, 75.89%, and 75.49%, respectively. These represent improvements of 8.54%, 4%, 9.82%, and 7% over the baseline model. All of these improvements over the baseline model were statistically significant (p< 0.05). Despite the increase in model parameters and training time, the performance gains significantly outweighed the additional computational cost, reflecting the model‚Äôs rational and efficient design. In conclusion, the ablation results fully demonstrate the complementarity and critical roles of ResBlock and SimAM. The complete 3D-CrackNet model significantly outperforms other combinations of components in terms of accuracy, recall, and overall performance, achieving an excellent balance between performance improvement and computational complexity. This validates the effectiveness and practical value of the proposed model for hyperspectral egg crack segmentation tasks. 4.4. Comparative ExperimentTo further validate the structural advantages of 3D-CrackNet in spatial‚Äìspectral feature extraction and crack detection, a series of comparative experiments were conducted with several representative convolutional neural network models as benchmarks. All models used the 15 feature bands selected by the CARS method as input, and training environments, hyperparameter settings, and data preprocessing were kept consistent. The 1D-CNN model classified solely based on pixel-level spectral vectors, ignoring spatial contextual features. The 2D-CNN model took single-band images as input and relied on spatial information, but it almost entirely lost spectral dimension features. The 3D-CNN (baseline) model jointly leveraged spatial and spectral information to achieve integrated spatial‚Äìspectral modeling. Building upon the 3D-CNN, the proposed 3D-CrackNet model incorporates a ResBlock and the SimAM attention mechanism to further enhance spatial‚Äìspectral feature representation.As shown in the training curves (Figure 11), 3D-CrackNet converges to a stable and significantly higher F1-score (approximately 0.75) after about 25 epochs, while also exhibiting the most rapid loss reduction and the lowest final loss value. According to the quantitative results inTable 3, the IoU of the 1D-CNN is only 29.29%, the lowest among all models, suggesting that relying solely on spectral information is insufficient for crack detection. With the incorporation of spatial information, the 2D-CNN shows a marked improvement over the 1D-CNN, achieving an IoU of 40.37% and an F1-score of 57.33%. However, due to the absence of spectral dimension features, it still suffers from significant limitations. The 3D-CNN, which simultaneously leverages spatial and spectral information, significantly outperforms the previous two models, attaining an IoU of 52.51% and an F1-score of 68.86%. Building upon this, 3D-CrackNet achieves a further breakthrough, with IoU and F1-score increasing to 60.62% and 75.49%, respectively‚Äîrepresenting improvements of 8.11% and 6.63% over the baseline 3D-CNN. These results place 3D-CrackNet‚Äôs performance among the leading results in deep learning-based crack detection.Figure 11.Training convergence curves and loss decline curves of different CNN models: (a) Trend of F1-score with Epochs; (b) Trend of loss with Epochs.Table 3.Performance comparison and analysis of different CNN models.The fundamental reason for the performance enhancement lies in the ResBlock‚Äôs ability to mitigate network degradation and strengthen deep feature propagation, while the SimAM attention mechanism allows the model to focus on small crack regions, thereby significantly improving its capability to detect subtle defects. Based on the comparative results and training curve analysis, it is evident that 3D-CrackNet demonstrates significant advantages in joint spatial‚Äìspectral feature representation and crack segmentation tasks using hyperspectral data, surpassing common CNN models in both accuracy and stability. This fully validates its effectiveness and feasibility for practical applications. 4.5. Visualization Analysis of Crack Detection ResultsTo visually evaluate the performance of the enhanced 3D-CrackNet model in egg crack detection tasks, representative samples were selected for comparative visualization analysis. The comparison involved three models: the 1D-CNN, which relies solely on spectral information, and the 2D-CNN, which utilizes only spatial information. The results are shown inFigure 12, where green lines represent the true crack positions (Ground Truth), and red regions denote the predicted outputs of each model. To provide a quantitative reference for these visual comparisons, the instance-level F1-score and Intersection over Union (IoU) metrics for each prediction are annotated at the bottom of the corresponding sub-images.Figure 12.Visual comparison of crack segmentation results (Green: Ground Truth; Red: Model Predictions). From left to right: Original image, Ground Truth (GT), 1D-CNN, 2D-CNN, 3D-CNN, and 3D-CrackNet.As observed in the figure, the 1D-CNN, which depends exclusively on spectral information, struggles to capture spatial structural features. It tends to produce incomplete detections and false alarms, particularly for slender and low-contrast cracks, often misclassifying background spots as cracks.(e.g., resulting in a low IoU of 0.2301 for the spotted egg in the second row) The 2D-CNN, by leveraging spatial context, depicts crack shapes more accurately; however, due to its neglect of fine-grained spectral features, it fails to detect smaller cracks and generates false positives in regions with complex textures. The 3D-CNN model, which integrates both spectral and spatial information, shows clear improvements over both the 1D-CNN and 2D-CNN, yet some missed and false detections still remain. In contrast, the 3D-CrackNet model effectively integrates multi-dimensional information through joint spatial‚Äìspectral modeling, which is crucial for distinguishing structural cracks from superficial interferences. The ResBlock module ensures stable deep feature propagation, while the SimAM attention mechanism dynamically weights discriminative spectral‚Äìspatial characteristics, thereby specifically enhancing crack-related features and suppressing responses from irrelevant background variations such as stains and natural shell textures. Quantitative analysis of the representative samples displayed inFigure 12further confirms these visual observations. Specifically, for challenging scenarios with complex spotted backgrounds (Row 2), while the comparative models struggled with IoUs below 0.45, 3D-CrackNet achieved a remarkable IoU of 0.8478 and an F1-score of 0.9176. Across all displayed samples, the proposed model consistently maintained high metric scores (e.g., F1 > 0.68 and IoU > 0.51), significantly outperforming the baseline models. As a result, the predictions exhibit optimal performance in terms of crack contour integrity, positional accuracy, and robustness against false positives caused by non-crack regions. The model successfully segments fine cracks while effectively avoiding the misclassification of eggshell spots, stains, or texture patterns as cracks, demonstrating a clear advantage in handling real-world variability.In summary, the visualization results inFigure 12clearly highlight the superiority of pixel-level segmentation. They not only validate the robustness and accuracy of 3D-CrackNet under complex background conditions but also underscore why precise pixel-wise delineation is essential for reliable and quantitative assessment of eggshell cracks‚Äîsomething that object-level detection alone cannot achieve. The visualization results corroborate the quantitative findings, demonstrating that 3D-CrackNet provides a robust and precise solution for automated, pixel-level egg crack inspection, which is essential for reliable quality assessment.",
            "4.1. Spectral Feature Analysis of Intact and Cracked Targets": "A systematic analysis of hyperspectral reflectance data from cracked and intact eggshell regions was conducted.Figure 7a illustrates the raw spectral curves for the two sample types in the 1000‚Äì2500 nm wavelength range. As observed, the raw data exhibit baseline drift and scattering effects, leading to considerable intra-class variation. To highlight the overall trend, the mean spectra for each class were calculated, as shown inFigure 7b. The spectral shapes demonstrate high consistency with significant overlap, with only slight variations in reflectance intensity. Specifically, cracked eggshells generally exhibit lower reflectance than intact eggshells, yet the overall spectral pattern remains consistent across all bands. To enhance spectral quality and accentuate subtle differences, the raw spectral data were pre-processed using the Savitzky‚ÄìGolay (SG) smoothing algorithm, combined with Multiplicative Scatter Correction (MSC) [25,26]. The results of this preprocessing are shown inFigure 7c, where the spectral curves are more compact and concentrated. This indicates that the preprocessing method effectively reduces baseline drift, scattering effects, and other non-chemical interferences. As a result, the mean spectral curve inFigure 7d is smoother and more standardized. Figure 7.Spectral profiles of intact and cracked eggshells: (a) original spectra; (b) mean original spectra; (c) spectra after SG + MSC preprocessing; (d) mean spectra after SG + MSC preprocessing. From a biochemical perspective, spectral characteristics at different wavelengths are closely tied to the primary chemical constituents of the eggshell and its underlying structures. Considering that the average eggshell thickness is approximately 0.30‚Äì0.40 mm, the incident near-infrared light (1000‚Äì2500 nm) exhibits sufficient penetration depth to pass through the calcified layer and interact with the shell membrane and outer albumen [27,28]. Consequently, the recorded spectra represent a composite signal of the shell and subsurface components. Previous studies have demonstrated that the absorption valleys near 1450 nm and 1940 nm correspond to the first and second overtone water absorption bands [29,30], which are detectable due to the presence of bound moisture in the underlying organic membrane and albumen. Although the calcified shell itself contains limited water, micro-cracks may increase the optical contribution of the exposed membrane, thereby influencing reflectance in these bands. Furthermore, the region between 2100 and 2300 nm has been linked to combination absorptions of C=O and N‚ÄìH functional groups associated with the protein-rich eggshell membrane [31]. Micro-cracks can alter the scattering path and partially expose this membrane material, resulting in subtle spectral differences within this range. Additionally, near-infrared scattering features related to the crystalline CaCO3structure may also be affected by microstructural disruption of the shell. Despite the preprocessing, the spectral curves of both classes still exhibit significant overlap across most wavelengths, indicating weak separability. Despite the preprocessing, the spectral curves of both classes still exhibit significant overlap across most wavelengths, indicating weak separability. The main challenge in hyperspectral crack detection lies in the high spectral redundancy across the broad 1000‚Äì2500 nm wavelength range, where adjacent bands are strongly correlated, with many contributing minimally to biochemical discrimination. This redundancy in-creases the computational complexity of data processing and model training and can obscure subtle spectral variations caused by cracks.",
            "4.2. Feature Band Extraction and Performance Analysis for Crack Detection": "Given the spectral redundancy and weak separability identified inSection 4.1, three feature band selection methods‚ÄîCARS, SPA, and RF‚Äîwere applied to reduce dimensionality, mitigate multicollinearity, and enhance crack detection performance. (1)Feature Band Selection Using the CARS Algorithm For the CARS algorithm, the number of iterations was set to 50, and the sampling ratio was set to 0.8. As illustrated inFigure 8, the feature selection process significantly reduces the number of wavelengths. InFigure 8a, the initial count of approximately 280 wavelengths rapidly decreases as iterations progress, indicating the efficient removal of redundant and irrelevant features.Figure 8b shows that the RMSECV, calculated via Monte Carlo cross-validation (MCCV), decreases steadily until reaching a minimum at the 30th iteration, suggesting optimal predictive performance at this point. After the 30th iteration, the RMSECV slightly increases as some critical features are inadvertently discarded. The regression coefficients inFigure 8c confirm that the 30th iteration marks the optimal selection, resulting in the identification of 15 key wavelengths. Figure 8.Feature wavelengths selection using the CARS method: (a) number of selected feature wavelengths; (b) RMSECV values across iterations; (c) regression coefficient paths, where each colored curve represents the change in the regression coefficient of a specific wavelength variable across sampling iterations; the blue vertical dashed line indicates the optimal number of sampling runs selected based on the minimal RMSECV. (2)Feature Band Selection Using the SPA Algorithm For the SPA algorithm, the maximum number of wavelengths was set to 20. As shown inFigure 9a, the RMSE value decreases sharply at first as more wavelengths are added, then plateaus, indicating the elimination of redundant information. For the SG + MSC preprocessed spectral data, 13 key wavelengths were selected, achieving the lowest RMSE and optimal predictive accuracy, as shown inFigure 9b. These 13 wavelengths cover multiple crucial regions of the spectrum, providing a comprehensive representation of the samples‚Äô spectral characteristics. Figure 9.Feature wavelength selection using the SPA method: (a) RMSE versus number of wavelengths; (b) distribution of selected wavelengths. (3)Feature Band Selection Using the RF Algorithm The RF algorithm obtains the selection probability distribution of each wavelength variable through multiple iterations of sampling (Figure 10). The horizontal axis represents the full-spectrum wavelength index, while the vertical axis denotes the probability of a variable being selected. To identify highly important bands, a selection probability threshold of 0.2 was set, with variables exceeding this threshold considered to be strongly correlated with the modeling objective. The RF method ultimately selected 23 key wavelengths, providing an alternative feature selection approach. Figure 10.Feature wavelength selection using the RF method: probability distribution of selected wavelengths. (4)Method Comparison and Performance Evaluation To evaluate the effectiveness of the three feature selection methods, the selected wavelengths from CARS, SPA, and RF were used as model inputs and compared with the full-band input. The results, summarized inTable 1, demonstrate that the CARS method outperforms all other methods across all evaluation metrics. Specifically, the IoU and F1 scores for CARS reached 59.99% and 74.99%, respectively‚Äîan improvement of approximately 8.2% and 6.8% over the full-band input‚Äîwhile reducing training time to 35.5 min. Despite a reduction of over 90% in data dimensionality, CARS maintained or even improved segmentation performance, effectively eliminating noise and redundancy while preserving the most discriminative features. In contrast, while the SPA method offers a training efficiency advantage, its accuracy metrics are slightly lower than those of CARS. The RF method performed well in terms of Precision but had a low Recall, resulting in overall performance inferior to both CARS and SPA. The full-band input method achieved the lowest scores across all metrics and incurred significantly higher training costs. In conclusion, similar to the spectral feature analysis inSection 4.1, the results from this feature band selection process demonstrate that the CARS method strikes the optimal balance between accuracy and efficiency. Therefore, the feature bands selected by CARS were adopted as model inputs for all subsequent experiments. Table 1.Performance comparison of different feature band selection methods and full-band input.",
            "4.3. Ablation Experiment": "To evaluate the individual contributions of the core components‚Äî3D-ResBlock and the SimAM attention mechanism‚Äîin the proposed 3D-CrackNet model, a comprehensive ablation study was conducted. A standard 3D-CNN served as the baseline. The performance impact of each component was assessed by progressively integrating them into the baseline model. All models were trained and tested under identical conditions using the 15 feature bands selected by the CARS method. To statistically validate the performance improvements, a paired t-test was performed on the evaluation metrics (IoU, Precision, Recall, F1-score) obtained from five independent runs of each model configuration. The results are summarized inTable 2. Table 2.Ablation experiment results and performance comparison. The baseline 3D-CNN model achieved IoU, Precision, Recall, and F1-score values of 52.08%, 71.09%, 66.07%, and 68.49%, respectively, serving as a reference for subsequent performance improvements. With the integration of the SimAM attention mechanism, the IoU increased to 53.95%, and Precision rose to 72.70%. Statistical analysis confirmed that the improvement in Precision was statistically significant (p< 0.05), indicating that this mechanism effectively enhances feature representation and spatial dependency modeling, thereby improving feature extraction performance. When the ResBlock module was incorporated, the IoU improved to 56.77%, and the F1-score increased to 72.42%. Both of these improvements were found to be statistically significant (p< 0.05), demonstrating its positive effect in strengthening deep feature propagation and mitigating gradient vanishing. By integrating both SimAM and ResBlock to form the complete 3D-CrackNet model, overall performance reached its peak: IoU, Precision, Recall, and F1-score achieved 60.62%, 75.09%, 75.89%, and 75.49%, respectively. These represent improvements of 8.54%, 4%, 9.82%, and 7% over the baseline model. All of these improvements over the baseline model were statistically significant (p< 0.05). Despite the increase in model parameters and training time, the performance gains significantly outweighed the additional computational cost, reflecting the model‚Äôs rational and efficient design. In conclusion, the ablation results fully demonstrate the complementarity and critical roles of ResBlock and SimAM. The complete 3D-CrackNet model significantly outperforms other combinations of components in terms of accuracy, recall, and overall performance, achieving an excellent balance between performance improvement and computational complexity. This validates the effectiveness and practical value of the proposed model for hyperspectral egg crack segmentation tasks.",
            "4.4. Comparative Experiment": "To further validate the structural advantages of 3D-CrackNet in spatial‚Äìspectral feature extraction and crack detection, a series of comparative experiments were conducted with several representative convolutional neural network models as benchmarks. All models used the 15 feature bands selected by the CARS method as input, and training environments, hyperparameter settings, and data preprocessing were kept consistent. The 1D-CNN model classified solely based on pixel-level spectral vectors, ignoring spatial contextual features. The 2D-CNN model took single-band images as input and relied on spatial information, but it almost entirely lost spectral dimension features. The 3D-CNN (baseline) model jointly leveraged spatial and spectral information to achieve integrated spatial‚Äìspectral modeling. Building upon the 3D-CNN, the proposed 3D-CrackNet model incorporates a ResBlock and the SimAM attention mechanism to further enhance spatial‚Äìspectral feature representation. As shown in the training curves (Figure 11), 3D-CrackNet converges to a stable and significantly higher F1-score (approximately 0.75) after about 25 epochs, while also exhibiting the most rapid loss reduction and the lowest final loss value. According to the quantitative results inTable 3, the IoU of the 1D-CNN is only 29.29%, the lowest among all models, suggesting that relying solely on spectral information is insufficient for crack detection. With the incorporation of spatial information, the 2D-CNN shows a marked improvement over the 1D-CNN, achieving an IoU of 40.37% and an F1-score of 57.33%. However, due to the absence of spectral dimension features, it still suffers from significant limitations. The 3D-CNN, which simultaneously leverages spatial and spectral information, significantly outperforms the previous two models, attaining an IoU of 52.51% and an F1-score of 68.86%. Building upon this, 3D-CrackNet achieves a further breakthrough, with IoU and F1-score increasing to 60.62% and 75.49%, respectively‚Äîrepresenting improvements of 8.11% and 6.63% over the baseline 3D-CNN. These results place 3D-CrackNet‚Äôs performance among the leading results in deep learning-based crack detection. Figure 11.Training convergence curves and loss decline curves of different CNN models: (a) Trend of F1-score with Epochs; (b) Trend of loss with Epochs. Table 3.Performance comparison and analysis of different CNN models. The fundamental reason for the performance enhancement lies in the ResBlock‚Äôs ability to mitigate network degradation and strengthen deep feature propagation, while the SimAM attention mechanism allows the model to focus on small crack regions, thereby significantly improving its capability to detect subtle defects. Based on the comparative results and training curve analysis, it is evident that 3D-CrackNet demonstrates significant advantages in joint spatial‚Äìspectral feature representation and crack segmentation tasks using hyperspectral data, surpassing common CNN models in both accuracy and stability. This fully validates its effectiveness and feasibility for practical applications.",
            "4.5. Visualization Analysis of Crack Detection Results": "To visually evaluate the performance of the enhanced 3D-CrackNet model in egg crack detection tasks, representative samples were selected for comparative visualization analysis. The comparison involved three models: the 1D-CNN, which relies solely on spectral information, and the 2D-CNN, which utilizes only spatial information. The results are shown inFigure 12, where green lines represent the true crack positions (Ground Truth), and red regions denote the predicted outputs of each model. To provide a quantitative reference for these visual comparisons, the instance-level F1-score and Intersection over Union (IoU) metrics for each prediction are annotated at the bottom of the corresponding sub-images. Figure 12.Visual comparison of crack segmentation results (Green: Ground Truth; Red: Model Predictions). From left to right: Original image, Ground Truth (GT), 1D-CNN, 2D-CNN, 3D-CNN, and 3D-CrackNet. As observed in the figure, the 1D-CNN, which depends exclusively on spectral information, struggles to capture spatial structural features. It tends to produce incomplete detections and false alarms, particularly for slender and low-contrast cracks, often misclassifying background spots as cracks.(e.g., resulting in a low IoU of 0.2301 for the spotted egg in the second row) The 2D-CNN, by leveraging spatial context, depicts crack shapes more accurately; however, due to its neglect of fine-grained spectral features, it fails to detect smaller cracks and generates false positives in regions with complex textures. The 3D-CNN model, which integrates both spectral and spatial information, shows clear improvements over both the 1D-CNN and 2D-CNN, yet some missed and false detections still remain. In contrast, the 3D-CrackNet model effectively integrates multi-dimensional information through joint spatial‚Äìspectral modeling, which is crucial for distinguishing structural cracks from superficial interferences. The ResBlock module ensures stable deep feature propagation, while the SimAM attention mechanism dynamically weights discriminative spectral‚Äìspatial characteristics, thereby specifically enhancing crack-related features and suppressing responses from irrelevant background variations such as stains and natural shell textures. Quantitative analysis of the representative samples displayed inFigure 12further confirms these visual observations. Specifically, for challenging scenarios with complex spotted backgrounds (Row 2), while the comparative models struggled with IoUs below 0.45, 3D-CrackNet achieved a remarkable IoU of 0.8478 and an F1-score of 0.9176. Across all displayed samples, the proposed model consistently maintained high metric scores (e.g., F1 > 0.68 and IoU > 0.51), significantly outperforming the baseline models. As a result, the predictions exhibit optimal performance in terms of crack contour integrity, positional accuracy, and robustness against false positives caused by non-crack regions. The model successfully segments fine cracks while effectively avoiding the misclassification of eggshell spots, stains, or texture patterns as cracks, demonstrating a clear advantage in handling real-world variability. In summary, the visualization results inFigure 12clearly highlight the superiority of pixel-level segmentation. They not only validate the robustness and accuracy of 3D-CrackNet under complex background conditions but also underscore why precise pixel-wise delineation is essential for reliable and quantitative assessment of eggshell cracks‚Äîsomething that object-level detection alone cannot achieve. The visualization results corroborate the quantitative findings, demonstrating that 3D-CrackNet provides a robust and precise solution for automated, pixel-level egg crack inspection, which is essential for reliable quality assessment.",
            "5. Discussion": "This study addresses the application requirements of HSI in non-destructive egg crack detection by proposing the 3D-CrackNet model, which integrates feature band selection, deep spatial‚Äìspectral fusion, and a lightweight attention mechanism for high-precision recognition of fine cracks. Comprehensive experimental results demonstrate that the proposed method substantially improves detection accuracy, robustness, and generalization capability compared to existing models. These findings not only validate the effectiveness of individual techniques but also highlight several key scientific challenges and technical principles involved in integrating hyperspectral data with deep learning models for agricultural and food inspection applications. At the dataset level, hyperspectral imagery, despite its rich spectral information, faces challenges such as high-dimensional redundancy, noise sensitivity, and the ‚Äúcurse of dimensionality.‚Äù These issues make the rational selection of feature bands critical to model performance. In this study, dimensionality reduction was achieved using the CARS algorithm, which retained spectral channels strongly correlated with crack defects, thus improving both computational efficiency and recognition accuracy. This indicates that data-driven feature compression is not only an effective means for algorithm acceleration but also crucial for enhancing a model‚Äôs generalization ability. In future efforts to construct larger-scale, multi-variety, and multi-condition datasets, band selection and feature refinement will remain central steps for improving model stability and performance. At the methodological level, 3D-CrackNet demonstrates the efficacy of joint spatial‚Äìspectral modeling for crack detection. Traditional classification methods such as Botta et al. [5] achieve high accuracy by making a single binary decision per egg. In contrast, 3D-CrackNet performs pixel-wise segmentation‚Äîa more challenging task that requires precise localization of crack boundaries. Although the pixel-wise F1-score (75.49%) is numerically lower than classification accuracy, it provides critical morphological information (e.g., crack dimensions and location) essential for industrial grading. This capability allows differentiation between cosmetic and structural cracks, which binary classification cannot achieve. Thus, while direct metric comparison may be misleading, the proposed method offers superior utility for fine-grained quality control in practical applications. At the application level, the proposed method provides technical support for automated, non-destructive egg crack detection. Compared with traditional manual inspection or single optical methods, the hyperspectral imaging‚Äìdeep learning combined approach significantly enhances detection consistency and objectivity. While the current model performs excellently in laboratory settings, further optimization is required in terms of computational efficiency, hardware compatibility, and real-time processing capabilities to meet the demands of large-scale industrial production. Furthermore, to achieve industrial-grade full-shell inspection, the proposed method can be extended to a multi-view system, for instance, by employing a rotary mechanism or a multi-camera array for data acquisition. The core algorithm of this study, 3D-CrackNet, is well-suited for processing multi-view data. The primary subsequent challenge will focus on the fusion of multi-view segmentation results and the generation of a comprehensive defect map. Finally, while hyperspectral imaging provides rich spectral information, its deployment in industrial production lines may be constrained by hardware cost, system integration complexity, and the need for high-speed acquisition modules. Furthermore, industrial grading requires continuous inspection on conveyor belts, necessitating a total processing time per egg that is significantly faster than achieved here. Although the core 3D-CrackNet model inference shows promise with a latency of approximately 0.9 s, future work must focus on developing a synchronized, high-throughput pipeline. This entails integrating high-speed cameras, real-time preprocessing hardware (e.g., FPGAs), and further model optimization to bridge the gap between laboratory proof-of-concept and commercial-scale application. These factors should be considered when translating the proposed approach into large-scale commercial applications.",
            "6. Conclusions": "This study presents a non-destructive egg crack detection method based on HSI and the improved 3D-CrackNet model, which integrates the CARS algorithm for feature band selection, a 3D residual module, and the SimAM attention mechanism. The CARS algorithm effectively reduced dimensionality, retaining 15 key bands that enhanced detection accuracy and computational efficiency. Experimental results demonstrate that 3D-CrackNet outperforms 1D-CNN, 2D-CNN, and conventional 3D-CNN models, achieving an F1-score of 75.49% and an IoU of 60.62%. Ablation studies confirmed that the integration of ResBlock and SimAM significantly improved model performance, particularly in fine crack detection under complex backgrounds. These findings highlight the potential of HSI and deep learning for automated, high-precision crack detection in agricultural products. Future work will focus on further model optimization for real-time industrial application and expanding the detection capabilities to multiple defect categories."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2304-8158/14/24/4183",
        "scraped_at": "2025-12-05 23:52:06"
    },
    {
        "title": "Engaging Older Adults to Guide the Development of Passive Home Health Monitoring to Support Aging in Place",
        "authors": "byElinor Randi Schoenfeld,Tracy Trimboli,Kaylyn Schwartz,Givenchy Ayisi-Boahene,Patricia Bruckenthal,Erez Zadok,Shelley HorwitzandFan Ye",
        "journal": "Sensors2025,25(24), 7413; https://doi.org/10.3390/s25247413 (registering¬†DOI) - 5 Dec 2025",
        "abstract": "HighlightsWhat are the main findings?Older adults have an interest in and willingness to have home installed image free sensors in their homes to monitor health. Their willingness was amplified with changes to their living arrangements, health status, and experience with others having a health event that required getting help.Newly retired participants were generally younger and more frequent technology users. Thus, they were more knowledgeable about and accepting of incorporating our proposed sensors in their homes.What are the implication of the main findings?The study underscores the importance of involving potential users in technology development to create effective and acceptable solutions for aging in place.To be accepted, home health monitoring systems must be cost-conscious, privacy preserving and flexible enough to accommodate individuals at different life phases and comfort levels, with different home environments and support systems.AbstractBy 2050, most adults aged 65 and older in the United States will want to age independently at home, a goal that will strain healthcare resources. Adults aged 50 and older (N = 112) were recruited for study participation between 2018 and 2022. They completed surveys and participated in discussion sessions to explore their needs and opinions regarding smart home sensors. Survey results indicated that older adults‚Äô comfort with smart home sensors increased with their perceived need for monitoring when home alone (OR = 1.46;p= 0.012) or sick/recovering from an illness (OR = 2.21;p< 0.001). When sick compared to when healthy, individuals were 2.65 times more likely to prefer installing multiple sensors in the living room, 1.75 times more likely in the kitchen, 3.66 times more likely in the bedroom, and 3.41 times more likely in the bathroom (p< 0.05). Regarding data sharing, participants were most willing to share information with healthcare providers and family members on a regular basis (80 and 81%, respectively) and 71% on a regular basis or when sick/recovering. Comfort with data sharing with professional caregivers (OR = 1.67;p= 0.0017) and monitoring companies (OR = 1.34;p= 0.030) significantly increased when sick/recovering. Discussion sessions highlighted overwhelming concerns about personal security/privacy, loss of independence, and ethical issues in data collection. Participants emphasized the need for new systems to be flexible, cost-effective, user-friendly, and respectful of user autonomy, accommodating diverse life stages, comfort levels, home environments, income levels, and support structures. Insights are now informing sensor data collection in our model home. Study findings underscore the importance of involving potential users in technology development to create effective and acceptable solutions for aging in place.Keywords:sensors;aging in place;smart homes;community engagement;passive remote monitoring;technology acceptance;older adults;health monitoring",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "The United States (US) population is undergoing a significant demographic shift, with a rapidly increasing number of older adults. Currently, 17.3% of the US population is 65 and older (57.8 million) [1,2]. By 2050, this figure will grow to 23% of the US population (82 million), with increasing diversity by race and ethnicity. More than ¬æ of older adults want to remain in their homes and communities for as long as possible [3,4]. This trend presents opportunities and challenges for healthcare, support services, and home health monitoring for this growing community-dwelling older adult population. While the older US population is growing, the healthcare workforce needed to support older adults is shrinking [5,6,7,8]. Thus, a role exists to further develop and utilize in-home sensors combined with artificial intelligence (AI) to create innovative living environments that support aging in place. These technologies range from passive, ambient sensors that do not require any physical or cognitive burden on users (e.g., radios) to monitor daily activities to wearable devices that track vital signs and detect falls [9,10,11,12,13,14,15,16,17]. With advancing technologies, personal and in-home medical emergency response systems, smart home security, and smart home devices are becoming more sophisticated and accepted [3]. By combining these systems with AI algorithms, one can analyze the generated data to learn an individual‚Äôs routines and detect subtle changes in behavior. These data may indicate a health issue, cognitive decline, or an emergency [18]. For example, decreased kitchen activity might suggest a loss of appetite, while changes in sleep patterns could indicate a developing health issue. By identifying these patterns, AI-powered systems can alert caregivers or healthcare providers, enabling early intervention and preventing more serious outcomes. Prior studies have identified a number of barriers to technology adoption, including: 1‚Äîresistance to divulging home difficulties to family members or healthcare providers for fear that they will have to give up their independence and leave their homes [19,20,21,22]; 2‚Äîstruggling with new technology, especially with increasing age and cognitive decline [23,24,25]; 3‚Äîfear of the high cost of purchasing and maintaining required devices and services [24,26,27,28,29,30,31]; 4‚Äîfeeling that current technology is not designed for users across the lifespan [14,24,30,32]. Despite these barriers, older US-based adults are growing more digitally connected [33]. COVID-19 isolation helped accelerate technology adoption, and recognition of the benefits of using technology to assist with detecting and monitoring illness and increasing age [24]. While 60% of older adults living in the US are aware of in-home sensors, fewer than 5% have adopted the use of these devices in their homes. They either use non-technology methods or are unaware of technology that may be available to support their health [24]. This trend is also observed in Canada. Although passive remote monitoring to support aging in place is publicly funded, utilization is low due to technology maturity and limited program awareness [34]. As technologies to support aging in place become more mature and user-friendly, attitudes and acceptance are slowly changing [3,9,35]. Our study aimed to identify the uses for, acceptance of, challenges, and barriers to adopting passive remote home monitoring to support aging in place, and how the attitudes and acceptance have changed. The primary objectives were to gain insights from community dwelling older adults‚Äô technology use and acceptance, data sharing preferences, opinions on home sensor placement, demographic differences in opinions, and interest in future technologies. We will then utilize knowledge gained to facilitate sensor development and deployment, addressing the needs and challenges of technology adoption to support aging in place.",
            "2. Materials and Methods": "The study employed a mixed-methods approach, combining surveys and discussion sessions with older adults to inform the development of novel ultrawideband wireless sensors for capturing physiologic and location data, thereby supporting aging in place. These measurements, combined with machine learning and AI-developed algorithms, have the potential to provide greater insights into changes in health among older adults aging in place. To ensure the future adoption of these new devices by community dwelling older adults and individuals with chronic conditions, we are engaging multiple consumer groups throughout the technology development and implementation process to facilitate their integration into home health monitoring activities. Community-dwelling older adults aged 50 and above from the New York metropolitan region within the United States were recruited through partnerships with community organizations and senior living communities. Recruitment methods included invited speaking engagements where the study team presented the study but did not collect identifiable information (using an unsigned consent form). Additionally, recruitment announcements were distributed by community partner organizations, and interested individuals contacted the study team, leading to the completion of signed consent forms or e-consent, where participant identification was known. To be eligible, participants had to be at least 50 years old, live independently in the community, be fluent in English, and provide their own consent. Individuals under 50, non-English speaking, or living in assisted living facilities or nursing homes were not eligible. Since our goal for technology development is to help older adults live in their own homes for as long as possible, we limited participation to community-dwelling older residents who did not require continuous care, as provided by assisted living or nursing homes. To ensure the protection of human subjects/study participants, Institutional Review Board (IRB) approval was initially obtained in 2018 from the Stony Brook University-wide IRB prior to the start of recruitment and data collection (IRB #878261). IRB review and approval of the study protocol and data collection tools are required to ensure the protection of human subjects throughout study conduct. Subsequent amendments were submitted to accommodate changes in recruitment and data collection settings (in-person, virtual, or hybrid) throughout the study. Data collection involved completing surveys (in-person or online) and participating in scripted discussion sessions offered in-person or via Zoom. Details about survey questions are summarized below. A copy of the full survey is available inSupplementary Materials S1. To help overcome barriers to in-person involvement, the study team offered sessions during daylight hours at locations familiar to participants and accessible to people with disabilities. We sought locations that were easily accessible on foot or by public transportation and provided parking at no cost to participants when applicable. Online sessions provided greater flexibility in scheduling study participation. The limiting factor for participation during COVID-19 was potential participants‚Äô access to the internet and Zoom (Zoom Video Communications, San Jose, CA, USA). Discussion sessions continued until knowledge saturation was achieved. All survey questions were captured as closed-ended categorical questions. The survey included demographic questions related to age, gender, education, and living arrangements. Age was grouped into 10-year intervals to protect participant privacy and reduce reluctance to disclose age. Given the limited sample size, education was grouped into less than college and college graduate, and living arrangements were recoded into living alone, living with someone, or living in a group setting. The survey included questions about home technology use and comfort with technology to gauge an individual‚Äôs potential acceptance of home sensor installation. With respect to acceptance of home sensor installation, participants were asked ‚Äúhow comfortable would you be to have contactless sensors installed in your home to monitor your health and wellbeing in general, home alone, sick, or have a health problem that required continuous monitoring‚Äù. The choices were not comfortable at all, a single sensor in some rooms, multiple sensors in some rooms, and comfortable having as many as you want in as many rooms as you want. We then asked participants, ‚ÄúIf you had contactless room-based sensors installed in your home, with whom and under what conditions would you be comfortable sharing the information generated from these sensors on a regular basis, and when you are sick or recovering from an illness that may demand immediate attention‚Äù. We inquired about their comfort sharing with health care providers (doctors, nurses, social workers), professional caregivers, a company that provides monitoring services, family, friends, and neighbors. The final question regarding sensors installed in the home asked about their comfort and the number of sensors by location in the home. We asked, ‚ÄúIn thinking about sensors as a tool to help you when you are either healthy or sick, in which locations would you be comfortable having them installed, living room, bedroom, bathroom, or kitchen?‚Äù For each location, we asked whether they would be comfortable with one sensor or multiple sensors when healthy, sick, or recovering. The survey took about 15 min to complete. Each discussion session lasted 1.5‚Äì2 h. Sessions began with obtaining or confirming informed consent and using a teach-back method to ensure participant understanding. As part of the session introductions, participants were asked to use only their first names for privacy, as the sessions were recorded. A project overview, goals, and a vignette/persona were presented to facilitate discussion about challenges related to aging in place, followed by questions about personal experiences with home and personal technology use. The vignette/persona highlighted the complexities of aging in place, family dynamics, and the challenges of meeting the needs of an older adult and their family. Sensor and security discussions included a summary of home-based sensor roles and an overview of data collection and security, led by project team members (FY and EZ). A copy of the discussion session presentation and questions posed to participants can be found inSupplementary Materials S2. Participants discussed their comfort levels with sensors, data sharing preferences (who, when, under what conditions), and explored privacy, security, ethical, and societal issues. Participants were encouraged to ask general questions about the study throughout the session. Demographic data, experiences using technologies, the number of home sensors, locations, and with whom and under what conditions they would share data were all collected via a self-completed survey, then analyzed with ages grouped categorically to preserve confidentiality. Study participants did not receive any monetary compensation for participation. REDCap (Research Electronic Data Capture, Vanderbilt University, KY, USA), a HIPAA-compliant data management system, was designed and used for e-consenting, online survey data collection, and data entry of forms completed on paper. Discussion sessions were recorded using Zoom or a voice recorder, and transcripts were subsequently generated. A scribe was present as a backup at each discussion session. All study session transcripts were reviewed and deidentified as needed to remove names, references to specific locations, and references to healthcare providers before the study team conducted analyses. All recordings and transcripts were stored in a secure, HIPAA-compliant, on-premises web-based tool that offered password protection and role-based file storage, maintained by the University‚Äôs Research IT group. Only the grant and study PIs (FY and ERS, respectively) had access to the recordings and transcripts before de-identification. One study team member reviewed and de-identified all transcripts before sharing with the study team for analysis (ERS). Because of the methods for survey completion and discussion group recordings, the study was unable to link survey responses to statements made by study participants during discussion sessions, thus protecting their privacy. Survey data were analyzed using IBM SPSS (version 29.0.0.0, IBM, NY, USA). Crosstabs and chi-square tests for significance were used for univariate analysis. Logistic regression was used to examine preferences regarding comfort with sensors in the home, sensor location, and with whom to share data generated by the study sensors, withp-values, odds ratios (ORs), and confidence intervals (CIs). The logistic models controlled for age, gender, home environment, education, and home living arrangements. Statistical significance in this study was defined as ap-value < 0.05. Data analysis of qualitative data from discussion sessions employed an inductive thematic analysis approach [13,33,34] involving a core multidisciplinary team comprising nursing (KS), geriatrics (TT), epidemiology (ERS), and community collaboration (GA). Team members independently reviewed transcripts, identified themes/subthemes, and then the team collectively reached consensus. Findings were then shared with the full multidisciplinary team (including social welfare, engineering, and computer science) to review results and to guide technology development. A thematic table summarizing the major themes and subthemes was generated for presentation.",
            "3. Results": "Study participants were recruited from four unaffiliated organizations that support community-dwelling older adults. These included 1‚Äîan urban housing complex designed specifically for adults aged 55 and older living independently in the community (55+ community); 2‚Äîtwo suburban university-affiliated organizations that provided programs for community-dwelling older adults (general community); and 3‚Äîan independent living division from a rural continuous care community (continuous care). The leadership from the 55+ and the continuous care communities recruited and invited participants to the in-person sessions and coordinated all in-person activities. No names or contact information were shared for these participants. Thus, these participants remained anonymous to the study team. The university-affiliated organizations sent recruitment emails to their members, providing instructions to visit the REDCap survey site or contact a study member (ERS), who would coordinate sessions directly with participants. Discussion sessions held in the 55+ community and the independent living section of the continuous care facility were in-person. Discussion sessions conducted with local community-dwelling residents began as inperson and moved to fully online due to COVID-19. 3.1. Survey Results3.1.1. Demographics (Table 1andFigure 1)A total of 112 community dwelling older adults were recruited from the general community and older adult focused independent living complexes. These individuals completed the study survey and participated in one of 11 discussion sessions. The older adult focused communities included a 55+ independent housing complex in an urban setting and an independent living complex within a rural continuous care community. In 2018, 24 individuals (21%) were recruited from the 55+ apartment complex. Over a 3-year time period (2019, 2020, 2022), 68 individuals (61%) were recruited from the general community. In 2022, 20 individuals (18%) were recruited from the independent living complex within the continuous care community.Participants ranged in age from 50 to 90+, with 70‚Äì79 year olds making up the largest group of participants (n = 50; 46%); 53% were female; 78% had a college education or higher; and 68% lived with another individual (e.g., spouse, significant other, family member). Overall, participants were technology users, with at least 75% using smartphones, computers, texting, and video conferencing. Over 80% had access to cable and the internet via Wi-Fi (Table 1).In a logistic regression analysis examining differences in technology use by demographics, participants in the 55+ community were more likely to report basic cell phone use than those recruited from the other two communities (OR = 8.4;p-value = 0.0022). Similar findings were seen for participants who reported living in a group setting (OR = 15.2;p-value = 0.0073). Participants living in the general community were significantly more likely to have cable in their home (OR = 3.6;p-value = 0.036). Female participants were more likely to report using a wearable health device (OR = 7.5;p-value = 0.01). No other demographic factors were significantly associated with technology use. Due to the timing of data collection, it was not possible to differentiate between the recruitment site and the impact of COVID-19 on technology use because of collinearity.Surveys were completed in person by 77 participants and online in REDCap by 35 participants. Discussion sessions were initially held in person in 2018 and 2019. We transitioned to Zoom in 2020 and 2022 as the COVID-19 pandemic took hold in the US. As the pandemic dissipated, we resumed in-person sessions in the latter part of 2022. Between 7 and 24 individuals participated in each discussion session. No sessions were held in 2021 due to the COVID-19 pandemic, which limited our ability to recruit participants. The recruitment timeline and data collection methods are shown inFigure 1. Two fully in-person sessions were held in 2018 (n = 24 participants); 3 hybrid sessions were held in 2019, with online consent, surveys, and discussion sessions held in person. In 2020, due to the COVID-19 pandemic, consent, surveys, and 2 discussion sessions were completed via Zoom (n = 15). In the latter part of 2022, as the United States began reopening, we were able to hold 3 fully in-person sessions (n = 20).Table 1.Demographics of survey and discussion group participants.Table 1.Demographics of survey and discussion group participants.(N = 112)Participant Demographicsn (%)Recruitment site55+ apartment complex in an urban setting24 (21)General community in a suburban setting68 (61)Independent living complex in a rural continuous care community20 (18)Age50‚Äì591 (1)60‚Äì6926 (24)70‚Äì7950 (46)80+32 (29)missing3Female Gender57 (53)Education<College24 (22)College graduatemissing83 (78)5Living arrangementsLive alone20 (19)Live with someone73 (68)Live in a group settingmissing14 (13)5Home technology usen (%)Have technology access at homeBasic cellphone (e.g., flip phone)48 (54)Smartphone (e.g., Apple iPhone, Samsung Galaxy)93 (89)Tablet (e.g., iPad, Samsung)67 (67)Computer (desktop or laptop)88 (85)Cable/satellite access (e.g., Spectrum, Verizon)88 (87)Text messaging on your cellphone96 (92)Video conferencing (e.g., Zoom)78 (75)Internet access via WiFi (wireless)93 (89)Home health monitoring (e.g., BP machine) (n = 44) *28 (64)Wearable health devices (e.g., fitbit, apple watch) (n = 42) *13 (31)Falls detection monitoring *6 (14)Very comfortable with technology useMaking or receiving a call on your cellphone90 (88)Using your tablet (e.g., iPad, Samsung)32 (71)Using your computer (desktop or laptop)33 (75)Using an app on smartphone, tablet or computer34 (74)Text messaging on your cellphone40 (87)Video conferencing30 (67)Connecting to the internet80 (82)Home health monitoring devices *22 (48)Wearable health devices *15 (33)Falls detection monitoring *5 (11)Telehealth *11 (26)* question not asked of all participants.Figure 1.Study timeline and data collection methods.Figure 1.Study timeline and data collection methods.3.1.2. Sensor PreferencesEarly in the study, participants‚Äô overwhelming negative reaction to the potential for sensors to produce any type of body visualization using thermal, depth, or wireless radio sensors influenced our sensor development direction. Participants‚Äô reactions to these sensor types led the engineering team to focus development efforts on ultrawideband (UWB) radio-based sensors, which do not collect or produce any visual information. Consequently, later sessions focused on potential uses and barriers to UWB sensor deployment in the home, rather than other sensor types.3.1.3. Comfort with Home Installed Sensors by Number of Sensors and Individual Health Status (Figure 2andTable 2)We inquired about comfort with home-installed sensors under different health statuses (in general, home alone, sick, or with a chronic condition), and the number of sensors to be installed (none, single sensor, multiple sensors, as many as needed). As shown inFigure 2, a greater percentage of participants were comfortable with having as many sensors as needed installed in the home across all three health statuses. There was a significant difference in the preference for as many sensors as needed by health status. In the event that a participant was sick, they would be 1.5 times more likely to want as many sensors in their home as needed. Compared with the general health scenario, they would be 2.21 times more likely to want as many sensors as needed when sick (Table 2).Figure 2.Comfort with home-installed wireless radio sensors by number of sensors installed‚Äîin general, when home alone, or when sick or had a health problem that requires continuous monitoring.Figure 2.Comfort with home-installed wireless radio sensors by number of sensors installed‚Äîin general, when home alone, or when sick or had a health problem that requires continuous monitoring.Table 2.Strength of the association between health status and installation of as many sensors as needed based on logistic regression analysis.Table 2.Strength of the association between health status and installation of as many sensors as needed based on logistic regression analysis.Health StatusOdds Ratio (OR) *95% Confidence Intervalp-ValueSick vs. Home alone1.521.13‚Äì2.040.006Sick vs. in general2.211.52‚Äì3.21<0.001Home alone vs. in general1.461.09‚Äì1.950.012* Based on the logistic regression model controlling for age, gender, home environment, and education.3.1.4. Comfort with the Location for Home-Installed Sensors by Health Status and Number of Sensors (Figure 3andTable 3)We inquired about the in-home locations (living room, bedroom, bathroom, kitchen) and the quantity (one sensor vs. multiple sensors) in which participants would be comfortable having sensors installed, whether they are healthy or sick/recovering. Participants were most comfortable having one sensor installed in the living room when they were healthy and multiple sensors when they were sick or recovering (Figure 3). The percentage of participants expressing interest in home-installed sensors increased during sickness or recovery, especially in more private areas (i.e., the bedroom and bathroom). As presented inTable 3, when sick compared to when healthy, individuals were 2.65 times more likely to prefer installing multiple sensors in the living room, 1.75 times more likely in the kitchen, 3.66 times more likely in the bedroom, and 3.41 times more likely in the bathroom (p< 0.05). When healthy individuals were significantly less likely to want multiple sensors in any room, which is reflected in the odds ratios (all <1.0). Overall, these data show that individuals were more comfortable with having more sensors installed in their homes when they are sick/recovering, consistent with the use of close patient monitoring during recovery from illness.Figure 3.In thinking about sensors as a tool to help you when you are either healthy or sick, in which locations would you be comfortable having them installed?Figure 3.In thinking about sensors as a tool to help you when you are either healthy or sick, in which locations would you be comfortable having them installed?Table 3.Strength of the association between the location of home-installed sensors by health status and number of sensors using logistic regression analysis.Table 3.Strength of the association between the location of home-installed sensors by health status and number of sensors using logistic regression analysis.Location Withinthe HomePreference for Single vs. Multiple SensorsOdds Ratio *95% Confidence Intervalp-ValueLiving roomWhen sickMultiple vs. single sensor0.950.62‚Äì1.460.805When sick vs. healthyMultiple sensors2.651.67‚Äì4.20<0.0001When sick vs. healthySingle sensors1.140.71‚Äì1.840.591When healthyMultiple vs. single sensor0.410.26‚Äì0.63<0.0001KitchenWhen sickMultiple vs. single sensor0.750.52‚Äì1.080.124When sick vs. healthyMultiple sensors1.751.12‚Äì2.710.013When sick vs. healthySingle sensors1.410.99‚Äì2.010.056When healthyMultiple vs. single sensor0.600.40‚Äì0.920.019BedroomWhen sickMultiple vs. single sensor0.620.39‚Äì0.990.047When sick vs. healthyMultiple sensors3.662.29‚Äì5.84<0.0001When sick vs. healthySingle sensors3.091.85‚Äì5.16<0.0001When healthyMultiple vs. single sensor0.520.34‚Äì0.790.0023BathroomWhen sickMultiple vs. single sensor0.920.60‚Äì1.420.71When sick vs. healthyMultiple sensors3.412.22‚Äì5.24<0.0001When sick vs. healthySingle sensors2.091.38‚Äì3.170.0005When healthyMultiple vs. single sensor0.570.39‚Äì0.830.0033* Based on the logistic regression model controlling for age, gender, home environment, education, and home living arrangements.3.1.5. Data Sharing (Figure 4andTable 4)We asked participants with whom they would be comfortable sharing sensor-generated data and under what conditions. Participants were more willing to share their sensor data with a healthcare provider (80% on a regular basis, 81% when sick/recovering) and family member (71% both on a regular basis or when sick/recovering) (Figure 4). Less than 60% were willing to share their data with professional caregivers, monitoring companies, friends, or neighbors on a regular basis or when sick/recovering. There was no significant difference in comfort with sharing data with healthcare providers and family on a regular basis or when sick (Table 4). Participants were significantly more likely to want to share data with Professional Caregivers and a Monitoring Company when they were sick or recovering from an illness than on a regular basis (ORs of 1.67 and 1.34, respectively). No significant differences were observed in data sharing with other groups. These findings reflect greater comfort and trust among healthcare providers and their families with very personal, confidential information related to an individual‚Äôs health and well-being. When sick/recovering, individuals are more willing to share their data with others, reflecting possibly a hope that such parties (e.g., professional caregivers, monitoring companies) might provide additional benefits during difficult times.Figure 4.If you had contactless room-based sensors installed in your home, with whom and under what conditions would you be comfortable sharing the information generated from these sensors?Figure 4.If you had contactless room-based sensors installed in your home, with whom and under what conditions would you be comfortable sharing the information generated from these sensors?Table 4.Strength of the association between health status and who an individual would be comfortable sharing their sensor data when sick versus on a regular basis from logistic regression analysis.Table 4.Strength of the association between health status and who an individual would be comfortable sharing their sensor data when sick versus on a regular basis from logistic regression analysis.With Whom to Share Sensor DataOdds Ratio (OR) *95% Confidence Intervalp-ValueHealthcare Provider1.000.67‚Äì1.000.989Professional Caregiver1.671.21‚Äì2.300.0017Monitoring Company1.341.03‚Äì1.740.030Family1.000.69‚Äì1.461.00Friends1.420.98‚Äì2.050.061Neighbors1.230.86‚Äì1.740.256* Based on the logistic regression model, controlling for age, gender, home environment, and home living arrangements. 3.2. Summary of Discussion Session Findings‚ÄîThemes Identified from the DiscussionsThematic analysis identified 3 major themes and six subthemes.Table 5provides a thematic table summarizing the themes, subthemes, and key findings from our discussion sessions, reflecting participants‚Äô perspectives on aging in place and home monitoring. In the following sections, we present details for each theme, including relevant participant quotes.Table 5.Summary of discussion sessions themes and key findings.3.2.1. Theme #1: Challenges to Aging in PlaceParticipants noted physical and technology related challenges to remaining independent and aging in place. Physical challenges included trouble with stairs, inability to reach cabinets, hearing loss, and trouble seeing the phone to dial. Difficulties with technology included the expense of buying and maintaining, the difficulty of use, and the need for assistance with repairs. During the COVID-19 pandemic, challenges included disruptions to daily routines, accessing food, and seeking help with household tasks or broken technology.Participants expressed concerns about being unable to care for themselves, particularly in an emergency, when living independently and far from their children, other family members, and/or neighbors. During the COVID-19 pandemic, participants were particularly concerned about disruptions to their daily schedules, accessing food, and seeking assistance with household tasks and technology when it was not functioning properly, or they were unsure how to use it. Participants noted that before the COVID-19 pandemic, they were accustomed to going places (e.g., museums, restaurants, senior centers, libraries), meeting with friends, and engaging in ‚Äúshopping as an activity‚Äù. These were all curtailed or stopped during the height of the COVID-19 pandemic. Participants expressed concerns about online shopping, as they preferred seeing and feeling products before making a purchase. Food shopping became one of their biggest challenges during the COVID-19 pandemic. Participants post-COVID-19 were interested in continuing to visit with friends.Participants discussed various lifestyle and living arrangement adaptations due to health changes, family and friend relocation, loss of loved ones, and retirement from work. Individuals who found themselves now living alone or who were home alone during the day were much more interested in having sensors in their homes to alert others if there was a health ‚Äúproblem.‚Äù A few participants mentioned health events that happened to friends of theirs (e.g., stroke, heart attack, fall) that resulted in their review of their own home safety procedures and how to get help in an emergency. These individuals were more interested in having technology to help monitor their health when they were home alone. Those with chronic conditions were more interested in learning about technologies to help them age in place. Newly retired participants who still had more disposable income, were also more technologically connected (e.g., ring doorbells, digital locks) than those who were further from retirement. Thus, they are more accepting of installing other devices in their homes than older participants. Many participants asked if, at some point, insurance might also cover our technology.3.2.2. Major Theme #2: Home Monitoring Concerns and QuestionsParticipants were overwhelmingly concerned about privacy and security, and questioned who would see their sensor-generated data and derived information, and how it would be used. They were also worried about the amount of data collected and the burden it could place on themselves, their loved ones, and their healthcare providers. Participants were concerned about the potential impact on their independence if their children knew too much about their activities. There was concern that if the children knew too much, they would ‚Äúhave the talk with them‚Äù about no longer being able to live in their own homes. Participants were overwhelmingly concerned about the type of images that the sensors would collect. No one wanted their children to see them without clothes (e.g., in the shower, changing in the bedroom) and did not want their families to know ‚Äúwhen they are entertaining‚Äù. Participants were adamant about maintaining control over when they would be monitored. They strongly felt they should be able to turn monitoring on and off to protect their privacy and that of visitors (e.g., grandchildren, house guests). They did not want young children monitored in their homes and were concerned about protecting visitors‚Äô privacy from the collection of sensor data.Participants expressed concerns about the burden of home sensing, specifically the added expense/cost of purchasing and maintaining the technology, especially for those on a fixed income. This concern was a continuation of discussions about the support for currently used technologies when they break, especially the challenges of finding support technicians to help one remain ‚Äúconnected‚Äù. Participants noted potential adoption barriers, including cultural differences and disabilities related to aging (e.g., vision and hearing loss, assistive device use). Participants under 70 years of age were more frequent technology users and thus were much more knowledgeable about and accepting of incorporating the proposed sensors into their homes.3.2.3. Major Theme #3: Caregiver ConcernsAlthough participants were older adults themselves, they also served as caregivers for spouses, siblings, or other relatives. As caregivers, they reported feeling tired and constantly worried about watching their significant others. Due to disabilities and/or cognitive decline, some significant others wander in and out of the home, placing a burden and pressure on the caregiver. These concerns extended to home safety. For example, the significant other may leave something on the stove, fall, and remain on the floor until someone finds them. Another significant challenge for participants was balancing their own lives as caregivers with the needs of the individuals they care for, such as attending doctor appointments, obtaining food, and visiting with friends. Caregivers felt they needed help monitoring their loved ones while out of the home, such as when running errands. ‚ÄúKnowing everything is ok at home while I am out would give me peace of mind‚Äù. They discussed the advantages of monitoring loved ones when they were unavailable. Participants noted that getting in the car and driving 1 h or more to ‚Äúrun‚Äù to a relative in an emergency becomes more challenging as they age. Others mentioned that their children had moved out of the region, so having them as emergency contacts was not helpful. A common challenge for study participants and all older adults is when to stay in their own home and when to move. Individuals who moved due to changes in their own or their significant other‚Äôs health reported losing their friends and support networks. Thus, they reported the need to develop new support connections.COVID-19 was also noted to have a significant impact on the older adults they care for. During the height of the COVID-19 pandemic, participants remained in touch with their friends and family via phone and other technologies (e.g., Zoom and FaceTime). Caregivers have reported that they were surprised at how much their relatives‚Äô health declined because of COVID-19 shutdowns and isolation, and how much their homes needed repairs when they finally visited in person. They stated that ‚Äúmuch was masked by only seeing my relative from the waist up‚Äù.",
            "3.1. Survey Results": "3.1.1. Demographics (Table 1andFigure 1)A total of 112 community dwelling older adults were recruited from the general community and older adult focused independent living complexes. These individuals completed the study survey and participated in one of 11 discussion sessions. The older adult focused communities included a 55+ independent housing complex in an urban setting and an independent living complex within a rural continuous care community. In 2018, 24 individuals (21%) were recruited from the 55+ apartment complex. Over a 3-year time period (2019, 2020, 2022), 68 individuals (61%) were recruited from the general community. In 2022, 20 individuals (18%) were recruited from the independent living complex within the continuous care community.Participants ranged in age from 50 to 90+, with 70‚Äì79 year olds making up the largest group of participants (n = 50; 46%); 53% were female; 78% had a college education or higher; and 68% lived with another individual (e.g., spouse, significant other, family member). Overall, participants were technology users, with at least 75% using smartphones, computers, texting, and video conferencing. Over 80% had access to cable and the internet via Wi-Fi (Table 1).In a logistic regression analysis examining differences in technology use by demographics, participants in the 55+ community were more likely to report basic cell phone use than those recruited from the other two communities (OR = 8.4;p-value = 0.0022). Similar findings were seen for participants who reported living in a group setting (OR = 15.2;p-value = 0.0073). Participants living in the general community were significantly more likely to have cable in their home (OR = 3.6;p-value = 0.036). Female participants were more likely to report using a wearable health device (OR = 7.5;p-value = 0.01). No other demographic factors were significantly associated with technology use. Due to the timing of data collection, it was not possible to differentiate between the recruitment site and the impact of COVID-19 on technology use because of collinearity.Surveys were completed in person by 77 participants and online in REDCap by 35 participants. Discussion sessions were initially held in person in 2018 and 2019. We transitioned to Zoom in 2020 and 2022 as the COVID-19 pandemic took hold in the US. As the pandemic dissipated, we resumed in-person sessions in the latter part of 2022. Between 7 and 24 individuals participated in each discussion session. No sessions were held in 2021 due to the COVID-19 pandemic, which limited our ability to recruit participants. The recruitment timeline and data collection methods are shown inFigure 1. Two fully in-person sessions were held in 2018 (n = 24 participants); 3 hybrid sessions were held in 2019, with online consent, surveys, and discussion sessions held in person. In 2020, due to the COVID-19 pandemic, consent, surveys, and 2 discussion sessions were completed via Zoom (n = 15). In the latter part of 2022, as the United States began reopening, we were able to hold 3 fully in-person sessions (n = 20).Table 1.Demographics of survey and discussion group participants.Table 1.Demographics of survey and discussion group participants.(N = 112)Participant Demographicsn (%)Recruitment site55+ apartment complex in an urban setting24 (21)General community in a suburban setting68 (61)Independent living complex in a rural continuous care community20 (18)Age50‚Äì591 (1)60‚Äì6926 (24)70‚Äì7950 (46)80+32 (29)missing3Female Gender57 (53)Education<College24 (22)College graduatemissing83 (78)5Living arrangementsLive alone20 (19)Live with someone73 (68)Live in a group settingmissing14 (13)5Home technology usen (%)Have technology access at homeBasic cellphone (e.g., flip phone)48 (54)Smartphone (e.g., Apple iPhone, Samsung Galaxy)93 (89)Tablet (e.g., iPad, Samsung)67 (67)Computer (desktop or laptop)88 (85)Cable/satellite access (e.g., Spectrum, Verizon)88 (87)Text messaging on your cellphone96 (92)Video conferencing (e.g., Zoom)78 (75)Internet access via WiFi (wireless)93 (89)Home health monitoring (e.g., BP machine) (n = 44) *28 (64)Wearable health devices (e.g., fitbit, apple watch) (n = 42) *13 (31)Falls detection monitoring *6 (14)Very comfortable with technology useMaking or receiving a call on your cellphone90 (88)Using your tablet (e.g., iPad, Samsung)32 (71)Using your computer (desktop or laptop)33 (75)Using an app on smartphone, tablet or computer34 (74)Text messaging on your cellphone40 (87)Video conferencing30 (67)Connecting to the internet80 (82)Home health monitoring devices *22 (48)Wearable health devices *15 (33)Falls detection monitoring *5 (11)Telehealth *11 (26)* question not asked of all participants.Figure 1.Study timeline and data collection methods.Figure 1.Study timeline and data collection methods. 3.1.2. Sensor PreferencesEarly in the study, participants‚Äô overwhelming negative reaction to the potential for sensors to produce any type of body visualization using thermal, depth, or wireless radio sensors influenced our sensor development direction. Participants‚Äô reactions to these sensor types led the engineering team to focus development efforts on ultrawideband (UWB) radio-based sensors, which do not collect or produce any visual information. Consequently, later sessions focused on potential uses and barriers to UWB sensor deployment in the home, rather than other sensor types. 3.1.3. Comfort with Home Installed Sensors by Number of Sensors and Individual Health Status (Figure 2andTable 2)We inquired about comfort with home-installed sensors under different health statuses (in general, home alone, sick, or with a chronic condition), and the number of sensors to be installed (none, single sensor, multiple sensors, as many as needed). As shown inFigure 2, a greater percentage of participants were comfortable with having as many sensors as needed installed in the home across all three health statuses. There was a significant difference in the preference for as many sensors as needed by health status. In the event that a participant was sick, they would be 1.5 times more likely to want as many sensors in their home as needed. Compared with the general health scenario, they would be 2.21 times more likely to want as many sensors as needed when sick (Table 2).Figure 2.Comfort with home-installed wireless radio sensors by number of sensors installed‚Äîin general, when home alone, or when sick or had a health problem that requires continuous monitoring.Figure 2.Comfort with home-installed wireless radio sensors by number of sensors installed‚Äîin general, when home alone, or when sick or had a health problem that requires continuous monitoring.Table 2.Strength of the association between health status and installation of as many sensors as needed based on logistic regression analysis.Table 2.Strength of the association between health status and installation of as many sensors as needed based on logistic regression analysis.Health StatusOdds Ratio (OR) *95% Confidence Intervalp-ValueSick vs. Home alone1.521.13‚Äì2.040.006Sick vs. in general2.211.52‚Äì3.21<0.001Home alone vs. in general1.461.09‚Äì1.950.012* Based on the logistic regression model controlling for age, gender, home environment, and education. 3.1.4. Comfort with the Location for Home-Installed Sensors by Health Status and Number of Sensors (Figure 3andTable 3)We inquired about the in-home locations (living room, bedroom, bathroom, kitchen) and the quantity (one sensor vs. multiple sensors) in which participants would be comfortable having sensors installed, whether they are healthy or sick/recovering. Participants were most comfortable having one sensor installed in the living room when they were healthy and multiple sensors when they were sick or recovering (Figure 3). The percentage of participants expressing interest in home-installed sensors increased during sickness or recovery, especially in more private areas (i.e., the bedroom and bathroom). As presented inTable 3, when sick compared to when healthy, individuals were 2.65 times more likely to prefer installing multiple sensors in the living room, 1.75 times more likely in the kitchen, 3.66 times more likely in the bedroom, and 3.41 times more likely in the bathroom (p< 0.05). When healthy individuals were significantly less likely to want multiple sensors in any room, which is reflected in the odds ratios (all <1.0). Overall, these data show that individuals were more comfortable with having more sensors installed in their homes when they are sick/recovering, consistent with the use of close patient monitoring during recovery from illness.Figure 3.In thinking about sensors as a tool to help you when you are either healthy or sick, in which locations would you be comfortable having them installed?Figure 3.In thinking about sensors as a tool to help you when you are either healthy or sick, in which locations would you be comfortable having them installed?Table 3.Strength of the association between the location of home-installed sensors by health status and number of sensors using logistic regression analysis.Table 3.Strength of the association between the location of home-installed sensors by health status and number of sensors using logistic regression analysis.Location Withinthe HomePreference for Single vs. Multiple SensorsOdds Ratio *95% Confidence Intervalp-ValueLiving roomWhen sickMultiple vs. single sensor0.950.62‚Äì1.460.805When sick vs. healthyMultiple sensors2.651.67‚Äì4.20<0.0001When sick vs. healthySingle sensors1.140.71‚Äì1.840.591When healthyMultiple vs. single sensor0.410.26‚Äì0.63<0.0001KitchenWhen sickMultiple vs. single sensor0.750.52‚Äì1.080.124When sick vs. healthyMultiple sensors1.751.12‚Äì2.710.013When sick vs. healthySingle sensors1.410.99‚Äì2.010.056When healthyMultiple vs. single sensor0.600.40‚Äì0.920.019BedroomWhen sickMultiple vs. single sensor0.620.39‚Äì0.990.047When sick vs. healthyMultiple sensors3.662.29‚Äì5.84<0.0001When sick vs. healthySingle sensors3.091.85‚Äì5.16<0.0001When healthyMultiple vs. single sensor0.520.34‚Äì0.790.0023BathroomWhen sickMultiple vs. single sensor0.920.60‚Äì1.420.71When sick vs. healthyMultiple sensors3.412.22‚Äì5.24<0.0001When sick vs. healthySingle sensors2.091.38‚Äì3.170.0005When healthyMultiple vs. single sensor0.570.39‚Äì0.830.0033* Based on the logistic regression model controlling for age, gender, home environment, education, and home living arrangements. 3.1.5. Data Sharing (Figure 4andTable 4)We asked participants with whom they would be comfortable sharing sensor-generated data and under what conditions. Participants were more willing to share their sensor data with a healthcare provider (80% on a regular basis, 81% when sick/recovering) and family member (71% both on a regular basis or when sick/recovering) (Figure 4). Less than 60% were willing to share their data with professional caregivers, monitoring companies, friends, or neighbors on a regular basis or when sick/recovering. There was no significant difference in comfort with sharing data with healthcare providers and family on a regular basis or when sick (Table 4). Participants were significantly more likely to want to share data with Professional Caregivers and a Monitoring Company when they were sick or recovering from an illness than on a regular basis (ORs of 1.67 and 1.34, respectively). No significant differences were observed in data sharing with other groups. These findings reflect greater comfort and trust among healthcare providers and their families with very personal, confidential information related to an individual‚Äôs health and well-being. When sick/recovering, individuals are more willing to share their data with others, reflecting possibly a hope that such parties (e.g., professional caregivers, monitoring companies) might provide additional benefits during difficult times.Figure 4.If you had contactless room-based sensors installed in your home, with whom and under what conditions would you be comfortable sharing the information generated from these sensors?Figure 4.If you had contactless room-based sensors installed in your home, with whom and under what conditions would you be comfortable sharing the information generated from these sensors?Table 4.Strength of the association between health status and who an individual would be comfortable sharing their sensor data when sick versus on a regular basis from logistic regression analysis.Table 4.Strength of the association between health status and who an individual would be comfortable sharing their sensor data when sick versus on a regular basis from logistic regression analysis.With Whom to Share Sensor DataOdds Ratio (OR) *95% Confidence Intervalp-ValueHealthcare Provider1.000.67‚Äì1.000.989Professional Caregiver1.671.21‚Äì2.300.0017Monitoring Company1.341.03‚Äì1.740.030Family1.000.69‚Äì1.461.00Friends1.420.98‚Äì2.050.061Neighbors1.230.86‚Äì1.740.256* Based on the logistic regression model, controlling for age, gender, home environment, and home living arrangements.",
            "3.1.1. Demographics (Table 1andFigure 1)": "A total of 112 community dwelling older adults were recruited from the general community and older adult focused independent living complexes. These individuals completed the study survey and participated in one of 11 discussion sessions. The older adult focused communities included a 55+ independent housing complex in an urban setting and an independent living complex within a rural continuous care community. In 2018, 24 individuals (21%) were recruited from the 55+ apartment complex. Over a 3-year time period (2019, 2020, 2022), 68 individuals (61%) were recruited from the general community. In 2022, 20 individuals (18%) were recruited from the independent living complex within the continuous care community. Participants ranged in age from 50 to 90+, with 70‚Äì79 year olds making up the largest group of participants (n = 50; 46%); 53% were female; 78% had a college education or higher; and 68% lived with another individual (e.g., spouse, significant other, family member). Overall, participants were technology users, with at least 75% using smartphones, computers, texting, and video conferencing. Over 80% had access to cable and the internet via Wi-Fi (Table 1). In a logistic regression analysis examining differences in technology use by demographics, participants in the 55+ community were more likely to report basic cell phone use than those recruited from the other two communities (OR = 8.4;p-value = 0.0022). Similar findings were seen for participants who reported living in a group setting (OR = 15.2;p-value = 0.0073). Participants living in the general community were significantly more likely to have cable in their home (OR = 3.6;p-value = 0.036). Female participants were more likely to report using a wearable health device (OR = 7.5;p-value = 0.01). No other demographic factors were significantly associated with technology use. Due to the timing of data collection, it was not possible to differentiate between the recruitment site and the impact of COVID-19 on technology use because of collinearity. Surveys were completed in person by 77 participants and online in REDCap by 35 participants. Discussion sessions were initially held in person in 2018 and 2019. We transitioned to Zoom in 2020 and 2022 as the COVID-19 pandemic took hold in the US. As the pandemic dissipated, we resumed in-person sessions in the latter part of 2022. Between 7 and 24 individuals participated in each discussion session. No sessions were held in 2021 due to the COVID-19 pandemic, which limited our ability to recruit participants. The recruitment timeline and data collection methods are shown inFigure 1. Two fully in-person sessions were held in 2018 (n = 24 participants); 3 hybrid sessions were held in 2019, with online consent, surveys, and discussion sessions held in person. In 2020, due to the COVID-19 pandemic, consent, surveys, and 2 discussion sessions were completed via Zoom (n = 15). In the latter part of 2022, as the United States began reopening, we were able to hold 3 fully in-person sessions (n = 20). Table 1.Demographics of survey and discussion group participants. Table 1.Demographics of survey and discussion group participants.(N = 112)Participant Demographicsn (%)Recruitment site55+ apartment complex in an urban setting24 (21)General community in a suburban setting68 (61)Independent living complex in a rural continuous care community20 (18)Age50‚Äì591 (1)60‚Äì6926 (24)70‚Äì7950 (46)80+32 (29)missing3Female Gender57 (53)Education<College24 (22)College graduatemissing83 (78)5Living arrangementsLive alone20 (19)Live with someone73 (68)Live in a group settingmissing14 (13)5Home technology usen (%)Have technology access at homeBasic cellphone (e.g., flip phone)48 (54)Smartphone (e.g., Apple iPhone, Samsung Galaxy)93 (89)Tablet (e.g., iPad, Samsung)67 (67)Computer (desktop or laptop)88 (85)Cable/satellite access (e.g., Spectrum, Verizon)88 (87)Text messaging on your cellphone96 (92)Video conferencing (e.g., Zoom)78 (75)Internet access via WiFi (wireless)93 (89)Home health monitoring (e.g., BP machine) (n = 44) *28 (64)Wearable health devices (e.g., fitbit, apple watch) (n = 42) *13 (31)Falls detection monitoring *6 (14)Very comfortable with technology useMaking or receiving a call on your cellphone90 (88)Using your tablet (e.g., iPad, Samsung)32 (71)Using your computer (desktop or laptop)33 (75)Using an app on smartphone, tablet or computer34 (74)Text messaging on your cellphone40 (87)Video conferencing30 (67)Connecting to the internet80 (82)Home health monitoring devices *22 (48)Wearable health devices *15 (33)Falls detection monitoring *5 (11)Telehealth *11 (26)* question not asked of all participants. Figure 1.Study timeline and data collection methods. Figure 1.Study timeline and data collection methods.",
            "3.1.2. Sensor Preferences": "Early in the study, participants‚Äô overwhelming negative reaction to the potential for sensors to produce any type of body visualization using thermal, depth, or wireless radio sensors influenced our sensor development direction. Participants‚Äô reactions to these sensor types led the engineering team to focus development efforts on ultrawideband (UWB) radio-based sensors, which do not collect or produce any visual information. Consequently, later sessions focused on potential uses and barriers to UWB sensor deployment in the home, rather than other sensor types.",
            "3.1.3. Comfort with Home Installed Sensors by Number of Sensors and Individual Health Status (Figure 2andTable 2)": "We inquired about comfort with home-installed sensors under different health statuses (in general, home alone, sick, or with a chronic condition), and the number of sensors to be installed (none, single sensor, multiple sensors, as many as needed). As shown inFigure 2, a greater percentage of participants were comfortable with having as many sensors as needed installed in the home across all three health statuses. There was a significant difference in the preference for as many sensors as needed by health status. In the event that a participant was sick, they would be 1.5 times more likely to want as many sensors in their home as needed. Compared with the general health scenario, they would be 2.21 times more likely to want as many sensors as needed when sick (Table 2). Figure 2.Comfort with home-installed wireless radio sensors by number of sensors installed‚Äîin general, when home alone, or when sick or had a health problem that requires continuous monitoring. Figure 2.Comfort with home-installed wireless radio sensors by number of sensors installed‚Äîin general, when home alone, or when sick or had a health problem that requires continuous monitoring. Table 2.Strength of the association between health status and installation of as many sensors as needed based on logistic regression analysis. Table 2.Strength of the association between health status and installation of as many sensors as needed based on logistic regression analysis.Health StatusOdds Ratio (OR) *95% Confidence Intervalp-ValueSick vs. Home alone1.521.13‚Äì2.040.006Sick vs. in general2.211.52‚Äì3.21<0.001Home alone vs. in general1.461.09‚Äì1.950.012* Based on the logistic regression model controlling for age, gender, home environment, and education.",
            "3.1.4. Comfort with the Location for Home-Installed Sensors by Health Status and Number of Sensors (Figure 3andTable 3)": "We inquired about the in-home locations (living room, bedroom, bathroom, kitchen) and the quantity (one sensor vs. multiple sensors) in which participants would be comfortable having sensors installed, whether they are healthy or sick/recovering. Participants were most comfortable having one sensor installed in the living room when they were healthy and multiple sensors when they were sick or recovering (Figure 3). The percentage of participants expressing interest in home-installed sensors increased during sickness or recovery, especially in more private areas (i.e., the bedroom and bathroom). As presented inTable 3, when sick compared to when healthy, individuals were 2.65 times more likely to prefer installing multiple sensors in the living room, 1.75 times more likely in the kitchen, 3.66 times more likely in the bedroom, and 3.41 times more likely in the bathroom (p< 0.05). When healthy individuals were significantly less likely to want multiple sensors in any room, which is reflected in the odds ratios (all <1.0). Overall, these data show that individuals were more comfortable with having more sensors installed in their homes when they are sick/recovering, consistent with the use of close patient monitoring during recovery from illness. Figure 3.In thinking about sensors as a tool to help you when you are either healthy or sick, in which locations would you be comfortable having them installed? Figure 3.In thinking about sensors as a tool to help you when you are either healthy or sick, in which locations would you be comfortable having them installed? Table 3.Strength of the association between the location of home-installed sensors by health status and number of sensors using logistic regression analysis. Table 3.Strength of the association between the location of home-installed sensors by health status and number of sensors using logistic regression analysis.Location Withinthe HomePreference for Single vs. Multiple SensorsOdds Ratio *95% Confidence Intervalp-ValueLiving roomWhen sickMultiple vs. single sensor0.950.62‚Äì1.460.805When sick vs. healthyMultiple sensors2.651.67‚Äì4.20<0.0001When sick vs. healthySingle sensors1.140.71‚Äì1.840.591When healthyMultiple vs. single sensor0.410.26‚Äì0.63<0.0001KitchenWhen sickMultiple vs. single sensor0.750.52‚Äì1.080.124When sick vs. healthyMultiple sensors1.751.12‚Äì2.710.013When sick vs. healthySingle sensors1.410.99‚Äì2.010.056When healthyMultiple vs. single sensor0.600.40‚Äì0.920.019BedroomWhen sickMultiple vs. single sensor0.620.39‚Äì0.990.047When sick vs. healthyMultiple sensors3.662.29‚Äì5.84<0.0001When sick vs. healthySingle sensors3.091.85‚Äì5.16<0.0001When healthyMultiple vs. single sensor0.520.34‚Äì0.790.0023BathroomWhen sickMultiple vs. single sensor0.920.60‚Äì1.420.71When sick vs. healthyMultiple sensors3.412.22‚Äì5.24<0.0001When sick vs. healthySingle sensors2.091.38‚Äì3.170.0005When healthyMultiple vs. single sensor0.570.39‚Äì0.830.0033* Based on the logistic regression model controlling for age, gender, home environment, education, and home living arrangements.",
            "3.1.5. Data Sharing (Figure 4andTable 4)": "We asked participants with whom they would be comfortable sharing sensor-generated data and under what conditions. Participants were more willing to share their sensor data with a healthcare provider (80% on a regular basis, 81% when sick/recovering) and family member (71% both on a regular basis or when sick/recovering) (Figure 4). Less than 60% were willing to share their data with professional caregivers, monitoring companies, friends, or neighbors on a regular basis or when sick/recovering. There was no significant difference in comfort with sharing data with healthcare providers and family on a regular basis or when sick (Table 4). Participants were significantly more likely to want to share data with Professional Caregivers and a Monitoring Company when they were sick or recovering from an illness than on a regular basis (ORs of 1.67 and 1.34, respectively). No significant differences were observed in data sharing with other groups. These findings reflect greater comfort and trust among healthcare providers and their families with very personal, confidential information related to an individual‚Äôs health and well-being. When sick/recovering, individuals are more willing to share their data with others, reflecting possibly a hope that such parties (e.g., professional caregivers, monitoring companies) might provide additional benefits during difficult times. Figure 4.If you had contactless room-based sensors installed in your home, with whom and under what conditions would you be comfortable sharing the information generated from these sensors? Figure 4.If you had contactless room-based sensors installed in your home, with whom and under what conditions would you be comfortable sharing the information generated from these sensors? Table 4.Strength of the association between health status and who an individual would be comfortable sharing their sensor data when sick versus on a regular basis from logistic regression analysis. Table 4.Strength of the association between health status and who an individual would be comfortable sharing their sensor data when sick versus on a regular basis from logistic regression analysis.With Whom to Share Sensor DataOdds Ratio (OR) *95% Confidence Intervalp-ValueHealthcare Provider1.000.67‚Äì1.000.989Professional Caregiver1.671.21‚Äì2.300.0017Monitoring Company1.341.03‚Äì1.740.030Family1.000.69‚Äì1.461.00Friends1.420.98‚Äì2.050.061Neighbors1.230.86‚Äì1.740.256* Based on the logistic regression model, controlling for age, gender, home environment, and home living arrangements.",
            "3.2. Summary of Discussion Session Findings‚ÄîThemes Identified from the Discussions": "Thematic analysis identified 3 major themes and six subthemes.Table 5provides a thematic table summarizing the themes, subthemes, and key findings from our discussion sessions, reflecting participants‚Äô perspectives on aging in place and home monitoring. In the following sections, we present details for each theme, including relevant participant quotes. Table 5.Summary of discussion sessions themes and key findings. 3.2.1. Theme #1: Challenges to Aging in PlaceParticipants noted physical and technology related challenges to remaining independent and aging in place. Physical challenges included trouble with stairs, inability to reach cabinets, hearing loss, and trouble seeing the phone to dial. Difficulties with technology included the expense of buying and maintaining, the difficulty of use, and the need for assistance with repairs. During the COVID-19 pandemic, challenges included disruptions to daily routines, accessing food, and seeking help with household tasks or broken technology.Participants expressed concerns about being unable to care for themselves, particularly in an emergency, when living independently and far from their children, other family members, and/or neighbors. During the COVID-19 pandemic, participants were particularly concerned about disruptions to their daily schedules, accessing food, and seeking assistance with household tasks and technology when it was not functioning properly, or they were unsure how to use it. Participants noted that before the COVID-19 pandemic, they were accustomed to going places (e.g., museums, restaurants, senior centers, libraries), meeting with friends, and engaging in ‚Äúshopping as an activity‚Äù. These were all curtailed or stopped during the height of the COVID-19 pandemic. Participants expressed concerns about online shopping, as they preferred seeing and feeling products before making a purchase. Food shopping became one of their biggest challenges during the COVID-19 pandemic. Participants post-COVID-19 were interested in continuing to visit with friends.Participants discussed various lifestyle and living arrangement adaptations due to health changes, family and friend relocation, loss of loved ones, and retirement from work. Individuals who found themselves now living alone or who were home alone during the day were much more interested in having sensors in their homes to alert others if there was a health ‚Äúproblem.‚Äù A few participants mentioned health events that happened to friends of theirs (e.g., stroke, heart attack, fall) that resulted in their review of their own home safety procedures and how to get help in an emergency. These individuals were more interested in having technology to help monitor their health when they were home alone. Those with chronic conditions were more interested in learning about technologies to help them age in place. Newly retired participants who still had more disposable income, were also more technologically connected (e.g., ring doorbells, digital locks) than those who were further from retirement. Thus, they are more accepting of installing other devices in their homes than older participants. Many participants asked if, at some point, insurance might also cover our technology. 3.2.2. Major Theme #2: Home Monitoring Concerns and QuestionsParticipants were overwhelmingly concerned about privacy and security, and questioned who would see their sensor-generated data and derived information, and how it would be used. They were also worried about the amount of data collected and the burden it could place on themselves, their loved ones, and their healthcare providers. Participants were concerned about the potential impact on their independence if their children knew too much about their activities. There was concern that if the children knew too much, they would ‚Äúhave the talk with them‚Äù about no longer being able to live in their own homes. Participants were overwhelmingly concerned about the type of images that the sensors would collect. No one wanted their children to see them without clothes (e.g., in the shower, changing in the bedroom) and did not want their families to know ‚Äúwhen they are entertaining‚Äù. Participants were adamant about maintaining control over when they would be monitored. They strongly felt they should be able to turn monitoring on and off to protect their privacy and that of visitors (e.g., grandchildren, house guests). They did not want young children monitored in their homes and were concerned about protecting visitors‚Äô privacy from the collection of sensor data.Participants expressed concerns about the burden of home sensing, specifically the added expense/cost of purchasing and maintaining the technology, especially for those on a fixed income. This concern was a continuation of discussions about the support for currently used technologies when they break, especially the challenges of finding support technicians to help one remain ‚Äúconnected‚Äù. Participants noted potential adoption barriers, including cultural differences and disabilities related to aging (e.g., vision and hearing loss, assistive device use). Participants under 70 years of age were more frequent technology users and thus were much more knowledgeable about and accepting of incorporating the proposed sensors into their homes. 3.2.3. Major Theme #3: Caregiver ConcernsAlthough participants were older adults themselves, they also served as caregivers for spouses, siblings, or other relatives. As caregivers, they reported feeling tired and constantly worried about watching their significant others. Due to disabilities and/or cognitive decline, some significant others wander in and out of the home, placing a burden and pressure on the caregiver. These concerns extended to home safety. For example, the significant other may leave something on the stove, fall, and remain on the floor until someone finds them. Another significant challenge for participants was balancing their own lives as caregivers with the needs of the individuals they care for, such as attending doctor appointments, obtaining food, and visiting with friends. Caregivers felt they needed help monitoring their loved ones while out of the home, such as when running errands. ‚ÄúKnowing everything is ok at home while I am out would give me peace of mind‚Äù. They discussed the advantages of monitoring loved ones when they were unavailable. Participants noted that getting in the car and driving 1 h or more to ‚Äúrun‚Äù to a relative in an emergency becomes more challenging as they age. Others mentioned that their children had moved out of the region, so having them as emergency contacts was not helpful. A common challenge for study participants and all older adults is when to stay in their own home and when to move. Individuals who moved due to changes in their own or their significant other‚Äôs health reported losing their friends and support networks. Thus, they reported the need to develop new support connections.COVID-19 was also noted to have a significant impact on the older adults they care for. During the height of the COVID-19 pandemic, participants remained in touch with their friends and family via phone and other technologies (e.g., Zoom and FaceTime). Caregivers have reported that they were surprised at how much their relatives‚Äô health declined because of COVID-19 shutdowns and isolation, and how much their homes needed repairs when they finally visited in person. They stated that ‚Äúmuch was masked by only seeing my relative from the waist up‚Äù.",
            "3.2.1. Theme #1: Challenges to Aging in Place": "Participants noted physical and technology related challenges to remaining independent and aging in place. Physical challenges included trouble with stairs, inability to reach cabinets, hearing loss, and trouble seeing the phone to dial. Difficulties with technology included the expense of buying and maintaining, the difficulty of use, and the need for assistance with repairs. During the COVID-19 pandemic, challenges included disruptions to daily routines, accessing food, and seeking help with household tasks or broken technology. Participants expressed concerns about being unable to care for themselves, particularly in an emergency, when living independently and far from their children, other family members, and/or neighbors. During the COVID-19 pandemic, participants were particularly concerned about disruptions to their daily schedules, accessing food, and seeking assistance with household tasks and technology when it was not functioning properly, or they were unsure how to use it. Participants noted that before the COVID-19 pandemic, they were accustomed to going places (e.g., museums, restaurants, senior centers, libraries), meeting with friends, and engaging in ‚Äúshopping as an activity‚Äù. These were all curtailed or stopped during the height of the COVID-19 pandemic. Participants expressed concerns about online shopping, as they preferred seeing and feeling products before making a purchase. Food shopping became one of their biggest challenges during the COVID-19 pandemic. Participants post-COVID-19 were interested in continuing to visit with friends. Participants discussed various lifestyle and living arrangement adaptations due to health changes, family and friend relocation, loss of loved ones, and retirement from work. Individuals who found themselves now living alone or who were home alone during the day were much more interested in having sensors in their homes to alert others if there was a health ‚Äúproblem.‚Äù A few participants mentioned health events that happened to friends of theirs (e.g., stroke, heart attack, fall) that resulted in their review of their own home safety procedures and how to get help in an emergency. These individuals were more interested in having technology to help monitor their health when they were home alone. Those with chronic conditions were more interested in learning about technologies to help them age in place. Newly retired participants who still had more disposable income, were also more technologically connected (e.g., ring doorbells, digital locks) than those who were further from retirement. Thus, they are more accepting of installing other devices in their homes than older participants. Many participants asked if, at some point, insurance might also cover our technology.",
            "3.2.2. Major Theme #2: Home Monitoring Concerns and Questions": "Participants were overwhelmingly concerned about privacy and security, and questioned who would see their sensor-generated data and derived information, and how it would be used. They were also worried about the amount of data collected and the burden it could place on themselves, their loved ones, and their healthcare providers. Participants were concerned about the potential impact on their independence if their children knew too much about their activities. There was concern that if the children knew too much, they would ‚Äúhave the talk with them‚Äù about no longer being able to live in their own homes. Participants were overwhelmingly concerned about the type of images that the sensors would collect. No one wanted their children to see them without clothes (e.g., in the shower, changing in the bedroom) and did not want their families to know ‚Äúwhen they are entertaining‚Äù. Participants were adamant about maintaining control over when they would be monitored. They strongly felt they should be able to turn monitoring on and off to protect their privacy and that of visitors (e.g., grandchildren, house guests). They did not want young children monitored in their homes and were concerned about protecting visitors‚Äô privacy from the collection of sensor data. Participants expressed concerns about the burden of home sensing, specifically the added expense/cost of purchasing and maintaining the technology, especially for those on a fixed income. This concern was a continuation of discussions about the support for currently used technologies when they break, especially the challenges of finding support technicians to help one remain ‚Äúconnected‚Äù. Participants noted potential adoption barriers, including cultural differences and disabilities related to aging (e.g., vision and hearing loss, assistive device use). Participants under 70 years of age were more frequent technology users and thus were much more knowledgeable about and accepting of incorporating the proposed sensors into their homes.",
            "3.2.3. Major Theme #3: Caregiver Concerns": "Although participants were older adults themselves, they also served as caregivers for spouses, siblings, or other relatives. As caregivers, they reported feeling tired and constantly worried about watching their significant others. Due to disabilities and/or cognitive decline, some significant others wander in and out of the home, placing a burden and pressure on the caregiver. These concerns extended to home safety. For example, the significant other may leave something on the stove, fall, and remain on the floor until someone finds them. Another significant challenge for participants was balancing their own lives as caregivers with the needs of the individuals they care for, such as attending doctor appointments, obtaining food, and visiting with friends. Caregivers felt they needed help monitoring their loved ones while out of the home, such as when running errands. ‚ÄúKnowing everything is ok at home while I am out would give me peace of mind‚Äù. They discussed the advantages of monitoring loved ones when they were unavailable. Participants noted that getting in the car and driving 1 h or more to ‚Äúrun‚Äù to a relative in an emergency becomes more challenging as they age. Others mentioned that their children had moved out of the region, so having them as emergency contacts was not helpful. A common challenge for study participants and all older adults is when to stay in their own home and when to move. Individuals who moved due to changes in their own or their significant other‚Äôs health reported losing their friends and support networks. Thus, they reported the need to develop new support connections. COVID-19 was also noted to have a significant impact on the older adults they care for. During the height of the COVID-19 pandemic, participants remained in touch with their friends and family via phone and other technologies (e.g., Zoom and FaceTime). Caregivers have reported that they were surprised at how much their relatives‚Äô health declined because of COVID-19 shutdowns and isolation, and how much their homes needed repairs when they finally visited in person. They stated that ‚Äúmuch was masked by only seeing my relative from the waist up‚Äù.",
            "4. Discussion": "4.1. Principal DiscoveriesIn total, we recruited 112 community participants who represent our future home-health monitoring users. Using a combination of surveys and discussion sessions, we explored in depth new areas of acceptance and adoption of sensor use for home deployment. Our study appears to be the first to explore sensor configurations by room and health status across different home living arrangements, as well as the conditions under which data is shared and with whom. Study findings have guided technology choices and are guiding our sensor system design. Based on our findings, we are creating a system that is flexible enough to accommodate individuals at different life phases and comfort levels, across varying home environments and support systems, while providing a secure, privacy-preserving environment. Though prior studies have identified older adults‚Äô comfort with and interest in individual components for home sensing, our study is unique in being more comprehensive in exploring the extent of home sensing, data sharing and the need for technology adaptability across the lifespan [31].Our survey found that participants had different opinions as to the amount and location of sensor deployment, depending on their health status. There was greater comfort with sharing home monitoring data with healthcare providers and family, whether healthy or sick. The extent to which individuals were comfortable sharing their data beyond health care providers and family increased when sick or recovering from an illness though comfort remained far less. From discussion sessions, we further identified that cost, privacy, security, the potential for system hacking, and the availability of a support system (e.g., family, friends, neighbors, professional caregivers) were major concerns. Participants noted the need for assistance with technology related issues after deployment, as reported in prior quantitative research [11,36]. Participants were very enthusiastic about the vision of our proposed sensor system. They felt that having home monitoring as they aged would provide themselves, loved ones, caregivers, and healthcare providers with valuable information to help them remain safe at home and offer insight into their health between healthcare provider visits. Based upon these findings, initial home deployment for home health monitoring may be most successful targeting individuals post hospitalization or those with chronic health conditions. These individuals may be more amenable to home health monitoring to support aging in place.Participants from 55+ communities and continuous care facilities were more comfortable with monitoring, as these settings already have in-room help cords and wearable alert systems in place for residents. Although participants noted their availability, it is not always acceptable or feasible. This suggests that individuals already engaged in continuous care arrangements are highly receptive to data sharing, likely viewing it as integral to their well-being and health management. Experiences with participants‚Äô own or others‚Äô changing health status helped participants appreciate how continuous home sensing could assist them in getting help when needed and in aging in place. COVID-19 has altered the perspective of older adults and their caregivers on the use of technology. Participants noted 1‚Äîincreased use of telehealth to connect with their medical care team; 2‚Äîuse of Zoom and FaceTime to overcome social isolation and stay connected, and willingness to continue to use these technologies; and 3‚Äîincreased acceptance of smartwatches, especially the Apple Watch, as it now includes a falls-detection alert system. Survey findings and discussion sessions reveal shifting attitudes among participants regarding home monitoring, particularly in relation to changes in health status, home environment, and the absence of onsite caregivers. While living rooms and kitchens generally showed high comfort, comfort in bedrooms and, particularly, bathrooms was lower across different home environments and health conditions, highlighting privacy concerns or perceived intrusiveness in more private spaces, which was further supported during discussions. However, for those facing declining health or requiring additional care, even bathrooms can offer high comfort with multiple sensors, indicating that the perceived need outweighs privacy concerns.The combined findings from our survey and discussion sessions are driving our sensor kit creation and home deployment. We envision developing a sensor kit that is inexpensive, self-contained, and remotely monitorable. Based on initial participant input, we have focused on using off-the-shelf ultra-wideband (UWB) sensors that do not generate any body images to address privacy concerns while remaining cost-effective for widespread deployment. We are pairing this UWB sensor with a Raspberry Pi for data collection, with data sent initially to an edge machine and then to a secure Cloud. All signal processing, sensing data extraction algorithms and code are custom-developed and reconfigurable to meet the varying needs of older adults aging in place. We have deployed 16 sensor kits in a campus-supported model home, where we are collecting participant data as they complete a series of scripted activities of daily living [16].Another strength of our study is the multidisciplinary team approach, which includes academics, healthcare providers, community leaders, and older adults who collectively participate in discussions throughout the technology development life cycle. This approach can positively impact future technology adoption [37]. Our research methods for engaging older adults align with prior work that explored the acceptance of technology among older adults. The strength of using a persona developed by community collaborators to guide discussions on technology development was supported by prior research [38,39]. Dupreez et al. employed community engagement methods to develop a persona that reflected the local community. This persona was then utilized to inform technology design, providing a human-centered approach through an iterative design methodology [38]. Tiersen et al. employed a participatory approach to understand the priorities of individuals living with dementia and their caregivers, and to inform the design and implementation of technologies that support these individuals‚Äô functional and psychological needs [39]. Similarly, including the opinions of multiple user groups in our study helped prioritize technology development components.Many prior studies did not employ community engagement in technology design; instead, they used a standard development, testing, and implementation format that did not include input from the potential user community until the technology was ready for testing or product rollout [40,41,42,43,44,45,46]. Only a few studies have used surveys to explore the concerns, needs, and interests of older adults regarding technology use, home health monitoring and the association with aging in place [30,36,47,48,49,50]. Each year, AARP conducts surveys of older adults about their current technology use. According to the 2025 AARP survey, a greater percentage of older adults are embracing technologies to facilitate aging in place and appreciate those that monitor and support their health. The needs of older adults partially contributed to this increase in technology acceptance during the COVID-19 pandemic, as they were more socially isolated [30,35]. These findings align with this study‚Äôs results, showing increases in technology use post-COVID (e.g., smartphones and Zoom). A national study in South Korea found that participants aged 75 and older were more likely to desire to age in place. Individuals with higher education and higher income were also more likely to age in place [50]. These findings help support the observations in our study. Though our participants are self-selected, their demographics are similar to those in the South Korean study by age, education, and economic status. The study participant demographics reflect the region of the US in which our study was conducted. Thus, we have confidence that the findings presented here reflect the broader voice of older adults aging in place in our region and can guide the development of new technologies to facilitate aging in place. 4.2. Comparison with Prior WorkOur findings are consistent with prior research that identified the perceived value of health monitoring technology [9,35,36,51,52,53,54,55,56], privacy/data sharing concerns, and data ownership [32,47,56,57,58,59,60,61,62], technology use confidence, and the burden of use [32,51,54,55,60,63,64,65,66], system flexibility [56,67], increased acceptance with age/frailty [43,49,68,69], or when it can help older adults live longer at home [51,70]. Prior work examined the role of home monitoring for activities of daily living/instrumental activities of daily living, sleep, mobility, falls, location tracking, and abnormal behaviors [35,71]. Similarly to other studies, older adults appeared accepting of the technologies as long as the cost was reasonable, benefits outweighed the concerns about use/data collected, privacy was preserved, the user maintained control over the technology, and access to the generated data to preserve independence [42,43,56,72,73]. Our study, along with others, supports the need for engagement with a community and potential users throughout the technology development lifecycle, thereby continuing to address user needs and strengthening our potential for successful adoption [32].4.2.1. Theme #1-Challenges to Aging in PlaceAccepting and using technologies to support aging in place was a major theme across our study. The strongest predictors of new technology adoption by older adults included the value of the technology, the impact of technology use on quality of life, and confidence in using the technology [51,74]. Resilient individuals are also more likely to adopt new technologies before they are needed, and as health declines [52]. Our participants who learned about friends‚Äô new adverse health events (e.g., heart attack, stroke) were more interested in adopting new technologies to detect health events or provide assistance earlier when needed.Our discussion group participants were more accepting of home monitoring following the COVID-19 pandemic. Studies conducted in Canada and the United States noted similar findings, with participants expressing their desire to continue using the technology they adopted during the COVID-19 pandemic [53,63]. Results from a Singaporean study conducted during COVID-19 found that digital technology acceptance had shifted from pre-COVID, with perceived benefits including ease of use, social influence, and confidence in use [55]. This further supports our participant observations and those from other studies that learning new technologies is more challenging without external support, education on use, and increased complexity [51,71]. In our findings, a participant stated ‚ÄúNewer technologies are not made for me but rather designed for younger people,‚Äù a sentiment supported by the 2024 AARP survey [24]. This further confirms the need for engagement throughout the technology development lifecycle to overcome barriers, educate, and allay fears of technology use and adoption.Another avenue to facilitate adoption identified in our discussion sessions is support from the older adults‚Äô families, extended families, friends, and neighbors. During the height of the COVID-19 pandemic, technology failures posed a significant challenge to its use. Without their support system, older adults found it difficult to maintain their technologies when they ‚Äúbroke‚Äù and to learn new technology on their own (e.g., Zoom). Though conducted pre-COVID-19, the importance of family in supporting older adults‚Äô technology adoption and use was identified by Dickman Portz [75]. As noted by our study and others, the widespread adoption of Zoom during the height of the COVID-19 pandemic provided a tool to stay connected with family and friends. There was overwhelming agreement that they would continue to use Zoom to ‚Äústay connected‚Äù beyond the pandemic [53,63].Participants expressed concerns about the cost of purchasing and supporting new technology, particularly when expendable income may decrease, and about accepting the trade-off between these expenses and the benefits of remaining in a familiar and comfortable environment. These concerns were similarly raised in studies conducted both inside and outside the United States [24,52,63]. Generational differences for monitoring acceptance identified in our study were documented in previous studies where those < 70 years of age [65,71] and those pre-retirement were more enthusiastic about accepting and using home environmental sensing [76].4.2.2. Theme #2-Home Monitoring Concerns and QuestionsA burgeoning field of research concerns the ethics, security, privacy, and trust associated with the expanding use of smart home technology [32,45,52,56,60,73,77]. The fear of data collection without consent and illegal data access ranked as top concerns by others [78,79]. There is a misconception that the aging population will readily accept medical technology due to health concerns, without a deeper understanding of who receives the information and how the data is presented [78,80]. Participants in our study were very concerned about who they would feel comfortable sharing sensor data with, under what conditions, and who in the home they would include in data collection. As more smart home technologies have entered the market, a dynamic tension has emerged between the benefits of monitoring and concerns about privacy, data breaches, and an affront to autonomy among older adults [48,60,81]. Prior investigations have identified fears of asking for help as a barrier to technology adoption, due to concerns that others would perceive them as no longer able to perform tasks, which can translate into a burden on others and a loss of control [59,80]. These findings offer insights into the concerns our participants expressed about aging in place and the reluctance to share sensor data, which could lead to questioning one‚Äôs independence. Crotty et al. found differences in perspectives between caregivers and older adults regarding what constitutes ‚Äúdata burden‚Äù. Older adults did not want to feel ‚Äúspied on‚Äù or ‚Äúlose control‚Äù over decisions if data were shared. The study concluded that there is not a ‚Äúone size fits all‚Äù model for data sharing among older adults and those who support them. This concept was included in all our discussion sessions and influenced how we are designing our sensor kit to offer options and accommodate individuals at different stages of aging with varying opinions and comfort levels regarding data sharing. As highlighted in our discussions, these concerns are more significant among older participants, as monitoring can impact their independence and autonomy. Dermody et al. found that their participants initially had very negative perceptions about home monitoring, but these perceptions changed as they learned more from discussions with investigators. Participants were generally uncomfortable using cameras and having their audio recorded [48]. Early in our study, participants expressed strong concerns about using visual data (including coarse-grained depth cameras), prompting our technology development team to focus on ultra-wideband sensors that use radio waves and produce no visual data. In the Dermody study, participants were also asked about their sensors‚Äô ability to detect family pets [48]. Our participants took these concerns one step further, asking that our system be able to be turned on and off to control who and when individuals are monitored (e.g., house guests, grandchildren).Individuals with the most and those with the least technological experience were more concerned about privacy and data sharing. Tech-savvy individuals were concerned about their existing knowledge base, while tech novices were concerned about the unknown. Prior studies support findings about this latter group [48,65,72]. The middle group was generally younger, adopted technology for themselves and their homes, and welcomed the inclusion of our technology to support aging. Similar trends were found in the United Kingdom [71]. Prior research has identified that with education and improved knowledge, older adults are more likely to be accepting of technologies that support aging in place and health monitoring. They noted a balance between giving up some privacy to maintain independence and obtaining support when needed [72].The burden on family and friends resulting from data overload was a recurring theme in our study and prior research with older adults [48,59,65]. In focus groups conducted at Tiger Place, a senior living community in the US state of Missouri, they assessed interest in and concerns about home monitoring. Participants felt there needed to be a balance between safety and privacy, and understanding the benefits of monitoring [82,83,84]. Healthcare providers and technology developers have also raised concerns about the role of smart device data in healthcare, its misuse, burden on the healthcare system, data ownership, and the lack of smart device data standards as a limitation to its adoption [56,60,61]. Our participants‚Äô concerns mirror concerns about the burden on individuals and healthcare providers to address the health changes identified, as well as about how the data would be used, which could infringe on privacy. To overcome these privacy and security challenges as described here, all potential users/technology consumers (e.g., business, academia, medicine, health insurance, providers, and users) must come together to speak to the challenges and benefits of such data collection while preserving the privacy and security of the individuals and the data that they generate.4.2.3. Theme #3 Caregiver ConcernsPrior studies have explored older adults‚Äô readiness to adopt smart home technologies. Previous research has shown that having a smart home provides ‚Äúeyes on‚Äù support to relieve caregivers‚Äô burdens and gives older adults confidence that continuous assistance is available [48,85]. This becomes even more important with long-distance caregiving, as noted in a systematic review of attitudes toward technology use by distant caregivers [81]. Many of our study participants noted their role in supporting family members with decreased mobility and/or cognitive decline. These participants were enthusiastic about the potential of our sensors to provide comfort when they could not physically be with their loved ones, which was especially challenging during and immediately following the height of the COVID-19 pandemic. Participants further noted their own need to leave home to do work, run errands, and make doctor appointments, among other things, and not have the peace of mind of knowing that ‚Äúeverything is ok at home‚Äù.Though prior studies mentioned cameras [85], newer technologies, like those we are developing, overcome older adults‚Äô privacy concerns while still addressing caregivers‚Äô concerns. A mixed-methods study by Piau et al. determined that caregivers were very enthusiastic about using a smart home to monitor older adults for falls, medication use, and changes in functioning. This finding adds to the uses identified by our respondents for smart technology‚Äôs role in supporting aging in place [79].Similarly to our findings, other studies noted the benefits of health monitoring in aiding medical care [65,71]. Younger participants in the Camp study noted the same trend as ours, with caregivers identifying the advantages of home monitoring for health status and well-being (e.g., fall detection) for older adults they care for [71]. In contrast, Egan et al. found that caregivers lacked confidence in the currently available technology to support caregiving, citing issues such as irrelevance (targeted towards younger individuals), cost, lack of support, and the technologies not meeting their needs [86]. Many of these challenges were identified in our discussions and continue to guide our technology development and testing. Our participants noted the challenges of convincing the older adults they care for to embrace monitoring technologies and how long it takes for all involved to agree. This trend was highlighted in a study by Thilo et al., who described the adverse reactions of older adults to recommendations for home monitoring devices and the need to balance monitoring with independence [87]. 4.3. LimitationsThe limitations of the study fall into three categories: participant demographics, methodological execution, and the scope of the technology investigated. The study faced demographic limitations, primarily in terms of educational and race/ethnicity diversity, which were attributed to the specific region where the research was conducted. The research was also confined to English-speaking participants due to the demographics of the local residents. The necessary transition to fully online recruitment and Zoom discussion sessions during the COVID-19 pandemic may have introduced sampling bias, potentially limiting the participant pool to those already possessing access to computers and the internet, thereby restricting applicability to communities with less computer and internet access, particularly older adults aged 80 and older. Despite these shortcomings, participant representation was balanced by sex, age, and home companionship. We plan to expand outreach beyond the local region and to diverse communities to address these demographic limitations in future work. Even with these limitations and COVID-19 adaptations, we may have identified the individuals who would become early adopters of our technology and thus those we should target to facilitate adoption.Methodological restrictions included data linkage and sample size. To preserve participant confidentiality, the study team chose not to link individual survey responses with the individual participant‚Äôs discussion session data. Though a standard methodology, it limits the ability to conduct analyses and sub-analyses exploring the associations between specific demographics (such as age or living arrangements) and the themes identified in the discussion groups. Additionally, the relatively small survey sample limited the ability to perform complex sub-analyses and to examine interactions in the survey data. An early study challenge was engaging in a discussion of technology use with an audience that lacked a frame of reference for the novel technologies we were exploring; this was resolved when the study team developed and used a vignette/persona to help participants understand the technology‚Äôs potential uses by first discussing a fictional individual.A limitation of the investigation‚Äôs scope was the decision to focus exclusively on contactless, home-installed sensors, without discussing wearable devices. Investigators chose this approach to eliminate the inherent burden of charging, wearing, and potential breakage associated with wearables, as well as long-term compliance challenges based on individual demographics, stage of life, device usability, etc. [88,89]. This specific focus on home-installed sensors was deemed appropriate because older individuals, especially those with worsening chronic conditions, tend to spend extended periods in their homes, making in-home monitoring a logical choice to support aging in place, as noted in prior studies by our team and others [71]. 4.4. Relevance to Future Technology Design and Adoption for Use by Older AdultsStudy findings continue to guide us as we build, test, and deploy our sensor kits in the community. The decisions made in choosing the sensors to use, identifying the chronic conditions that are important to older adults and healthcare providers for predictive analytics development, and the community and provider groups we are working with are all direct results of this study‚Äôs findings. We believe that through community engagement and discovery, researchers can develop stronger ties with future users and learn together what will and will not work to bring technology to market. History shows that technology developers who do not involve potential users throughout development and deployment tend to produce technologies that lack widespread adoption. The methods we have chosen enable us to learn from and work with our target communities, which will help ease our next steps in home testing and deployment.The knowledge gained from this study has applicability to any technology development for older adults. The overarching theme from this study is that a one-size-fits-all technology to support aging in place will not be embraced. The technology must be easy enough for the older adult to use, non-burdensome, affordable, shareable with others, privacy-preserving, and provide peace of mind for caregivers. The major takeaway is that as individuals experience different health concerns with age, their aging-in-place needs and the people involved in their care will change. Thus, technologies need to be flexible enough to accommodate these changes and remain relevant to users and the individuals who support them. From discussion sessions, we also learned that for home technology implementation, the work to support the technology (e.g., maintenance, troubleshooting) is as important as, if not more important than, the development itself. If a technology breaks and there is no one to support it, people will stop using it. Older adults need a support system to keep their technologies running. We encourage others to consider these factors when developing technologies to support older adults, incorporating community engagement and user discussions into their technology development to support successful translation into adoption.",
            "4.1. Principal Discoveries": "In total, we recruited 112 community participants who represent our future home-health monitoring users. Using a combination of surveys and discussion sessions, we explored in depth new areas of acceptance and adoption of sensor use for home deployment. Our study appears to be the first to explore sensor configurations by room and health status across different home living arrangements, as well as the conditions under which data is shared and with whom. Study findings have guided technology choices and are guiding our sensor system design. Based on our findings, we are creating a system that is flexible enough to accommodate individuals at different life phases and comfort levels, across varying home environments and support systems, while providing a secure, privacy-preserving environment. Though prior studies have identified older adults‚Äô comfort with and interest in individual components for home sensing, our study is unique in being more comprehensive in exploring the extent of home sensing, data sharing and the need for technology adaptability across the lifespan [31]. Our survey found that participants had different opinions as to the amount and location of sensor deployment, depending on their health status. There was greater comfort with sharing home monitoring data with healthcare providers and family, whether healthy or sick. The extent to which individuals were comfortable sharing their data beyond health care providers and family increased when sick or recovering from an illness though comfort remained far less. From discussion sessions, we further identified that cost, privacy, security, the potential for system hacking, and the availability of a support system (e.g., family, friends, neighbors, professional caregivers) were major concerns. Participants noted the need for assistance with technology related issues after deployment, as reported in prior quantitative research [11,36]. Participants were very enthusiastic about the vision of our proposed sensor system. They felt that having home monitoring as they aged would provide themselves, loved ones, caregivers, and healthcare providers with valuable information to help them remain safe at home and offer insight into their health between healthcare provider visits. Based upon these findings, initial home deployment for home health monitoring may be most successful targeting individuals post hospitalization or those with chronic health conditions. These individuals may be more amenable to home health monitoring to support aging in place. Participants from 55+ communities and continuous care facilities were more comfortable with monitoring, as these settings already have in-room help cords and wearable alert systems in place for residents. Although participants noted their availability, it is not always acceptable or feasible. This suggests that individuals already engaged in continuous care arrangements are highly receptive to data sharing, likely viewing it as integral to their well-being and health management. Experiences with participants‚Äô own or others‚Äô changing health status helped participants appreciate how continuous home sensing could assist them in getting help when needed and in aging in place. COVID-19 has altered the perspective of older adults and their caregivers on the use of technology. Participants noted 1‚Äîincreased use of telehealth to connect with their medical care team; 2‚Äîuse of Zoom and FaceTime to overcome social isolation and stay connected, and willingness to continue to use these technologies; and 3‚Äîincreased acceptance of smartwatches, especially the Apple Watch, as it now includes a falls-detection alert system. Survey findings and discussion sessions reveal shifting attitudes among participants regarding home monitoring, particularly in relation to changes in health status, home environment, and the absence of onsite caregivers. While living rooms and kitchens generally showed high comfort, comfort in bedrooms and, particularly, bathrooms was lower across different home environments and health conditions, highlighting privacy concerns or perceived intrusiveness in more private spaces, which was further supported during discussions. However, for those facing declining health or requiring additional care, even bathrooms can offer high comfort with multiple sensors, indicating that the perceived need outweighs privacy concerns. The combined findings from our survey and discussion sessions are driving our sensor kit creation and home deployment. We envision developing a sensor kit that is inexpensive, self-contained, and remotely monitorable. Based on initial participant input, we have focused on using off-the-shelf ultra-wideband (UWB) sensors that do not generate any body images to address privacy concerns while remaining cost-effective for widespread deployment. We are pairing this UWB sensor with a Raspberry Pi for data collection, with data sent initially to an edge machine and then to a secure Cloud. All signal processing, sensing data extraction algorithms and code are custom-developed and reconfigurable to meet the varying needs of older adults aging in place. We have deployed 16 sensor kits in a campus-supported model home, where we are collecting participant data as they complete a series of scripted activities of daily living [16]. Another strength of our study is the multidisciplinary team approach, which includes academics, healthcare providers, community leaders, and older adults who collectively participate in discussions throughout the technology development life cycle. This approach can positively impact future technology adoption [37]. Our research methods for engaging older adults align with prior work that explored the acceptance of technology among older adults. The strength of using a persona developed by community collaborators to guide discussions on technology development was supported by prior research [38,39]. Dupreez et al. employed community engagement methods to develop a persona that reflected the local community. This persona was then utilized to inform technology design, providing a human-centered approach through an iterative design methodology [38]. Tiersen et al. employed a participatory approach to understand the priorities of individuals living with dementia and their caregivers, and to inform the design and implementation of technologies that support these individuals‚Äô functional and psychological needs [39]. Similarly, including the opinions of multiple user groups in our study helped prioritize technology development components. Many prior studies did not employ community engagement in technology design; instead, they used a standard development, testing, and implementation format that did not include input from the potential user community until the technology was ready for testing or product rollout [40,41,42,43,44,45,46]. Only a few studies have used surveys to explore the concerns, needs, and interests of older adults regarding technology use, home health monitoring and the association with aging in place [30,36,47,48,49,50]. Each year, AARP conducts surveys of older adults about their current technology use. According to the 2025 AARP survey, a greater percentage of older adults are embracing technologies to facilitate aging in place and appreciate those that monitor and support their health. The needs of older adults partially contributed to this increase in technology acceptance during the COVID-19 pandemic, as they were more socially isolated [30,35]. These findings align with this study‚Äôs results, showing increases in technology use post-COVID (e.g., smartphones and Zoom). A national study in South Korea found that participants aged 75 and older were more likely to desire to age in place. Individuals with higher education and higher income were also more likely to age in place [50]. These findings help support the observations in our study. Though our participants are self-selected, their demographics are similar to those in the South Korean study by age, education, and economic status. The study participant demographics reflect the region of the US in which our study was conducted. Thus, we have confidence that the findings presented here reflect the broader voice of older adults aging in place in our region and can guide the development of new technologies to facilitate aging in place.",
            "4.2. Comparison with Prior Work": "Our findings are consistent with prior research that identified the perceived value of health monitoring technology [9,35,36,51,52,53,54,55,56], privacy/data sharing concerns, and data ownership [32,47,56,57,58,59,60,61,62], technology use confidence, and the burden of use [32,51,54,55,60,63,64,65,66], system flexibility [56,67], increased acceptance with age/frailty [43,49,68,69], or when it can help older adults live longer at home [51,70]. Prior work examined the role of home monitoring for activities of daily living/instrumental activities of daily living, sleep, mobility, falls, location tracking, and abnormal behaviors [35,71]. Similarly to other studies, older adults appeared accepting of the technologies as long as the cost was reasonable, benefits outweighed the concerns about use/data collected, privacy was preserved, the user maintained control over the technology, and access to the generated data to preserve independence [42,43,56,72,73]. Our study, along with others, supports the need for engagement with a community and potential users throughout the technology development lifecycle, thereby continuing to address user needs and strengthening our potential for successful adoption [32]. 4.2.1. Theme #1-Challenges to Aging in PlaceAccepting and using technologies to support aging in place was a major theme across our study. The strongest predictors of new technology adoption by older adults included the value of the technology, the impact of technology use on quality of life, and confidence in using the technology [51,74]. Resilient individuals are also more likely to adopt new technologies before they are needed, and as health declines [52]. Our participants who learned about friends‚Äô new adverse health events (e.g., heart attack, stroke) were more interested in adopting new technologies to detect health events or provide assistance earlier when needed.Our discussion group participants were more accepting of home monitoring following the COVID-19 pandemic. Studies conducted in Canada and the United States noted similar findings, with participants expressing their desire to continue using the technology they adopted during the COVID-19 pandemic [53,63]. Results from a Singaporean study conducted during COVID-19 found that digital technology acceptance had shifted from pre-COVID, with perceived benefits including ease of use, social influence, and confidence in use [55]. This further supports our participant observations and those from other studies that learning new technologies is more challenging without external support, education on use, and increased complexity [51,71]. In our findings, a participant stated ‚ÄúNewer technologies are not made for me but rather designed for younger people,‚Äù a sentiment supported by the 2024 AARP survey [24]. This further confirms the need for engagement throughout the technology development lifecycle to overcome barriers, educate, and allay fears of technology use and adoption.Another avenue to facilitate adoption identified in our discussion sessions is support from the older adults‚Äô families, extended families, friends, and neighbors. During the height of the COVID-19 pandemic, technology failures posed a significant challenge to its use. Without their support system, older adults found it difficult to maintain their technologies when they ‚Äúbroke‚Äù and to learn new technology on their own (e.g., Zoom). Though conducted pre-COVID-19, the importance of family in supporting older adults‚Äô technology adoption and use was identified by Dickman Portz [75]. As noted by our study and others, the widespread adoption of Zoom during the height of the COVID-19 pandemic provided a tool to stay connected with family and friends. There was overwhelming agreement that they would continue to use Zoom to ‚Äústay connected‚Äù beyond the pandemic [53,63].Participants expressed concerns about the cost of purchasing and supporting new technology, particularly when expendable income may decrease, and about accepting the trade-off between these expenses and the benefits of remaining in a familiar and comfortable environment. These concerns were similarly raised in studies conducted both inside and outside the United States [24,52,63]. Generational differences for monitoring acceptance identified in our study were documented in previous studies where those < 70 years of age [65,71] and those pre-retirement were more enthusiastic about accepting and using home environmental sensing [76]. 4.2.2. Theme #2-Home Monitoring Concerns and QuestionsA burgeoning field of research concerns the ethics, security, privacy, and trust associated with the expanding use of smart home technology [32,45,52,56,60,73,77]. The fear of data collection without consent and illegal data access ranked as top concerns by others [78,79]. There is a misconception that the aging population will readily accept medical technology due to health concerns, without a deeper understanding of who receives the information and how the data is presented [78,80]. Participants in our study were very concerned about who they would feel comfortable sharing sensor data with, under what conditions, and who in the home they would include in data collection. As more smart home technologies have entered the market, a dynamic tension has emerged between the benefits of monitoring and concerns about privacy, data breaches, and an affront to autonomy among older adults [48,60,81]. Prior investigations have identified fears of asking for help as a barrier to technology adoption, due to concerns that others would perceive them as no longer able to perform tasks, which can translate into a burden on others and a loss of control [59,80]. These findings offer insights into the concerns our participants expressed about aging in place and the reluctance to share sensor data, which could lead to questioning one‚Äôs independence. Crotty et al. found differences in perspectives between caregivers and older adults regarding what constitutes ‚Äúdata burden‚Äù. Older adults did not want to feel ‚Äúspied on‚Äù or ‚Äúlose control‚Äù over decisions if data were shared. The study concluded that there is not a ‚Äúone size fits all‚Äù model for data sharing among older adults and those who support them. This concept was included in all our discussion sessions and influenced how we are designing our sensor kit to offer options and accommodate individuals at different stages of aging with varying opinions and comfort levels regarding data sharing. As highlighted in our discussions, these concerns are more significant among older participants, as monitoring can impact their independence and autonomy. Dermody et al. found that their participants initially had very negative perceptions about home monitoring, but these perceptions changed as they learned more from discussions with investigators. Participants were generally uncomfortable using cameras and having their audio recorded [48]. Early in our study, participants expressed strong concerns about using visual data (including coarse-grained depth cameras), prompting our technology development team to focus on ultra-wideband sensors that use radio waves and produce no visual data. In the Dermody study, participants were also asked about their sensors‚Äô ability to detect family pets [48]. Our participants took these concerns one step further, asking that our system be able to be turned on and off to control who and when individuals are monitored (e.g., house guests, grandchildren).Individuals with the most and those with the least technological experience were more concerned about privacy and data sharing. Tech-savvy individuals were concerned about their existing knowledge base, while tech novices were concerned about the unknown. Prior studies support findings about this latter group [48,65,72]. The middle group was generally younger, adopted technology for themselves and their homes, and welcomed the inclusion of our technology to support aging. Similar trends were found in the United Kingdom [71]. Prior research has identified that with education and improved knowledge, older adults are more likely to be accepting of technologies that support aging in place and health monitoring. They noted a balance between giving up some privacy to maintain independence and obtaining support when needed [72].The burden on family and friends resulting from data overload was a recurring theme in our study and prior research with older adults [48,59,65]. In focus groups conducted at Tiger Place, a senior living community in the US state of Missouri, they assessed interest in and concerns about home monitoring. Participants felt there needed to be a balance between safety and privacy, and understanding the benefits of monitoring [82,83,84]. Healthcare providers and technology developers have also raised concerns about the role of smart device data in healthcare, its misuse, burden on the healthcare system, data ownership, and the lack of smart device data standards as a limitation to its adoption [56,60,61]. Our participants‚Äô concerns mirror concerns about the burden on individuals and healthcare providers to address the health changes identified, as well as about how the data would be used, which could infringe on privacy. To overcome these privacy and security challenges as described here, all potential users/technology consumers (e.g., business, academia, medicine, health insurance, providers, and users) must come together to speak to the challenges and benefits of such data collection while preserving the privacy and security of the individuals and the data that they generate. 4.2.3. Theme #3 Caregiver ConcernsPrior studies have explored older adults‚Äô readiness to adopt smart home technologies. Previous research has shown that having a smart home provides ‚Äúeyes on‚Äù support to relieve caregivers‚Äô burdens and gives older adults confidence that continuous assistance is available [48,85]. This becomes even more important with long-distance caregiving, as noted in a systematic review of attitudes toward technology use by distant caregivers [81]. Many of our study participants noted their role in supporting family members with decreased mobility and/or cognitive decline. These participants were enthusiastic about the potential of our sensors to provide comfort when they could not physically be with their loved ones, which was especially challenging during and immediately following the height of the COVID-19 pandemic. Participants further noted their own need to leave home to do work, run errands, and make doctor appointments, among other things, and not have the peace of mind of knowing that ‚Äúeverything is ok at home‚Äù.Though prior studies mentioned cameras [85], newer technologies, like those we are developing, overcome older adults‚Äô privacy concerns while still addressing caregivers‚Äô concerns. A mixed-methods study by Piau et al. determined that caregivers were very enthusiastic about using a smart home to monitor older adults for falls, medication use, and changes in functioning. This finding adds to the uses identified by our respondents for smart technology‚Äôs role in supporting aging in place [79].Similarly to our findings, other studies noted the benefits of health monitoring in aiding medical care [65,71]. Younger participants in the Camp study noted the same trend as ours, with caregivers identifying the advantages of home monitoring for health status and well-being (e.g., fall detection) for older adults they care for [71]. In contrast, Egan et al. found that caregivers lacked confidence in the currently available technology to support caregiving, citing issues such as irrelevance (targeted towards younger individuals), cost, lack of support, and the technologies not meeting their needs [86]. Many of these challenges were identified in our discussions and continue to guide our technology development and testing. Our participants noted the challenges of convincing the older adults they care for to embrace monitoring technologies and how long it takes for all involved to agree. This trend was highlighted in a study by Thilo et al., who described the adverse reactions of older adults to recommendations for home monitoring devices and the need to balance monitoring with independence [87].",
            "4.2.1. Theme #1-Challenges to Aging in Place": "Accepting and using technologies to support aging in place was a major theme across our study. The strongest predictors of new technology adoption by older adults included the value of the technology, the impact of technology use on quality of life, and confidence in using the technology [51,74]. Resilient individuals are also more likely to adopt new technologies before they are needed, and as health declines [52]. Our participants who learned about friends‚Äô new adverse health events (e.g., heart attack, stroke) were more interested in adopting new technologies to detect health events or provide assistance earlier when needed. Our discussion group participants were more accepting of home monitoring following the COVID-19 pandemic. Studies conducted in Canada and the United States noted similar findings, with participants expressing their desire to continue using the technology they adopted during the COVID-19 pandemic [53,63]. Results from a Singaporean study conducted during COVID-19 found that digital technology acceptance had shifted from pre-COVID, with perceived benefits including ease of use, social influence, and confidence in use [55]. This further supports our participant observations and those from other studies that learning new technologies is more challenging without external support, education on use, and increased complexity [51,71]. In our findings, a participant stated ‚ÄúNewer technologies are not made for me but rather designed for younger people,‚Äù a sentiment supported by the 2024 AARP survey [24]. This further confirms the need for engagement throughout the technology development lifecycle to overcome barriers, educate, and allay fears of technology use and adoption. Another avenue to facilitate adoption identified in our discussion sessions is support from the older adults‚Äô families, extended families, friends, and neighbors. During the height of the COVID-19 pandemic, technology failures posed a significant challenge to its use. Without their support system, older adults found it difficult to maintain their technologies when they ‚Äúbroke‚Äù and to learn new technology on their own (e.g., Zoom). Though conducted pre-COVID-19, the importance of family in supporting older adults‚Äô technology adoption and use was identified by Dickman Portz [75]. As noted by our study and others, the widespread adoption of Zoom during the height of the COVID-19 pandemic provided a tool to stay connected with family and friends. There was overwhelming agreement that they would continue to use Zoom to ‚Äústay connected‚Äù beyond the pandemic [53,63]. Participants expressed concerns about the cost of purchasing and supporting new technology, particularly when expendable income may decrease, and about accepting the trade-off between these expenses and the benefits of remaining in a familiar and comfortable environment. These concerns were similarly raised in studies conducted both inside and outside the United States [24,52,63]. Generational differences for monitoring acceptance identified in our study were documented in previous studies where those < 70 years of age [65,71] and those pre-retirement were more enthusiastic about accepting and using home environmental sensing [76].",
            "4.2.2. Theme #2-Home Monitoring Concerns and Questions": "A burgeoning field of research concerns the ethics, security, privacy, and trust associated with the expanding use of smart home technology [32,45,52,56,60,73,77]. The fear of data collection without consent and illegal data access ranked as top concerns by others [78,79]. There is a misconception that the aging population will readily accept medical technology due to health concerns, without a deeper understanding of who receives the information and how the data is presented [78,80]. Participants in our study were very concerned about who they would feel comfortable sharing sensor data with, under what conditions, and who in the home they would include in data collection. As more smart home technologies have entered the market, a dynamic tension has emerged between the benefits of monitoring and concerns about privacy, data breaches, and an affront to autonomy among older adults [48,60,81]. Prior investigations have identified fears of asking for help as a barrier to technology adoption, due to concerns that others would perceive them as no longer able to perform tasks, which can translate into a burden on others and a loss of control [59,80]. These findings offer insights into the concerns our participants expressed about aging in place and the reluctance to share sensor data, which could lead to questioning one‚Äôs independence. Crotty et al. found differences in perspectives between caregivers and older adults regarding what constitutes ‚Äúdata burden‚Äù. Older adults did not want to feel ‚Äúspied on‚Äù or ‚Äúlose control‚Äù over decisions if data were shared. The study concluded that there is not a ‚Äúone size fits all‚Äù model for data sharing among older adults and those who support them. This concept was included in all our discussion sessions and influenced how we are designing our sensor kit to offer options and accommodate individuals at different stages of aging with varying opinions and comfort levels regarding data sharing. As highlighted in our discussions, these concerns are more significant among older participants, as monitoring can impact their independence and autonomy. Dermody et al. found that their participants initially had very negative perceptions about home monitoring, but these perceptions changed as they learned more from discussions with investigators. Participants were generally uncomfortable using cameras and having their audio recorded [48]. Early in our study, participants expressed strong concerns about using visual data (including coarse-grained depth cameras), prompting our technology development team to focus on ultra-wideband sensors that use radio waves and produce no visual data. In the Dermody study, participants were also asked about their sensors‚Äô ability to detect family pets [48]. Our participants took these concerns one step further, asking that our system be able to be turned on and off to control who and when individuals are monitored (e.g., house guests, grandchildren). Individuals with the most and those with the least technological experience were more concerned about privacy and data sharing. Tech-savvy individuals were concerned about their existing knowledge base, while tech novices were concerned about the unknown. Prior studies support findings about this latter group [48,65,72]. The middle group was generally younger, adopted technology for themselves and their homes, and welcomed the inclusion of our technology to support aging. Similar trends were found in the United Kingdom [71]. Prior research has identified that with education and improved knowledge, older adults are more likely to be accepting of technologies that support aging in place and health monitoring. They noted a balance between giving up some privacy to maintain independence and obtaining support when needed [72]. The burden on family and friends resulting from data overload was a recurring theme in our study and prior research with older adults [48,59,65]. In focus groups conducted at Tiger Place, a senior living community in the US state of Missouri, they assessed interest in and concerns about home monitoring. Participants felt there needed to be a balance between safety and privacy, and understanding the benefits of monitoring [82,83,84]. Healthcare providers and technology developers have also raised concerns about the role of smart device data in healthcare, its misuse, burden on the healthcare system, data ownership, and the lack of smart device data standards as a limitation to its adoption [56,60,61]. Our participants‚Äô concerns mirror concerns about the burden on individuals and healthcare providers to address the health changes identified, as well as about how the data would be used, which could infringe on privacy. To overcome these privacy and security challenges as described here, all potential users/technology consumers (e.g., business, academia, medicine, health insurance, providers, and users) must come together to speak to the challenges and benefits of such data collection while preserving the privacy and security of the individuals and the data that they generate.",
            "4.2.3. Theme #3 Caregiver Concerns": "Prior studies have explored older adults‚Äô readiness to adopt smart home technologies. Previous research has shown that having a smart home provides ‚Äúeyes on‚Äù support to relieve caregivers‚Äô burdens and gives older adults confidence that continuous assistance is available [48,85]. This becomes even more important with long-distance caregiving, as noted in a systematic review of attitudes toward technology use by distant caregivers [81]. Many of our study participants noted their role in supporting family members with decreased mobility and/or cognitive decline. These participants were enthusiastic about the potential of our sensors to provide comfort when they could not physically be with their loved ones, which was especially challenging during and immediately following the height of the COVID-19 pandemic. Participants further noted their own need to leave home to do work, run errands, and make doctor appointments, among other things, and not have the peace of mind of knowing that ‚Äúeverything is ok at home‚Äù. Though prior studies mentioned cameras [85], newer technologies, like those we are developing, overcome older adults‚Äô privacy concerns while still addressing caregivers‚Äô concerns. A mixed-methods study by Piau et al. determined that caregivers were very enthusiastic about using a smart home to monitor older adults for falls, medication use, and changes in functioning. This finding adds to the uses identified by our respondents for smart technology‚Äôs role in supporting aging in place [79]. Similarly to our findings, other studies noted the benefits of health monitoring in aiding medical care [65,71]. Younger participants in the Camp study noted the same trend as ours, with caregivers identifying the advantages of home monitoring for health status and well-being (e.g., fall detection) for older adults they care for [71]. In contrast, Egan et al. found that caregivers lacked confidence in the currently available technology to support caregiving, citing issues such as irrelevance (targeted towards younger individuals), cost, lack of support, and the technologies not meeting their needs [86]. Many of these challenges were identified in our discussions and continue to guide our technology development and testing. Our participants noted the challenges of convincing the older adults they care for to embrace monitoring technologies and how long it takes for all involved to agree. This trend was highlighted in a study by Thilo et al., who described the adverse reactions of older adults to recommendations for home monitoring devices and the need to balance monitoring with independence [87].",
            "4.3. Limitations": "The limitations of the study fall into three categories: participant demographics, methodological execution, and the scope of the technology investigated. The study faced demographic limitations, primarily in terms of educational and race/ethnicity diversity, which were attributed to the specific region where the research was conducted. The research was also confined to English-speaking participants due to the demographics of the local residents. The necessary transition to fully online recruitment and Zoom discussion sessions during the COVID-19 pandemic may have introduced sampling bias, potentially limiting the participant pool to those already possessing access to computers and the internet, thereby restricting applicability to communities with less computer and internet access, particularly older adults aged 80 and older. Despite these shortcomings, participant representation was balanced by sex, age, and home companionship. We plan to expand outreach beyond the local region and to diverse communities to address these demographic limitations in future work. Even with these limitations and COVID-19 adaptations, we may have identified the individuals who would become early adopters of our technology and thus those we should target to facilitate adoption. Methodological restrictions included data linkage and sample size. To preserve participant confidentiality, the study team chose not to link individual survey responses with the individual participant‚Äôs discussion session data. Though a standard methodology, it limits the ability to conduct analyses and sub-analyses exploring the associations between specific demographics (such as age or living arrangements) and the themes identified in the discussion groups. Additionally, the relatively small survey sample limited the ability to perform complex sub-analyses and to examine interactions in the survey data. An early study challenge was engaging in a discussion of technology use with an audience that lacked a frame of reference for the novel technologies we were exploring; this was resolved when the study team developed and used a vignette/persona to help participants understand the technology‚Äôs potential uses by first discussing a fictional individual. A limitation of the investigation‚Äôs scope was the decision to focus exclusively on contactless, home-installed sensors, without discussing wearable devices. Investigators chose this approach to eliminate the inherent burden of charging, wearing, and potential breakage associated with wearables, as well as long-term compliance challenges based on individual demographics, stage of life, device usability, etc. [88,89]. This specific focus on home-installed sensors was deemed appropriate because older individuals, especially those with worsening chronic conditions, tend to spend extended periods in their homes, making in-home monitoring a logical choice to support aging in place, as noted in prior studies by our team and others [71].",
            "4.4. Relevance to Future Technology Design and Adoption for Use by Older Adults": "Study findings continue to guide us as we build, test, and deploy our sensor kits in the community. The decisions made in choosing the sensors to use, identifying the chronic conditions that are important to older adults and healthcare providers for predictive analytics development, and the community and provider groups we are working with are all direct results of this study‚Äôs findings. We believe that through community engagement and discovery, researchers can develop stronger ties with future users and learn together what will and will not work to bring technology to market. History shows that technology developers who do not involve potential users throughout development and deployment tend to produce technologies that lack widespread adoption. The methods we have chosen enable us to learn from and work with our target communities, which will help ease our next steps in home testing and deployment. The knowledge gained from this study has applicability to any technology development for older adults. The overarching theme from this study is that a one-size-fits-all technology to support aging in place will not be embraced. The technology must be easy enough for the older adult to use, non-burdensome, affordable, shareable with others, privacy-preserving, and provide peace of mind for caregivers. The major takeaway is that as individuals experience different health concerns with age, their aging-in-place needs and the people involved in their care will change. Thus, technologies need to be flexible enough to accommodate these changes and remain relevant to users and the individuals who support them. From discussion sessions, we also learned that for home technology implementation, the work to support the technology (e.g., maintenance, troubleshooting) is as important as, if not more important than, the development itself. If a technology breaks and there is no one to support it, people will stop using it. Older adults need a support system to keep their technologies running. We encourage others to consider these factors when developing technologies to support older adults, incorporating community engagement and user discussions into their technology development to support successful translation into adoption.",
            "5. Conclusions": "This study highlighted the perceived benefits, challenges, and concerns that older adults shared regarding the adoption of home-based 24/7 sensor data collection for general health monitoring and detection of health changes. Utilizing both surveys and discussion group sessions, participants shared insights into the benefits and drawbacks of using technology to support their health and safety, highlighting the need for flexible, cost-conscious, and user-friendly systems that respect their autonomy. The findings underscore the importance of including potential users throughout the technology development process to create solutions that are both acceptable and effective for the growing aging population. Our study identified numerous physical, mental, and support challenges that need to be addressed in any technology development to facilitate adoption by this growing population segment. Configurations appropriate for a young older adult may not be sufficient or usable by that individual as they age into their 80s and beyond. Thus, technology development must be flexible and adaptive, tailored to an individual‚Äôs changing needs as they age, respecting users‚Äô comfort with home monitoring and adapting to their preferences regarding data sharing and in-home sensor location. Future work should also focus on integrating health-related data generated from smart home monitoring into a comprehensive health monitoring program. Full community engagement and acceptance will be required throughout the technology development, testing, and deployment lifecycle to ensure the successful development of sensor technology that is age-friendly and supportive of all potential users."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/1424-8220/25/24/7413",
        "scraped_at": "2025-12-05 23:52:13"
    },
    {
        "title": "Global Mapping of Population Exposure to Upstream Gas Flaring Using Integrated VIIRS Nightfire and GHSL Data, 2016‚Äì2023, with Projections to 2030",
        "authors": "bySotiris Zikas,Christos Christakis,Loukas-Moysis Misthos,Ioannis Psomadakis,Angeliki I. Katsafadou,Ioannis Tsilikas,George C. Fthenakis,Vasilis VasiliouandYiannis Kiouvrekis",
        "journal": "Toxics2025,13(12), 1053;https://doi.org/10.3390/toxics13121053- 5 Dec 2025",
        "abstract": "Gas flaring from upstream oil and gas production remains a significant source of air pollution and toxic emissions, with major implications for human health and climate. However, the number of people living near flaring has not been quantified globally. This study presents the first worldwide, settlement-scale assessment of populations living within 1 km and 3 km of active upstream flare sites between 2016 and 2023, with projections to 2030. Using the VIIRS Nightfire satellite product, which provides global detections of high-temperature combustion sources, and the Global Human Settlement Layer (GHSL) population and settlement data, we developed a transparent and reproducible geospatial workflow to compute proximity-based exposure indicators by buffering flare locations and intersecting them with population rasters The analysis provides consistent estimates across five settlement categories: rural, peri-urban/suburban, semi-dense urban, dense urban, and urban centres. The VIIRS-based flaring time series combined with GHSL projections allows us to estimate how many people are likely to live near upstream flares under current flaring patterns by 2030. Results show that exposure is concentrated in a few oil-producing countries. Nigeria remains the most affected, with over 100,000 urban residents exposed in 2023. India and Pakistan dominate peri-urban and semi-urban exposures, while Indonesia and Iraq persist as multi-settlement hotspots. Although moderate declines are observed in China and Iran, little progress is evident in Nigeria, Mexico, and Indonesia. Projections for 2030 suggest exposure will increase substantially, driven by population growth and urban expansion, with about 2.7 million people living within 1 km and 14.8 million within 3 km of flaring sites. The findings establish the first globally consistent baseline for population exposure to gas flaring, supporting the monitoring and mitigation objectives of the Zero Routine Flaring by 2030 initiative.Keywords:gasflaring;population exposure;upstream oil and gas;VIIRS nightfire;GHSL;SMOD;urbanization;health risk;spatial analysis;2030 projections",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Gas flaring refers to the combustion of natural gas released during oil extraction, particularly when adequate infrastructure for its collection or utilization is lacking [1]. Understanding the extent of flaring is crucial not only for assessing its contribution to greenhouse gas emissions but also for evaluating its impacts on ecosystems and public health. Although the environmental consequences‚Äîsuch as air pollution and climate change‚Äîare widely recognized, the direct and indirect effects on people living near flaring sites are increasingly a cause for concern. Emissions from flaring significantly degrade air quality and exacerbate climate change [2]. Within the framework of the Paris Agreement (2016), nations committed to reducing their emissions [3]. In alignment with these objectives, the World Bank launched the ‚ÄúZero Routine Flaring by 2030‚Äù initiative, which aims to eliminate the routine burning of approximately 140 bcm of natural gas annually, thereby preventing the release of about 300 million tons of CO2[4]. Achieving this target is of vital importance, as gas flaring not only affects the climate but also has direct health implications for populations residing near oil and gas facilities. Fawole et al. [5] strongly criticize existing literature, arguing that the exclusion of flaring from emission inventories and global climate models results in an underestimation of pollutants. They propose a conceptual framework for developing more accurate and resilient emission factors for flaring activities. From an environmental and public health perspective (Figure 1), it is important to note that flaring releases not only CO2but also methane (CH4), which has a much higher global warming potential‚Äîover 80 times greater than CO2over a 20-year horizon. According to World Bank estimates, in 2022, flaring accounted for approximately 357 million tons of CO2-equivalent emissions, of which 42 million tons were methane, a quantity comparable to the emissions from about three million vehicles. Figure 1.Infographic illustrating the process of gas flaring, highlighting emitted pollutants and their impacts on air pollution, climate change, and human health. With respect to public health, studies conducted across various regions associate flaring with increased respiratory problems, premature deaths, and chronic diseases among residents living near flaring sites [6,7]. The risks are particularly elevated for pregnant women and newborns [8], and higher rates of childhood asthma and other respiratory disorders have been documented [9]. Ajugwo‚Äôs study [10] vividly highlights the health impacts of gas flaring in Nigeria. Janitz et al. [11] investigated health risks associated with natural gas drilling, particularly adverse outcomes for newborns. A comprehensive review of the literature on human exposure and health risks linked to oil extraction is provided by Johnston et al. [12]. From an initial pool of 2236 publications, 63 relevant studies were identified: 22 human, 5 occupational, 5 animal, 6 experimental, and 31 community-based. The review underscores multiple exposure pathways‚Äîair, soil, water, and waste‚Äîthrough which communities are affected. Evidence links oil extraction to a range of adverse health outcomes, including cancer, liver dysfunction, immunodeficiency, and neurological disorders. Furthermore, McKenzie et al. [13] examined the relationship between the intensity of oil and gas (O&G) activity and biological indicators of cardiovascular health, finding that higher activity intensity correlates with an increased augmentation index (a measure of arterial stiffness). Finally, the global and national burden of disease associated with flaring is assessed by Motte et al. [14] using disability-adjusted life years (DALYs). Their evaluation links flaring emissions with mid-point indicators commonly used in life cycle assessment (LCA), such as climate change. The results indicate that, globally, flaring contributes approximately 4.83 √ó 105DALYs per year, equivalent to 6.19 √ó 10‚àí5DALYs per person per year. All of the above highlight the necessity and rationale for the present research. Scope and Innovation of the StudyThis study quantifies the number of people living in proximity to oil and gas flaring facilities on a global scale. The analysis focuses on flares from upstream production facilities and does not include midstream energy system activities. We use the term exposure in accordance with the definitions established by the United Nations Office for Disaster Risk Reduction (UNDRR) and the United Nations Office for Outer Space Affairs (UN-SPIDER) [15,16]. UNDRR defines exposure as ‚Äúthe situation of people, infrastructure, housing, production capacities and other tangible human assets located in hazard-prone areas‚Äù, while UN-SPIDER specifies that measures of exposure may include ‚Äúthe number of people or types of assets in an area‚Äù. Following this internationally recognized terminology, we quantify exposure as the number of individuals residing within a defined proximity to gas-flaring sites, which are treated as localized environmental hazards. This spatially based measure reflects the presence of populations in hazard-prone areas and is distinct from hazard-intensity metrics (e.g., volume of gas flared or pollutant concentrations), which relate to the severity or magnitude of potential impacts rather than to exposure itself.We define proximity indicators as the number of individuals residing within 1 km and 3 km of active flaring sites during the period 2016‚Äì2023. To estimate the spatial distribution of the population, we employ the Visible Infrared Imaging Radiometer Suite (VIIRS) Nightfire dataset in combination with GHSL POP R2023A and SMOD Level 2, enabling the assessment of exposure across settlement categories (urban centre, dense-urban, semi-dense urban, suburban/peri-urban, and rural). We use population and settlement information from the Global Human Settlement Layer (GHSL) POP R2023A and the Settlement Model (SMOD) Level 2. GHSL POP R2023A provides global gridded population estimates at 100 m to 1 km resolution for multiple years and projections to 2030, while SMOD Level 2 classifies each grid cell into settlement types (urban centre, dense urban cluster, semi-dense urban cluster, suburban/peri-urban, rural cluster). Together, these datasets allow us to quantify how many people in each settlement class live within defined distances of flaring sites.The VIIRS is a satellite instrument that measures visible and infrared light to observe Earth‚Äôs surface and atmosphere. Its Nightfire data product is the primary global source for detecting and quantifying gas flaring from space [17].To date, global studies have examined flaring intensity and emitted volumes, while epidemiological and environmental research has typically focused on specific countries or local communities. What has been missing is a global and systematic representation of how many people live near flares from production facilities, using a unified methodology that ensures spatial and temporal comparability.The present study contributes to the growing body of evidence linking residential proximity to flaring with adverse health effects. Epidemiological research has documented associations between flaring and respiratory morbidity [7], as well as preterm birth and reduced birth weight [8]. Consistent with these observations, reviews and satellite-enabled assessments indicate that flaring contributes to air-pollution mixtures relevant for population health, particularly fine particulate matter and co-pollutants in affected regions [5,18,19].The innovation of this study lies in several key aspects. First, it represents the first global, settlement-specific quantification of populations residing near upstream flares, applying a consistent methodology across multiple years. Second, it provides a comparable eight-year time series (2016‚Äì2023) to monitor trends and assess policy impacts. Third, the indicators are designed for direct use by governmental and international organizations, supporting the prioritization of interventions and identification of exposure hotspots. Fourth, the methodology is transparent and reproducible, featuring a straightforward workflow that can be re-executed as VIIRS or population data are updated. Fifth, the analysis highlights spatial exposure patterns across settlement types, showing where populations live near to flares are most concentrated and how these patterns evolve over time. Sixth, the analytical scope is explicitly aligned with the domain where commitments to eliminate routine flaring apply (upstream production). Finally, the resulting dataset can be directly linked with health, socioeconomic, or environmental data in future studies, without the need to redefine exposure metrics.It should be noted that the study does not simulate atmospheric pollutant dispersion nor directly estimate health outcomes; rather, the results describe potential exposure, not disease burden.",
            "Scope and Innovation of the Study": "This study quantifies the number of people living in proximity to oil and gas flaring facilities on a global scale. The analysis focuses on flares from upstream production facilities and does not include midstream energy system activities. We use the term exposure in accordance with the definitions established by the United Nations Office for Disaster Risk Reduction (UNDRR) and the United Nations Office for Outer Space Affairs (UN-SPIDER) [15,16]. UNDRR defines exposure as ‚Äúthe situation of people, infrastructure, housing, production capacities and other tangible human assets located in hazard-prone areas‚Äù, while UN-SPIDER specifies that measures of exposure may include ‚Äúthe number of people or types of assets in an area‚Äù. Following this internationally recognized terminology, we quantify exposure as the number of individuals residing within a defined proximity to gas-flaring sites, which are treated as localized environmental hazards. This spatially based measure reflects the presence of populations in hazard-prone areas and is distinct from hazard-intensity metrics (e.g., volume of gas flared or pollutant concentrations), which relate to the severity or magnitude of potential impacts rather than to exposure itself. We define proximity indicators as the number of individuals residing within 1 km and 3 km of active flaring sites during the period 2016‚Äì2023. To estimate the spatial distribution of the population, we employ the Visible Infrared Imaging Radiometer Suite (VIIRS) Nightfire dataset in combination with GHSL POP R2023A and SMOD Level 2, enabling the assessment of exposure across settlement categories (urban centre, dense-urban, semi-dense urban, suburban/peri-urban, and rural). We use population and settlement information from the Global Human Settlement Layer (GHSL) POP R2023A and the Settlement Model (SMOD) Level 2. GHSL POP R2023A provides global gridded population estimates at 100 m to 1 km resolution for multiple years and projections to 2030, while SMOD Level 2 classifies each grid cell into settlement types (urban centre, dense urban cluster, semi-dense urban cluster, suburban/peri-urban, rural cluster). Together, these datasets allow us to quantify how many people in each settlement class live within defined distances of flaring sites. The VIIRS is a satellite instrument that measures visible and infrared light to observe Earth‚Äôs surface and atmosphere. Its Nightfire data product is the primary global source for detecting and quantifying gas flaring from space [17]. To date, global studies have examined flaring intensity and emitted volumes, while epidemiological and environmental research has typically focused on specific countries or local communities. What has been missing is a global and systematic representation of how many people live near flares from production facilities, using a unified methodology that ensures spatial and temporal comparability. The present study contributes to the growing body of evidence linking residential proximity to flaring with adverse health effects. Epidemiological research has documented associations between flaring and respiratory morbidity [7], as well as preterm birth and reduced birth weight [8]. Consistent with these observations, reviews and satellite-enabled assessments indicate that flaring contributes to air-pollution mixtures relevant for population health, particularly fine particulate matter and co-pollutants in affected regions [5,18,19]. The innovation of this study lies in several key aspects. First, it represents the first global, settlement-specific quantification of populations residing near upstream flares, applying a consistent methodology across multiple years. Second, it provides a comparable eight-year time series (2016‚Äì2023) to monitor trends and assess policy impacts. Third, the indicators are designed for direct use by governmental and international organizations, supporting the prioritization of interventions and identification of exposure hotspots. Fourth, the methodology is transparent and reproducible, featuring a straightforward workflow that can be re-executed as VIIRS or population data are updated. Fifth, the analysis highlights spatial exposure patterns across settlement types, showing where populations live near to flares are most concentrated and how these patterns evolve over time. Sixth, the analytical scope is explicitly aligned with the domain where commitments to eliminate routine flaring apply (upstream production). Finally, the resulting dataset can be directly linked with health, socioeconomic, or environmental data in future studies, without the need to redefine exposure metrics. It should be noted that the study does not simulate atmospheric pollutant dispersion nor directly estimate health outcomes; rather, the results describe potential exposure, not disease burden.",
            "2. Materials and Methods": "2.1. The Dataset, Data Collection, and Data PreparationThe dataset [20] contains site-level estimates of annual gas flaring volumes for 102 countries from 2012 to 2023 (seeTable 1). The data set comprises 145,642 site-year records representing approximately 90,230 unique flare locations, identified by country and geographic coordinates. Over the full period, the total estimated flared volume is 1721.75 billion cubic meters (bcm). Flaring is predominantly onshore (80.24%), with offshore flaring contributing 19.76%. The dataset supports analyses of temporal trends, geographic patterns, and comparisons across asset types (OIL/GAS/LNG) and flare magnitude classes.Table 1.Description of variables included in the flared gas dataset, including variable names, data types, measurement units, and brief explanations of their meanings.Administrative boundary data (country polygons) were retrieved from the Natural Earth Geoportal [21], while urbanization degree and population density data in raster format were sourced from the Global Human Settlement Layer (GHSL) maintained by the European Commission [22]. The GHSL provides consistent, global-scale spatial data on where people live and how settlements evolve, making it essential for studies linking population exposure to environmental or industrial risks [22].All spatial datasets were projected into a common coordinate reference system using the Mollweide projection, and any spatial inconsistencies in the country shapefile were corrected to ensure accuracy in subsequent analyses.The total number of flaring sites per country was calculated and stored within the attribute table of the country shapefile. Geographic centroids were also computed for each country to facilitate cartographic visualization using proportional red symbols, whose size reflects the number of flare sites. To estimate the population potentially exposed to emissions, buffer zones of 1 and 3 km were generated around each flaring site. Previous epidemiological and environmental studies have shown that health risks associated with flaring and other combustion-related emissions decrease substantially with increasing distance, with notable reductions beyond  3‚Äì5 km [19] from the source. For instance, traffic-related pollutants such as PM2.5 and NO2decline sharply at distances greater than 3 km from major roads [23], while studies of gas flaring report elevated risks of adverse birth outcomes within approximately 5 km [24]. Based on this evidence, a maximum buffer of 5 km could have been considered. However, in densely populated urban areas, 5 km circles frequently overlap, leading to excessive double-counting and potential misrepresentation of exposure. To balance epidemiological relevance with spatial accuracy, we therefore adopted buffer distances of 1 km and 3 km. These thresholds capture the populations most at risk of direct exposure while minimizing the artificial inflation of exposed counts caused by overlapping buffers in high-density settlement zones. Overlapping buffers were dissolved to prevent double-counting of affected populations. The resulting buffer zones were intersected with national boundaries, and zonal statistics were computed on the population raster to estimate the total population residing within these areas. The calculated population data were then joined with the country shapefile and visualized with choropleth maps to represent exposure levels. We therefore selected 1 km to represent immediate toxicological exposure and 3 km to capture near-immediate gradients of health risk. Furthermore, to obtain a deeper understanding of average population exposure per flaring site, the total affected population in each country was divided by the number of active flaring sites in that country. These normalized values were subsequently linked to the respective country shapefiles and visualized for comparative analysis. The use of distance-based proxies does not account for meteorological variation, combustion efficiency, flare-stack design, or emission intensity. However, these proxies are widely applied in environmental health research and provide a reproducible, scalable basis for global exposure estimation. The VIIRS Nightfire dataset may underdetect low-temperature or intermittent flares, particularly in regions with persistent cloud cover or low radiative efficiency; nonetheless, it remains the most validated and comprehensive dataset currently available for global flaring surveillance [25,26].2.1.1. Global Distribution of Gas Flaring Sites (2023)Figure 2presents the global distribution of gas flaring points recorded in 2023. Each point represents a confirmed flaring site. High concentrations of flaring activity are visible in Russia and the Caspian Region, as well as in the Middle East, including Iran, Iraq, and Saudi Arabia. In North America, the United States shows widespread flaring, with notable clusters in Texas and coastal regions. In West and Central Africa, dense flare clusters are observed in Nigeria and parts of Angola. South America shows significant flaring in Venezuela, Colombia, Brazil, and Argentina. Europe exhibits moderate flare concentrations in Eastern Europe and the North Sea basin. Asia shows scattered flaring across China, India, and Southeast Asia, with visible activity in offshore areas. Australia displays dispersed flaring sites, while Canada shows flare activity in oil sands and shale regions. Smaller, isolated flare sites are present in New Zealand, Papua New Guinea, and several island nations. Western and Central Europe show minimal flaring activity. No flaring points are recorded in Antarctica and Greenland.Figure 2.The distribution of flaring points per country.2.1.2. National-Level Distribution of Gas Flaring Sites (2023)Figure 3presents the number of recorded gas flaring sites per country for the year 2023, using graduated symbols to represent flare counts. While the number of flaring sites indicates spatial proliferation, it does not directly translate into the magnitude of flaring, as the volume of gas burned varies by site. For this reason, the analysis is complemented byFigure 3, which reports flaring volumes at the country level, providing a weighted representation of overall flaring intensity. This national-scale visualization categorizes countries into five classes based on flare activity, highlighting the spatial concentration and relative burden of flaring practices worldwide. A small group of countries (a) emerges with exceptionally high flare counts, ranging between 1532 and 3020 sites. This includes Russia, which continues to lead globally in total flare numbers due to its extensive upstream oil and gas infrastructure. The United States also ranks high in this category, with flaring concentrated in shale-rich regions such as the Permian Basin and the Bakken Formation, where infrastructure constraints and surges in unconventional extraction have contributed to persistent flare activity. Iran and Iraq are similarly prominent, with routine flaring across widespread oil-producing regions in the absence of sufficient gas-recovery systems. Nigeria completes this high-burden group, with extensive flaring centered in the Niger Delta, reflecting both infrastructural deficits and regulatory challenges. A second group of countries (b) falls within the intermediate range of 406 to 1531 flares. These include China, Libya, Indonesia, and Venezuela, where flaring reflects both growing demand for domestic energy. Algeria and Angola also belong to this group, representing major hydrocarbon producers in Africa with ongoing difficulties in implementing effective flare mitigation strategies. Brazil and India round out this category, driven by increasing oil and gas activities, including offshore developments. In the third and fourth categories (c), which include countries with 38 to 405 flare sites, we observe a mix of mature and emerging oil producers, including Egypt, Mexico, Kazakhstan, Argentina, and several countries in Southeast Asia. Flaring in these cases tends to be more localized and may be seasonal or operationally dependent, often corresponding to specific basins or infrastructure types. The last group (d) consists of countries with minimal flare activity, ranging from 1 to 37 sites. These are widely distributed and include several European nations, parts of Southern Africa, Central America, and smaller island states. Low flare counts in these countries often correspond to limited oil extraction activity, stronger regulatory enforcement, or the successful implementation of gas recovery and utilization systems.Figure 3.The volume of flaring points per country.2.1.3. Analysis of Total Volume per Total Flares (2016‚Äì2023)Figure 4is showing all countries that appeared in the top 10 at least once from 2016 to 2023. This gives a complete picture of how each significant country has changed over time. Based on data from 2016 to 2023 on Total Volume per Total Flares, several clear trends and patterns emerge across countries.Figure 4.Total volume per flare (2016‚Äì2023) for the top-10 countries.Iraq stands out unmistakably as the leading country every single year, starting with an extremely high volume of 139.3 bcm in 2016 and maintaining values above 89 bcm in most years, with a slight dip in 2020 to 77.6 bcm. This consistent dominance suggests either particularly high flare volumes, lower flare counts, or a combination of both, possibly due to structural characteristics of its oil and gas infrastructure. Iran (Islamic Republic) appears regularly in the top 4, with values ranging from 45.6 bcm in 2020 to a peak of 66.8 bcm in 2016 and again in 2023 at 66.7 bcm. Its stable presence in the upper tier reflects sustained high-volume flaring with some year-to-year variation. Venezuela remains in the top 5 throughout the period, despite a general downward trend from 89.1 bcm in 2016 to 43.7 bcm in 2022. In 2023, a slight rebound to 47.8 bcm is observed, which may suggest operational recovery or renewed flaring activity. Angola, initially at 90.1 bcm in 2016, has shown a steady decline, reaching 40.8 bcm in 2023. This pattern might indicate improvements in flare efficiency or reductions in production volumes. Libya has been consistently observed since 2017, with increasing flare volume, peaking at 56.8 bcm in 2023. This upward trend may reflect expanded operations or worsening flare control. Republic of the Congo enters the top rankings in 2018 and maintains moderate values thereafter 52.8 bcm in 2018; 34‚Äì37 bcm in later years, highlighting a growing role in global flare activity. Nigeria, Algeria, and Cameroon appear frequently in the mid-tier range 30‚Äì45 bcm, showing moderate but relatively stable flare volumes over time. Malaysia and Mexico generally rank in the lower half of the top 10 list. Malaysia bcm drops out after 2020, while Mexico bcm consistently shows lower values, at 30‚Äì32 bcm, indicating relatively lower flare intensity. 2.2. Methodology: Urbanization Classification Using SMODThe study adopts the Settlement Model (SMOD) layers [27], which implement the Degree of Urbanisation Stage I methodology as recommended by the United Nations Statistical Commission. The classification was applied to the global population grid produced by the Joint Research Centre (JRC) for the years 1975 to 2030 in 5-year intervals.The SMOD layers are determined by integrating built-up surface data extracted from Landsat and Sentinel-2 imagery (GHS-BUILT-S R2023A) with gridded population data from the CIESIN GPW v4.11 dataset (GHS-POP R2023A). The version used in this analysis (SMOD R2023A v2) reflects updated definitions, including refinements to the classification of Semi-Dense Urban Clusters.We utilized the SMOD at Level 2 (L2) detail, enabling precise classification of settlement patterns. Specifically, five urbanization classes were analyzed:30: Urban Centre Grid Cell23: Dense Urban Cluster Grid Cell22: Semi-Dense Urban Cluster Grid Cell21: Suburban or Peri-Urban Grid Cell13: Rural Cluster Grid CellAll volumetric quantities of oil and gas are expressed in billion cubic meters (bcm). Although the term ‚Äúbcm‚Äù is most commonly used for natural gas, expressing liquid petroleum volumes in cubic meters facilitates direct comparison on a metric basis.These classes provide a hierarchical structure to assess spatial and temporal variations in population exposure and urban development. Aggregation to a broader classification (L1) is also supported when needed. The study employed a combination of Python 3.12 geospatial libraries and professional GIS software to conduct spatial analysis and visualization. Core Python tools included Pandas and NumPy for data handling, GeoPandas and Shapely for vector geometry operations, Rasterio for reading and processing raster datasets, Pyproj for coordinate system transformations, Contextily for basemap integration, and Folium for interactive mapping. In addition to these libraries, the analysis was supported by QGIS [28], an open-source desktop GIS platform, for visual inspection, spatial data validation, and map layout design. ArcGIS [29], ESRI [30] were also used for advanced geoprocessing and high-precision cartographic outputs. This integrated toolset enabled robust geospatial workflows throughout the study.We do not report margins of error or confidence intervals because our results are not based on predictions or statistical sampling. Instead, they are derived through a deterministic geospatial computation: overlaying high-resolution population rasters with buffer zones around flaring sites and directly aggregating the counts. The outcome is therefore a precise calculation of exposed populations given the input datasets, rather than an estimate with statistical uncertainty. Any potential inaccuracies arise from the underlying data sources (e.g., VIIRS flare detection, population distribution in GHSL), not from stochastic variation in our workflow.Data for 2016 and 2017 were processed using the GHSL 2015 dataset, whereas data for all subsequent years were derived from the 2020 dataset to reflect the closest available temporal match. The most recent calculations were performed using flaring data from 2023, together with GHSL population projections for 2030. The GHSL population datasets are available at spatial resolutions of 100 m and 1000 m. For population exposure estimations, the 100 m dataset was primarily utilized. However, for calculations involving SMOD, a compatibility issue required using the 1000 √ó 1000 m population dataset to ensure alignment with the SMOD data structure.",
            "2.1. The Dataset, Data Collection, and Data Preparation": "The dataset [20] contains site-level estimates of annual gas flaring volumes for 102 countries from 2012 to 2023 (seeTable 1). The data set comprises 145,642 site-year records representing approximately 90,230 unique flare locations, identified by country and geographic coordinates. Over the full period, the total estimated flared volume is 1721.75 billion cubic meters (bcm). Flaring is predominantly onshore (80.24%), with offshore flaring contributing 19.76%. The dataset supports analyses of temporal trends, geographic patterns, and comparisons across asset types (OIL/GAS/LNG) and flare magnitude classes. Table 1.Description of variables included in the flared gas dataset, including variable names, data types, measurement units, and brief explanations of their meanings. Administrative boundary data (country polygons) were retrieved from the Natural Earth Geoportal [21], while urbanization degree and population density data in raster format were sourced from the Global Human Settlement Layer (GHSL) maintained by the European Commission [22]. The GHSL provides consistent, global-scale spatial data on where people live and how settlements evolve, making it essential for studies linking population exposure to environmental or industrial risks [22]. All spatial datasets were projected into a common coordinate reference system using the Mollweide projection, and any spatial inconsistencies in the country shapefile were corrected to ensure accuracy in subsequent analyses. The total number of flaring sites per country was calculated and stored within the attribute table of the country shapefile. Geographic centroids were also computed for each country to facilitate cartographic visualization using proportional red symbols, whose size reflects the number of flare sites. To estimate the population potentially exposed to emissions, buffer zones of 1 and 3 km were generated around each flaring site. Previous epidemiological and environmental studies have shown that health risks associated with flaring and other combustion-related emissions decrease substantially with increasing distance, with notable reductions beyond  3‚Äì5 km [19] from the source. For instance, traffic-related pollutants such as PM2.5 and NO2decline sharply at distances greater than 3 km from major roads [23], while studies of gas flaring report elevated risks of adverse birth outcomes within approximately 5 km [24]. Based on this evidence, a maximum buffer of 5 km could have been considered. However, in densely populated urban areas, 5 km circles frequently overlap, leading to excessive double-counting and potential misrepresentation of exposure. To balance epidemiological relevance with spatial accuracy, we therefore adopted buffer distances of 1 km and 3 km. These thresholds capture the populations most at risk of direct exposure while minimizing the artificial inflation of exposed counts caused by overlapping buffers in high-density settlement zones. Overlapping buffers were dissolved to prevent double-counting of affected populations. The resulting buffer zones were intersected with national boundaries, and zonal statistics were computed on the population raster to estimate the total population residing within these areas. The calculated population data were then joined with the country shapefile and visualized with choropleth maps to represent exposure levels. We therefore selected 1 km to represent immediate toxicological exposure and 3 km to capture near-immediate gradients of health risk. Furthermore, to obtain a deeper understanding of average population exposure per flaring site, the total affected population in each country was divided by the number of active flaring sites in that country. These normalized values were subsequently linked to the respective country shapefiles and visualized for comparative analysis. The use of distance-based proxies does not account for meteorological variation, combustion efficiency, flare-stack design, or emission intensity. However, these proxies are widely applied in environmental health research and provide a reproducible, scalable basis for global exposure estimation. The VIIRS Nightfire dataset may underdetect low-temperature or intermittent flares, particularly in regions with persistent cloud cover or low radiative efficiency; nonetheless, it remains the most validated and comprehensive dataset currently available for global flaring surveillance [25,26]. 2.1.1. Global Distribution of Gas Flaring Sites (2023)Figure 2presents the global distribution of gas flaring points recorded in 2023. Each point represents a confirmed flaring site. High concentrations of flaring activity are visible in Russia and the Caspian Region, as well as in the Middle East, including Iran, Iraq, and Saudi Arabia. In North America, the United States shows widespread flaring, with notable clusters in Texas and coastal regions. In West and Central Africa, dense flare clusters are observed in Nigeria and parts of Angola. South America shows significant flaring in Venezuela, Colombia, Brazil, and Argentina. Europe exhibits moderate flare concentrations in Eastern Europe and the North Sea basin. Asia shows scattered flaring across China, India, and Southeast Asia, with visible activity in offshore areas. Australia displays dispersed flaring sites, while Canada shows flare activity in oil sands and shale regions. Smaller, isolated flare sites are present in New Zealand, Papua New Guinea, and several island nations. Western and Central Europe show minimal flaring activity. No flaring points are recorded in Antarctica and Greenland.Figure 2.The distribution of flaring points per country. 2.1.2. National-Level Distribution of Gas Flaring Sites (2023)Figure 3presents the number of recorded gas flaring sites per country for the year 2023, using graduated symbols to represent flare counts. While the number of flaring sites indicates spatial proliferation, it does not directly translate into the magnitude of flaring, as the volume of gas burned varies by site. For this reason, the analysis is complemented byFigure 3, which reports flaring volumes at the country level, providing a weighted representation of overall flaring intensity. This national-scale visualization categorizes countries into five classes based on flare activity, highlighting the spatial concentration and relative burden of flaring practices worldwide. A small group of countries (a) emerges with exceptionally high flare counts, ranging between 1532 and 3020 sites. This includes Russia, which continues to lead globally in total flare numbers due to its extensive upstream oil and gas infrastructure. The United States also ranks high in this category, with flaring concentrated in shale-rich regions such as the Permian Basin and the Bakken Formation, where infrastructure constraints and surges in unconventional extraction have contributed to persistent flare activity. Iran and Iraq are similarly prominent, with routine flaring across widespread oil-producing regions in the absence of sufficient gas-recovery systems. Nigeria completes this high-burden group, with extensive flaring centered in the Niger Delta, reflecting both infrastructural deficits and regulatory challenges. A second group of countries (b) falls within the intermediate range of 406 to 1531 flares. These include China, Libya, Indonesia, and Venezuela, where flaring reflects both growing demand for domestic energy. Algeria and Angola also belong to this group, representing major hydrocarbon producers in Africa with ongoing difficulties in implementing effective flare mitigation strategies. Brazil and India round out this category, driven by increasing oil and gas activities, including offshore developments. In the third and fourth categories (c), which include countries with 38 to 405 flare sites, we observe a mix of mature and emerging oil producers, including Egypt, Mexico, Kazakhstan, Argentina, and several countries in Southeast Asia. Flaring in these cases tends to be more localized and may be seasonal or operationally dependent, often corresponding to specific basins or infrastructure types. The last group (d) consists of countries with minimal flare activity, ranging from 1 to 37 sites. These are widely distributed and include several European nations, parts of Southern Africa, Central America, and smaller island states. Low flare counts in these countries often correspond to limited oil extraction activity, stronger regulatory enforcement, or the successful implementation of gas recovery and utilization systems.Figure 3.The volume of flaring points per country. 2.1.3. Analysis of Total Volume per Total Flares (2016‚Äì2023)Figure 4is showing all countries that appeared in the top 10 at least once from 2016 to 2023. This gives a complete picture of how each significant country has changed over time. Based on data from 2016 to 2023 on Total Volume per Total Flares, several clear trends and patterns emerge across countries.Figure 4.Total volume per flare (2016‚Äì2023) for the top-10 countries.Iraq stands out unmistakably as the leading country every single year, starting with an extremely high volume of 139.3 bcm in 2016 and maintaining values above 89 bcm in most years, with a slight dip in 2020 to 77.6 bcm. This consistent dominance suggests either particularly high flare volumes, lower flare counts, or a combination of both, possibly due to structural characteristics of its oil and gas infrastructure. Iran (Islamic Republic) appears regularly in the top 4, with values ranging from 45.6 bcm in 2020 to a peak of 66.8 bcm in 2016 and again in 2023 at 66.7 bcm. Its stable presence in the upper tier reflects sustained high-volume flaring with some year-to-year variation. Venezuela remains in the top 5 throughout the period, despite a general downward trend from 89.1 bcm in 2016 to 43.7 bcm in 2022. In 2023, a slight rebound to 47.8 bcm is observed, which may suggest operational recovery or renewed flaring activity. Angola, initially at 90.1 bcm in 2016, has shown a steady decline, reaching 40.8 bcm in 2023. This pattern might indicate improvements in flare efficiency or reductions in production volumes. Libya has been consistently observed since 2017, with increasing flare volume, peaking at 56.8 bcm in 2023. This upward trend may reflect expanded operations or worsening flare control. Republic of the Congo enters the top rankings in 2018 and maintains moderate values thereafter 52.8 bcm in 2018; 34‚Äì37 bcm in later years, highlighting a growing role in global flare activity. Nigeria, Algeria, and Cameroon appear frequently in the mid-tier range 30‚Äì45 bcm, showing moderate but relatively stable flare volumes over time. Malaysia and Mexico generally rank in the lower half of the top 10 list. Malaysia bcm drops out after 2020, while Mexico bcm consistently shows lower values, at 30‚Äì32 bcm, indicating relatively lower flare intensity.",
            "2.1.1. Global Distribution of Gas Flaring Sites (2023)": "Figure 2presents the global distribution of gas flaring points recorded in 2023. Each point represents a confirmed flaring site. High concentrations of flaring activity are visible in Russia and the Caspian Region, as well as in the Middle East, including Iran, Iraq, and Saudi Arabia. In North America, the United States shows widespread flaring, with notable clusters in Texas and coastal regions. In West and Central Africa, dense flare clusters are observed in Nigeria and parts of Angola. South America shows significant flaring in Venezuela, Colombia, Brazil, and Argentina. Europe exhibits moderate flare concentrations in Eastern Europe and the North Sea basin. Asia shows scattered flaring across China, India, and Southeast Asia, with visible activity in offshore areas. Australia displays dispersed flaring sites, while Canada shows flare activity in oil sands and shale regions. Smaller, isolated flare sites are present in New Zealand, Papua New Guinea, and several island nations. Western and Central Europe show minimal flaring activity. No flaring points are recorded in Antarctica and Greenland. Figure 2.The distribution of flaring points per country.",
            "2.1.2. National-Level Distribution of Gas Flaring Sites (2023)": "Figure 3presents the number of recorded gas flaring sites per country for the year 2023, using graduated symbols to represent flare counts. While the number of flaring sites indicates spatial proliferation, it does not directly translate into the magnitude of flaring, as the volume of gas burned varies by site. For this reason, the analysis is complemented byFigure 3, which reports flaring volumes at the country level, providing a weighted representation of overall flaring intensity. This national-scale visualization categorizes countries into five classes based on flare activity, highlighting the spatial concentration and relative burden of flaring practices worldwide. A small group of countries (a) emerges with exceptionally high flare counts, ranging between 1532 and 3020 sites. This includes Russia, which continues to lead globally in total flare numbers due to its extensive upstream oil and gas infrastructure. The United States also ranks high in this category, with flaring concentrated in shale-rich regions such as the Permian Basin and the Bakken Formation, where infrastructure constraints and surges in unconventional extraction have contributed to persistent flare activity. Iran and Iraq are similarly prominent, with routine flaring across widespread oil-producing regions in the absence of sufficient gas-recovery systems. Nigeria completes this high-burden group, with extensive flaring centered in the Niger Delta, reflecting both infrastructural deficits and regulatory challenges. A second group of countries (b) falls within the intermediate range of 406 to 1531 flares. These include China, Libya, Indonesia, and Venezuela, where flaring reflects both growing demand for domestic energy. Algeria and Angola also belong to this group, representing major hydrocarbon producers in Africa with ongoing difficulties in implementing effective flare mitigation strategies. Brazil and India round out this category, driven by increasing oil and gas activities, including offshore developments. In the third and fourth categories (c), which include countries with 38 to 405 flare sites, we observe a mix of mature and emerging oil producers, including Egypt, Mexico, Kazakhstan, Argentina, and several countries in Southeast Asia. Flaring in these cases tends to be more localized and may be seasonal or operationally dependent, often corresponding to specific basins or infrastructure types. The last group (d) consists of countries with minimal flare activity, ranging from 1 to 37 sites. These are widely distributed and include several European nations, parts of Southern Africa, Central America, and smaller island states. Low flare counts in these countries often correspond to limited oil extraction activity, stronger regulatory enforcement, or the successful implementation of gas recovery and utilization systems. Figure 3.The volume of flaring points per country.",
            "2.1.3. Analysis of Total Volume per Total Flares (2016‚Äì2023)": "Figure 4is showing all countries that appeared in the top 10 at least once from 2016 to 2023. This gives a complete picture of how each significant country has changed over time. Based on data from 2016 to 2023 on Total Volume per Total Flares, several clear trends and patterns emerge across countries. Figure 4.Total volume per flare (2016‚Äì2023) for the top-10 countries. Iraq stands out unmistakably as the leading country every single year, starting with an extremely high volume of 139.3 bcm in 2016 and maintaining values above 89 bcm in most years, with a slight dip in 2020 to 77.6 bcm. This consistent dominance suggests either particularly high flare volumes, lower flare counts, or a combination of both, possibly due to structural characteristics of its oil and gas infrastructure. Iran (Islamic Republic) appears regularly in the top 4, with values ranging from 45.6 bcm in 2020 to a peak of 66.8 bcm in 2016 and again in 2023 at 66.7 bcm. Its stable presence in the upper tier reflects sustained high-volume flaring with some year-to-year variation. Venezuela remains in the top 5 throughout the period, despite a general downward trend from 89.1 bcm in 2016 to 43.7 bcm in 2022. In 2023, a slight rebound to 47.8 bcm is observed, which may suggest operational recovery or renewed flaring activity. Angola, initially at 90.1 bcm in 2016, has shown a steady decline, reaching 40.8 bcm in 2023. This pattern might indicate improvements in flare efficiency or reductions in production volumes. Libya has been consistently observed since 2017, with increasing flare volume, peaking at 56.8 bcm in 2023. This upward trend may reflect expanded operations or worsening flare control. Republic of the Congo enters the top rankings in 2018 and maintains moderate values thereafter 52.8 bcm in 2018; 34‚Äì37 bcm in later years, highlighting a growing role in global flare activity. Nigeria, Algeria, and Cameroon appear frequently in the mid-tier range 30‚Äì45 bcm, showing moderate but relatively stable flare volumes over time. Malaysia and Mexico generally rank in the lower half of the top 10 list. Malaysia bcm drops out after 2020, while Mexico bcm consistently shows lower values, at 30‚Äì32 bcm, indicating relatively lower flare intensity.",
            "2.2. Methodology: Urbanization Classification Using SMOD": "The study adopts the Settlement Model (SMOD) layers [27], which implement the Degree of Urbanisation Stage I methodology as recommended by the United Nations Statistical Commission. The classification was applied to the global population grid produced by the Joint Research Centre (JRC) for the years 1975 to 2030 in 5-year intervals. The SMOD layers are determined by integrating built-up surface data extracted from Landsat and Sentinel-2 imagery (GHS-BUILT-S R2023A) with gridded population data from the CIESIN GPW v4.11 dataset (GHS-POP R2023A). The version used in this analysis (SMOD R2023A v2) reflects updated definitions, including refinements to the classification of Semi-Dense Urban Clusters. We utilized the SMOD at Level 2 (L2) detail, enabling precise classification of settlement patterns. Specifically, five urbanization classes were analyzed: 30: Urban Centre Grid Cell23: Dense Urban Cluster Grid Cell22: Semi-Dense Urban Cluster Grid Cell21: Suburban or Peri-Urban Grid Cell13: Rural Cluster Grid Cell All volumetric quantities of oil and gas are expressed in billion cubic meters (bcm). Although the term ‚Äúbcm‚Äù is most commonly used for natural gas, expressing liquid petroleum volumes in cubic meters facilitates direct comparison on a metric basis. These classes provide a hierarchical structure to assess spatial and temporal variations in population exposure and urban development. Aggregation to a broader classification (L1) is also supported when needed. The study employed a combination of Python 3.12 geospatial libraries and professional GIS software to conduct spatial analysis and visualization. Core Python tools included Pandas and NumPy for data handling, GeoPandas and Shapely for vector geometry operations, Rasterio for reading and processing raster datasets, Pyproj for coordinate system transformations, Contextily for basemap integration, and Folium for interactive mapping. In addition to these libraries, the analysis was supported by QGIS [28], an open-source desktop GIS platform, for visual inspection, spatial data validation, and map layout design. ArcGIS [29], ESRI [30] were also used for advanced geoprocessing and high-precision cartographic outputs. This integrated toolset enabled robust geospatial workflows throughout the study. We do not report margins of error or confidence intervals because our results are not based on predictions or statistical sampling. Instead, they are derived through a deterministic geospatial computation: overlaying high-resolution population rasters with buffer zones around flaring sites and directly aggregating the counts. The outcome is therefore a precise calculation of exposed populations given the input datasets, rather than an estimate with statistical uncertainty. Any potential inaccuracies arise from the underlying data sources (e.g., VIIRS flare detection, population distribution in GHSL), not from stochastic variation in our workflow. Data for 2016 and 2017 were processed using the GHSL 2015 dataset, whereas data for all subsequent years were derived from the 2020 dataset to reflect the closest available temporal match. The most recent calculations were performed using flaring data from 2023, together with GHSL population projections for 2030. The GHSL population datasets are available at spatial resolutions of 100 m and 1000 m. For population exposure estimations, the 100 m dataset was primarily utilized. However, for calculations involving SMOD, a compatibility issue required using the 1000 √ó 1000 m population dataset to ensure alignment with the SMOD data structure.",
            "3. Results": "It is important to note that the countries shown in the heatmaps and the trend chart are not the top 12, 15, or 17 countries for each individual year, but rather the set of countries that appear among the top 5 or top 10 highest-impact cases at least once during the period 2016‚Äì2023. Some countries exhibit high population exposure within 1 km of flaring sites only in specific years, while in other years they fall below the upper ranks. For this reason, the final list includes countries that appear repeatedly over the eight-year window, even if they do not consistently rank in the top positions each year. This also explains why some heatmaps display 12, others 15, and others 17 countries instead of a fixed number, such as the top 10, since several countries reach notable exposure values intermittently rather than continuously. This approach ensures that the analysis captures all countries that ever reach a high exposure value during the study period, rather than limiting the selection to those that consistently rank highly every year. The heatmaps illustrate this clearly: several countries exhibit pronounced exposure peaks only in particular years, confirming that the selected set is based on overall prominence across the full period rather than on strict year-by-year rankings. 3.1. Population Within 1 km of Flares Elements by Country (2016‚Äì2023)InFigure 5, we present a time series (2016‚Äì2023) of each country‚Äôs maximum population residing in a single 1 km2grid cell.Figure 6a highlights the twelve countries that consistently rank among the global top ten over this period, revealing a remarkably stable group of high-density locations.Figure 5.Global maps of the maximum number of people residing within 1 km of active upstream flaring sites, 2016‚Äì2023. Panels: (a) 2023, (b) 2022, (c) 2021, (d) 2020, (e) 2019, (f) 2018, (g) 2017, (h) 2016, and (i) the legend. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S1‚ÄìS8).Figure 6.(a) Heatmap of population near flares by country and year. (b) Total population within 1 km of flaring elements, 2016‚Äì2023. (c) Countries that ever ranked in the top 5 by population living within 1 km of flaring sites during 2016‚Äì2023. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S9‚ÄìS11).Figure 6b,c summarizes the total number of people living within 1 km of flaring sites and identifies the five most affected countries for each year. In 2016, roughly 1,296,816 people lived near flares, with Nigeria, Indonesia, Russia, Pakistan, and Iraq most affected. The following year, the total declined to 1,202,945, led by Indonesia, China, India, Iraq, and Iran. In 2018, the total number rose again to 1,296,488, driven by Indonesia, Iraq, India, Iran, and China, before falling to 1,247,252 in 2019 (Indonesia, Iraq, Iran, India, Pakistan). In 2020, population exposure peaked at 1,480,563‚Äîprimarily in Indonesia, India, Iraq, Pakistan, and Iran‚Äîthen decreased to 1,423,045 in 2021 (Indonesia, India, Iraq, Iran, China) and 1,223,337 in 2022 (Indonesia, Iraq, India, Pakistan, China). By 2023, the total reached its lowest point at 971,618, with Indonesia, India, Iraq, Pakistan, and China remaining the most affected. Throughout 2016‚Äì2023, Indonesia, India, Iraq, Pakistan, China, and Iran consistently appear among the five countries with the largest populations near flaring sites. 3.2. Population Within 3 km of Flares Elements by Country (2016‚Äì2023)InFigure 7, we show a time series (2016‚Äì2023) of each country‚Äôs maximum population within a radius of 3 km.Figure 8a highlights the twelve countries that appear in the annual top ten over this period, revealing a remarkably stable group of high-density locations.Figure 7.Global maps showing, for each country, the maximum number of people residing within 3 km of active upstream flaring sites for the years (a) 2023, (b) 2022, (c) 2021, (d) 2020, (e) 2019, (f) 2018, (g) 2017, and (h) 2016. (i) The legend. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S12‚ÄìS19).Figure 8.(a) Heatmap of population near flares by country and year (3 km radius). (b) Total population within 3 km of flaring elements for the period 2016‚Äì2023. (c) Countries that ever ranked in the top 5 by population living within 3 km of flaring sites during 2016‚Äì2023. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S20‚ÄìS22).Figure 8b,c summarizes the total number of people living within 3 km of flaring sites and identifies the five most affected countries each year. In 2016, approximately 7,127,174 people lived within 3 km of flares across nine countries, with Indonesia (1,479,452), Nigeria (1,375,098), Pakistan (803,243), the Russian Federation (760,320), and Iran (632,853) most exposed; in 2017, the total rose to 7,297,205, led by Nigeria (1,434,636), China (989,203), Iraq (847,668), Pakistan (833,324), and India (783,149); in 2018 it climbed to 8,378,305, driven by Indonesia (1,628,811), India (1,141,450), Iraq (1,098,070), China (979,707), and Iran (927,997); 2019 saw a decline to 7,088,776, with Indonesia (1,445,555), Iran (1,200,590), Iraq (1,056,994), India (871,525), and Pakistan (647,885) most affected; in 2020 exposure peaked at 8,808,104‚Äîprimarily in Nigeria (1,803,833), India (1,473,674), Iraq (1,165,139), Pakistan (1,110,260), and China (853,826); 2021 recorded 8,643,855 people near flares, notably in Nigeria (1,794,524), India (1,676,036), Iraq (1,183,083), Pakistan (936,520), and China (859,549); in 2022 the total fell to 7,235,113, led by Nigeria (1,364,012), Iraq (1,165,038), India (1,125,643), Pakistan (924,009), and China (786,599); and by 2023 it reached its lowest point at 6,026,231, with Indonesia (1,465,760), India (965,446), Iraq (705,791), China (586,830), and Pakistan (576,843) remaining the most exposed. 3.3. Population near Urban Centre Areas by Country: 2016‚Äì2023This report analyzes the population living in urban centres within 1 km of gas flaring sites in countries that enter the top 10 by exposed population at least once during the period 2016‚Äì2023. These data provide insights into population exposure to flaring in densely populated urban hubs.The largest absolute exposures to flaring occur in dense urban centres (Figure 9andFigure 10), where very large populations are concentrated. Nigeria is the dominant case globally, beginning with approximately 140,000 exposed individuals in 2016, peaking near 180,000 in 2018, and remaining above 100,000 in 2023. India showed extremely high exposure in 2016, exceeding 200,000, but this dropped sharply to around 20,000 by 2019, partially recovered to  70,000 in 2021, and fell again to fewer than 10,000 by 2023. This pattern suggests either rapid regulatory change or inconsistencies in the detection of the satellite record. Indonesia maintained a consistently high exposure, ranging between 60,000 and 100,000, with  50,000 still affected in 2023. Pakistan also showed a steady increase, peaking at  50,000 in 2020 before modest declines thereafter. Other countries, including Venezuela, Mexico, Iraq, and Bangladesh, recorded moderate exposures in the 20,000‚Äì50,000 range. By contrast, exposures in China, Iran, Egypt, and the Gulf States remained relatively low. These findings highlight that urban centres in Nigeria, Indonesia, and Pakistan are the most persistent global hotspots of human exposure to gas flaring, whereas India‚Äôs dramatic reduction warrants further investigation. Urban centre exposures are of particular concern because they affect the largest absolute populations, compounding risks in environments already burdened by air pollution and high population density.Figure 9.Global maps showing, for each country, the maximum number of people residing in urban centre areas within a 1 km radius of active flaring sites for the years (a) 2023, (b) 2022, (c) 2021, (d) 2020, (e) 2019, (f) 2018, (g) 2017, (h) 2016. (i) The legend. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S23‚ÄìS30).Figure 10.Urban centre population living within 1 km of active flaring sites for countries that enter the top 10 at least once during 2016‚Äì2023: (a) Heatmap visualization showing temporal and geographic patterns. (b) Comparative totals by country and year. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S31 and S32). 3.4. Population near Dense Urban Areas by Country: 2016‚Äì2023This report focuses on the population living in dense urban areas within 1 km of gas flaring sites in countries that enter the top 10 by exposed population at least once during the period 2016‚Äì2023. The data reflect population exposure to flaring in highly populated urban zones.Dense urban clusters represent the largest single concentrations of populations living near gas flaring (Figure 11andFigure 12). Nigeria stands out as the most affected country, with consistently high exposure levels averaging around 70,000‚Äì80,000 individuals annually and peaking at more than 100,000 in 2020. Iran and Indonesia form the second tier of hotspots: both countries recorded steady increases to peaks of approximately 80,000 and 77,000, respectively, in 2020 before declining to 47,000‚Äì56,000 by 2023. Iraq also exhibited significant exposure, rising to nearly 47,000 in 2020 before falling to  32,000 by 2023. China and India, while among the world‚Äôs largest flare producers, registered more moderate dense urban exposures of  30,000‚Äì40,000, reflecting different spatial overlaps between flaring sites and dense urban settlements. Mexico remained stable in the 25,000‚Äì30,000 range throughout the period, while other producer states, such as Saudi Arabia, the United Arab Emirates, Algeria, and Venezuela, recorded comparatively low exposures (<20,000). Importantly, 2020 marked the global peak in dense urban exposure, likely reflecting both production dynamics and pandemic-related disruptions. These findings highlight that dense urban flare exposure is concentrated in a handful of producer states, with Nigeria, Iran, Indonesia, and Iraq emerging as critical global hotspots where mitigation would deliver the greatest benefits to human health.Figure 11.Global maps showing, for each country, the maximum number of people residing in dense urban areas within a 1 km radius of active flaring sites for the years (a) 2023, (b) 2022, (c) 2021, (d) 2020, (e) 2019, (f) 2018, (g) 2017, (h) 2016. (i) The legend. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S33‚ÄìS40).Figure 12.Dense urban population living within 1 km of active flaring sites for countries that enter the top 10 at least once during 2016‚Äì2023: (a) Heatmap visualization showing temporal and geographic patterns. (b) Comparative totals by country and year. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S41 and S42). 3.5. Population near Semi-Dense Urban Areas by Country: 2016‚Äì2023Semi-dense urban settlements exposed to flaring within 1 km (Figure 13andFigure 14) represent a smaller but highly vulnerable category of affected populations. India consistently accounts for the largest exposures, beginning at approximately 20,000 individuals in 2016, declining to about 13,000 by 2019, then rising again to nearly 18,000 in 2022, before a slight reduction in 2023. China and Indonesia form the second tier, with China reaching 16,000‚Äì17,000 exposed individuals by 2020 but dropping sharply to 6000 in 2022 before recovering to around 10,000 in 2023, while Indonesia maintained a steady upward trajectory to nearly 10,000. Pakistan and Venezuela experienced temporary peaks above 10,000 around 2018‚Äì2020 but later declined, whereas Sudan consistently showed smaller but non-negligible exposures of around 5000‚Äì8000. Other countries, such as Iraq, Kazakhstan, Libya, Malaysia, and Peru, remained below 5000 throughout the study period. These results illustrate that, although absolute population numbers in semi-dense urban clusters are lower than in rural or peri-urban contexts, the relative risk is higher due to population density and existing environmental burdens. Persistent hotspots in India and fluctuating but significant exposures in China underscore the importance of targeted flare-mitigation measures in semi-urban areas where vulnerable populations may be disproportionately affected.Figure 13.Global maps showing, for each country, the maximum number of people residing in semi-dense urban areas within a 1 km radius of active flaring sites for the years (a) 2023, (b) 2022, (c) 2021, (d) 2020, (e) 2019, (f) 2018, (g) 2017, (h) 2016. (i) The legend. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S43‚ÄìS50).Figure 14.Semi-dense urban population living within 1 km of active flaring sites for countries that enter the top 10 at least once during 2016‚Äì2023: (a) Heatmap visualization showing temporal and geographic trends. (b) Comparative totals by country and year. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S51 and S52). 3.6. Population near Suburban Areas by Country: 2016‚Äì2023Suburban populations within 1 km of flares (Figure 15andFigure 16) are concentrated primarily in South and Southeast Asia. India and Pakistan emerge as the most affected countries, together accounting for the majority of global exposure in this settlement category. India exhibits a consistent upward trend, rising from approximately 45,000 exposed individuals in 2016 to more than 62,000 in 2023, making it the single largest suburban exposure hotspot. Pakistan reached a peak of nearly 65,000 in 2020, then declined somewhat in subsequent years, but still recorded more than 45,000 exposed individuals in 2023. Indonesia represents the third-highest case, maintaining a steady upward trend to nearly 40,000 in 2023. Other producer states, including Nigeria, Iraq, and Iran, sustained medium levels of suburban exposure ranging from 25,000 to 35,000 over the study period, with little evidence of decline. By contrast, several countries, most notably China, Russia, and Saudi Arabia, recorded much lower suburban exposure (<15,000 by 2023), in some cases reflecting downward trends. Taken together, these results highlight that suburban and peri-urban communities in India and Pakistan are currently the most affected globally by gas flaring activity, while persistent medium exposure in Nigeria, Iraq, and Indonesia underscores a continued lack of effective mitigation in key producing regions.Figure 15.Global maps showing, for each country, the maximum number of people residing in suburban/peri-urban areas within a 1 km radius of active flaring sites for the years (a) 2023, (b) 2022, (c) 2021, (d) 2020, (e) 2019, (f) 2018, (g) 2017, (h) 2016. (i) The legend. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S53‚ÄìS60).Figure 16.Suburban population living within 1 km of active flaring sites for countries that enter the top 10 at least once during 2016‚Äì2023: (a) Heatmap showing temporal and geographic distribution. (b) Comparative totals by country and year. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S61 and S62). 3.7. Population near Rural Areas by Country: 2016‚Äì2023Analysis of rural cluster populations (Figure 17andFigure 18) living within 1 km of flares between 2016 and 2023 reveals persistent and uneven exposure across the top flaring countries. Iraq, Nigeria, and Mexico consistently record the largest exposed populations, with Iraq peaking at more than 50,000 people in 2018 before declining to approximately 35,000 by 2023. Nigeria maintained consistently high exposure levels of around 38,000‚Äì40,000 throughout most of the study period, with only a modest decline in the final years. Mexico similarly shows little improvement, with rural populations exposed to flares remaining stable at approximately 38,000‚Äì39,000 across the full period. By contrast, China and India exhibit sharp declines: China peaked at nearly 30,000 exposed individuals in 2017 but fell to under 10,000 by 2023, while India followed a similar downward trajectory after 2020. Medium-exposure countries such as Iran, Indonesia, and Colombia maintained levels between 10,000 and 20,000, with only gradual reductions. Lower-exposure countries, including the Russian Federation, Syria, Oman, and Pakistan, remained relatively stable near 10,000‚Äì15,000. These results highlight that while some countries appear to benefit from stricter regulation or energy transitions, others, particularly Nigeria and Mexico, have seen little progress, leaving large rural populations persistently exposed to flaring-related emissions.Figure 17.Global maps showing, for each country, the maximum number of people residing inrural areaswithin a 1 km radius of active flaring sites for the years (a) 2023, (b) 2022, (c) 2021, (d) 2020, (e) 2019, (f) 2018, (g) 2017, (h) 2016. (i) The legend. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S63‚ÄìS70).Figure 18.Rural population living within 1 km of active flaring sites for countries that enter the top 10 at least once during 2016‚Äì2023: (a) Heatmap visualization showing temporal and geographic patterns. (b) Comparative totals by country and year. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S71 and S72). 3.8. Predictive Modeling of Human Exposure to Gas Flaring in 2030This section presents predicted estimates of the population expected to be exposed to gas flaring activities by the year 2030, based on flaring activity recorded in 2023, combined with GHSL population projections for 2030 (Table 2). These projections provide a forward-looking perspective, offering critical insights into how demographic trends and urban expansion will influence the spatial distribution of at-risk populations in the coming years. The analysis is designed to support planning and policy interventions to mitigate the health and environmental impacts of gas flaring, in alignment with global objectives, such as the Zero Routine Flaring by 2030 initiative.Table 2.Projected population exposure to gas flaring in 2030 by proximity and settlement type (SMOD classification).3.8.1. Overall Population ExposureThe results indicate a substantial projected increase in the number of people residing near flaring sites by 2030. Within a 1 km radius, the total affected population is predicted to reach approximately 2.70 million individuals (2,700,770.64). Expanding the radius to 3 km dramatically increases the projected exposure to 14.85 million individuals (14,853,912.41). This represents a more than fivefold increase when the buffer is extended from 1 km to 3 km, reflecting the far-reaching potential of flaring emissions and the population growth expected in areas surrounding oil and gas production sites. These projections suggest that without significant mitigation measures, millions of people will remain at heightened risk from direct and indirect impacts of flaring.3.8.2. Projected Distribution by Settlement Type (SMOD Classification)To better understand the dynamics of population growth and exposure, the analysis incorporates Settlement Model (SMOD) Level 2 classifications, which categorize human settlements into five distinct classes: Rural Clusters (SMOD 13), Suburban/Peri-Urban Areas (SMOD 21), Semi-Dense Urban Clusters (SMOD 22), Dense Urban Clusters (SMOD 23), and Urban Centres (SMOD 30). This classification enables the identification of spatial and demographic trends that will shape future exposure scenarios.3.8.3. 1 km Proximity‚ÄîImmediate Impact ZoneWithin the 1 km buffer, the projected exposure patterns for 2030 reveal a significant concentration of populations in urban and densely developed areas. Dense Urban Clusters (SMOD 23) and Urban Centres (SMOD 30) each account for approximately 637,406 and 631,131 exposed individuals, respectively, representing nearly half of the total population at this distance. Suburban/Peri-Urban Areas (SMOD 21) are projected to have 439,954 exposed individuals, underscoring the role of expanding metropolitan peripheries in shaping future risk. Rural Clusters (SMOD 13) are expected to include 374,329 affected individuals, highlighting persistent risks for populations in less developed regions. Semi-Dense Urban Clusters (SMOD 22) have the lowest predicted exposure, with 104,087 individuals, indicating that transitional settlement zones remain relatively less impacted compared to urban cores and peri-urban fringes. These findings suggest that by 2030, urban populations will bear the greatest direct exposure, exacerbating existing health and environmental stressors in cities.3.8.4. 3 km Proximity‚ÄîBroader Influence ZoneWhen the analysis is extended to a 3 km radius, the population at risk increases dramatically, particularly within high-density urban areas. Urban Centres (SMOD 30) dominate the projections, with an estimated 6.32 million people exposed, representing 42.6% of the total population within this buffer. Dense Urban Clusters (SMOD 23) are projected to reach 2.82 million exposed individuals, highlighting the continued vulnerability of secondary urban zones. Suburban/Peri-Urban Areas (SMOD 21) show a sharp increase to 2.54 million individuals, reflecting rapid outward urban sprawl and the increasing overlap between residential zones and industrial activities. Rural Clusters (SMOD 13) will encompass approximately 963,257 exposed individuals, while Semi-Dense Urban Clusters (SMOD 22) are projected to reach 456,676, remaining the smallest group overall. 3.9. Synthesis of Main Exposure PatternsThe analysis of potential population exposure to upstream gas flaring between 2016 and 2023 reveals a three-fold pattern critical for mitigation efforts. Firstly, exposure is highly concentrated geographically, with a small group of large oil producers, namely India, Indonesia, Iraq, Nigeria, Pakistan, and China, consistently accounting for the majority of people living within 1 km and 3 km of flaring sites. Secondly, the structure of exposure is strongly dependent on settlement type: at the closest proximity (1 km), urban centers host a large fraction of exposed individuals, while a wider radius (3 km) shows the majority of exposed populations residing in a mix of urban, peri-urban, and rural clusters, reflecting how settlement growth has encroached upon the spatial extent of flaring. Finally, temporal trends are heterogeneous: While countries such as China and Iran show gradual declines in exposed populations, others, such as Nigeria, Mexico, and Indonesia, exhibit persistent or only weakly declining exposure, despite international commitments to reduce flaring. These findings collectively underscore that (i) mitigation can be effectively prioritized in a select number of high-exposure countries, and (ii) without immediate, targeted intervention, the ongoing processes of urbanization and demographic growth are projected to increase the total number of people living near flaring sites by 2030.",
            "3.1. Population Within 1 km of Flares Elements by Country (2016‚Äì2023)": "InFigure 5, we present a time series (2016‚Äì2023) of each country‚Äôs maximum population residing in a single 1 km2grid cell.Figure 6a highlights the twelve countries that consistently rank among the global top ten over this period, revealing a remarkably stable group of high-density locations. Figure 5.Global maps of the maximum number of people residing within 1 km of active upstream flaring sites, 2016‚Äì2023. Panels: (a) 2023, (b) 2022, (c) 2021, (d) 2020, (e) 2019, (f) 2018, (g) 2017, (h) 2016, and (i) the legend. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S1‚ÄìS8). Figure 6.(a) Heatmap of population near flares by country and year. (b) Total population within 1 km of flaring elements, 2016‚Äì2023. (c) Countries that ever ranked in the top 5 by population living within 1 km of flaring sites during 2016‚Äì2023. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S9‚ÄìS11). Figure 6b,c summarizes the total number of people living within 1 km of flaring sites and identifies the five most affected countries for each year. In 2016, roughly 1,296,816 people lived near flares, with Nigeria, Indonesia, Russia, Pakistan, and Iraq most affected. The following year, the total declined to 1,202,945, led by Indonesia, China, India, Iraq, and Iran. In 2018, the total number rose again to 1,296,488, driven by Indonesia, Iraq, India, Iran, and China, before falling to 1,247,252 in 2019 (Indonesia, Iraq, Iran, India, Pakistan). In 2020, population exposure peaked at 1,480,563‚Äîprimarily in Indonesia, India, Iraq, Pakistan, and Iran‚Äîthen decreased to 1,423,045 in 2021 (Indonesia, India, Iraq, Iran, China) and 1,223,337 in 2022 (Indonesia, Iraq, India, Pakistan, China). By 2023, the total reached its lowest point at 971,618, with Indonesia, India, Iraq, Pakistan, and China remaining the most affected. Throughout 2016‚Äì2023, Indonesia, India, Iraq, Pakistan, China, and Iran consistently appear among the five countries with the largest populations near flaring sites.",
            "3.2. Population Within 3 km of Flares Elements by Country (2016‚Äì2023)": "InFigure 7, we show a time series (2016‚Äì2023) of each country‚Äôs maximum population within a radius of 3 km.Figure 8a highlights the twelve countries that appear in the annual top ten over this period, revealing a remarkably stable group of high-density locations. Figure 7.Global maps showing, for each country, the maximum number of people residing within 3 km of active upstream flaring sites for the years (a) 2023, (b) 2022, (c) 2021, (d) 2020, (e) 2019, (f) 2018, (g) 2017, and (h) 2016. (i) The legend. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S12‚ÄìS19). Figure 8.(a) Heatmap of population near flares by country and year (3 km radius). (b) Total population within 3 km of flaring elements for the period 2016‚Äì2023. (c) Countries that ever ranked in the top 5 by population living within 3 km of flaring sites during 2016‚Äì2023. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S20‚ÄìS22). Figure 8b,c summarizes the total number of people living within 3 km of flaring sites and identifies the five most affected countries each year. In 2016, approximately 7,127,174 people lived within 3 km of flares across nine countries, with Indonesia (1,479,452), Nigeria (1,375,098), Pakistan (803,243), the Russian Federation (760,320), and Iran (632,853) most exposed; in 2017, the total rose to 7,297,205, led by Nigeria (1,434,636), China (989,203), Iraq (847,668), Pakistan (833,324), and India (783,149); in 2018 it climbed to 8,378,305, driven by Indonesia (1,628,811), India (1,141,450), Iraq (1,098,070), China (979,707), and Iran (927,997); 2019 saw a decline to 7,088,776, with Indonesia (1,445,555), Iran (1,200,590), Iraq (1,056,994), India (871,525), and Pakistan (647,885) most affected; in 2020 exposure peaked at 8,808,104‚Äîprimarily in Nigeria (1,803,833), India (1,473,674), Iraq (1,165,139), Pakistan (1,110,260), and China (853,826); 2021 recorded 8,643,855 people near flares, notably in Nigeria (1,794,524), India (1,676,036), Iraq (1,183,083), Pakistan (936,520), and China (859,549); in 2022 the total fell to 7,235,113, led by Nigeria (1,364,012), Iraq (1,165,038), India (1,125,643), Pakistan (924,009), and China (786,599); and by 2023 it reached its lowest point at 6,026,231, with Indonesia (1,465,760), India (965,446), Iraq (705,791), China (586,830), and Pakistan (576,843) remaining the most exposed.",
            "3.3. Population near Urban Centre Areas by Country: 2016‚Äì2023": "This report analyzes the population living in urban centres within 1 km of gas flaring sites in countries that enter the top 10 by exposed population at least once during the period 2016‚Äì2023. These data provide insights into population exposure to flaring in densely populated urban hubs. The largest absolute exposures to flaring occur in dense urban centres (Figure 9andFigure 10), where very large populations are concentrated. Nigeria is the dominant case globally, beginning with approximately 140,000 exposed individuals in 2016, peaking near 180,000 in 2018, and remaining above 100,000 in 2023. India showed extremely high exposure in 2016, exceeding 200,000, but this dropped sharply to around 20,000 by 2019, partially recovered to  70,000 in 2021, and fell again to fewer than 10,000 by 2023. This pattern suggests either rapid regulatory change or inconsistencies in the detection of the satellite record. Indonesia maintained a consistently high exposure, ranging between 60,000 and 100,000, with  50,000 still affected in 2023. Pakistan also showed a steady increase, peaking at  50,000 in 2020 before modest declines thereafter. Other countries, including Venezuela, Mexico, Iraq, and Bangladesh, recorded moderate exposures in the 20,000‚Äì50,000 range. By contrast, exposures in China, Iran, Egypt, and the Gulf States remained relatively low. These findings highlight that urban centres in Nigeria, Indonesia, and Pakistan are the most persistent global hotspots of human exposure to gas flaring, whereas India‚Äôs dramatic reduction warrants further investigation. Urban centre exposures are of particular concern because they affect the largest absolute populations, compounding risks in environments already burdened by air pollution and high population density. Figure 9.Global maps showing, for each country, the maximum number of people residing in urban centre areas within a 1 km radius of active flaring sites for the years (a) 2023, (b) 2022, (c) 2021, (d) 2020, (e) 2019, (f) 2018, (g) 2017, (h) 2016. (i) The legend. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S23‚ÄìS30). Figure 10.Urban centre population living within 1 km of active flaring sites for countries that enter the top 10 at least once during 2016‚Äì2023: (a) Heatmap visualization showing temporal and geographic patterns. (b) Comparative totals by country and year. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S31 and S32).",
            "3.4. Population near Dense Urban Areas by Country: 2016‚Äì2023": "This report focuses on the population living in dense urban areas within 1 km of gas flaring sites in countries that enter the top 10 by exposed population at least once during the period 2016‚Äì2023. The data reflect population exposure to flaring in highly populated urban zones. Dense urban clusters represent the largest single concentrations of populations living near gas flaring (Figure 11andFigure 12). Nigeria stands out as the most affected country, with consistently high exposure levels averaging around 70,000‚Äì80,000 individuals annually and peaking at more than 100,000 in 2020. Iran and Indonesia form the second tier of hotspots: both countries recorded steady increases to peaks of approximately 80,000 and 77,000, respectively, in 2020 before declining to 47,000‚Äì56,000 by 2023. Iraq also exhibited significant exposure, rising to nearly 47,000 in 2020 before falling to  32,000 by 2023. China and India, while among the world‚Äôs largest flare producers, registered more moderate dense urban exposures of  30,000‚Äì40,000, reflecting different spatial overlaps between flaring sites and dense urban settlements. Mexico remained stable in the 25,000‚Äì30,000 range throughout the period, while other producer states, such as Saudi Arabia, the United Arab Emirates, Algeria, and Venezuela, recorded comparatively low exposures (<20,000). Importantly, 2020 marked the global peak in dense urban exposure, likely reflecting both production dynamics and pandemic-related disruptions. These findings highlight that dense urban flare exposure is concentrated in a handful of producer states, with Nigeria, Iran, Indonesia, and Iraq emerging as critical global hotspots where mitigation would deliver the greatest benefits to human health. Figure 11.Global maps showing, for each country, the maximum number of people residing in dense urban areas within a 1 km radius of active flaring sites for the years (a) 2023, (b) 2022, (c) 2021, (d) 2020, (e) 2019, (f) 2018, (g) 2017, (h) 2016. (i) The legend. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S33‚ÄìS40). Figure 12.Dense urban population living within 1 km of active flaring sites for countries that enter the top 10 at least once during 2016‚Äì2023: (a) Heatmap visualization showing temporal and geographic patterns. (b) Comparative totals by country and year. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S41 and S42).",
            "3.5. Population near Semi-Dense Urban Areas by Country: 2016‚Äì2023": "Semi-dense urban settlements exposed to flaring within 1 km (Figure 13andFigure 14) represent a smaller but highly vulnerable category of affected populations. India consistently accounts for the largest exposures, beginning at approximately 20,000 individuals in 2016, declining to about 13,000 by 2019, then rising again to nearly 18,000 in 2022, before a slight reduction in 2023. China and Indonesia form the second tier, with China reaching 16,000‚Äì17,000 exposed individuals by 2020 but dropping sharply to 6000 in 2022 before recovering to around 10,000 in 2023, while Indonesia maintained a steady upward trajectory to nearly 10,000. Pakistan and Venezuela experienced temporary peaks above 10,000 around 2018‚Äì2020 but later declined, whereas Sudan consistently showed smaller but non-negligible exposures of around 5000‚Äì8000. Other countries, such as Iraq, Kazakhstan, Libya, Malaysia, and Peru, remained below 5000 throughout the study period. These results illustrate that, although absolute population numbers in semi-dense urban clusters are lower than in rural or peri-urban contexts, the relative risk is higher due to population density and existing environmental burdens. Persistent hotspots in India and fluctuating but significant exposures in China underscore the importance of targeted flare-mitigation measures in semi-urban areas where vulnerable populations may be disproportionately affected. Figure 13.Global maps showing, for each country, the maximum number of people residing in semi-dense urban areas within a 1 km radius of active flaring sites for the years (a) 2023, (b) 2022, (c) 2021, (d) 2020, (e) 2019, (f) 2018, (g) 2017, (h) 2016. (i) The legend. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S43‚ÄìS50). Figure 14.Semi-dense urban population living within 1 km of active flaring sites for countries that enter the top 10 at least once during 2016‚Äì2023: (a) Heatmap visualization showing temporal and geographic trends. (b) Comparative totals by country and year. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S51 and S52).",
            "3.6. Population near Suburban Areas by Country: 2016‚Äì2023": "Suburban populations within 1 km of flares (Figure 15andFigure 16) are concentrated primarily in South and Southeast Asia. India and Pakistan emerge as the most affected countries, together accounting for the majority of global exposure in this settlement category. India exhibits a consistent upward trend, rising from approximately 45,000 exposed individuals in 2016 to more than 62,000 in 2023, making it the single largest suburban exposure hotspot. Pakistan reached a peak of nearly 65,000 in 2020, then declined somewhat in subsequent years, but still recorded more than 45,000 exposed individuals in 2023. Indonesia represents the third-highest case, maintaining a steady upward trend to nearly 40,000 in 2023. Other producer states, including Nigeria, Iraq, and Iran, sustained medium levels of suburban exposure ranging from 25,000 to 35,000 over the study period, with little evidence of decline. By contrast, several countries, most notably China, Russia, and Saudi Arabia, recorded much lower suburban exposure (<15,000 by 2023), in some cases reflecting downward trends. Taken together, these results highlight that suburban and peri-urban communities in India and Pakistan are currently the most affected globally by gas flaring activity, while persistent medium exposure in Nigeria, Iraq, and Indonesia underscores a continued lack of effective mitigation in key producing regions. Figure 15.Global maps showing, for each country, the maximum number of people residing in suburban/peri-urban areas within a 1 km radius of active flaring sites for the years (a) 2023, (b) 2022, (c) 2021, (d) 2020, (e) 2019, (f) 2018, (g) 2017, (h) 2016. (i) The legend. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S53‚ÄìS60). Figure 16.Suburban population living within 1 km of active flaring sites for countries that enter the top 10 at least once during 2016‚Äì2023: (a) Heatmap showing temporal and geographic distribution. (b) Comparative totals by country and year. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S61 and S62).",
            "3.7. Population near Rural Areas by Country: 2016‚Äì2023": "Analysis of rural cluster populations (Figure 17andFigure 18) living within 1 km of flares between 2016 and 2023 reveals persistent and uneven exposure across the top flaring countries. Iraq, Nigeria, and Mexico consistently record the largest exposed populations, with Iraq peaking at more than 50,000 people in 2018 before declining to approximately 35,000 by 2023. Nigeria maintained consistently high exposure levels of around 38,000‚Äì40,000 throughout most of the study period, with only a modest decline in the final years. Mexico similarly shows little improvement, with rural populations exposed to flares remaining stable at approximately 38,000‚Äì39,000 across the full period. By contrast, China and India exhibit sharp declines: China peaked at nearly 30,000 exposed individuals in 2017 but fell to under 10,000 by 2023, while India followed a similar downward trajectory after 2020. Medium-exposure countries such as Iran, Indonesia, and Colombia maintained levels between 10,000 and 20,000, with only gradual reductions. Lower-exposure countries, including the Russian Federation, Syria, Oman, and Pakistan, remained relatively stable near 10,000‚Äì15,000. These results highlight that while some countries appear to benefit from stricter regulation or energy transitions, others, particularly Nigeria and Mexico, have seen little progress, leaving large rural populations persistently exposed to flaring-related emissions. Figure 17.Global maps showing, for each country, the maximum number of people residing inrural areaswithin a 1 km radius of active flaring sites for the years (a) 2023, (b) 2022, (c) 2021, (d) 2020, (e) 2019, (f) 2018, (g) 2017, (h) 2016. (i) The legend. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S63‚ÄìS70). Figure 18.Rural population living within 1 km of active flaring sites for countries that enter the top 10 at least once during 2016‚Äì2023: (a) Heatmap visualization showing temporal and geographic patterns. (b) Comparative totals by country and year. For full-resolution images and detailed visual analysis, please refer to theSupplementary Materials (Figures S71 and S72).",
            "3.8. Predictive Modeling of Human Exposure to Gas Flaring in 2030": "This section presents predicted estimates of the population expected to be exposed to gas flaring activities by the year 2030, based on flaring activity recorded in 2023, combined with GHSL population projections for 2030 (Table 2). These projections provide a forward-looking perspective, offering critical insights into how demographic trends and urban expansion will influence the spatial distribution of at-risk populations in the coming years. The analysis is designed to support planning and policy interventions to mitigate the health and environmental impacts of gas flaring, in alignment with global objectives, such as the Zero Routine Flaring by 2030 initiative. Table 2.Projected population exposure to gas flaring in 2030 by proximity and settlement type (SMOD classification). 3.8.1. Overall Population ExposureThe results indicate a substantial projected increase in the number of people residing near flaring sites by 2030. Within a 1 km radius, the total affected population is predicted to reach approximately 2.70 million individuals (2,700,770.64). Expanding the radius to 3 km dramatically increases the projected exposure to 14.85 million individuals (14,853,912.41). This represents a more than fivefold increase when the buffer is extended from 1 km to 3 km, reflecting the far-reaching potential of flaring emissions and the population growth expected in areas surrounding oil and gas production sites. These projections suggest that without significant mitigation measures, millions of people will remain at heightened risk from direct and indirect impacts of flaring. 3.8.2. Projected Distribution by Settlement Type (SMOD Classification)To better understand the dynamics of population growth and exposure, the analysis incorporates Settlement Model (SMOD) Level 2 classifications, which categorize human settlements into five distinct classes: Rural Clusters (SMOD 13), Suburban/Peri-Urban Areas (SMOD 21), Semi-Dense Urban Clusters (SMOD 22), Dense Urban Clusters (SMOD 23), and Urban Centres (SMOD 30). This classification enables the identification of spatial and demographic trends that will shape future exposure scenarios. 3.8.3. 1 km Proximity‚ÄîImmediate Impact ZoneWithin the 1 km buffer, the projected exposure patterns for 2030 reveal a significant concentration of populations in urban and densely developed areas. Dense Urban Clusters (SMOD 23) and Urban Centres (SMOD 30) each account for approximately 637,406 and 631,131 exposed individuals, respectively, representing nearly half of the total population at this distance. Suburban/Peri-Urban Areas (SMOD 21) are projected to have 439,954 exposed individuals, underscoring the role of expanding metropolitan peripheries in shaping future risk. Rural Clusters (SMOD 13) are expected to include 374,329 affected individuals, highlighting persistent risks for populations in less developed regions. Semi-Dense Urban Clusters (SMOD 22) have the lowest predicted exposure, with 104,087 individuals, indicating that transitional settlement zones remain relatively less impacted compared to urban cores and peri-urban fringes. These findings suggest that by 2030, urban populations will bear the greatest direct exposure, exacerbating existing health and environmental stressors in cities. 3.8.4. 3 km Proximity‚ÄîBroader Influence ZoneWhen the analysis is extended to a 3 km radius, the population at risk increases dramatically, particularly within high-density urban areas. Urban Centres (SMOD 30) dominate the projections, with an estimated 6.32 million people exposed, representing 42.6% of the total population within this buffer. Dense Urban Clusters (SMOD 23) are projected to reach 2.82 million exposed individuals, highlighting the continued vulnerability of secondary urban zones. Suburban/Peri-Urban Areas (SMOD 21) show a sharp increase to 2.54 million individuals, reflecting rapid outward urban sprawl and the increasing overlap between residential zones and industrial activities. Rural Clusters (SMOD 13) will encompass approximately 963,257 exposed individuals, while Semi-Dense Urban Clusters (SMOD 22) are projected to reach 456,676, remaining the smallest group overall.",
            "3.8.1. Overall Population Exposure": "The results indicate a substantial projected increase in the number of people residing near flaring sites by 2030. Within a 1 km radius, the total affected population is predicted to reach approximately 2.70 million individuals (2,700,770.64). Expanding the radius to 3 km dramatically increases the projected exposure to 14.85 million individuals (14,853,912.41). This represents a more than fivefold increase when the buffer is extended from 1 km to 3 km, reflecting the far-reaching potential of flaring emissions and the population growth expected in areas surrounding oil and gas production sites. These projections suggest that without significant mitigation measures, millions of people will remain at heightened risk from direct and indirect impacts of flaring.",
            "3.8.2. Projected Distribution by Settlement Type (SMOD Classification)": "To better understand the dynamics of population growth and exposure, the analysis incorporates Settlement Model (SMOD) Level 2 classifications, which categorize human settlements into five distinct classes: Rural Clusters (SMOD 13), Suburban/Peri-Urban Areas (SMOD 21), Semi-Dense Urban Clusters (SMOD 22), Dense Urban Clusters (SMOD 23), and Urban Centres (SMOD 30). This classification enables the identification of spatial and demographic trends that will shape future exposure scenarios.",
            "3.8.3. 1 km Proximity‚ÄîImmediate Impact Zone": "Within the 1 km buffer, the projected exposure patterns for 2030 reveal a significant concentration of populations in urban and densely developed areas. Dense Urban Clusters (SMOD 23) and Urban Centres (SMOD 30) each account for approximately 637,406 and 631,131 exposed individuals, respectively, representing nearly half of the total population at this distance. Suburban/Peri-Urban Areas (SMOD 21) are projected to have 439,954 exposed individuals, underscoring the role of expanding metropolitan peripheries in shaping future risk. Rural Clusters (SMOD 13) are expected to include 374,329 affected individuals, highlighting persistent risks for populations in less developed regions. Semi-Dense Urban Clusters (SMOD 22) have the lowest predicted exposure, with 104,087 individuals, indicating that transitional settlement zones remain relatively less impacted compared to urban cores and peri-urban fringes. These findings suggest that by 2030, urban populations will bear the greatest direct exposure, exacerbating existing health and environmental stressors in cities.",
            "3.8.4. 3 km Proximity‚ÄîBroader Influence Zone": "When the analysis is extended to a 3 km radius, the population at risk increases dramatically, particularly within high-density urban areas. Urban Centres (SMOD 30) dominate the projections, with an estimated 6.32 million people exposed, representing 42.6% of the total population within this buffer. Dense Urban Clusters (SMOD 23) are projected to reach 2.82 million exposed individuals, highlighting the continued vulnerability of secondary urban zones. Suburban/Peri-Urban Areas (SMOD 21) show a sharp increase to 2.54 million individuals, reflecting rapid outward urban sprawl and the increasing overlap between residential zones and industrial activities. Rural Clusters (SMOD 13) will encompass approximately 963,257 exposed individuals, while Semi-Dense Urban Clusters (SMOD 22) are projected to reach 456,676, remaining the smallest group overall.",
            "3.9. Synthesis of Main Exposure Patterns": "The analysis of potential population exposure to upstream gas flaring between 2016 and 2023 reveals a three-fold pattern critical for mitigation efforts. Firstly, exposure is highly concentrated geographically, with a small group of large oil producers, namely India, Indonesia, Iraq, Nigeria, Pakistan, and China, consistently accounting for the majority of people living within 1 km and 3 km of flaring sites. Secondly, the structure of exposure is strongly dependent on settlement type: at the closest proximity (1 km), urban centers host a large fraction of exposed individuals, while a wider radius (3 km) shows the majority of exposed populations residing in a mix of urban, peri-urban, and rural clusters, reflecting how settlement growth has encroached upon the spatial extent of flaring. Finally, temporal trends are heterogeneous: While countries such as China and Iran show gradual declines in exposed populations, others, such as Nigeria, Mexico, and Indonesia, exhibit persistent or only weakly declining exposure, despite international commitments to reduce flaring. These findings collectively underscore that (i) mitigation can be effectively prioritized in a select number of high-exposure countries, and (ii) without immediate, targeted intervention, the ongoing processes of urbanization and demographic growth are projected to increase the total number of people living near flaring sites by 2030.",
            "4. Discussion": "This study presents the first globally harmonized assessment of populations residing near active upstream gas-flaring sites, with consistent spatial and temporal coverage from 2016 to 2023 and projections to 2030. Findings indicate that potential population exposure to flaring is highly concentrated, with six countries, India, Indonesia, Iraq, Nigeria, Pakistan, and China, accounting for over 60% of the global annual exposure. A notable trend is the increasing spatial overlap between active flaring locations and urban or peri-urban areas, suggesting that ongoing urban expansion is intersecting with oil and gas infrastructure, thereby raising concerns about population-level exposure to flaring emissions. These patterns align with the literature, indicating that proximity-based metrics can capture meaningful gradients in potential exposure around upstream oil and gas activity. Proximity bands of 1 km and 3 km were used as exposure proxies, following methodologies used in prior studies reporting health associations within similar ranges [19]. Emissions commonly associated with gas flaring, such as nitrogen oxides (NOx), sulphur dioxide (SO2), volatile organic compounds (VOCs, including benzene), fine particulate matter (PM2.5), and black carbon, are known to adversely affect respiratory, cardiovascular, and developmental health [5,18]. Analysis of global exposure patterns revealed that more than 40% of individuals living within 1 km of flaring sites reside in urban or dense urban settlements. In many of these settings, access to healthcare and environmental monitoring infrastructure remains limited. Prior studies have highlighted that health impacts from upstream oil and gas activity tend to disproportionately affect socioeconomically marginalised groups [7]. The intersection of high population density with flaring activity underscores the need to frame gas flaring as a public health issue in addition to its environmental and climate dimensions. Beyond human health, gas flaring may also pose risks to animal and ecosystem health, in accordance with a One Health perspective. Studies have linked flaring exposure to reproductive and immunological effects in livestock near oil and gas sites [31,32], and biomonitoring using animal health indicators has proven effective in contaminated rural settings [33]. Such cross-species impacts highlight the relevance of integrated monitoring approaches in which human and animal populations share exposure pathways. The high-resolution, globally consistent exposure estimates developed in this study offer an opportunity to support public health surveillance, environmental justice analyses, and air quality planning. These data can be integrated into epidemiological studies, particularly when linked with atmospheric dispersion modelling or health registry data. As recommended by prior reviews, combining settlement-resolved proximity with clinical and registry outcomes can improve exposure‚Äìresponse estimation and clarify vulnerable subgroups [12]. Implications for mitigation follow directly from the emissions profile and technological literature. Enclosed or otherwise improved flare systems, stronger capture and utilisation of associated gas, and operational controls have been shown to reduce pollutant releases and safety risks [2]. By identifying where flaring intersects with large population centres, the present results can help prioritise deployments of these technologies to maximise health co-benefits. Interpretation of these findings should consider methodological constraints inherent to large-scale geospatial analyses. Although pollutant concentrations and health outcomes were not directly measured, the settlement-level population exposure estimates presented here fill a critical data gap and offer a robust foundation for environmental health research, surveillance planning, and risk assessment. The present study complements these findings by identifying where such exposures are most widespread and persistent, thereby offering a practical basis for evaluating mitigation progress and potential health co-benefits. The projections for 2030 reveal several critical insights into the future dynamics of populations residing near flaring sites. By 2030, more than 60% of the exposed population within 3 km will reside in urban centres or dense urban clusters, illustrating how urban expansion and population growth are intensifying flaring-related risks. These trends are consistent with broader global urbanization forecasts, suggesting that regulatory and technological interventions must prioritize urban contexts to protect the largest absolute populations. The sharp increase in peri-urban exposure, from 439,954 individuals at 1 km to 2.54 million at 3 km, underscores the challenge of managing industrial activities on the urban fringe. Without targeted land-use planning and stricter zoning regulations, these areas may become critical hotspots for human-flaring interactions. Although rural populations constitute a smaller proportion of the total population, their projected exposure remains significant, particularly given limited access to healthcare and environmental monitoring infrastructure. Nearly one million rural residents are expected to live within 3 km of active flaring sites by 2030, indicating that mitigation strategies must extend beyond urban areas to ensure equity. These projections highlight the urgent need for strategic interventions as the world approaches 2030. Without substantial progress toward the Zero Routine Flaring by 2030 target, millions of individuals will continue to face elevated health risks, especially in urban and peri-urban zones. Policy measures should focus on urban hotspot mitigation, where absolute exposure levels will be highest, by strengthening monitoring and regulation in rapidly growing peri-urban regions and providing targeted support for rural populations to address their heightened vulnerability and resource constraints.",
            "5. Conclusions": "This study introduces the first globally consistent dataset that quantifies population proximity to upstream gas flaring, broken down by settlement type, spanning 2016‚Äì2023 and incorporating projections for 2030. The resulting annually comparable, proximity-based estimates provide a powerful and scalable technique for identifying hotspots, guiding public health surveillance, and informing mitigation strategies in regions where flaring intersects with dense human settlements. By offering a reproducible and transparent spatial framework, the study supports integration into environmental health research, policy planning, and climate action. This work also lays the groundwork for linking proximity quality modelling and epidemiological analysis, and future efforts may further enhance its impact through integrated approaches such as One Health, particularly in shared human‚Äìanimal environments. The 2030 projections demonstrate a widening spatial and demographic footprint of flaring-related risks, driven by urban growth and continued industrial activity. If current trends persist, more than 14 million people worldwide will live within 3 km of flaring sites, with a substantial proportion residing in urban areas. These findings reinforce the importance of integrating predictive population models with geospatial flaring data to guide evidence-based policies and interventions, ensuring that mitigation efforts are effectively targeted to protect the most vulnerable populations. These findings offer an actionable framework for toxicological risk assessment, enabling policymakers, regulators, and health practitioners to prioritize hotspots for monitoring and intervention."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2305-6304/13/12/1053",
        "scraped_at": "2025-12-05 23:52:24"
    },
    {
        "title": "Prediction, Uncertainty Quantification, and ANN-Assisted Operation of Anaerobic Digestion Guided by Entropy Using Machine Learning",
        "authors": "byZhipeng Zhuang,Xiaoshan Liu,Jing Jin,Ziwen Li,Yanheng Liu,Adriano TavaresandDalin Li",
        "journal": "Entropy2025,27(12), 1233;https://doi.org/10.3390/e27121233- 5 Dec 2025",
        "abstract": "Anaerobic digestion (AD) is a nonlinear and disturbance-sensitive process in which instability is often induced by feedstock variability and biological fluctuations. To address this challenge, this study develops an entropy-guided machine learning framework that integrates parameter prediction, uncertainty quantification, and entropy-based evaluation of AD operation. Using six months of industrial data (~10,000 samples), three models‚Äîsupport vector machine (SVM), random forest (RF), and artificial neural network (ANN)‚Äîwere compared for predicting biogas yield, fermentation temperature, and volatile fatty acid (VFA) concentration. The ANN achieved the highest performance (accuracy = 96%, F1 = 0.95, root mean square error (RMSE) = 1.2 m3/t) and also exhibited the lowest prediction error entropy, indicating reduced uncertainty compared to RF and SVM. Feature entropy and permutation analysis consistently identified feed solids, organic matter, and feed rate as the most influential variables (>85% contribution), in agreement with the RF importance ranking. When applied as a real-time prediction and decision-support tool in the plant (‚Äúsensor ‚Üí prediction ‚Üí programmable logic controller (PLC)/operation ‚Üí feedback‚Äù), the ANN model was associated with a reduction in gas-yield fluctuation from approximately ¬±18% to ¬±5%, a decrease in process entropy, and an improvement in operational stability of about 23%. Techno-economic and life-cycle assessments further indicated a 12‚Äì15 USD/t lower operating cost, 8‚Äì10% energy savings, and 5‚Äì7% CO2reduction compared with baseline operation. Overall, this study demonstrates that combining machine learning with entropy-based uncertainty analysis offers a reliable and interpretable pathway for more stable and low-carbon AD operation.Keywords:anaerobic digestion;machine learning;error entropy;uncertainty quantification;ANN-assisted operation",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Driven by global carbon neutrality goals, anaerobic digestion (AD) has become a core technology for organic solid waste treatment and renewable energy production [1]. AD transforms municipal and food waste into biogas (55‚Äì65% CH4) and nutrient-rich digestate, achieving simultaneous waste reduction and energy recovery in line with circular economy principles [2]. Over the past decade, AD has evolved from laboratory-scale (<1 m3) to industrial-scale reactors (>1000 m3); for example, Schmack Biogas plants (3000‚Äì180,000 t/a) have been widely deployed in Germany and Sweden [3]. Under China‚Äôs dual-carbon strategy, AD supports the ‚Äúwaste‚Äìenergy‚Äìfertilizer‚Äù pathway. Jiang et al. [1] reported that biogas can partially substitute fossil fuels, while digestate enhances soil fertility. However, full-scale AD processes remain highly sensitive to feedstock fluctuations and operational disturbances. Key parameters‚Äîsolids (20‚Äì35%), organic matter (15‚Äì40%), temperature (30‚Äì60 ¬∞C), and feed rate (0.5‚Äì2.0 t/h)‚Äîgovern system stability; improper regulation leads to volatile fatty acid (VFA) accumulation and gas-yield variations exceeding ¬±18%, inhibiting methanogenesis [4,5]. Conventional threshold-based and offline control strategies can achieve acceptable performance in many AD plants, but they often require intensive manual tuning and may struggle to maintain near-optimal operation under strong nonlinearity and frequent feedstock changes. This motivates exploring data-driven approaches that can provide real-time prediction and decision support to complement existing control schemes. Machine learning (ML) provides a data-driven alternative. Models such as support vector machine (SVM), random forest (RF), and artificial neural network (ANN) are capable of learning complex nonlinear relationships in AD processes. Rutland et al. [6] demonstrated their predictive capability, while Zhai et al. [7] reported 96% gas-yield accuracy with <0.3 g/L VFA concentration error. However, most studies remain confined to the laboratory or pilot scale, with dataset sizes typically below 5000 samples (n< 5000, wherendenotes the number of observations), and focus only on prediction rather than real-time regulation. Furthermore, existing approaches seldom address predictive uncertainty or process disorder‚Äîfactors that fundamentally determine whether a prediction model can be trusted for decision-making in industrial environments. To address these limitations, this study investigates a full-scale AD plant treating approximately 30 t/d of organic solid waste, using around 10,000 real operation samples. An entropy-guided machine learning framework is developed that integrates parameter prediction, uncertainty quantification, and operation-oriented assessment. From an entropy-based perspective, prediction error entropy is used to quantify model uncertainty, while process entropy describes system stability under real operating conditions. The main contributions of this study are summarized as follows: Industrial-scale dataset and reproducible evaluation pipeline. A six-month, 9823-sample dataset is constructed from a full-scale AD plant. A unified pipeline‚Äîincluding data cleaning, anomaly removal, normalization, temporal K‚ÄìS splitting, five-fold cross-validation, and rolling window evaluation‚Äîensures data reliability and model generalizability.Entropy-aware machine learning and interpretable validation. The ANN outperforms SVM and RF in predicting biogas yield, temperature, and VFA. Beyond accuracy metrics, error entropy is introduced to characterize predictive uncertainty. Feed solids, organic matter, and feed rate are consistently identified as the dominant variables through feature importance and entropy increase analysis.ANN-assisted operation deployment and process entropy reduction. The optimized ANN is embedded into a real-time feedback loop (‚Äúsensor ‚Üí prediction ‚Üí programmable logic controller (PLC) ‚Üí feedback‚Äù), reducing gas-yield fluctuation from ¬±18% to ¬±5% and improving process stability by approximately 23%. This improvement is accompanied by a measurable reduction in process entropy, demonstrating enhanced system order, energy efficiency, and low-carbon potential.",
            "2. Materials and Methods": "2.1. Data Sources and Preprocessing2.1.1. Data Acquisition and Anaerobic Digestion ProcessThe dataset was collected over six consecutive months from a continuously operating full-scale AD facility treating approximately 30 t/d of organic solid waste. Unlike laboratory or pilot plants, this system adopts an integrated configuration that couples multiple reactor types in series to enhance operational stability, resistance to feed fluctuations, and conversion efficiency (Figure 1). The process train consists of a vertical plug-flow reactor designed for high-solid substrates and long hydraulic retention times, followed by a horizontal plug-flow reactor that accommodates rapid organic loading variations and a vertical aerated stirred tank reactor that enhances mixing homogeneity and maintains microbial activity through intermittent aeration. The three reactors share a common biogas collection header and operate as a single digestion line with a total working volume on the order of 1000 m3, which is typical for industrial AD plants at this throughput. This hybrid layout represents a representative configuration in modern industrial AD, as it combines the structural stability of plug-flow digestion with the flexibility of continuous stirring. After digestion and mechanical solid‚Äìliquid separation, the effluent consistently maintains a moisture content of ‚â§40%, meeting local discharge regulations and enabling its reuse as a soil conditioner or as recycled inoculum to maintain microbial balance.Figure 1.Schematic diagram of the AD process.Throughout the operation period, key physicochemical data were continuously monitored by online sensors, the supervisory control and data acquisition (SCADA) system, and periodic laboratory analyses. Input parameters included feed solids (20‚Äì35%), organic matter content (15‚Äì40%), pH (6.5‚Äì8.0), dissolved oxygen (0.1‚Äì0.5 mg/L), and feed rate (0.5‚Äì2.0 t/h), representing critical drivers of hydrolysis, acidogenesis, and methanogenesis. Reactor temperature, pH, dissolved oxygen, and feed flow rate were measured by industrial online instruments and logged at regular intervals via the SCADA system, whereas feed solids, organic matter, total solids, and VFA concentration were obtained from grab samples analyzed in the onsite laboratory according to standard methods.Specifically, reactor temperature was monitored using Pt100-class thermoresistive probes with typical accuracies better than ¬±0.1 ¬∞C, while pH was measured with industrial gel-filled electrodes (accuracy ¬±0.02 pH units). Dissolved oxygen was monitored using optical luminescence-based DO sensors with an accuracy of approximately ¬±0.1 mg/L, and feed flow rate was recorded using a magnetic flow meter with an accuracy better than ¬±1% of full scale. These specifications are representative of standard online instruments widely deployed in full-scale AD facilities and ensure that the logged signals are sufficiently precise for model training and operational monitoring.In parallel with data acquisition, the industrial automation system relied on a programmable logic controller (PLC) equipped with conventional feedback control loops. Reactor temperature was regulated through a PID controller that modulated a steam-control valve, with typical actuator constraints including a minimum opening of 5%, a maximum opening of 95%, and valve response times on the order of 1‚Äì3 s. Feed flow was controlled by a variable-frequency pump whose operating limits (0.5‚Äì2.0 t/h) matched the measured flow ranges reported inTable 1, while intermittent aeration in the stirred tank was governed by time-based duty cycles implemented in the PLC logic.Table 1.Operating ranges and thresholds of input‚Äìoutput variables in the AD system.The PLC executed its control routines at a base cycle time of approximately 200‚Äì500 ms, ensuring real-time responsiveness to temperature and flow deviations. In contrast, the SCADA system recorded sensor values at its native 5 min logging interval, and the ANN model‚Äîrunning on an external industrial workstation‚Äîrequired less than 1 s per inference. This architecture ensured that ANN computations did not interfere with real-time PLC feedback but instead operated as a higher-level advisory layer. Notably, no ANN-derived signal was directly transmitted to actuators; operator adjustments based on ANN predictions followed the plant‚Äôs standard 30‚Äì90 min operational decision cycle, consistent with industrial practice.These details clarify the interaction between machine learning components, online instrumentation, and the underlying automatic control infrastructure and provide the operational boundaries within which the ANN-based prediction module was integrated.Biogas yield (m3/t) is defined as the daily biogas volume at normal temperature and pressure (NTP) divided by the corresponding daily mass of fresh feed, i.e., a specific yield per ton of feedstock. The output indicators used for modeling‚Äîbiogas yield (m3/t), reactor temperature (¬∞C), and VFA concentration (g/L)‚Äîwere thus used to evaluate system performance and detect metabolic imbalance. These variables were selected not only due to their engineering measurability but also because they directly correspond to microbial activity, mass-transfer characteristics, and thermodynamic constraints of the AD process. To reduce systematic errors and ensure temporal consistency, all online sensors were calibrated weekly and cross-validated against laboratory measurements following standard operating procedures.In addition, outlier values were removed only when they clearly reflected sensor malfunction or physically impossible measurements, such as negative flow readings, dissolved oxygen spikes incompatible with anaerobic conditions, or corrupted SCADA packets flagged during instrument diagnostics. Outliers were identified based on engineering limits and cross-checked against laboratory measurements to avoid filtering out meaningful process dynamics. Importantly, operational fluctuations‚Äîincluding VFA increases during load shocks, feed disturbances, and seasonal temperature variations‚Äîwere fully retained to preserve genuine variability in the dataset.To ensure that the dataset captured real operational variability, samples were collected under three representative conditions: stable feeding and temperature control, load-shock periods caused by abrupt feed changes, and seasonal variations affecting ambient and reactor temperatures. Typical observations under these conditions are presented inTable 2, whileTable 1further summarizes statistical ranges, engineering thresholds, sample sizes (‚âà10,000 valid records), and measurement methods. As summarized inTable 2, feed solids, pH, dissolved oxygen, temperature, and feed rate were monitored online, whereas organic matter, total solids, and VFA concentration were measured in the laboratory. All recorded values remained within industrially accepted boundaries, ensuring the reliability, completeness, and applicability of the dataset for subsequent machine learning modeling.Table 2.Representative system parameter values under different conditions.2.1.2. Data Preprocessing MethodsTo ensure data integrity and suitability for modeling, standard preprocessing procedures were applied [8]. Raw operational records contained minor noise due to sensor drift and operational disturbances. Outliers beyond industrial or statistical limits‚Äîsuch as temperature > 80 ¬∞C, feed solids > 40%, or organic matter > 50% (‚âà2.3% of samples)‚Äîas well as physically impossible values (e.g., negative gas yield) were removed [9,10].All continuous variables were normalized to the range [0,1] using min‚Äìmax scaling to eliminate dimensional inconsistencies:x‚Ä≤=x‚àíxminxmax‚àíxminx‚Ä≤=x‚àíxminxmax‚àíxmin(1)wherexxis the raw value andxminxminandxmaxxmaxdenote the minimum and maximum of each variable.The cleaned dataset was randomly split into training, validation, and test sets (7:2:1). A Kolmogorov‚ÄìSmirnov (K‚ÄìS) test confirmed no significant statistical differences (p> 0.05) among the three subsets [11]. To further assess generalization under time-dependent disturbances, five-fold cross-validation [12] and rolling window prediction [13,14] were implemented.Feature selection was conducted using Pearson correlation analysis, computed only on the training set to prevent information leakage. The correlation coefficient between variableXand targetYis defined as:ùëüùëãùëå=‚àëùëõùëñ=1(ùëãùëñ‚àíùëãÓÉµÓÉ∑ÓÉ∂ÓÉ∂ÓÉ∂ÓÉ∂)(ùëåùëñ‚àíùëåÓÉµÓÉ∑ÓÉ∂ÓÉ∂ÓÉ∂ÓÉ∂)‚àëùëõùëñ=1(ùëãùëñ‚àíùëãÓÉµÓÉ∑ÓÉ∂ÓÉ∂ÓÉ∂ÓÉ∂)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö‚àëùëõùëñ=1(ùëåùëñ‚àíùëåÓÉµÓÉ∑ÓÉ∂ÓÉ∂ÓÉ∂ÓÉ∂)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àörXY=‚àëi=1n(Xi‚àíX¬Ø)(Yi‚àíY¬Ø)‚àëi=1n(Xi‚àíX¬Ø)2‚àëi=1n(Yi‚àíY¬Ø)2(2)As shown inFigure 2, feed solid content exhibited strong correlations with biogas yield (r = 0.90), fermentation temperature (r = 0.85), and VFA concentration (r = 0.60). Organic matter content and feed rate also showed significant correlations, while pH and dissolved oxygen presented weak correlations and were treated as auxiliary stability indicators. Therefore, feed solids, organic matter, and feed rate were retained as core predictive variables.Figure 2.Correlation matrix of AD characteristics.After aggregating real-time data into fixed time intervals and removing incomplete records, a final dataset of approximately 10,000 valid samples was obtained for model development. 2.2. Machine Learning Model Design and Evaluation MetricsTo predict biogas yield, fermentation temperature, and VFA concentration in industrial AD, three mainstream models were adopted‚Äîsupport vector machine (SVM), random forest (RF), and artificial neural network (ANN)‚Äîchosen for their complementary strengths in nonlinear modeling and interpretability [15,16,17,18,19,20,21,22]. All models use the normalized inputs defined in Equation (1) (Section 2.1.2) and a unified train/validation/test protocol (7:2:1 with cross-validation and rolling window evaluation) to ensure comparability and robustness [23]. Hyperparameter ranges and optimal values are summarized inTable 3.Table 3.Hyperparameter settings and optimal values for each machine learning model.2.2.1. Support Vector Machine (SVM)SVM was used for both classification (high/low gas yield) and regression. A radial basis function (RBF) kernel maps inputs to a high-dimensional feature space:ùëò(xùëñ,x)=exp(‚àíùõæ‚Äñxùëñ‚àíx‚Äñ2)k(xi,x)=exp(‚àíŒ≥‚Äñxi‚àíx‚Äñ2)(3)and the classification decision function isùë¶ÃÇ=sign(‚àëùëñ=1ùëõùõºùëñùë¶ùëñùëò(xùëñ,x)+ùëè)y^=sign(‚àëi=1nŒ±iyik(xi,x)+b)(4)For support-vector regression, theœµ‚àíœµ‚àíloss is adopted:ùêøùúÄ(ùë¶,ùëì(x))=max(0,|ùë¶‚àíùëì(x)|‚àíùúÄ)LŒµ(y,f(x))=max0,y‚àíf(x)‚àíŒµ(5)with penalty C and kernel width Œ≥ tuned by grid search (Table 3). Classification labels follow the engineering threshold ‚â•70 m3/t (high) versus <70 m3/t (low), consistent withSection 3.1.SVM is effective for small-sample nonlinear tasks, mapping coupled factors such as feed solid content, organic matter, and feed rate via the radial basis function (RBF) kernel (Equation (2)). For classification, the decision function follows Equation (4), where the input vector comprises normalized S (20‚Äì35%), OM (15‚Äì40%), and F (0.5‚Äì2.0 t/h). Samples with gas yield ‚â•70 m3/t are labeled +1 and <70 m3/t as ‚àí1, where 70 m3/t corresponds to the engineering lower bound of acceptable biogas productivity in the studied industrial plant; yields below this threshold are routinely treated as low-performance conditions requiring inspection or adjustment. Hyperparameters were optimized as C = 10 and Œ≥ = 0.05 (Table 3). For regression, the Œµ-insensitive loss (Equation (7)) was adopted to ensure robustness in continuous predictions.2.2.2. Random Forest (RF)RF aggregates B bootstrap trees to reduce variance and improve generalization [16]. Classification uses majority votingùë¶ÃÇ=mode{‚Ñéùëè(ùê±)}ùêµùëè=1y^=mode{hb(x)}b=1B(6)and regression uses the ensemble meanùë¶ÃÇ=1ùêµ‚àëùëè=1ùêµ‚Ñéùëè(ùê±)y^=1B‚àëb=1Bhb(x)(7)Model error is quantified by mean squared error (MSE)MSE=1ùëõ‚àëùëñ=1ùëõ‚éõ‚éù‚éú‚éúùë¶ùëñ‚àíùë¶ÃÇùëñ‚éû‚é†‚éü‚éü2MSE=1n‚àëi=1n(yi‚àíy^i)2(8)Key hyperparameters‚Äînumber of treesB, maximum depth, and features per split mtry‚Äîwere tuned via grid/cross-validation; the feature-importance ranking reported inSection 3.2is computed from the trained forest.RF employs ensemble averaging to mitigate overfitting and quantify feature importance (Figure 3). Classification uses majority voting (Equation (6)), regression takes the mean of tree outputs (Equation (7)), and model error is measured by mean-squared error (Equation (8)). Optimal parameters‚ÄîB = 100, depth = 18, mtry = 2‚Äîwere obtained through grid/cross-validation (Table 3). The resulting feature-importance ranking (Section 3.2) reveals each variable‚Äôs contribution to biogas performance.Figure 3.Schematic diagram of random forest structure.2.2.3. Artificial Neural Networks (ANNs)The ANN is a two-layer fully connected feed-forward network with hidden sizes [128, 64] and ReLU activations:ReLU(ùëß)=max(0,ùëß),ReLU(z)=max(0,z),(9)and a three-neuron output layer predicting biogas, temperature, and VFA simultaneously [17,18,19]. Training uses Adam optimization with L2 regularization and early stopping under the objective‚Ñí=1ùëÅ‚àëùëñ=1ùëÅ‚Äñùê≤ÃÇùëñ‚àíùê≤ùëñ‚Äñ22+ùúÜ‚ÄñùúÉ‚Äñ22L=1N‚àëi=1N‚Äñy^i‚àíyi‚Äñ22+Œª‚ÄñŒ∏‚Äñ22(10)where Œ∏ denotes network parameters. The chosen architecture balances accuracy with minute-level inference requirements for online control.Evaluation metrics. Classification performance is reported with Precision, Recall, and F1-score [24,25]:Precision=ùëáùëÉùëáùëÉ+ùêπùëÉ,Recall=ùëáùëÉùëáùëÉ+ùêπùëÅ,F1=2‚ãÖPrecision‚ãÖRecallPrecision+RecallPrecision=TPTP+FP,Recall=TPTP+FN,F1=2‚ãÖPrecision‚ãÖRecallPrecision+Recall(11)and AUROC is provided to assess threshold-independent separability. Regression accuracy is assessed by RMSE for each target (biogas m3/t, temperature ¬∞C, VFA g/L) [26]:RMSE=1ùëõ‚àëùëñ=1ùëõ(ùë¶ùëñ‚àíùë¶ÃÇùëñ)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àíÓÑ¥‚é∑ÓÑ≥ÓÑ≥RMSE=1n‚àëi=1n(yi‚àíy^i)2(12)Using a single preprocessing/validation pipeline for all models ensures that differences in reported metrics arise from model capability rather than data handling, enabling fair comparison on the same industrial dataset [23].The ANN model consists of a two-layer fully connected feed-forward network ([128, 64] neurons;Figure 4) using ReLU activation (Equation (9)) and an output layer predicting biogas yield, temperature, and VFA simultaneously.Figure 4.Schematic diagram of the artificial neural network structure.Training adopted the Adam optimizer, L2 regularization, and early stopping with the loss function (Equation (10)). Optimal hyperparameters‚Äîlearning rate = 0.001, batch = 64, and Œª = 0.001‚Äîwere determined via cross/grid search (Table 3). The model maintains high accuracy with an inference time well below the one-minute control cycle.To address the ‚Äúblack-box‚Äù issue, a lightweight architecture + regularization + early stopping strategy was applied, with interpretability discussed inSection 3.1.To justify the selection of the final ANN architecture, multiple alternative configurations were evaluated, including networks with 1‚Äì3 hidden layers, 16‚Äì64 neurons per layer, and different activation functions (ReLU, tanh) within the grid-search range listed inTable 3. These variants were compared using five-fold cross-validation to assess predictive accuracy, generalization performance, and inference time. The selected architecture (two hidden layers with 32 neurons each and ReLU activation) achieved the best balance between accuracy and stability while keeping the inference time below one millisecond, which is necessary for real-time deployment within the PLC/SCADA environment. Deeper or wider networks showed only marginal accuracy improvement but exhibited higher variance across folds and increased risk of overfitting, whereas shallower architectures resulted in reduced predictive performance. Therefore, the chosen ANN represents the optimal trade-off between predictive capability, robustness, and computational efficiency for industrial application. 2.3. Model Evaluation MetricsModel performance was assessed for both classification and regression tasks [24,25,26].Classification metrics include Precision (Equation (11)), Recall (Equation (11)), and F1-score (Equation (11)) to balance prediction reliability under class imbalance; AUROC complements these by evaluating threshold-independent robustness.Regression performance was quantified by Root Mean Square Error (RMSE) (Equation (12)) for biogas yield (m3/t), temperature (¬∞C), and VFA (g/L); lower RMSE indicates stronger predictive capability and generalization.Combining classification and regression assessments ensures comprehensive and reliable evaluation of model performance within industrial AD applications. 2.4. Entropy-Based Uncertainty Quantification MethodAD is a nonlinear and disturbance-sensitive process, and traditional performance metrics such as RMSE or accuracy reflect average prediction errors but cannot measure prediction uncertainty or process disorder [9,10]. To address this gap and align with the entropy-driven scope of entropy, this study introduces information entropy to quantify (i) model prediction uncertainty and (ii) operational stability.2.4.1. Error Entropy for Prediction UncertaintyFor each model, the prediction error is defined ase=y‚àíyÃÇe=y‚àíy^. Its uncertainty is quantified using Shannon error entropy:H(e)=‚àí‚à´pe(Œæ)lnpe(Œæ)dŒæH(e)=‚àí‚à´pe(Œæ)lnpe(Œæ)dŒæ(13)wherepe(Œæ)pe(Œæ)is the probability density of the error. Kernel density estimation (KDE) was used to estimatepepewith Gaussian kernel and Silverman‚Äôs bandwidth rule [27]. Lower entropy corresponds to a more concentrated error distribution, indicating both smaller variance and higher predictive confidence. Error entropy was calculated on the test set for all models (ANN, RF, SVM) and summarized in a comparison table inSection 3(instead of new figures) [28].2.4.2. Entropy Increase for Feature ContributionTo evaluate how each input variable reduces prediction uncertainty, a permutation-based conditional entropy approach was applied [29]. For each featureXj,we randomly permuted its values to break its relationship with the target [30]. The resulting increase in error entropy is:ŒîHj=H(e(j))‚àíH(e)ŒîHj=H(e(j))‚àíH(e)(14)wheree(j)e(j)is the error after permuting featureXj. A higherŒîHjŒîHjindicates that this feature contributes more to uncertainty reduction. This approach is consistent with RF feature importance yet grounded in information theory.2.4.3. Process Entropy for Operational StabilityTo assess macroscopic system disorder, the AD process is divided into several discrete operating states (normal, VFA accumulation, overload, and temperature deviation). In each observation window, the probability of each state isœÄkaœÄka. The process entropy is calculated as:Sproc=‚àí‚àëk=1KœÄklnœÄkSproc=‚àí‚àëk=1KœÄklnœÄk(15)LowerSprocSprocindicates a more ordered and stable process. This metric was used to compare system stability before and after ANN-assisted operation (reported inSection 3.3). No additional figure is introduced; a simple table may be used if necessary.",
            "2.1. Data Sources and Preprocessing": "2.1.1. Data Acquisition and Anaerobic Digestion ProcessThe dataset was collected over six consecutive months from a continuously operating full-scale AD facility treating approximately 30 t/d of organic solid waste. Unlike laboratory or pilot plants, this system adopts an integrated configuration that couples multiple reactor types in series to enhance operational stability, resistance to feed fluctuations, and conversion efficiency (Figure 1). The process train consists of a vertical plug-flow reactor designed for high-solid substrates and long hydraulic retention times, followed by a horizontal plug-flow reactor that accommodates rapid organic loading variations and a vertical aerated stirred tank reactor that enhances mixing homogeneity and maintains microbial activity through intermittent aeration. The three reactors share a common biogas collection header and operate as a single digestion line with a total working volume on the order of 1000 m3, which is typical for industrial AD plants at this throughput. This hybrid layout represents a representative configuration in modern industrial AD, as it combines the structural stability of plug-flow digestion with the flexibility of continuous stirring. After digestion and mechanical solid‚Äìliquid separation, the effluent consistently maintains a moisture content of ‚â§40%, meeting local discharge regulations and enabling its reuse as a soil conditioner or as recycled inoculum to maintain microbial balance.Figure 1.Schematic diagram of the AD process.Throughout the operation period, key physicochemical data were continuously monitored by online sensors, the supervisory control and data acquisition (SCADA) system, and periodic laboratory analyses. Input parameters included feed solids (20‚Äì35%), organic matter content (15‚Äì40%), pH (6.5‚Äì8.0), dissolved oxygen (0.1‚Äì0.5 mg/L), and feed rate (0.5‚Äì2.0 t/h), representing critical drivers of hydrolysis, acidogenesis, and methanogenesis. Reactor temperature, pH, dissolved oxygen, and feed flow rate were measured by industrial online instruments and logged at regular intervals via the SCADA system, whereas feed solids, organic matter, total solids, and VFA concentration were obtained from grab samples analyzed in the onsite laboratory according to standard methods.Specifically, reactor temperature was monitored using Pt100-class thermoresistive probes with typical accuracies better than ¬±0.1 ¬∞C, while pH was measured with industrial gel-filled electrodes (accuracy ¬±0.02 pH units). Dissolved oxygen was monitored using optical luminescence-based DO sensors with an accuracy of approximately ¬±0.1 mg/L, and feed flow rate was recorded using a magnetic flow meter with an accuracy better than ¬±1% of full scale. These specifications are representative of standard online instruments widely deployed in full-scale AD facilities and ensure that the logged signals are sufficiently precise for model training and operational monitoring.In parallel with data acquisition, the industrial automation system relied on a programmable logic controller (PLC) equipped with conventional feedback control loops. Reactor temperature was regulated through a PID controller that modulated a steam-control valve, with typical actuator constraints including a minimum opening of 5%, a maximum opening of 95%, and valve response times on the order of 1‚Äì3 s. Feed flow was controlled by a variable-frequency pump whose operating limits (0.5‚Äì2.0 t/h) matched the measured flow ranges reported inTable 1, while intermittent aeration in the stirred tank was governed by time-based duty cycles implemented in the PLC logic.Table 1.Operating ranges and thresholds of input‚Äìoutput variables in the AD system.The PLC executed its control routines at a base cycle time of approximately 200‚Äì500 ms, ensuring real-time responsiveness to temperature and flow deviations. In contrast, the SCADA system recorded sensor values at its native 5 min logging interval, and the ANN model‚Äîrunning on an external industrial workstation‚Äîrequired less than 1 s per inference. This architecture ensured that ANN computations did not interfere with real-time PLC feedback but instead operated as a higher-level advisory layer. Notably, no ANN-derived signal was directly transmitted to actuators; operator adjustments based on ANN predictions followed the plant‚Äôs standard 30‚Äì90 min operational decision cycle, consistent with industrial practice.These details clarify the interaction between machine learning components, online instrumentation, and the underlying automatic control infrastructure and provide the operational boundaries within which the ANN-based prediction module was integrated.Biogas yield (m3/t) is defined as the daily biogas volume at normal temperature and pressure (NTP) divided by the corresponding daily mass of fresh feed, i.e., a specific yield per ton of feedstock. The output indicators used for modeling‚Äîbiogas yield (m3/t), reactor temperature (¬∞C), and VFA concentration (g/L)‚Äîwere thus used to evaluate system performance and detect metabolic imbalance. These variables were selected not only due to their engineering measurability but also because they directly correspond to microbial activity, mass-transfer characteristics, and thermodynamic constraints of the AD process. To reduce systematic errors and ensure temporal consistency, all online sensors were calibrated weekly and cross-validated against laboratory measurements following standard operating procedures.In addition, outlier values were removed only when they clearly reflected sensor malfunction or physically impossible measurements, such as negative flow readings, dissolved oxygen spikes incompatible with anaerobic conditions, or corrupted SCADA packets flagged during instrument diagnostics. Outliers were identified based on engineering limits and cross-checked against laboratory measurements to avoid filtering out meaningful process dynamics. Importantly, operational fluctuations‚Äîincluding VFA increases during load shocks, feed disturbances, and seasonal temperature variations‚Äîwere fully retained to preserve genuine variability in the dataset.To ensure that the dataset captured real operational variability, samples were collected under three representative conditions: stable feeding and temperature control, load-shock periods caused by abrupt feed changes, and seasonal variations affecting ambient and reactor temperatures. Typical observations under these conditions are presented inTable 2, whileTable 1further summarizes statistical ranges, engineering thresholds, sample sizes (‚âà10,000 valid records), and measurement methods. As summarized inTable 2, feed solids, pH, dissolved oxygen, temperature, and feed rate were monitored online, whereas organic matter, total solids, and VFA concentration were measured in the laboratory. All recorded values remained within industrially accepted boundaries, ensuring the reliability, completeness, and applicability of the dataset for subsequent machine learning modeling.Table 2.Representative system parameter values under different conditions. 2.1.2. Data Preprocessing MethodsTo ensure data integrity and suitability for modeling, standard preprocessing procedures were applied [8]. Raw operational records contained minor noise due to sensor drift and operational disturbances. Outliers beyond industrial or statistical limits‚Äîsuch as temperature > 80 ¬∞C, feed solids > 40%, or organic matter > 50% (‚âà2.3% of samples)‚Äîas well as physically impossible values (e.g., negative gas yield) were removed [9,10].All continuous variables were normalized to the range [0,1] using min‚Äìmax scaling to eliminate dimensional inconsistencies:x‚Ä≤=x‚àíxminxmax‚àíxminx‚Ä≤=x‚àíxminxmax‚àíxmin(1)wherexxis the raw value andxminxminandxmaxxmaxdenote the minimum and maximum of each variable.The cleaned dataset was randomly split into training, validation, and test sets (7:2:1). A Kolmogorov‚ÄìSmirnov (K‚ÄìS) test confirmed no significant statistical differences (p> 0.05) among the three subsets [11]. To further assess generalization under time-dependent disturbances, five-fold cross-validation [12] and rolling window prediction [13,14] were implemented.Feature selection was conducted using Pearson correlation analysis, computed only on the training set to prevent information leakage. The correlation coefficient between variableXand targetYis defined as:ùëüùëãùëå=‚àëùëõùëñ=1(ùëãùëñ‚àíùëãÓÉµÓÉ∑ÓÉ∂ÓÉ∂ÓÉ∂ÓÉ∂)(ùëåùëñ‚àíùëåÓÉµÓÉ∑ÓÉ∂ÓÉ∂ÓÉ∂ÓÉ∂)‚àëùëõùëñ=1(ùëãùëñ‚àíùëãÓÉµÓÉ∑ÓÉ∂ÓÉ∂ÓÉ∂ÓÉ∂)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö‚àëùëõùëñ=1(ùëåùëñ‚àíùëåÓÉµÓÉ∑ÓÉ∂ÓÉ∂ÓÉ∂ÓÉ∂)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àörXY=‚àëi=1n(Xi‚àíX¬Ø)(Yi‚àíY¬Ø)‚àëi=1n(Xi‚àíX¬Ø)2‚àëi=1n(Yi‚àíY¬Ø)2(2)As shown inFigure 2, feed solid content exhibited strong correlations with biogas yield (r = 0.90), fermentation temperature (r = 0.85), and VFA concentration (r = 0.60). Organic matter content and feed rate also showed significant correlations, while pH and dissolved oxygen presented weak correlations and were treated as auxiliary stability indicators. Therefore, feed solids, organic matter, and feed rate were retained as core predictive variables.Figure 2.Correlation matrix of AD characteristics.After aggregating real-time data into fixed time intervals and removing incomplete records, a final dataset of approximately 10,000 valid samples was obtained for model development.",
            "2.1.1. Data Acquisition and Anaerobic Digestion Process": "The dataset was collected over six consecutive months from a continuously operating full-scale AD facility treating approximately 30 t/d of organic solid waste. Unlike laboratory or pilot plants, this system adopts an integrated configuration that couples multiple reactor types in series to enhance operational stability, resistance to feed fluctuations, and conversion efficiency (Figure 1). The process train consists of a vertical plug-flow reactor designed for high-solid substrates and long hydraulic retention times, followed by a horizontal plug-flow reactor that accommodates rapid organic loading variations and a vertical aerated stirred tank reactor that enhances mixing homogeneity and maintains microbial activity through intermittent aeration. The three reactors share a common biogas collection header and operate as a single digestion line with a total working volume on the order of 1000 m3, which is typical for industrial AD plants at this throughput. This hybrid layout represents a representative configuration in modern industrial AD, as it combines the structural stability of plug-flow digestion with the flexibility of continuous stirring. After digestion and mechanical solid‚Äìliquid separation, the effluent consistently maintains a moisture content of ‚â§40%, meeting local discharge regulations and enabling its reuse as a soil conditioner or as recycled inoculum to maintain microbial balance. Figure 1.Schematic diagram of the AD process. Throughout the operation period, key physicochemical data were continuously monitored by online sensors, the supervisory control and data acquisition (SCADA) system, and periodic laboratory analyses. Input parameters included feed solids (20‚Äì35%), organic matter content (15‚Äì40%), pH (6.5‚Äì8.0), dissolved oxygen (0.1‚Äì0.5 mg/L), and feed rate (0.5‚Äì2.0 t/h), representing critical drivers of hydrolysis, acidogenesis, and methanogenesis. Reactor temperature, pH, dissolved oxygen, and feed flow rate were measured by industrial online instruments and logged at regular intervals via the SCADA system, whereas feed solids, organic matter, total solids, and VFA concentration were obtained from grab samples analyzed in the onsite laboratory according to standard methods. Specifically, reactor temperature was monitored using Pt100-class thermoresistive probes with typical accuracies better than ¬±0.1 ¬∞C, while pH was measured with industrial gel-filled electrodes (accuracy ¬±0.02 pH units). Dissolved oxygen was monitored using optical luminescence-based DO sensors with an accuracy of approximately ¬±0.1 mg/L, and feed flow rate was recorded using a magnetic flow meter with an accuracy better than ¬±1% of full scale. These specifications are representative of standard online instruments widely deployed in full-scale AD facilities and ensure that the logged signals are sufficiently precise for model training and operational monitoring. In parallel with data acquisition, the industrial automation system relied on a programmable logic controller (PLC) equipped with conventional feedback control loops. Reactor temperature was regulated through a PID controller that modulated a steam-control valve, with typical actuator constraints including a minimum opening of 5%, a maximum opening of 95%, and valve response times on the order of 1‚Äì3 s. Feed flow was controlled by a variable-frequency pump whose operating limits (0.5‚Äì2.0 t/h) matched the measured flow ranges reported inTable 1, while intermittent aeration in the stirred tank was governed by time-based duty cycles implemented in the PLC logic. Table 1.Operating ranges and thresholds of input‚Äìoutput variables in the AD system. The PLC executed its control routines at a base cycle time of approximately 200‚Äì500 ms, ensuring real-time responsiveness to temperature and flow deviations. In contrast, the SCADA system recorded sensor values at its native 5 min logging interval, and the ANN model‚Äîrunning on an external industrial workstation‚Äîrequired less than 1 s per inference. This architecture ensured that ANN computations did not interfere with real-time PLC feedback but instead operated as a higher-level advisory layer. Notably, no ANN-derived signal was directly transmitted to actuators; operator adjustments based on ANN predictions followed the plant‚Äôs standard 30‚Äì90 min operational decision cycle, consistent with industrial practice. These details clarify the interaction between machine learning components, online instrumentation, and the underlying automatic control infrastructure and provide the operational boundaries within which the ANN-based prediction module was integrated. Biogas yield (m3/t) is defined as the daily biogas volume at normal temperature and pressure (NTP) divided by the corresponding daily mass of fresh feed, i.e., a specific yield per ton of feedstock. The output indicators used for modeling‚Äîbiogas yield (m3/t), reactor temperature (¬∞C), and VFA concentration (g/L)‚Äîwere thus used to evaluate system performance and detect metabolic imbalance. These variables were selected not only due to their engineering measurability but also because they directly correspond to microbial activity, mass-transfer characteristics, and thermodynamic constraints of the AD process. To reduce systematic errors and ensure temporal consistency, all online sensors were calibrated weekly and cross-validated against laboratory measurements following standard operating procedures. In addition, outlier values were removed only when they clearly reflected sensor malfunction or physically impossible measurements, such as negative flow readings, dissolved oxygen spikes incompatible with anaerobic conditions, or corrupted SCADA packets flagged during instrument diagnostics. Outliers were identified based on engineering limits and cross-checked against laboratory measurements to avoid filtering out meaningful process dynamics. Importantly, operational fluctuations‚Äîincluding VFA increases during load shocks, feed disturbances, and seasonal temperature variations‚Äîwere fully retained to preserve genuine variability in the dataset. To ensure that the dataset captured real operational variability, samples were collected under three representative conditions: stable feeding and temperature control, load-shock periods caused by abrupt feed changes, and seasonal variations affecting ambient and reactor temperatures. Typical observations under these conditions are presented inTable 2, whileTable 1further summarizes statistical ranges, engineering thresholds, sample sizes (‚âà10,000 valid records), and measurement methods. As summarized inTable 2, feed solids, pH, dissolved oxygen, temperature, and feed rate were monitored online, whereas organic matter, total solids, and VFA concentration were measured in the laboratory. All recorded values remained within industrially accepted boundaries, ensuring the reliability, completeness, and applicability of the dataset for subsequent machine learning modeling. Table 2.Representative system parameter values under different conditions.",
            "2.1.2. Data Preprocessing Methods": "To ensure data integrity and suitability for modeling, standard preprocessing procedures were applied [8]. Raw operational records contained minor noise due to sensor drift and operational disturbances. Outliers beyond industrial or statistical limits‚Äîsuch as temperature > 80 ¬∞C, feed solids > 40%, or organic matter > 50% (‚âà2.3% of samples)‚Äîas well as physically impossible values (e.g., negative gas yield) were removed [9,10]. All continuous variables were normalized to the range [0,1] using min‚Äìmax scaling to eliminate dimensional inconsistencies:x‚Ä≤=x‚àíxminxmax‚àíxminx‚Ä≤=x‚àíxminxmax‚àíxmin(1)wherexxis the raw value andxminxminandxmaxxmaxdenote the minimum and maximum of each variable. The cleaned dataset was randomly split into training, validation, and test sets (7:2:1). A Kolmogorov‚ÄìSmirnov (K‚ÄìS) test confirmed no significant statistical differences (p> 0.05) among the three subsets [11]. To further assess generalization under time-dependent disturbances, five-fold cross-validation [12] and rolling window prediction [13,14] were implemented. Feature selection was conducted using Pearson correlation analysis, computed only on the training set to prevent information leakage. The correlation coefficient between variableXand targetYis defined as:ùëüùëãùëå=‚àëùëõùëñ=1(ùëãùëñ‚àíùëãÓÉµÓÉ∑ÓÉ∂ÓÉ∂ÓÉ∂ÓÉ∂)(ùëåùëñ‚àíùëåÓÉµÓÉ∑ÓÉ∂ÓÉ∂ÓÉ∂ÓÉ∂)‚àëùëõùëñ=1(ùëãùëñ‚àíùëãÓÉµÓÉ∑ÓÉ∂ÓÉ∂ÓÉ∂ÓÉ∂)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö‚àëùëõùëñ=1(ùëåùëñ‚àíùëåÓÉµÓÉ∑ÓÉ∂ÓÉ∂ÓÉ∂ÓÉ∂)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àörXY=‚àëi=1n(Xi‚àíX¬Ø)(Yi‚àíY¬Ø)‚àëi=1n(Xi‚àíX¬Ø)2‚àëi=1n(Yi‚àíY¬Ø)2(2) As shown inFigure 2, feed solid content exhibited strong correlations with biogas yield (r = 0.90), fermentation temperature (r = 0.85), and VFA concentration (r = 0.60). Organic matter content and feed rate also showed significant correlations, while pH and dissolved oxygen presented weak correlations and were treated as auxiliary stability indicators. Therefore, feed solids, organic matter, and feed rate were retained as core predictive variables. Figure 2.Correlation matrix of AD characteristics. After aggregating real-time data into fixed time intervals and removing incomplete records, a final dataset of approximately 10,000 valid samples was obtained for model development.",
            "2.2. Machine Learning Model Design and Evaluation Metrics": "To predict biogas yield, fermentation temperature, and VFA concentration in industrial AD, three mainstream models were adopted‚Äîsupport vector machine (SVM), random forest (RF), and artificial neural network (ANN)‚Äîchosen for their complementary strengths in nonlinear modeling and interpretability [15,16,17,18,19,20,21,22]. All models use the normalized inputs defined in Equation (1) (Section 2.1.2) and a unified train/validation/test protocol (7:2:1 with cross-validation and rolling window evaluation) to ensure comparability and robustness [23]. Hyperparameter ranges and optimal values are summarized inTable 3. Table 3.Hyperparameter settings and optimal values for each machine learning model. 2.2.1. Support Vector Machine (SVM)SVM was used for both classification (high/low gas yield) and regression. A radial basis function (RBF) kernel maps inputs to a high-dimensional feature space:ùëò(xùëñ,x)=exp(‚àíùõæ‚Äñxùëñ‚àíx‚Äñ2)k(xi,x)=exp(‚àíŒ≥‚Äñxi‚àíx‚Äñ2)(3)and the classification decision function isùë¶ÃÇ=sign(‚àëùëñ=1ùëõùõºùëñùë¶ùëñùëò(xùëñ,x)+ùëè)y^=sign(‚àëi=1nŒ±iyik(xi,x)+b)(4)For support-vector regression, theœµ‚àíœµ‚àíloss is adopted:ùêøùúÄ(ùë¶,ùëì(x))=max(0,|ùë¶‚àíùëì(x)|‚àíùúÄ)LŒµ(y,f(x))=max0,y‚àíf(x)‚àíŒµ(5)with penalty C and kernel width Œ≥ tuned by grid search (Table 3). Classification labels follow the engineering threshold ‚â•70 m3/t (high) versus <70 m3/t (low), consistent withSection 3.1.SVM is effective for small-sample nonlinear tasks, mapping coupled factors such as feed solid content, organic matter, and feed rate via the radial basis function (RBF) kernel (Equation (2)). For classification, the decision function follows Equation (4), where the input vector comprises normalized S (20‚Äì35%), OM (15‚Äì40%), and F (0.5‚Äì2.0 t/h). Samples with gas yield ‚â•70 m3/t are labeled +1 and <70 m3/t as ‚àí1, where 70 m3/t corresponds to the engineering lower bound of acceptable biogas productivity in the studied industrial plant; yields below this threshold are routinely treated as low-performance conditions requiring inspection or adjustment. Hyperparameters were optimized as C = 10 and Œ≥ = 0.05 (Table 3). For regression, the Œµ-insensitive loss (Equation (7)) was adopted to ensure robustness in continuous predictions. 2.2.2. Random Forest (RF)RF aggregates B bootstrap trees to reduce variance and improve generalization [16]. Classification uses majority votingùë¶ÃÇ=mode{‚Ñéùëè(ùê±)}ùêµùëè=1y^=mode{hb(x)}b=1B(6)and regression uses the ensemble meanùë¶ÃÇ=1ùêµ‚àëùëè=1ùêµ‚Ñéùëè(ùê±)y^=1B‚àëb=1Bhb(x)(7)Model error is quantified by mean squared error (MSE)MSE=1ùëõ‚àëùëñ=1ùëõ‚éõ‚éù‚éú‚éúùë¶ùëñ‚àíùë¶ÃÇùëñ‚éû‚é†‚éü‚éü2MSE=1n‚àëi=1n(yi‚àíy^i)2(8)Key hyperparameters‚Äînumber of treesB, maximum depth, and features per split mtry‚Äîwere tuned via grid/cross-validation; the feature-importance ranking reported inSection 3.2is computed from the trained forest.RF employs ensemble averaging to mitigate overfitting and quantify feature importance (Figure 3). Classification uses majority voting (Equation (6)), regression takes the mean of tree outputs (Equation (7)), and model error is measured by mean-squared error (Equation (8)). Optimal parameters‚ÄîB = 100, depth = 18, mtry = 2‚Äîwere obtained through grid/cross-validation (Table 3). The resulting feature-importance ranking (Section 3.2) reveals each variable‚Äôs contribution to biogas performance.Figure 3.Schematic diagram of random forest structure. 2.2.3. Artificial Neural Networks (ANNs)The ANN is a two-layer fully connected feed-forward network with hidden sizes [128, 64] and ReLU activations:ReLU(ùëß)=max(0,ùëß),ReLU(z)=max(0,z),(9)and a three-neuron output layer predicting biogas, temperature, and VFA simultaneously [17,18,19]. Training uses Adam optimization with L2 regularization and early stopping under the objective‚Ñí=1ùëÅ‚àëùëñ=1ùëÅ‚Äñùê≤ÃÇùëñ‚àíùê≤ùëñ‚Äñ22+ùúÜ‚ÄñùúÉ‚Äñ22L=1N‚àëi=1N‚Äñy^i‚àíyi‚Äñ22+Œª‚ÄñŒ∏‚Äñ22(10)where Œ∏ denotes network parameters. The chosen architecture balances accuracy with minute-level inference requirements for online control.Evaluation metrics. Classification performance is reported with Precision, Recall, and F1-score [24,25]:Precision=ùëáùëÉùëáùëÉ+ùêπùëÉ,Recall=ùëáùëÉùëáùëÉ+ùêπùëÅ,F1=2‚ãÖPrecision‚ãÖRecallPrecision+RecallPrecision=TPTP+FP,Recall=TPTP+FN,F1=2‚ãÖPrecision‚ãÖRecallPrecision+Recall(11)and AUROC is provided to assess threshold-independent separability. Regression accuracy is assessed by RMSE for each target (biogas m3/t, temperature ¬∞C, VFA g/L) [26]:RMSE=1ùëõ‚àëùëñ=1ùëõ(ùë¶ùëñ‚àíùë¶ÃÇùëñ)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àíÓÑ¥‚é∑ÓÑ≥ÓÑ≥RMSE=1n‚àëi=1n(yi‚àíy^i)2(12)Using a single preprocessing/validation pipeline for all models ensures that differences in reported metrics arise from model capability rather than data handling, enabling fair comparison on the same industrial dataset [23].The ANN model consists of a two-layer fully connected feed-forward network ([128, 64] neurons;Figure 4) using ReLU activation (Equation (9)) and an output layer predicting biogas yield, temperature, and VFA simultaneously.Figure 4.Schematic diagram of the artificial neural network structure.Training adopted the Adam optimizer, L2 regularization, and early stopping with the loss function (Equation (10)). Optimal hyperparameters‚Äîlearning rate = 0.001, batch = 64, and Œª = 0.001‚Äîwere determined via cross/grid search (Table 3). The model maintains high accuracy with an inference time well below the one-minute control cycle.To address the ‚Äúblack-box‚Äù issue, a lightweight architecture + regularization + early stopping strategy was applied, with interpretability discussed inSection 3.1.To justify the selection of the final ANN architecture, multiple alternative configurations were evaluated, including networks with 1‚Äì3 hidden layers, 16‚Äì64 neurons per layer, and different activation functions (ReLU, tanh) within the grid-search range listed inTable 3. These variants were compared using five-fold cross-validation to assess predictive accuracy, generalization performance, and inference time. The selected architecture (two hidden layers with 32 neurons each and ReLU activation) achieved the best balance between accuracy and stability while keeping the inference time below one millisecond, which is necessary for real-time deployment within the PLC/SCADA environment. Deeper or wider networks showed only marginal accuracy improvement but exhibited higher variance across folds and increased risk of overfitting, whereas shallower architectures resulted in reduced predictive performance. Therefore, the chosen ANN represents the optimal trade-off between predictive capability, robustness, and computational efficiency for industrial application.",
            "2.2.1. Support Vector Machine (SVM)": "SVM was used for both classification (high/low gas yield) and regression. A radial basis function (RBF) kernel maps inputs to a high-dimensional feature space:ùëò(xùëñ,x)=exp(‚àíùõæ‚Äñxùëñ‚àíx‚Äñ2)k(xi,x)=exp(‚àíŒ≥‚Äñxi‚àíx‚Äñ2)(3)and the classification decision function isùë¶ÃÇ=sign(‚àëùëñ=1ùëõùõºùëñùë¶ùëñùëò(xùëñ,x)+ùëè)y^=sign(‚àëi=1nŒ±iyik(xi,x)+b)(4) For support-vector regression, theœµ‚àíœµ‚àíloss is adopted:ùêøùúÄ(ùë¶,ùëì(x))=max(0,|ùë¶‚àíùëì(x)|‚àíùúÄ)LŒµ(y,f(x))=max0,y‚àíf(x)‚àíŒµ(5)with penalty C and kernel width Œ≥ tuned by grid search (Table 3). Classification labels follow the engineering threshold ‚â•70 m3/t (high) versus <70 m3/t (low), consistent withSection 3.1. SVM is effective for small-sample nonlinear tasks, mapping coupled factors such as feed solid content, organic matter, and feed rate via the radial basis function (RBF) kernel (Equation (2)). For classification, the decision function follows Equation (4), where the input vector comprises normalized S (20‚Äì35%), OM (15‚Äì40%), and F (0.5‚Äì2.0 t/h). Samples with gas yield ‚â•70 m3/t are labeled +1 and <70 m3/t as ‚àí1, where 70 m3/t corresponds to the engineering lower bound of acceptable biogas productivity in the studied industrial plant; yields below this threshold are routinely treated as low-performance conditions requiring inspection or adjustment. Hyperparameters were optimized as C = 10 and Œ≥ = 0.05 (Table 3). For regression, the Œµ-insensitive loss (Equation (7)) was adopted to ensure robustness in continuous predictions.",
            "2.2.2. Random Forest (RF)": "RF aggregates B bootstrap trees to reduce variance and improve generalization [16]. Classification uses majority votingùë¶ÃÇ=mode{‚Ñéùëè(ùê±)}ùêµùëè=1y^=mode{hb(x)}b=1B(6)and regression uses the ensemble meanùë¶ÃÇ=1ùêµ‚àëùëè=1ùêµ‚Ñéùëè(ùê±)y^=1B‚àëb=1Bhb(x)(7) Model error is quantified by mean squared error (MSE)MSE=1ùëõ‚àëùëñ=1ùëõ‚éõ‚éù‚éú‚éúùë¶ùëñ‚àíùë¶ÃÇùëñ‚éû‚é†‚éü‚éü2MSE=1n‚àëi=1n(yi‚àíy^i)2(8) Key hyperparameters‚Äînumber of treesB, maximum depth, and features per split mtry‚Äîwere tuned via grid/cross-validation; the feature-importance ranking reported inSection 3.2is computed from the trained forest. RF employs ensemble averaging to mitigate overfitting and quantify feature importance (Figure 3). Classification uses majority voting (Equation (6)), regression takes the mean of tree outputs (Equation (7)), and model error is measured by mean-squared error (Equation (8)). Optimal parameters‚ÄîB = 100, depth = 18, mtry = 2‚Äîwere obtained through grid/cross-validation (Table 3). The resulting feature-importance ranking (Section 3.2) reveals each variable‚Äôs contribution to biogas performance. Figure 3.Schematic diagram of random forest structure.",
            "2.2.3. Artificial Neural Networks (ANNs)": "The ANN is a two-layer fully connected feed-forward network with hidden sizes [128, 64] and ReLU activations:ReLU(ùëß)=max(0,ùëß),ReLU(z)=max(0,z),(9)and a three-neuron output layer predicting biogas, temperature, and VFA simultaneously [17,18,19]. Training uses Adam optimization with L2 regularization and early stopping under the objective‚Ñí=1ùëÅ‚àëùëñ=1ùëÅ‚Äñùê≤ÃÇùëñ‚àíùê≤ùëñ‚Äñ22+ùúÜ‚ÄñùúÉ‚Äñ22L=1N‚àëi=1N‚Äñy^i‚àíyi‚Äñ22+Œª‚ÄñŒ∏‚Äñ22(10)where Œ∏ denotes network parameters. The chosen architecture balances accuracy with minute-level inference requirements for online control. Evaluation metrics. Classification performance is reported with Precision, Recall, and F1-score [24,25]:Precision=ùëáùëÉùëáùëÉ+ùêπùëÉ,Recall=ùëáùëÉùëáùëÉ+ùêπùëÅ,F1=2‚ãÖPrecision‚ãÖRecallPrecision+RecallPrecision=TPTP+FP,Recall=TPTP+FN,F1=2‚ãÖPrecision‚ãÖRecallPrecision+Recall(11)and AUROC is provided to assess threshold-independent separability. Regression accuracy is assessed by RMSE for each target (biogas m3/t, temperature ¬∞C, VFA g/L) [26]:RMSE=1ùëõ‚àëùëñ=1ùëõ(ùë¶ùëñ‚àíùë¶ÃÇùëñ)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àíÓÑ¥‚é∑ÓÑ≥ÓÑ≥RMSE=1n‚àëi=1n(yi‚àíy^i)2(12) Using a single preprocessing/validation pipeline for all models ensures that differences in reported metrics arise from model capability rather than data handling, enabling fair comparison on the same industrial dataset [23]. The ANN model consists of a two-layer fully connected feed-forward network ([128, 64] neurons;Figure 4) using ReLU activation (Equation (9)) and an output layer predicting biogas yield, temperature, and VFA simultaneously. Figure 4.Schematic diagram of the artificial neural network structure. Training adopted the Adam optimizer, L2 regularization, and early stopping with the loss function (Equation (10)). Optimal hyperparameters‚Äîlearning rate = 0.001, batch = 64, and Œª = 0.001‚Äîwere determined via cross/grid search (Table 3). The model maintains high accuracy with an inference time well below the one-minute control cycle. To address the ‚Äúblack-box‚Äù issue, a lightweight architecture + regularization + early stopping strategy was applied, with interpretability discussed inSection 3.1. To justify the selection of the final ANN architecture, multiple alternative configurations were evaluated, including networks with 1‚Äì3 hidden layers, 16‚Äì64 neurons per layer, and different activation functions (ReLU, tanh) within the grid-search range listed inTable 3. These variants were compared using five-fold cross-validation to assess predictive accuracy, generalization performance, and inference time. The selected architecture (two hidden layers with 32 neurons each and ReLU activation) achieved the best balance between accuracy and stability while keeping the inference time below one millisecond, which is necessary for real-time deployment within the PLC/SCADA environment. Deeper or wider networks showed only marginal accuracy improvement but exhibited higher variance across folds and increased risk of overfitting, whereas shallower architectures resulted in reduced predictive performance. Therefore, the chosen ANN represents the optimal trade-off between predictive capability, robustness, and computational efficiency for industrial application.",
            "2.3. Model Evaluation Metrics": "Model performance was assessed for both classification and regression tasks [24,25,26]. Classification metrics include Precision (Equation (11)), Recall (Equation (11)), and F1-score (Equation (11)) to balance prediction reliability under class imbalance; AUROC complements these by evaluating threshold-independent robustness.Regression performance was quantified by Root Mean Square Error (RMSE) (Equation (12)) for biogas yield (m3/t), temperature (¬∞C), and VFA (g/L); lower RMSE indicates stronger predictive capability and generalization. Combining classification and regression assessments ensures comprehensive and reliable evaluation of model performance within industrial AD applications.",
            "2.4. Entropy-Based Uncertainty Quantification Method": "AD is a nonlinear and disturbance-sensitive process, and traditional performance metrics such as RMSE or accuracy reflect average prediction errors but cannot measure prediction uncertainty or process disorder [9,10]. To address this gap and align with the entropy-driven scope of entropy, this study introduces information entropy to quantify (i) model prediction uncertainty and (ii) operational stability. 2.4.1. Error Entropy for Prediction UncertaintyFor each model, the prediction error is defined ase=y‚àíyÃÇe=y‚àíy^. Its uncertainty is quantified using Shannon error entropy:H(e)=‚àí‚à´pe(Œæ)lnpe(Œæ)dŒæH(e)=‚àí‚à´pe(Œæ)lnpe(Œæ)dŒæ(13)wherepe(Œæ)pe(Œæ)is the probability density of the error. Kernel density estimation (KDE) was used to estimatepepewith Gaussian kernel and Silverman‚Äôs bandwidth rule [27]. Lower entropy corresponds to a more concentrated error distribution, indicating both smaller variance and higher predictive confidence. Error entropy was calculated on the test set for all models (ANN, RF, SVM) and summarized in a comparison table inSection 3(instead of new figures) [28]. 2.4.2. Entropy Increase for Feature ContributionTo evaluate how each input variable reduces prediction uncertainty, a permutation-based conditional entropy approach was applied [29]. For each featureXj,we randomly permuted its values to break its relationship with the target [30]. The resulting increase in error entropy is:ŒîHj=H(e(j))‚àíH(e)ŒîHj=H(e(j))‚àíH(e)(14)wheree(j)e(j)is the error after permuting featureXj. A higherŒîHjŒîHjindicates that this feature contributes more to uncertainty reduction. This approach is consistent with RF feature importance yet grounded in information theory. 2.4.3. Process Entropy for Operational StabilityTo assess macroscopic system disorder, the AD process is divided into several discrete operating states (normal, VFA accumulation, overload, and temperature deviation). In each observation window, the probability of each state isœÄkaœÄka. The process entropy is calculated as:Sproc=‚àí‚àëk=1KœÄklnœÄkSproc=‚àí‚àëk=1KœÄklnœÄk(15)LowerSprocSprocindicates a more ordered and stable process. This metric was used to compare system stability before and after ANN-assisted operation (reported inSection 3.3). No additional figure is introduced; a simple table may be used if necessary.",
            "2.4.1. Error Entropy for Prediction Uncertainty": "For each model, the prediction error is defined ase=y‚àíyÃÇe=y‚àíy^. Its uncertainty is quantified using Shannon error entropy:H(e)=‚àí‚à´pe(Œæ)lnpe(Œæ)dŒæH(e)=‚àí‚à´pe(Œæ)lnpe(Œæ)dŒæ(13)wherepe(Œæ)pe(Œæ)is the probability density of the error. Kernel density estimation (KDE) was used to estimatepepewith Gaussian kernel and Silverman‚Äôs bandwidth rule [27]. Lower entropy corresponds to a more concentrated error distribution, indicating both smaller variance and higher predictive confidence. Error entropy was calculated on the test set for all models (ANN, RF, SVM) and summarized in a comparison table inSection 3(instead of new figures) [28].",
            "2.4.2. Entropy Increase for Feature Contribution": "To evaluate how each input variable reduces prediction uncertainty, a permutation-based conditional entropy approach was applied [29]. For each featureXj,we randomly permuted its values to break its relationship with the target [30]. The resulting increase in error entropy is:ŒîHj=H(e(j))‚àíH(e)ŒîHj=H(e(j))‚àíH(e)(14)wheree(j)e(j)is the error after permuting featureXj. A higherŒîHjŒîHjindicates that this feature contributes more to uncertainty reduction. This approach is consistent with RF feature importance yet grounded in information theory.",
            "2.4.3. Process Entropy for Operational Stability": "To assess macroscopic system disorder, the AD process is divided into several discrete operating states (normal, VFA accumulation, overload, and temperature deviation). In each observation window, the probability of each state isœÄkaœÄka. The process entropy is calculated as:Sproc=‚àí‚àëk=1KœÄklnœÄkSproc=‚àí‚àëk=1KœÄklnœÄk(15) LowerSprocSprocindicates a more ordered and stable process. This metric was used to compare system stability before and after ANN-assisted operation (reported inSection 3.3). No additional figure is introduced; a simple table may be used if necessary.",
            "3. Results and Analysis": "3.1. Model Performance ComparisonAs shown inTable 4, the three models exhibit clear performance differences in distinguishing high- and low-yield samples. The ANN achieves the best overall results, with Accuracy, Recall, and F1 around 0.95 and AUROC at 0.98, indicating stable classification across thresholds. RF follows (0.90‚Äì0.94), while SVM performs slightly lower (0.88‚Äì0.91). Accuracy denotes total correct rate, Recall measures the detection of high-yield cases, F1 balances both, and AUROC represents threshold-independent robustness. The ANN‚Äôs multilayer structure effectively captures nonlinear relationships among feed solids, organic matter, and feed rate, consistent with prior ANN research [30,31].Table 4.Performance comparison of three machine learning models.In regression tasks, the ANN likewise showed superior precision: RMSE values of 1.2 m3/t (biogas), 0.5 ¬∞C (temperature), and 0.3 g/L (VFA) were all below those of RF (1.8, 0.9, 0.6) and SVM (2.1, 1.2, 0.8). Its average R2= 0.94 exceeded RF (0.88) and SVM (0.82) [32], confirming the ANN‚Äôs high accuracy and robustness for industrial applications. 3.2. Feature Importance and Entropy-Based Uncertainty AnalysisFigure 5presents RF-based feature importance: feed solids (42%), organic matter (30%), and feed rate (18%) together account for ‚âà90% of the total importance, identifying them as the dominant operational factors in the present plant. pH (4%), dissolved oxygen (5%), and total solids (3%) contribute less and mainly serve as stability indicators within the observed operating window. These outcomes align with mechanism analyses [22,33]: excessive solids cause scum formation and mass-transfer limitations; low solids induce hydraulic overload; organic content affects methane yield and VFA accumulation risks; and feed rate governs hydraulic retention time (HRT). Accordingly, under the normal operating conditions captured in this dataset, the three core variables constitute the primary levers for AD optimization.Figure 5.Weighted percentage of input characteristics for AD performance.Model explainability is strengthened in two complementary ways. First, the RF ranking is consistent with tendencies learned by the ANN, indicating agreement across model classes regarding the relative influence of inputs. Second, a lightweight ANN combined with cross-validation constrains complexity and mitigates overfitting, improving the balance between predictive accuracy and interpretability. This ‚Äústructure control + cross-validation‚Äù strategy alleviates black-box concerns while preserving performance (seeFigure 5).From an information-theoretic perspective, the RF results can be interpreted via Shannon entropy and information gain as defined inSection 2.4: features that most reduce the output uncertainty are precisely those with the highest RF importance, again highlighting feed solids, organic matter, and feed rate as primary drivers. For the regression tasks (biogas yield, temperature, and VFA concentration), permutation tests further corroborate this finding: shuffling any of the three core variables yields a clear increase in prediction error (ŒîRMSE), whereas permuting pH or dissolved oxygen produces only marginal changes, consistent with their role as secondary stability indicators under the studied conditions. It should be emphasized that the dataset does not contain severe acidification events or strong oxygen ingress; therefore, the low importance of pH and dissolved oxygen reflects their limited variation around well-controlled set-points in this plant, rather than a lack of relevance under failure scenarios.To quantify predictive uncertainty, we evaluate prediction error entropy on the held-out test sets (definitions and estimators inSection 2.4). Across all targets, the ANN exhibits the lowest error entropy H(e), followed by RF and then SVM. Thus, the ANN not only attains lower RMSE but also concentrates residuals more tightly, indicating higher predictive certainty beyond accuracy alone. At the variable level, the entropy-increase index ŒîHj (feature permutation) shows the largest rises for feed solids, organic matter, and feed rate, while pH and dissolved oxygen induce minimal changes‚Äîmirroring both the RF ranking and the ŒîRMSE pattern.Finally, local interpretability from ANN average input-gradient norms (seeSection 2.4) remains consistent with the global picture: sensitivities with respect to feed solids, organic matter, and feed rate are systematically higher than those for pH and dissolved oxygen.Overall, the convergence across RF importance, information-gain interpretation, permutation-based ŒîRMSE, error entropy H(e), and entropy-increase ŒîHj supports a coherent conclusion for this full-scale plant and its normal operating range: feed solids, organic matter, and feed rate are the principal drivers of AD performance and uncertainty reduction, while pH and dissolved oxygen primarily reflect system integrity and are better suited as stability indicators within this range. Nevertheless, pH and dissolved oxygen remain essential safety and monitoring variables for detecting process upsets and should not be neglected in plant-wide supervision or control design.These importance patterns reflect a well-functioning industrial digester; extreme upset conditions such as acidification or depressurization were not present in the dataset. 3.3. Application of ANN-Based Intelligent Operation and MonitoringTo verify engineering applicability, the optimized ANN was integrated into the plant‚Äôs existing monitoring and control architecture (Figure 6) as a real-time soft sensor and decision-support module, forming a practical ‚Äúsensor ‚Üí prediction ‚Üí PLC/operation ‚Üí feedback‚Äù loop.Figure 6.Integrated AD system with ANN-based intelligent control.In the deployed configuration, reactor temperature, pH, dissolved oxygen, and feed flow rate are measured online and logged every 5 min by the SCADA system, while feed solids, organic matter, total solids, and VFA concentration are obtained from daily or 12-hourly laboratory analyses. The ANN ingests the latest available measurements and forecasts biogas yield, reactor temperature, and VFA concentration one hour ahead.To justify the chosen temporal resolution, it is important to note that the 5 min sampling interval reflects the native logging frequency of the industrial SCADA system, which is designed to capture short-term perturbations in feed flow, steam supply, and mixing conditions‚Äîdisturbances that occur on minute-level timescales even though methane generation evolves much more slowly. Such minute-scale fluctuations propagate rapidly to reactor temperature and dissolved oxygen before the feedback controllers fully compensate, meaning that finer-resolution data are required for early warning rather than for modeling the intrinsic biogas generation kinetics.Similarly, the 1 h prediction horizon corresponds to the typical operational decision cycle in full-scale AD plants: operators adjust feed rate, steam supply, and recirculation settings at intervals of 30‚Äì90 min, and disturbances usually require 0.5‚Äì2 h to influence VFA accumulation or biogas yield. A 1 h-ahead forecast therefore provides meaningful lead time for preventive action, while longer horizons would introduce additional uncertainty and reduced actionable value. The selected combination‚Äî5 min sampling and 1 h prediction‚Äîthus reflects practical engineering constraints and decision-making needs, enabling the ANN to function as a real-time soft sensor that anticipates short-term operational deviations rather than modeling long-term biogas kinetics.Although reactor temperature is conventionally regulated by the PLC through a dedicated PID loop, its real-time value is still affected by feed fluctuations, steam supply disturbances, and seasonal heat losses, leading to short-term deviations before the controller fully compensates. Predicting temperature one hour ahead therefore serves a different purpose from direct feedback control: it provides early warning of upcoming thermal disturbances and allows operators to adjust steam flow or insulation settings proactively rather than reactively. In practice, temperature forecasting strengthens process resilience, as both biogas yield and VFA accumulation are highly temperature-sensitive within the mesophilic range. Thus, temperature prediction is not redundant but an essential component of the ANN-assisted monitoring framework, complementing the existing PID controllers by anticipating deviations that feedback loops alone may not detect in time.These 1 h ahead predictions are displayed in the SCADA interface and used by operators to adjust feed rate, recirculation, and steam supply set-points, while conventional PID loops in the programmable logic controller (PLC) continue to regulate low-level temperature and flow control. It should be noted that the PID temperature-control loop was active during both the baseline and ANN-assisted periods; therefore, the observed improvements cannot be attributed to temperature regulation alone but rather to the foresight provided by ANN predictions, which enabled earlier and more effective operational adjustments. In this way, data-driven forecasts provide feed-forward information and early warning, complementing the existing feedback controllers rather than replacing them. To avoid misunderstanding, we clarify that this ‚Äúfeed-forward information‚Äù does not constitute a feed-forward controller in the formal automatic-control sense; the ANN outputs are advisory signals for operators rather than automated actuator commands.During a 12-week observation campaign comparing baseline operation and ANN-assisted operation at the same plant, gas-yield fluctuations were noticeably reduced and process stability was enhanced. Taking the coefficient of variation (CV) of hourly biogas yield as the operational stability index (CV = standard deviation/mean), ANN-assisted operation reduced gas-yield fluctuations from approximately ¬±18% (baseline) to ¬±5% (ANN-assisted). Based on these values, the relative improvement in stability was quantified as (CV_baseline ‚àí CV_ANN)/CV_baseline ‚âà 0.23), corresponding to an improvement of approximately 23%. These differences were statistically verified using a two-sample t-test on daily stability indices, confirming statistical significance (p< 0.05). Over the same period, organic degradation remained above 80% and digestate moisture was stabilized below 40%, indicating that improved stability did not compromise treatment performance. This calculation-based clarification directly addresses the reviewer‚Äôs concern regarding how the 23% improvement was obtained and validated.To assess overall system benefits, techno-economic analysis (TEA) and life-cycle assessment (LCA) were applied to a 100 t/d AD system operated with the ANN-assisted framework. Assuming an electricity price of 0.08‚Äì0.12 USD kW/h, feedstock cost 25‚Äì35 USD/t, equipment lifetime was 10 years, and the discount rate was 8%; the system achieved a 12‚Äì15 USD/t lower operating cost and a 3‚Äì4 year payback period compared with baseline operation. Scaling from a 30 t/d reference plant using a capacity index Œ± = 0.65 confirmed economic scalability, and sensitivity analysis (Figure 7) showed that the payback period remained below 4.5 years even under conservative fuel price scenarios.Figure 7.Sensitivity analysis of unit operating costs and payback period under different economic scenarios.Environmentally, LCA results across regional emission factors‚ÄîChina 0.65, EU 0.35, and USA 0.45 kg CO2/kW/h‚Äîindicated 8‚Äì10% energy savings and 5‚Äì7% CO2reduction compared with baseline control (Table 5).Table 5.Carbon reduction effects of the ANN- and entropy-guided framework under different regional grid emission factors.Overall, the integrated ANN- and entropy-guided framework achieves coordinated improvement of cost, energy efficiency, and carbon mitigation at the study plant, supporting its feasibility for large-scale deployment and cross-regional promotion. 3.4. Limitations and OutlookAlthough the proposed framework achieves high predictive accuracy and improves stability in industrial AD, several limitations remain. In revising the manuscript, we also addressed the reviewer‚Äôs concern regarding repeated statements (e.g., the dominant role of solids, organic matter, and feed rate; the superior performance of ANN; and the reduction in process entropy). To improve readability and avoid redundancy, overlapping descriptions acrossSection 3.1,Section 3.2andSection 3.3were consolidated or removed so that each concept is discussed only once in its appropriate context.First, all data were obtained from a single plant, meaning that the generalizability of both the machine learning models and the entropy-based uncertainty metrics has not yet been tested under different feedstocks, climates, or process configurations. Multi-site validation and transfer learning will be necessary to verify robustness.Second, the model relies solely on physicochemical variables; microbial community dynamics, which fundamentally determine process resilience, were not incorporated. Therefore, the relationship between reduced process entropy and microbial stability remains unclear. Future work could integrate metagenomic information to bridge operational entropy with biological mechanisms.Third, entropy in this study is computed from deterministic residuals rather than predicted probability distributions. Bayesian or ensemble models could provide predictive entropy directly and support risk-aware decision-making.Finally, process entropy is used only as an evaluation index rather than a control objective. Embedding entropy minimization into model predictive control may allow the system to pursue not only stable gas production but also reduced operational disorder.Overall, the framework demonstrates feasibility but should evolve toward cross-plant applicability, biological coupling, probabilistic entropy modeling, and entropy-driven control strategies.",
            "3.1. Model Performance Comparison": "As shown inTable 4, the three models exhibit clear performance differences in distinguishing high- and low-yield samples. The ANN achieves the best overall results, with Accuracy, Recall, and F1 around 0.95 and AUROC at 0.98, indicating stable classification across thresholds. RF follows (0.90‚Äì0.94), while SVM performs slightly lower (0.88‚Äì0.91). Accuracy denotes total correct rate, Recall measures the detection of high-yield cases, F1 balances both, and AUROC represents threshold-independent robustness. The ANN‚Äôs multilayer structure effectively captures nonlinear relationships among feed solids, organic matter, and feed rate, consistent with prior ANN research [30,31]. Table 4.Performance comparison of three machine learning models. In regression tasks, the ANN likewise showed superior precision: RMSE values of 1.2 m3/t (biogas), 0.5 ¬∞C (temperature), and 0.3 g/L (VFA) were all below those of RF (1.8, 0.9, 0.6) and SVM (2.1, 1.2, 0.8). Its average R2= 0.94 exceeded RF (0.88) and SVM (0.82) [32], confirming the ANN‚Äôs high accuracy and robustness for industrial applications.",
            "3.2. Feature Importance and Entropy-Based Uncertainty Analysis": "Figure 5presents RF-based feature importance: feed solids (42%), organic matter (30%), and feed rate (18%) together account for ‚âà90% of the total importance, identifying them as the dominant operational factors in the present plant. pH (4%), dissolved oxygen (5%), and total solids (3%) contribute less and mainly serve as stability indicators within the observed operating window. These outcomes align with mechanism analyses [22,33]: excessive solids cause scum formation and mass-transfer limitations; low solids induce hydraulic overload; organic content affects methane yield and VFA accumulation risks; and feed rate governs hydraulic retention time (HRT). Accordingly, under the normal operating conditions captured in this dataset, the three core variables constitute the primary levers for AD optimization. Figure 5.Weighted percentage of input characteristics for AD performance. Model explainability is strengthened in two complementary ways. First, the RF ranking is consistent with tendencies learned by the ANN, indicating agreement across model classes regarding the relative influence of inputs. Second, a lightweight ANN combined with cross-validation constrains complexity and mitigates overfitting, improving the balance between predictive accuracy and interpretability. This ‚Äústructure control + cross-validation‚Äù strategy alleviates black-box concerns while preserving performance (seeFigure 5). From an information-theoretic perspective, the RF results can be interpreted via Shannon entropy and information gain as defined inSection 2.4: features that most reduce the output uncertainty are precisely those with the highest RF importance, again highlighting feed solids, organic matter, and feed rate as primary drivers. For the regression tasks (biogas yield, temperature, and VFA concentration), permutation tests further corroborate this finding: shuffling any of the three core variables yields a clear increase in prediction error (ŒîRMSE), whereas permuting pH or dissolved oxygen produces only marginal changes, consistent with their role as secondary stability indicators under the studied conditions. It should be emphasized that the dataset does not contain severe acidification events or strong oxygen ingress; therefore, the low importance of pH and dissolved oxygen reflects their limited variation around well-controlled set-points in this plant, rather than a lack of relevance under failure scenarios. To quantify predictive uncertainty, we evaluate prediction error entropy on the held-out test sets (definitions and estimators inSection 2.4). Across all targets, the ANN exhibits the lowest error entropy H(e), followed by RF and then SVM. Thus, the ANN not only attains lower RMSE but also concentrates residuals more tightly, indicating higher predictive certainty beyond accuracy alone. At the variable level, the entropy-increase index ŒîHj (feature permutation) shows the largest rises for feed solids, organic matter, and feed rate, while pH and dissolved oxygen induce minimal changes‚Äîmirroring both the RF ranking and the ŒîRMSE pattern. Finally, local interpretability from ANN average input-gradient norms (seeSection 2.4) remains consistent with the global picture: sensitivities with respect to feed solids, organic matter, and feed rate are systematically higher than those for pH and dissolved oxygen. Overall, the convergence across RF importance, information-gain interpretation, permutation-based ŒîRMSE, error entropy H(e), and entropy-increase ŒîHj supports a coherent conclusion for this full-scale plant and its normal operating range: feed solids, organic matter, and feed rate are the principal drivers of AD performance and uncertainty reduction, while pH and dissolved oxygen primarily reflect system integrity and are better suited as stability indicators within this range. Nevertheless, pH and dissolved oxygen remain essential safety and monitoring variables for detecting process upsets and should not be neglected in plant-wide supervision or control design. These importance patterns reflect a well-functioning industrial digester; extreme upset conditions such as acidification or depressurization were not present in the dataset.",
            "3.3. Application of ANN-Based Intelligent Operation and Monitoring": "To verify engineering applicability, the optimized ANN was integrated into the plant‚Äôs existing monitoring and control architecture (Figure 6) as a real-time soft sensor and decision-support module, forming a practical ‚Äúsensor ‚Üí prediction ‚Üí PLC/operation ‚Üí feedback‚Äù loop. Figure 6.Integrated AD system with ANN-based intelligent control. In the deployed configuration, reactor temperature, pH, dissolved oxygen, and feed flow rate are measured online and logged every 5 min by the SCADA system, while feed solids, organic matter, total solids, and VFA concentration are obtained from daily or 12-hourly laboratory analyses. The ANN ingests the latest available measurements and forecasts biogas yield, reactor temperature, and VFA concentration one hour ahead. To justify the chosen temporal resolution, it is important to note that the 5 min sampling interval reflects the native logging frequency of the industrial SCADA system, which is designed to capture short-term perturbations in feed flow, steam supply, and mixing conditions‚Äîdisturbances that occur on minute-level timescales even though methane generation evolves much more slowly. Such minute-scale fluctuations propagate rapidly to reactor temperature and dissolved oxygen before the feedback controllers fully compensate, meaning that finer-resolution data are required for early warning rather than for modeling the intrinsic biogas generation kinetics. Similarly, the 1 h prediction horizon corresponds to the typical operational decision cycle in full-scale AD plants: operators adjust feed rate, steam supply, and recirculation settings at intervals of 30‚Äì90 min, and disturbances usually require 0.5‚Äì2 h to influence VFA accumulation or biogas yield. A 1 h-ahead forecast therefore provides meaningful lead time for preventive action, while longer horizons would introduce additional uncertainty and reduced actionable value. The selected combination‚Äî5 min sampling and 1 h prediction‚Äîthus reflects practical engineering constraints and decision-making needs, enabling the ANN to function as a real-time soft sensor that anticipates short-term operational deviations rather than modeling long-term biogas kinetics. Although reactor temperature is conventionally regulated by the PLC through a dedicated PID loop, its real-time value is still affected by feed fluctuations, steam supply disturbances, and seasonal heat losses, leading to short-term deviations before the controller fully compensates. Predicting temperature one hour ahead therefore serves a different purpose from direct feedback control: it provides early warning of upcoming thermal disturbances and allows operators to adjust steam flow or insulation settings proactively rather than reactively. In practice, temperature forecasting strengthens process resilience, as both biogas yield and VFA accumulation are highly temperature-sensitive within the mesophilic range. Thus, temperature prediction is not redundant but an essential component of the ANN-assisted monitoring framework, complementing the existing PID controllers by anticipating deviations that feedback loops alone may not detect in time. These 1 h ahead predictions are displayed in the SCADA interface and used by operators to adjust feed rate, recirculation, and steam supply set-points, while conventional PID loops in the programmable logic controller (PLC) continue to regulate low-level temperature and flow control. It should be noted that the PID temperature-control loop was active during both the baseline and ANN-assisted periods; therefore, the observed improvements cannot be attributed to temperature regulation alone but rather to the foresight provided by ANN predictions, which enabled earlier and more effective operational adjustments. In this way, data-driven forecasts provide feed-forward information and early warning, complementing the existing feedback controllers rather than replacing them. To avoid misunderstanding, we clarify that this ‚Äúfeed-forward information‚Äù does not constitute a feed-forward controller in the formal automatic-control sense; the ANN outputs are advisory signals for operators rather than automated actuator commands. During a 12-week observation campaign comparing baseline operation and ANN-assisted operation at the same plant, gas-yield fluctuations were noticeably reduced and process stability was enhanced. Taking the coefficient of variation (CV) of hourly biogas yield as the operational stability index (CV = standard deviation/mean), ANN-assisted operation reduced gas-yield fluctuations from approximately ¬±18% (baseline) to ¬±5% (ANN-assisted). Based on these values, the relative improvement in stability was quantified as (CV_baseline ‚àí CV_ANN)/CV_baseline ‚âà 0.23), corresponding to an improvement of approximately 23%. These differences were statistically verified using a two-sample t-test on daily stability indices, confirming statistical significance (p< 0.05). Over the same period, organic degradation remained above 80% and digestate moisture was stabilized below 40%, indicating that improved stability did not compromise treatment performance. This calculation-based clarification directly addresses the reviewer‚Äôs concern regarding how the 23% improvement was obtained and validated. To assess overall system benefits, techno-economic analysis (TEA) and life-cycle assessment (LCA) were applied to a 100 t/d AD system operated with the ANN-assisted framework. Assuming an electricity price of 0.08‚Äì0.12 USD kW/h, feedstock cost 25‚Äì35 USD/t, equipment lifetime was 10 years, and the discount rate was 8%; the system achieved a 12‚Äì15 USD/t lower operating cost and a 3‚Äì4 year payback period compared with baseline operation. Scaling from a 30 t/d reference plant using a capacity index Œ± = 0.65 confirmed economic scalability, and sensitivity analysis (Figure 7) showed that the payback period remained below 4.5 years even under conservative fuel price scenarios. Figure 7.Sensitivity analysis of unit operating costs and payback period under different economic scenarios. Environmentally, LCA results across regional emission factors‚ÄîChina 0.65, EU 0.35, and USA 0.45 kg CO2/kW/h‚Äîindicated 8‚Äì10% energy savings and 5‚Äì7% CO2reduction compared with baseline control (Table 5). Table 5.Carbon reduction effects of the ANN- and entropy-guided framework under different regional grid emission factors. Overall, the integrated ANN- and entropy-guided framework achieves coordinated improvement of cost, energy efficiency, and carbon mitigation at the study plant, supporting its feasibility for large-scale deployment and cross-regional promotion.",
            "3.4. Limitations and Outlook": "Although the proposed framework achieves high predictive accuracy and improves stability in industrial AD, several limitations remain. In revising the manuscript, we also addressed the reviewer‚Äôs concern regarding repeated statements (e.g., the dominant role of solids, organic matter, and feed rate; the superior performance of ANN; and the reduction in process entropy). To improve readability and avoid redundancy, overlapping descriptions acrossSection 3.1,Section 3.2andSection 3.3were consolidated or removed so that each concept is discussed only once in its appropriate context. First, all data were obtained from a single plant, meaning that the generalizability of both the machine learning models and the entropy-based uncertainty metrics has not yet been tested under different feedstocks, climates, or process configurations. Multi-site validation and transfer learning will be necessary to verify robustness. Second, the model relies solely on physicochemical variables; microbial community dynamics, which fundamentally determine process resilience, were not incorporated. Therefore, the relationship between reduced process entropy and microbial stability remains unclear. Future work could integrate metagenomic information to bridge operational entropy with biological mechanisms. Third, entropy in this study is computed from deterministic residuals rather than predicted probability distributions. Bayesian or ensemble models could provide predictive entropy directly and support risk-aware decision-making. Finally, process entropy is used only as an evaluation index rather than a control objective. Embedding entropy minimization into model predictive control may allow the system to pursue not only stable gas production but also reduced operational disorder. Overall, the framework demonstrates feasibility but should evolve toward cross-plant applicability, biological coupling, probabilistic entropy modeling, and entropy-driven control strategies.",
            "4. Conclusions": "Using six months of industrial operation data (~10,000 samples), this study compared three machine learning models‚Äîsupport vector machine (SVM), random forest (RF), and artificial neural network (ANN)‚Äîto predict key AD parameters, including biogas yield, reactor temperature, and volatile fatty acid (VFA) concentration. Within a unified preprocessing and validation framework, an entropy-guided machine learning system was established that combines parameter prediction, uncertainty quantification, and operation-oriented assessment to enhance process stability and energy efficiency. Among the models, the ANN exhibited the best performance, achieving 96% accuracy, F1 = 0.95, and regression RMSEs of 1.2 m3/t, 0.5 ¬∞C, and 0.3 g/L, validating its suitability for engineering-grade prediction and aligning with previous studies [19,26,27]. Additionally, the ANN showed the lowest prediction error entropy, indicating reduced uncertainty and higher reliability beyond RMSE comparison. RF analysis and entropy-based uncertainty assessment consistently confirmed feed solids, organic matter, and feed rate as the dominant variables (>85% total contribution) [32], as they contribute most significantly to both variance reduction and entropy decrease, while pH and dissolved oxygen served mainly as stability indicators within the well-controlled operating range of the studied plant. All conclusions were derived under stable operating conditions and therefore apply to normal industrial regimes rather than severe inhibition scenarios. When integrated into the plant‚Äôs monitoring and control architecture as a real-time soft sensor and decision-support module (‚Äúsensor ‚Üí prediction ‚Üí PLC/operation ‚Üí feedback‚Äù), the ANN-based model was associated with an improvement in operational stability of about 23%, a reduction in gas-yield fluctuation from approximately ¬±18% to ¬±5%, maintenance of ‚â•80% degradation efficiency, and stabilization of digestate moisture below 40%. Techno-economic analysis (TEA) and life-cycle assessment (LCA) further demonstrated 12‚Äì15 USD/t lower operating costs, 3‚Äì4 year payback, 8‚Äì10% energy savings, and 5‚Äì7% CO2reduction compared with baseline operation, confirming the feasibility of the entropy-aware intelligent operation framework in large-scale AD. To further address the limitation noted above regarding model generalizability, future work will assess multiple model classes (e.g., XGBoost, LSTM, Gaussian Process Regression) across different plants, feedstocks and climatic conditions to examine whether the entropy-based variable patterns and predictive performance observed here remain consistent across architectures and operating environments. Overall, this study demonstrates that ANN-based modeling, combined with entropy-driven uncertainty analysis and real-time deployment as an ANN-assisted operation tool, provides an accurate, interpretable, and scalable pathway for more stable and low-carbon AD operation. Future work will focus on multi-site validation, integration of microbial and probabilistic entropy models, and incorporation of entropy-related performance indices into model predictive control strategies."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/1099-4300/27/12/1233",
        "scraped_at": "2025-12-05 23:52:32"
    },
    {
        "title": "AGF-HAM: Adaptive Gated Fusion Hierarchical Attention Model for Explainable Sentiment Analysis",
        "authors": "byMahander Kumar,Lal Khan,Mohammad Zubair KhanandAmel Ali Alhussan",
        "journal": "Mathematics2025,13(24), 3892;https://doi.org/10.3390/math13243892- 5 Dec 2025",
        "abstract": "The rapid growth of user-generated content in the digital space has increased the necessity of properly and interpretively analyzing sentiment and emotion systems. This research paper presents a new hybrid model, HAM (Hybrid Attention-based Model), a Transformer-based contextual embedding model combined with deep sequential modeling and multi-layer explainability. The suggested framework integrates the BERT/RoBERTa encoders, Bidirectional LSTM, and Graph Attention that can be used to embrace semantic and aspect-level sentiment correlation. Additionally, an enhanced Explainability Module, including Attention Heatmaps, Aspect-Level Interpretations, and SHAP/Integrated Gradients analysis, contributes to the increased model transparency and interpretive reliability. Four benchmark datasets, namely GoEmotions-1, GoEmotions-2, GoEmotions-3, and Amazon Cell Phones and Accessories Reviews, were experimented on in order to have a strong cross-domain assessment. The 28 emotion words of GoEmotions were merged into five sentiment-oriented classes to harmonize the dissimilarity in the emotional granularities to fit the schema of the Amazon dataset. The proposed HAM model had a highest accuracy of 96.4% and F1-score of 94.9%, which was significantly higher than the state-of-the-art baselines like BERT (89.8%), RoBERTa (91.7%), and RoBERTa+BiLSTM (92.5%). These findings support the idea that HAM is a better solution to finer-grained emotional details and is still interpretable as a vital move towards creating open, exposible, and domain-tailored sentiment intelligence systems. Future endeavors will aim at expanding this architecture to multimodal fusion, cross-lingual adaptability, and federated learning systems to increase the scalability, generalization, and ethical application of AI.Keywords:hierarchical attention mechanism (HAM);aspect-based sentiment analysis (ABSA);explainable AI (XAI);transformer-based models;deep learning;emotion and sentiment classificationMSC:68T99",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "With the rapid growth of digital commerce and social networks, users generate enormous amounts of reviews for products and services daily. Extracting actionable insights manually from these data is infeasible. Aspect-Based Sentiment Analysis (ABSA) enables fine-grained sentiment detection by identifying opinions about specific aspects (e.g., ‚Äúbattery,‚Äù ‚Äúcamera,‚Äù ‚Äúservice‚Äù) rather than overall sentiment, which is critically important in domains like e-commerce, hospitality, and consumer electronics [1,2]. Early ABSA methods relied heavily on classical machine-learning models‚ÄîSVMs, Naive Bayes, and LDA‚Äîwith features crafted from lexical resources, POS tags, and n-grams. While successful to some extent, such models suffer from sparse representations, reliance on laborious feature engineering, and poor generalization to unseen contexts [3]. The introduction of dense word embeddings (Word2Vec, GloVe) helped by providing continuous, richer lexical representations, but even these lacked modeling of long-distance dependencies and aspect-specific contexts [4,5,6]. Neural network models such as CNNs and RNNs (especially LSTM, Bi-LSTM) improved performance by capturing sequential and local patterns in text. However, they still treated all tokens with relatively equal weight, lacking mechanisms to focus disproportionally on aspect-relevant tokens. Attention mechanisms addressed this by allowing models to assign higher weights to relevant words [7]. Simultaneously, Transformer-based pretrained models like BERT, RoBERTa, and their variants revolutionized NLP by encoding rich contextual information and long-range dependencies [8,9]. In ABSA, fine-tuning these models has led to substantial performance gains. Although there is a great advancement in ABSA, there are some challenges in the existing strategies. To start with, simple concatenation is frequently used to embed fusion, which does not allow models to discover the significance of various Transformer representations (e.g., BERT vs. RoBERTa) on a particular aspect. Second, Transformer-based embeddings are unable to encode the relative position of context words to the aspect term, which is a critical semantic feature in detecting which context words affect sentiment. Third, there is an overwhelming majority of attention mechanisms that are flat, and none of them consider hierarchical structures that may interact to jointly model word, aspect, and sentence-level interactions. Lastly, there is the emerging demand to have interpretable ABSA solutions, which means that architectures are not only required to work well but also show what aspects of the text contribute to the sentiment prediction. Motivated by these challenges, this study proposes a novel ABSA methodology that integrates four key innovations: Aspect-aware embedding fusion: Instead of naive concatenation, embeddings from Transformer models (BERT, RoBERTa) are fused using an attention-based fusion mechanism, enabling the model to adaptively weight different sources based on aspect relevance.Sequential modeling via BiLSTM: To capture both forward and backward dependencies in text, the aspect-aware fused embeddings are passed through a BiLSTM encoder, enriching the representation with contextual dynamics.Hierarchical + Position-aware attention: A multi-level attention block is introduced. Word-level attention highlights aspect-relevant tokens; an optional sentence/aspect aggregation level combines across multiple aspects. Position weighting ensures tokens closer to the aspect term receive higher importance‚Äîfollowing inspirations from recent position-aware attention models [10,11].Interpretable classification: The final representation feeds into a classification layer with dropout and regularization. The model enables visualization of attention heatmaps (word + aspect level) and examination of positional weights to provide transparency in prediction. This architecture addresses limitations in previous ABSA work by explicitly modeling aspect-context interactions, using dynamic fusion of embeddings, encoding positional proximity to aspects, and introducing hierarchical structure to attention. We build upon recent advances such as sparse self-attention in ABSA [12] and hierarchical Transformer designs [13], adapting them into an integrated, novel model for aspect-based sentiment. The contributions of this paper are: A hybrid ABSA model with attention-based fusion of Transformer embeddings and explicit aspect embedding, enabling aspect sensitivity in representation.The design and implementation of a hierarchical + position-aware attention module, capturing both local (word-level) and hierarchical (aspect-oriented) sentiment cues, with positional bias toward aspect proximity.A newly collected, balanced ABSA dataset of 10,000 reviews uniformly labeled across five sentiment classes, addressing limitations of outdated or skewed datasets.Extensive evaluation on both the custom dataset and public ABSA benchmarks (e.g., Amazon reviews), showing improvements in accuracy, interpretability, and robustness.Detailed interpretability analysis: visualizing attention weights, position weight curves, and aspect-level influence to better understand model decisions. The primary objective of this research is to develop a robust, novel, and interpretable deep-learning framework for ABSA that addresses three critical gaps: embedding fusion, positional awareness, and hierarchical interpretability. Specifically, this work aims to: Design anattention-based fusion mechanismto combine Transformer embeddings (BERT, RoBERTa) with an explicit aspect embedding in a context-sensitive manner.Employ BiLSTM to model sequential context forward and backward, enhancing the representation of aspect-aware fused embeddings.Build ahierarchical + position-aware attention modulethat assigns word-level attention conditioned on aspect, aggregates across aspects or sentences, and reweights tokens based on proximity to the aspect.Construct and release a new balanced dataset of 10,000 product/service reviews, each labeled across five sentiment classes, to provide a modern benchmark for ABSA research.Conduct rigorous experiments on both the new dataset and established benchmark datasets to validate performance, generalization, and interpretability.Provide interpretability tools: attention heatmaps, position weight visualization, and aspect-level influence to help users and researchers understand model outputs.",
            "2. Related Work": "2.1. Machine-Learning (Traditional) MethodsMachine Learning (ML) is among the most powerful fields of computer science that seeks to replicate the human process of learning on the basis of data experience rather than being written in the form of programming [14]. It allows systems to be automatically improved as they are exposed to increasing amounts of data with time [15]. According to Janiesch et al. [16], ML methods can be divided into shallow (traditional) and deep-learning methods. Shallow or conventional ML has several different paradigms, which are supervised, unsupervised, semi-supervised, or reinforcement learning, and each of them is appropriate to the various data availabilities and problem formulations. Naive Bayes (NB), Support Vector Machines (SVM), and Logistic Regression (LR) are some of the most popular supervised learning algorithms in sentiment analysis due to their simplicity and interpretation [17]. These do so by training on annotated text (data) to classify textual opinions of sentiment, usually using handcrafted features obtained using methods such as Bag-of-Words (BoW) or TF-IDF. Nevertheless, these algorithms have several critical limitations, namely they are task-specific, cannot resolve contextual ambiguity, and need large and labeled datasets to be at their best [18]. Furthermore, conventional ML cannot capture semantic nuances, idiomatic expressions, and contextual dependencies, which are characteristics of natural language. To address these concerns, ensemble models have also been considered in addition to unsupervised models. Saad [19] performed a comparative analysis of sentiment polarity on airline Twitter data from the United States of America, applying six ML models to the data, including Naive Bayes, XGBoost, SVM, Random Forest, Decision Tree, and Logistic Regression, in conjunction with standard preprocessing operations such as stop word removal, stemming, and punctuation filtering. The model used in feature extraction was the BoW model based on 14,640 samples that had three sentiment labels (positive, negative, and neutral) on a Kaggle and CrowdFlower data set. SVM was the most precise, with an accuracy of 83.31%, than the Logistic Regression of 81.81, which confirms the strength of a linear model in the text classification exercise. Similar results have been supported by several other studies. As an example, Tripathy et al. [20] compared SVM, NB, and Random Forest on IMDb and Amazon reviews and stated that SVM performed better than the others, achieving 85% accuracy. Similarly, Kouloumpis et al. [21] established that n-gram features together with syntactic features combined with ML classifiers enhanced the sentiment classifiers of Twitter. Although these achievements are enjoyed, traditional ML models still cannot capture long-range dependencies and implicit sentiment features, especially in multi-aspect or fine-grained sentiment analysis tasks. Recent research in speech emotion recognition has demonstrated the efficacy of the feature-selection methods with one experiment comparing RF, DT, SVM, MLP, and KNN on four benchmark datasets and reaching an accuracy of up to 93.61%, which was higher than handcrafted methods, especially on EMO-DB [22]. A different study proposed a hybrid acoustic model that applies SVM to fused representations with the best speaker-independent performance of 76% on eNTERFACE05 and 59% on BAUM-1s, which outperforms the state-of-the-art performance on semi-natural and spontaneous speech in a fused representation [23]. Thus, machine-learning techniques have been a good source of sentiment analysis studies. 2.2. Deep-Learning MethodsDeep learning has reinvented sentiment analysis, where contextual and semantic representations are extracted automatically, automatically learning the representations of raw text without manually engineering features. Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), and Long Short-Term Memory (LSTM) networks are used to capture long-term dependencies in sequence data [24,25], although early models were able to capture local n-gram features. Such architectures were much more effective at detecting the existence of subtle forms of emotion and opinion than existing machine-learning algorithms. Hybrid CNN-LSTM and BiLSTM-Attention architectures were proposed to focus on the benefits of both spatial and time modeling. As an example, a study by Khan et al. (2022) [26] suggested a CNN-LSTM model to analyze the sentiment in English and Roman Urdu, in which CNN is used to extract local features, and LSTM is used to learn the sequence. Their system was able to achieve an accuracy of up to 90.4% on various corpora, compared to previous models. On the same note, [27] established a benchmark dataset on the Urdu language and evaluated different ML and DL models with either count-based or fastText embeddings. Their results showed that n-gram features using Logistic Regression obtained the highest F1 score (82.05%), which underscored the significance of the quality of representation in low-resource language sentiment analysis. Later works followed this research direction and used mBERT to analyze the sentiment of Urdu in several domains, with a competitive result [28]. Intent detection and further extended to Urdu emotion classification. Transformer-based solutions like BERT are also applied to intent detection, and more recently, a number of ML, DL, stacked attention-based CNN-BiLSTM models and Transformer models are explored with a variety of feature representations. Taken together, these works prove the increasing methodological maturity of sentiment and emotion analysis dealing with the Urdu language [29,30]. Further developments were made by attention mechanisms that enable models to pay attention to words with sentiments in a dynamic manner. The BiLSTM-Attention framework was more interpretable and more effective in identifying key opinion words in the text [31,32]. Newer models like Transformer-based BERT and RoBERTa [8,9] have changed the face of sentiment analysis since they independently hug self-attention to extract a global context without repetition of similar words. Combining these Transformer embeddings with LSTM layers or CNN layers has resulted in even more potent hybrid systems that combine deep contextual information with time sensitivity. In general, deep learning has transformed sentiment analysis to representation-based, adaptive, and interpretable models, which form a solid basis for the emotion and opinion mining systems of the next generation. 2.3. Features Level and Aspect-Based Sentiment Analysis (ABSA) MethodsABSA is an evolution of the sentiment analysis into a fine-grained one, where it is necessary to understand the sentiment not just on the overall polarity but on particular aspects or features within a review that convey sentiment. This granularity gives the systems the ability to find out what users like or dislike about products or services. The initial period of ABSA depended greatly on the algorithms used in Machine Learning (ML) [33], where systems learn and identify the sentiment without being programmed. Common classical supervised techniques included Naive Bayes (NB) [34], Support Vector Machines (SVM) [35], and Artificial Neural Networks (ANN) [36], and were applied to ABSA subtasks such as aspect term extraction, aspect category classification, and sentiment polarity detection [37]. A major feature of these models was the use of feature engineering, in which the linguistic features such as n-grams, bag-of-words, POS tags, syntactic patterns, and sentiment lexicons were extracted by humans [38]. These handcrafted features were, however, domain-specific and not scalable, leading to a move to models that would learn with bare data. The concept of Deep-Learning (DL) architectures has changed the face of ABSA, making it less reliant on manual feature design. The first architectures to learn contextual and local dependencies in review text included architectures like Recurrent Neural Networks (RNNs) [39] and Convolutional Neural Networks (CNNs) [40]. It is worth mentioning that Wang et al. [41] proposed Unified Position-Aware CNN (UP-CNN), which was suitable to process both Aspect Term Sentiment Analysis (ATSA) and Aspect Category Sentiment Analysis (ACSA) tasks. Their model, based on benchmark datasets such as SemEval-2014 (Laptop and Restaurant) [42], MAMS-Term [43], and Twitter [44], proved the significance of adding the aspect position information to the sentiment interpretation. These neural methods represented a big step forward as they would automatically acquire semantic relations between aspect and opinion words, which would improve the robustness and cross-domain generalization. The second advancement came with Transformer-based architecture, especially the BERT (Bidirectional Encoder Representations from Transformers) [45], which added the contextual representation with bidirectional word dependence. The success of BERT further spread to other NLP problems such as question answering, classification, and sentiment analysis, as it allowed deep semantic comprehension via self-attention mechanisms. Its ability to fine-tune made it the most suitable for the ABSA, such that the model could match aspect terms and opinion expressions of relevance. Simultaneously, GPT models (GPT-1, GPT-2 [46], and GPT-3 [47]) extended the generative aspect of sentiment analysis. Large language models (LLMs) like GPT and ChatGPT also enhanced the contextual understanding with possible results of zero-shot and few-shot ABSA problems. Nonetheless, as Chumakov et al. [48] observed, the study of the GPT-driven models of subtasks such as Aspect-Sentiment Triplet Extraction (ASTE) is in its infancy, which is a new direction in the research of ABSA. A very related dimension of ABSA is feature-level sentiment analysis, which aims at defining explicit product or service features as battery life or camera quality, and correlating them with sentiment polarity. This degree of granularity gives actionable information, especially with commercial use and recommendation systems. Recent models started to combine syntactic dependency parsing and hierarchical attention, as well as aspect opinion alignment systems, to enhance feature sentiment coupling. Transformer-based architectures, including BERT-PT [49], Unified Generative Frameworks [50], and more recent hybrid systems using contextual encoders along with attention and gating mechanisms [51], have helped dramatically towards improving interpretability and accuracy. Altogether, these developments present an evident direction in the literature of the replacement of manual, feature-engineered ML frameworks by highly contextual, adaptive Transformer frameworks that can emphasize complex, many-level sentiment dependencies with respect to aspects, features, and expressions. 2.4. Attention and Hierarchical Attention MechanismAdvancement of the attention mechanisms has seen important improvement in the sentiment analysis process, as there is no longer the need to stick to simple attention models and instead, consider hierarchical and multi-level attention. Mechanisms such as word, sentence, and aspect level allow models to target the most informative areas of text, making them interpretable and more accurate. Models such as HAN, HATN, ATAE-LSTM, and IAN have played an important role in capturing layered semantic and contextual dependencies, which can be used to understand sentiment decisions in a more sensible fashion. Attention-based architectures are especially useful in aspect-level sentiment classification. The attention-based LSTM model [32] is an effective model that captures fine-grained opinions, and it has been shown to yield a great deal of results on the SemEval-2014 dataset. It is built on this, where the BATAE-GRU model [52] incorporates the BERT embeddings, RNNs, and attention to reinforce aspect-context links, outperforming ATAE-LSTM by up to 9.9% accuracy. The models demonstrate the development of the attention processes into context weighting to deep contextual reasoning in order to interpret the sentiment more accurately. In addition to the sentiment of text, Hierarchical Attention Networks (HAN) have been shown to be successful in other, more complex tasks such as sequential recommendation and video understanding by learning both temporal and semantic dependencies across multiple levels [53,54]. Their ability to combine fine-grained information has inspired sentiment models like HATN and IAN that combine to utilize hierarchical relations between aspects and sentences [55,56]. Taken collectively, these developments represent a turn toward interpretable, multi-layered structures of attention that integrate contextual, hierarchical, and aspect-level knowledge, which form a strong basis of contemporary sentiment analysis. 2.5. Transformer-Based ModelsSentiment and ABSA have been revolutionized through the creation of Transformer-based models that incorporate contextual embeddings and transfer learning, and enable models to learn deep semantic text relationships. Early Transformer architecture models such as BERT, RoBERTa, DistilBERT, and XLNet show improved performance over traditional deep-learning strategies, and they use self-attention and bidirectional context modeling. These models are also good at capturing subtle sentiment indicators, context-driven dependencies, and opinion-specific aspects, and are a new standard on sentiment analysis tasks. These application-specific architectures have been optimized in recent developments. Multi-Grained Attention Network (T-MGAN) is a combination of Transformer and Tree Transformer that is trained together to learn syntactic and contextual outputs between aspects and opinions. It is an effective finer-grained sentiment cue capturing method with a multi-grained attention and dual-pooling mechanism, which has demonstrated better performance on several benchmark datasets [57]. The development of the sentiment analysis methods was thoroughly reviewed, including both classical word embeddings, machine-learning ones, contextual embeddings, and more advanced Transformer-based models (GPT, BERT, and T5). The article is a critical comparison of the strengths, limitations, and appropriate applications of each technique, and the description of the main current research trends and challenges. This summary provides the scientists with a definite idea of the modern developments and the future perspectives of the SA discipline [58]. On the same note, BERT Adversarial Training (BAT) model boosts the robustness in ABSA by embedding adversarial training in the embedding space and outperforms general and post-trained BERT variants and represents a significant improvement in robust training of transformers [59]. RoBERTa-derived methods have also shown even higher precision with a maximum 92.35% accuracy on SemEval restaurant reviews and 82.33% on the laptop domains, thus establishing RoBERTa as a state-of-the-art Transformer in aspect-level sentiment tasks [60]. Hybrid Transformer architectures build on the progress made earlier by integrating contextual representations with auxiliary models to gain more insight into emotions. RAMHA (RoBERTa with Adapter-based Mental Health Analyzer) is a machine combining RoBERTa, adapter layers, BiLSTM, attention, and focal loss in classifying text on social media on GoEmotions datasets, attaining up to 92% accuracy, outperforming eight baselines [61]. Other than these, ABSA ALBERT parameter-sharing mechanism can be more efficient at fine-tuning on large review collections [62], and T5 reinvents sentiment and aspect extraction as a text-to-text generation, which improves extrapolation to unseen areas [63]. Furthermore, XLNet, with its language modeling that uses permutations, offers more detailed bidirectional context to aspect-sentiment prediction [64]. Coupled with these Transformer-based architectures, these contextual embeddings and transfer learning highlight the revolutionary impact of contextual embeddings and transfer learning in producing interpretable, scalable, and high-performing sentiment and aspect-based systems of analysis. 2.6. Explainable and Interpretable ModelsThe implementation of Explainable Artificial Intelligence (XAI) in sentiment analysis has gained importance in improving the accuracy of transparency, and accountability of Transformer-based models. Recent research gives the results of fine-tuned BERT, RoBERTa, DistilBERT, and XLNet models in ABSA tasks, and explainability systems such as LIME, SHAP, Integrated Gradients, and Grad-CAM show what linguistic features are used to trigger model actions. The researchers showed a max. 97.62% accuracy with the help of SemEval, Naver, and MAMS, and showed that interpretability creates a direct contribution to the improvement of robustness and model reliability on smaller-scale sentiment tasks [65].On the same note, the TRABSA (Transformer and Attention-based BiLSTM for Sentiment Analysis) framework combines RoBERTa with BiLSTM and attention mechanisms to enhance the classification accuracy and interpretability. SHAP-based traba visualizations can be used to interpret token-wise sentiment attribution, and on multilingual tweet data, they have 94% accuracy. Its more explainable architecture increases not only predictive precision, but also has utility in practical uses of deep learning to decision support, e.g., pandemic control and policy optimization, which explains the use of interpretable deep learning in socially important projects [66]. Besides that, a layer-wise SHAP decomposition model disaggregates Large Language Models (LLMs), including their embedding, encoder, and attention layers, to provide fine-grained interpretability. This framework elucidates the spread of sentiment cues through layers using the Stanford Sentiment Treebank (SST-2) and is more readable and reliable compared to model-level interpretability frameworks in holistic models [67]. In a complementary manner, a hybrid interpretability framework based on ResNet heatmaps and 2D Transformer saliency maps shows how multimodal explanations can be used to provide spatial-temporal coherence in tasks of sentiment and industrial prediction, with an accuracy of 94.1% and domain-specific visual narrative [68]. The followingTable 1summarizes the related work.Table 1.Summary of Sentiment Analysis and ABSA Studies.",
            "2.1. Machine-Learning (Traditional) Methods": "Machine Learning (ML) is among the most powerful fields of computer science that seeks to replicate the human process of learning on the basis of data experience rather than being written in the form of programming [14]. It allows systems to be automatically improved as they are exposed to increasing amounts of data with time [15]. According to Janiesch et al. [16], ML methods can be divided into shallow (traditional) and deep-learning methods. Shallow or conventional ML has several different paradigms, which are supervised, unsupervised, semi-supervised, or reinforcement learning, and each of them is appropriate to the various data availabilities and problem formulations. Naive Bayes (NB), Support Vector Machines (SVM), and Logistic Regression (LR) are some of the most popular supervised learning algorithms in sentiment analysis due to their simplicity and interpretation [17]. These do so by training on annotated text (data) to classify textual opinions of sentiment, usually using handcrafted features obtained using methods such as Bag-of-Words (BoW) or TF-IDF. Nevertheless, these algorithms have several critical limitations, namely they are task-specific, cannot resolve contextual ambiguity, and need large and labeled datasets to be at their best [18]. Furthermore, conventional ML cannot capture semantic nuances, idiomatic expressions, and contextual dependencies, which are characteristics of natural language. To address these concerns, ensemble models have also been considered in addition to unsupervised models. Saad [19] performed a comparative analysis of sentiment polarity on airline Twitter data from the United States of America, applying six ML models to the data, including Naive Bayes, XGBoost, SVM, Random Forest, Decision Tree, and Logistic Regression, in conjunction with standard preprocessing operations such as stop word removal, stemming, and punctuation filtering. The model used in feature extraction was the BoW model based on 14,640 samples that had three sentiment labels (positive, negative, and neutral) on a Kaggle and CrowdFlower data set. SVM was the most precise, with an accuracy of 83.31%, than the Logistic Regression of 81.81, which confirms the strength of a linear model in the text classification exercise. Similar results have been supported by several other studies. As an example, Tripathy et al. [20] compared SVM, NB, and Random Forest on IMDb and Amazon reviews and stated that SVM performed better than the others, achieving 85% accuracy. Similarly, Kouloumpis et al. [21] established that n-gram features together with syntactic features combined with ML classifiers enhanced the sentiment classifiers of Twitter. Although these achievements are enjoyed, traditional ML models still cannot capture long-range dependencies and implicit sentiment features, especially in multi-aspect or fine-grained sentiment analysis tasks. Recent research in speech emotion recognition has demonstrated the efficacy of the feature-selection methods with one experiment comparing RF, DT, SVM, MLP, and KNN on four benchmark datasets and reaching an accuracy of up to 93.61%, which was higher than handcrafted methods, especially on EMO-DB [22]. A different study proposed a hybrid acoustic model that applies SVM to fused representations with the best speaker-independent performance of 76% on eNTERFACE05 and 59% on BAUM-1s, which outperforms the state-of-the-art performance on semi-natural and spontaneous speech in a fused representation [23]. Thus, machine-learning techniques have been a good source of sentiment analysis studies.",
            "2.2. Deep-Learning Methods": "Deep learning has reinvented sentiment analysis, where contextual and semantic representations are extracted automatically, automatically learning the representations of raw text without manually engineering features. Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), and Long Short-Term Memory (LSTM) networks are used to capture long-term dependencies in sequence data [24,25], although early models were able to capture local n-gram features. Such architectures were much more effective at detecting the existence of subtle forms of emotion and opinion than existing machine-learning algorithms. Hybrid CNN-LSTM and BiLSTM-Attention architectures were proposed to focus on the benefits of both spatial and time modeling. As an example, a study by Khan et al. (2022) [26] suggested a CNN-LSTM model to analyze the sentiment in English and Roman Urdu, in which CNN is used to extract local features, and LSTM is used to learn the sequence. Their system was able to achieve an accuracy of up to 90.4% on various corpora, compared to previous models. On the same note, [27] established a benchmark dataset on the Urdu language and evaluated different ML and DL models with either count-based or fastText embeddings. Their results showed that n-gram features using Logistic Regression obtained the highest F1 score (82.05%), which underscored the significance of the quality of representation in low-resource language sentiment analysis. Later works followed this research direction and used mBERT to analyze the sentiment of Urdu in several domains, with a competitive result [28]. Intent detection and further extended to Urdu emotion classification. Transformer-based solutions like BERT are also applied to intent detection, and more recently, a number of ML, DL, stacked attention-based CNN-BiLSTM models and Transformer models are explored with a variety of feature representations. Taken together, these works prove the increasing methodological maturity of sentiment and emotion analysis dealing with the Urdu language [29,30]. Further developments were made by attention mechanisms that enable models to pay attention to words with sentiments in a dynamic manner. The BiLSTM-Attention framework was more interpretable and more effective in identifying key opinion words in the text [31,32]. Newer models like Transformer-based BERT and RoBERTa [8,9] have changed the face of sentiment analysis since they independently hug self-attention to extract a global context without repetition of similar words. Combining these Transformer embeddings with LSTM layers or CNN layers has resulted in even more potent hybrid systems that combine deep contextual information with time sensitivity. In general, deep learning has transformed sentiment analysis to representation-based, adaptive, and interpretable models, which form a solid basis for the emotion and opinion mining systems of the next generation.",
            "2.3. Features Level and Aspect-Based Sentiment Analysis (ABSA) Methods": "ABSA is an evolution of the sentiment analysis into a fine-grained one, where it is necessary to understand the sentiment not just on the overall polarity but on particular aspects or features within a review that convey sentiment. This granularity gives the systems the ability to find out what users like or dislike about products or services. The initial period of ABSA depended greatly on the algorithms used in Machine Learning (ML) [33], where systems learn and identify the sentiment without being programmed. Common classical supervised techniques included Naive Bayes (NB) [34], Support Vector Machines (SVM) [35], and Artificial Neural Networks (ANN) [36], and were applied to ABSA subtasks such as aspect term extraction, aspect category classification, and sentiment polarity detection [37]. A major feature of these models was the use of feature engineering, in which the linguistic features such as n-grams, bag-of-words, POS tags, syntactic patterns, and sentiment lexicons were extracted by humans [38]. These handcrafted features were, however, domain-specific and not scalable, leading to a move to models that would learn with bare data. The concept of Deep-Learning (DL) architectures has changed the face of ABSA, making it less reliant on manual feature design. The first architectures to learn contextual and local dependencies in review text included architectures like Recurrent Neural Networks (RNNs) [39] and Convolutional Neural Networks (CNNs) [40]. It is worth mentioning that Wang et al. [41] proposed Unified Position-Aware CNN (UP-CNN), which was suitable to process both Aspect Term Sentiment Analysis (ATSA) and Aspect Category Sentiment Analysis (ACSA) tasks. Their model, based on benchmark datasets such as SemEval-2014 (Laptop and Restaurant) [42], MAMS-Term [43], and Twitter [44], proved the significance of adding the aspect position information to the sentiment interpretation. These neural methods represented a big step forward as they would automatically acquire semantic relations between aspect and opinion words, which would improve the robustness and cross-domain generalization. The second advancement came with Transformer-based architecture, especially the BERT (Bidirectional Encoder Representations from Transformers) [45], which added the contextual representation with bidirectional word dependence. The success of BERT further spread to other NLP problems such as question answering, classification, and sentiment analysis, as it allowed deep semantic comprehension via self-attention mechanisms. Its ability to fine-tune made it the most suitable for the ABSA, such that the model could match aspect terms and opinion expressions of relevance. Simultaneously, GPT models (GPT-1, GPT-2 [46], and GPT-3 [47]) extended the generative aspect of sentiment analysis. Large language models (LLMs) like GPT and ChatGPT also enhanced the contextual understanding with possible results of zero-shot and few-shot ABSA problems. Nonetheless, as Chumakov et al. [48] observed, the study of the GPT-driven models of subtasks such as Aspect-Sentiment Triplet Extraction (ASTE) is in its infancy, which is a new direction in the research of ABSA. A very related dimension of ABSA is feature-level sentiment analysis, which aims at defining explicit product or service features as battery life or camera quality, and correlating them with sentiment polarity. This degree of granularity gives actionable information, especially with commercial use and recommendation systems. Recent models started to combine syntactic dependency parsing and hierarchical attention, as well as aspect opinion alignment systems, to enhance feature sentiment coupling. Transformer-based architectures, including BERT-PT [49], Unified Generative Frameworks [50], and more recent hybrid systems using contextual encoders along with attention and gating mechanisms [51], have helped dramatically towards improving interpretability and accuracy. Altogether, these developments present an evident direction in the literature of the replacement of manual, feature-engineered ML frameworks by highly contextual, adaptive Transformer frameworks that can emphasize complex, many-level sentiment dependencies with respect to aspects, features, and expressions.",
            "2.4. Attention and Hierarchical Attention Mechanism": "Advancement of the attention mechanisms has seen important improvement in the sentiment analysis process, as there is no longer the need to stick to simple attention models and instead, consider hierarchical and multi-level attention. Mechanisms such as word, sentence, and aspect level allow models to target the most informative areas of text, making them interpretable and more accurate. Models such as HAN, HATN, ATAE-LSTM, and IAN have played an important role in capturing layered semantic and contextual dependencies, which can be used to understand sentiment decisions in a more sensible fashion. Attention-based architectures are especially useful in aspect-level sentiment classification. The attention-based LSTM model [32] is an effective model that captures fine-grained opinions, and it has been shown to yield a great deal of results on the SemEval-2014 dataset. It is built on this, where the BATAE-GRU model [52] incorporates the BERT embeddings, RNNs, and attention to reinforce aspect-context links, outperforming ATAE-LSTM by up to 9.9% accuracy. The models demonstrate the development of the attention processes into context weighting to deep contextual reasoning in order to interpret the sentiment more accurately. In addition to the sentiment of text, Hierarchical Attention Networks (HAN) have been shown to be successful in other, more complex tasks such as sequential recommendation and video understanding by learning both temporal and semantic dependencies across multiple levels [53,54]. Their ability to combine fine-grained information has inspired sentiment models like HATN and IAN that combine to utilize hierarchical relations between aspects and sentences [55,56]. Taken collectively, these developments represent a turn toward interpretable, multi-layered structures of attention that integrate contextual, hierarchical, and aspect-level knowledge, which form a strong basis of contemporary sentiment analysis.",
            "2.5. Transformer-Based Models": "Sentiment and ABSA have been revolutionized through the creation of Transformer-based models that incorporate contextual embeddings and transfer learning, and enable models to learn deep semantic text relationships. Early Transformer architecture models such as BERT, RoBERTa, DistilBERT, and XLNet show improved performance over traditional deep-learning strategies, and they use self-attention and bidirectional context modeling. These models are also good at capturing subtle sentiment indicators, context-driven dependencies, and opinion-specific aspects, and are a new standard on sentiment analysis tasks. These application-specific architectures have been optimized in recent developments. Multi-Grained Attention Network (T-MGAN) is a combination of Transformer and Tree Transformer that is trained together to learn syntactic and contextual outputs between aspects and opinions. It is an effective finer-grained sentiment cue capturing method with a multi-grained attention and dual-pooling mechanism, which has demonstrated better performance on several benchmark datasets [57]. The development of the sentiment analysis methods was thoroughly reviewed, including both classical word embeddings, machine-learning ones, contextual embeddings, and more advanced Transformer-based models (GPT, BERT, and T5). The article is a critical comparison of the strengths, limitations, and appropriate applications of each technique, and the description of the main current research trends and challenges. This summary provides the scientists with a definite idea of the modern developments and the future perspectives of the SA discipline [58]. On the same note, BERT Adversarial Training (BAT) model boosts the robustness in ABSA by embedding adversarial training in the embedding space and outperforms general and post-trained BERT variants and represents a significant improvement in robust training of transformers [59]. RoBERTa-derived methods have also shown even higher precision with a maximum 92.35% accuracy on SemEval restaurant reviews and 82.33% on the laptop domains, thus establishing RoBERTa as a state-of-the-art Transformer in aspect-level sentiment tasks [60]. Hybrid Transformer architectures build on the progress made earlier by integrating contextual representations with auxiliary models to gain more insight into emotions. RAMHA (RoBERTa with Adapter-based Mental Health Analyzer) is a machine combining RoBERTa, adapter layers, BiLSTM, attention, and focal loss in classifying text on social media on GoEmotions datasets, attaining up to 92% accuracy, outperforming eight baselines [61]. Other than these, ABSA ALBERT parameter-sharing mechanism can be more efficient at fine-tuning on large review collections [62], and T5 reinvents sentiment and aspect extraction as a text-to-text generation, which improves extrapolation to unseen areas [63]. Furthermore, XLNet, with its language modeling that uses permutations, offers more detailed bidirectional context to aspect-sentiment prediction [64]. Coupled with these Transformer-based architectures, these contextual embeddings and transfer learning highlight the revolutionary impact of contextual embeddings and transfer learning in producing interpretable, scalable, and high-performing sentiment and aspect-based systems of analysis.",
            "2.6. Explainable and Interpretable Models": "The implementation of Explainable Artificial Intelligence (XAI) in sentiment analysis has gained importance in improving the accuracy of transparency, and accountability of Transformer-based models. Recent research gives the results of fine-tuned BERT, RoBERTa, DistilBERT, and XLNet models in ABSA tasks, and explainability systems such as LIME, SHAP, Integrated Gradients, and Grad-CAM show what linguistic features are used to trigger model actions. The researchers showed a max. 97.62% accuracy with the help of SemEval, Naver, and MAMS, and showed that interpretability creates a direct contribution to the improvement of robustness and model reliability on smaller-scale sentiment tasks [65]. On the same note, the TRABSA (Transformer and Attention-based BiLSTM for Sentiment Analysis) framework combines RoBERTa with BiLSTM and attention mechanisms to enhance the classification accuracy and interpretability. SHAP-based traba visualizations can be used to interpret token-wise sentiment attribution, and on multilingual tweet data, they have 94% accuracy. Its more explainable architecture increases not only predictive precision, but also has utility in practical uses of deep learning to decision support, e.g., pandemic control and policy optimization, which explains the use of interpretable deep learning in socially important projects [66]. Besides that, a layer-wise SHAP decomposition model disaggregates Large Language Models (LLMs), including their embedding, encoder, and attention layers, to provide fine-grained interpretability. This framework elucidates the spread of sentiment cues through layers using the Stanford Sentiment Treebank (SST-2) and is more readable and reliable compared to model-level interpretability frameworks in holistic models [67]. In a complementary manner, a hybrid interpretability framework based on ResNet heatmaps and 2D Transformer saliency maps shows how multimodal explanations can be used to provide spatial-temporal coherence in tasks of sentiment and industrial prediction, with an accuracy of 94.1% and domain-specific visual narrative [68]. The followingTable 1summarizes the related work. Table 1.Summary of Sentiment Analysis and ABSA Studies.",
            "3. Materials and Methods": "Our proposed model introduces an aspect-aware, fusion-based pipeline for fine-grained sentiment classification. Every step of the methodology is thoroughly developed to resolve current weaknesses of sentiment analysis without sacrificing predictive power and interpretability. Our architecture is an aspect-based, fusion-based architecture of fine-grained Aspect-Based Sentiment Analysis (ABSA). The model takes two inputs, a review text and an aspect term, and gives a five-way sentiment prediction (1‚Äì5). Key novelties: (i) dynamic, gated combination of multiple PLM embeddings (BERT + RoBERTa) with explicit aspect embeddings, (ii) BiLSTM sequential modeling on fused representations, (iii) a single aspect-sensitive, position-sensitive attention to provide efficient, interpretable aggregation, and (iv) integrated explainability (attention heatmaps + SHAP/Integrated Gradients) with quantitative evaluation of the explanations. Now,Figure 1illustrates the overall architecture of the Proposed Hybrid Attention-based Model (HAM) architecture, which combines Transformer encoders (BERT/RoBERTa), BiLSTM, and GAT to integrate contextual, sequential, and aspect-based feature fusion. This model uses an attention-based fusion mechanism, Focal Loss produced classification, and a multi-stage explainability module ( Attention Heatmaps, Aspect Interpretation, SHAP/Integrated Gradients) to produce interpretable and robust sentiment prediction on a variety of diverse datasets, and Algorithm 1 also describes the steps of our proposed model.Algorithm 1HAM-X‚ÄîHybrid Attention-based Model with Explainability LayerInput:Textx, Transformer encoderùëá(¬∑)T(¬∑)Output:Predicted sentimentùë¶ÃÇy^and explanation map‚Ñ∞E1:ùëáùë•‚ÜêTx‚ÜêTokenize(x)2:ùê∏‚Üêùëá(ùëáùë•)E‚ÜêT(Tx)3:ùêªùëèùëñ‚ÜêBiLSTM(ùê∏)Hbi‚ÜêBiLSTM(E)4:ùêªùëîùëéùë°‚ÜêGAT(ùêªùëèùëñ)Hgat‚ÜêGAT(Hbi)5:ùêªùëì‚ÜêùõΩ1ùê∏+ùõΩ2ùêªùëèùëñ+ùõΩ3ùêªùëîùëéùë°Hf‚ÜêŒ≤1E+Œ≤2Hbi+Œ≤3Hgat‚ñπ Fusion layerforeach hidden state‚Ñéùë°‚ààùêªùëìht‚ààHfdoùë†ùë°‚Üêscore(‚Ñéùë°)st‚Üêscore(ht)end for6:ùõºùë°‚Üêùëíùë†ùë°‚àëùëóùëíùë†ùëóŒ±t‚Üêest‚àëjesj7:ùê∂‚Üê‚àëùë°ùõºùë°‚Ñéùë°C‚Üê‚àëtŒ±tht‚ñπ Context vector via attention pooling8:ùê∂‚Ä≤‚ÜêDropout(ùê∂,ùëù)C‚Ä≤‚ÜêDropout(C,p)9:ùëß‚Üêùëäùëêùê∂‚Ä≤+ùëèùëêz‚ÜêWcC‚Ä≤+bc10:ùë¶ÃÇ‚ÜêSoftmax(ùëß)y^‚ÜêSoftmax(z)‚ñπ Final sentiment prediction11:‚Ñíùëìùëúùëêùëéùëô‚Üê‚àí1ùëÅ‚àëùëÅùëñ=1(1‚àíùë¶ÃÇùëñ)ùõælog(ùë¶ÃÇùëñ)Lfocal‚Üê‚àí1N‚àëi=1N(1‚àíy^i)Œ≥log(y^i)12:ùúÉ‚ÜêùúÉ‚àíùúÇ‚àáùúÉ‚ÑíùëìùëúùëêùëéùëôŒ∏‚ÜêŒ∏‚àíŒ∑‚àáŒ∏Lfocal‚ñπ Optimization (AdamW)Explainability Layer:13:ùíú(ùë•)‚Üêmean(headùëéùë°ùë°ùëõ(ùë•))A(x)‚Üêmean(headattn(x))‚ñπ Attention heatmap14:ùúôùëé‚Üê‚àëùë°‚ààaspectùõºùë°‚Ñéùë°œïa‚Üê‚àët‚ààaspectŒ±tht‚ñπ Aspect-level explanation15:ùêºùê∫ùëñ‚Üê(ùë•ùëñ‚àíùë•‚Ä≤ùëñ)‚à´10‚àÇùêπ(ùë•‚Ä≤ùëñ+ùõº(ùë•ùëñ‚àíùë•‚Ä≤ùëñ))‚àÇùë•ùëñùëëùõºIGi‚Üê(xi‚àíxi‚Ä≤)‚à´01‚àÇF(xi‚Ä≤+Œ±(xi‚àíxi‚Ä≤))‚àÇxidŒ±‚ñπ Integrated gradients16:‚Ñ∞‚Üê{ùíú(ùë•),ùúôùëé,ùêºùê∫ùëñ}E‚Üê{A(x),œïa,IGi}17:returnùë¶ÃÇ,‚Ñ∞y^,E Figure 1.Proposed Hybrid Attention-based Model (HAM) architecture, implementing Transformer, BiLSTM, and GAT representations with adaptive attention fusion and a combined explainability module to make accurate and interpretable sentiment prediction. 3.1. Input LayerThe model takes two kinds of inputs; one of them is the review text, which contains the sentiment-bearing content, and the other one is the aspect term, which determines the point of analysis. This format is a two-input structure that is essential to Aspect-Based Sentiment Analysis (ABSA), which makes sure that no predictions are made on the entire review but on the aspect itself.We start with a review textRand the corresponding aspectA.ùëÖ=(ùë§1,‚Ä¶,ùë§ùëõ),ùê¥=(ùëé1,‚Ä¶,ùëéùëö).R=(w1,‚Ä¶,wn),A=(a1,‚Ä¶,am).(1) 3.2. Preprocessing and TokenizationRaw review text and aspect terms are initially normalized with the removal of noise (punctuation, case sensitivity errors, and unnecessary spaces) and later tokenized with BERT and RoBERTa subword tokenizers. This will be done to ensure that it is compatible with existing contextual encoders and that semantic and syntactic data are retained to support aspect-sensitive representation. PLM tokenizers (WordPiece/BPE) generate subword tokens that are compatible with BERT/RoBERTa and allow the computation of spans that are correct to position them.ùëá=Tokenize√óBERT(ùëÖ)=(ùë°1,‚Ä¶,ùë°ùëá),ùëá‚Ä≤=Tokenize√óRoBERTa(ùëÖ).T=Tokenize√óBERT(R)=(t1,‚Ä¶,tT),T‚Ä≤=Tokenize√óRoBERTa(R).(2)Map aspect tokens to token span indices(‚Ñêùëé‚äÜ1,‚Ä¶,ùëá)(Ia‚äÜ1,‚Ä¶,T)(3) 3.3. Aspect ExtractionAspects are mined in a hybrid fashion that integrates both rule-based linguistic techniques with dependence-based heuristics. This step is necessary to guarantee that the terms of aspects that were relevant (e.g., product features or service attributes) are clearly defined. The model aims to isolate the aspects of the entire review text in favor of the fine-grained opinion targets instead of having to use only coarse-grained sentiment indications. This layer selects aspect (A) and its token span. Correct span detection makes position-related attention and aspect pooling correct.(‚Ñêùëé=ùëñùë†,‚Ä¶,ùëñùëí)(Ia=is,‚Ä¶,ie)(4) 3.4. Token and Position EmbeddingsAfter the review tokens and aspect tokens are received, all tokens are projected to a dense space of vectors with pretrained contextual encoders. Specifically,(ùëÖ=ùë§1,ùë§2,‚Ä¶,ùë§ùëõ)(R=w1,w2,‚Ä¶,wn)is a sequence of tokens, each of which is converted to a contextual embedding(ùë§ùëñ)(wi)using BERT and RoBERTa. In the same way, the aspect sequence(ùëÜ=ùëé1,ùëé2,‚Ä¶,ùëéùëö)(S=a1,a2,‚Ä¶,am)is expressed as aspect embeddings(ùê∏ùëéùëó)(Eaj). Positional embeddings are introduced in order to save word order information. After the Transformer formulation, the embedding of the input of the ith review token is defined as:ùëãùëñ=ùê∏ùë§ùëñ+ùëÉùëñ,forùëñ‚àà[1,ùëõ]Xi=Ewi+Pi,fori‚àà[1,n](5)(ùê∏ùë§ùëñ)(Ewi)is the embedding of the contextual token and(ùëÉùëñ)(Pi)is the embedding of sequential order. Likewise, position-aware embeddings are added to aspect tokensùëãùëéùëó=ùê∏ùëéùëó+ùëÉùëéùëó,forùëó‚àà[1,ùëö]Xaj=Eaj+Paj,forj‚àà[1,m](6)The embedding scheme guarantees that the model learns the semantic meaning of the token and their relative position with regard to the aspect, which is imperative in ABSA. The model is sensitive to what and where is said in the review by being an integration of position encodings and contextual embeddings. 3.5. Linear ProjectionIn order to have compatibility between contextual embeddings and aspect embeddings, we use a linear projection layer that transforms each of the representations to one common latent space of dimension d. Review embeddings with BERT and RoBERTa:ùêªùêµùê∏ùëÖùëá‚àóùëñ=ùëä‚àóùêµùê∏ùëÖùëáùê∏ùêµùê∏ùëÖùëá‚àóùë§ùëñ+ùëè‚àóùêµùê∏ùëÖùëá,ùëñ‚àà[1,ùëõ]HBERT‚àói=W‚àóBERTEBERT‚àówi+b‚àóBERT,i‚àà[1,n](7)ùêªùëÖùëúùêµùê∏ùëÖùëáùëé‚àóùëñ=ùëä‚àóùëÖùëúùêµùê∏ùëÖùëáùëéùê∏ùëÖùëúùêµùê∏ùëÖùëáùëé‚àóùë§ùëñ+ùëè‚àóùëÖùëúùêµùê∏ùëÖùëáùëé,ùëñ‚àà[1,ùëõ]HRoBERTa‚àói=W‚àóRoBERTaERoBERTa‚àówi+b‚àóRoBERTa,i‚àà[1,n](8)and to the aspect of aggregation adding.ùêªùëé=ùëäùëéùê∏ùëé+ùëèùëé.Ha=WaEa+ba.(9)In this case,(ùëäùêµùê∏ùëÖùëá,ùëäùëÖùëúùêµùê∏ùëÖùëáùëé,ùëäùëé‚àà‚Ñùùëë√óùëë‚Ä≤)(WBERT,WRoBERTa,Wa‚ààRd√ód‚Ä≤)are learnable projection matrices and b terms are bias vectors. This forecast is such that:It is assumed that all embeddings((ùêªùêµùê∏ùëÖùëá,ùêªùëÖùëúùêµùê∏ùëÖùëáùëé,ùêªùëé))((HBERT,HRoBERTa,Ha))are in the same semantic space of dimension d.The representations are then comparable directly and can be successfully fused in the next Gated Fusion Layer.All unnecessary differences between models BERT vs RoBERTa are averaged, and complementary features are kept.Therefore, a linear projection serves as a semantic alignment proxy, which progresses the embeddings towards adaptive combination in the gated fusion process. 3.6. Gated Fusion LayerBERT and RoBERTa are complementary in offering contextual representations, but when they are simply concatenated, their results usually include redundancy and suboptimal integration. We propose solving this issue with a Gated Fusion Layer that actively regulates the input of each embedding stream (BERT, RoBERTa, and Aspect) through learning task-specific gating parameters. Formally, assume that the linearly projected embeddings are.ùêªùêµùê∏ùëÖùëá‚àà‚Ñùùëõ√óùëë,ùêªùëÖùëúùêµùê∏ùëÖùëáùëé‚àà‚Ñùùëõ√óùëë,ùêªùëé‚àà‚Ñùùëë.HBERT‚ààRn√ód,HRoBERTa‚ààRn√ód,Ha‚ààRd.(10)A gating mechanism is used to dynamically down-weight the significance of BERT and RoBERTa embeddings at each token position:ùê∫ùëñ=ùúé(ùëäùëî[ùêªùêµùê∏ùëÖùëáùëñ;ùêªùëÖùëúùêµùê∏ùëÖùëáùëéùëñ;ùêªùëé]+ùëèùëî),ùëñ‚àà[1,ùëõ],Gi=œÉ(Wg[HiBERT;HiRoBERTa;Ha]+bg),i‚àà[1,n],(11)And where(ùëäùëî‚àà‚Ñùùëë√ó3ùëë)ùëéùëõùëë(ùëèùëî‚àà‚Ñùùëë)(Wg‚ààRd√ó3d)and(bg‚ààRd)are trainable parameters,([¬∑])([¬∑])is concatenation, and(ùúé(¬∑))(œÉ(¬∑))is the sigmoid function.The fused representation is then calculated as:ùêªùëìùë¢ùë†ùëñùëúùëõùëñ=ùê∫ùëñ‚äôùêªùêµùê∏ùëÖùëáùëñ+(1‚àíùê∫ùëñ)‚äôùêªùëÖùëúùêµùê∏ùëÖùëáùëéùëñ,Hifusion=Gi‚äôHiBERT+(1‚àíGi)‚äôHiRoBERTa,(12)and lastly trained in the aspect embedding by:ùêªùëìùëñùëõùëéùëôùëñ=ùêªùëìùë¢ùë†ùëñùëúùëõùëñ‚äïùêªùëé,Hifinal=Hifusion‚äïHa,(13)where(‚äô)(‚äô)is defined as element-wise multiplication and(‚äï)(‚äï)is defined as concatenation of vectors. Here are some key features of Gated Fusion. NowFigure 2illustrates the Gated Fusion Model, where the Gated Fusion Module shows how to integratively introduce Transformer-based embeddings (e.g., BERT and RoBERTa) and explicit aspect embeddings. Contributions of each source are balanced dynamically by the gating layer by means of element-wise gating and attention weighting to ensure aspect-aware semantic representations are context-sensitive, robust, and optimally fused before sequence modeling.Figure 2.Gated Fusion Module with Transformer and aspect embeddings based on dynamic element-wise gating and aspect attention weighting to create semantic representations that are robust, aspect-aware, and sensitive to attention.Adaptive weighting: The model is trained to apply BERT or RoBERTa features based on the token and context.Aspect-conscious integration: The fusion is still biased towards the opinion target but not generic sentiment by incorporating the aspect embedding in the gating function.Redundancy minimization: Gating filters rather than concatenating both embeddings results in redundancy minimization.Interpretability: Gating scores are visualizable to learn which model had a more significant impact on a decision of a particular token.In this way, the Gated Fusion Layer is the semantic integration point, ensuring that the heterogenous embeddings are combined into one, aspect-sensitive representation, which is then sent to the downstream BiLSTM + Attention layer to reason sequentially. 3.7. Bidirectional Long Short-Term Memory (BiLSTM)Following gated fusion, aspect-aware embeddings(ùêªùëìùëñùëõùëéùëô)(Hfinal)are then fed into a Bidirectional Long Short-Term Memory (BiLSTM) network to capture sequential interaction and syntax in the review text. In contrast to conventional RNNs, LSTMs have gating mechanisms, which reduce vanishing gradients, allowing them to learn long-term contextual dependencies. The two-way variant also implies that the past (left context) and future (right context) information were coded at the same time. In the abstract, provided the fused input sequence:ùêªùëìùëñùëõùëéùëô=ùêªùëìùëñùëõùëéùëô1,ùêªùëìùëñùëõùëéùëô2,‚Ä¶,ùêªùëìùëñùëõùëéùëôùëõ,ùêªùëìùëñùëõùëéùëôùëñ‚àà‚Ñùùëë,Hfinal=H1final,H2final,‚Ä¶,Hnfinal,Hifinal‚ààRd,(14)The forward and backward LSTMs calculate hidden states as:‚Ñé‚Üíùëñ=LSTMùëì(ùêªùëìùëñùëõùëéùëô√óùëñ,‚Ñé‚Üí√óùëñ‚àí1),h‚Üíi=LSTMf(Hfinal√ói,h‚Üí√ói‚àí1),(15)‚Ñé‚Üêùëñ=LSTMùëè(ùêªùëìùëñùëõùëéùëô√óùëñ,‚Ñé‚Üê√óùëñ+1),h‚Üêi=LSTMb(Hfinal√ói,h‚Üê√ói+1),(16)In which(‚Ñé‚Üíùëñ)(h‚Üíi)represents the encoding of information starting at the start until token i, and(‚Ñé‚Üêùëñ)(h‚Üêi)represents the encoding of information starting at the end, all the way back to token i.The two-directional hidden states are combined to obtain the BiLSTM representation at that time step.‚Ñéùëñ=[‚Ñé‚Üíùëñ;‚Ñé‚Üêùëñ],‚Ñéùëñ‚àà‚Ñù2ùëë.hi=[h‚Üíi;h‚Üêi],hi‚ààR2d.(17)In this way, the BiLSTM generates the sequence of outputs.ùêªùêµùëñùêøùëÜùëáùëÄ=‚Ñé1,‚Ñé2,‚Ä¶,‚Ñéùëõ,ùêªùêµùëñùêøùëÜùëáùëÄ‚àà‚Ñùùëõ√ó2ùëë.HBiLSTM=h1,h2,‚Ä¶,hn,HBiLSTM‚ààRn√ó2d.(18)The importance of BiLSTM in ABSA.Sequential reasoning: Reasoning between opinion words at different distances, for example ‚Äúnot good‚Äù.Aspect alignment: The model captures the aspect-conditioned sentiment by learning a contextual flow around the aspect that involves the aspect preceding context and the aspect succeeding context.Complementary to transformers: Transformers are more effective at contextualizing on a global scale, whereas BiLSTM strengthens local, position-sensitive dependencies, which prove particularly useful in aspect-based tasks.The structure of the architecture is based on a combination of deep contextual embeddings (BERT, RoBERTa) and sequential structure modeling (BiLSTM) to provide semantic richness and contextual accuracy to support strong sentiment classification. 3.8. Hierarchical Attention Mechanism (HAM)The proposed model uses a HAM module to overcome the necessity of refining the BiLSTM outputs and allowing the aspect-based focus. In comparison to traditional single-layer attention, HAM incorporates three complementary views: word-level, position-level, and aspect-level attention, such that the sentiment decision does not simply rely on the token semantics but also the location of words and their connection to the target aspect. Therefore, here,Figure 3illustrates the Multi-level Attention Process HAM module, where word-level attention brings out aspect-relevant tokens, and sentence-level attention combines these representations into aspect-specific contextual vectors. Position-conscious weighting is an additional refinement of attention that gives higher emphasis on tokens that are close to the aspect term, which maximizes interpretability and sentiment discrimination.Figure 3.Multi-level attention process represented by the Hierarchical Attention Mechanism (HAM) module, which shows word-level and sentence-level attention with position-sensitive weighting to give stress on aspect-relevant tokens and create interpretable, aspect-specific contextual representations.3.8.1. Word-Level AttentionAt this stage, the model allocates the weight of importance to each contextual hidden state(‚Ñéùëñ)(hi)of the BiLSTM, where examples of our sentiment-bearing words are given, and excellent, poor, and irrelevant tokens are suppressed.ùõºùëñ=exp(‚Ñé‚ä§ùëñùëäùë§ùë¢ùë§)‚àëùëõùëò=1exp(‚Ñé‚ä§ùëòùëäùë§ùë¢ùë§)Œ±i=exp(hi‚ä§Wwuw)‚àëk=1nexp(hk‚ä§Wwuw)(19)where(ùëäùë§)(Ww)is a trainable projection matrix and(ùë¢ùë§)(uw)is a word-level context vector. The summary representation is:‚Ñéùë§ùëúùëüùëë=‚àëùëñ=1ùëõùõºùëñ‚Ñéùëñhword=‚àëi=1nŒ±ihi(20)This makes sure that semantic salience is saved from the sequence of reviews.3.8.2. Position-Level AttentionThe significance of words is not enough in ABSA, because in many cases, the sentiment polarity is determined by the relative closeness to the aspect. The position-level attention, therefore, brings in distance-conscious weighting, where the tokens nearer to the aspect have more weight. At position (i) and aspect centered at position (j):ùëùùëñ=11+|ùëñ‚àíùëó|pi=11+|i‚àíj|(21)Then, a normalized position attention score is used:ùõΩùëñ=exp(ùëùùëñ¬∑(‚Ñé‚ä§ùëñùëäùëùùë¢ùëù))‚àëùëõùëò=1exp(ùëùùëò¬∑(‚Ñé‚ä§ùëòùëäùëùùë¢ùëù))Œ≤i=exp(pi¬∑(hi‚ä§Wpup))‚àëk=1nexp(pk¬∑(hk‚ä§Wpup))(22)and the position-sensitive representation is:‚Ñéùëùùëúùë†=‚àëùëñ=1ùëõùõΩùëñ‚Ñéùëñhpos=‚àëi=1nŒ≤ihi(23)This level makes sure that the model puts tokens in close proximity to the aspect (e.g., in, ‚ÄúThe camera quality of this phone is outstanding,‚Äù the phrase ‚Äúoutstanding‚Äù must be more closely associated with ‚Äúcamera‚Äù than with words far away).3.8.3. Aspect-Level AttentionThe last attention layer is used to align review tokens with the specific aspect representationùê∏ùëéEato yield aspect-conditioned sentiment.ùõæùëñ=exp((‚Ñé‚ä§ùëñùëäùëéùê∏ùëé))‚àëùëõùëò=1exp((‚Ñé‚ä§ùëòùëäùëéùê∏ùëé))Œ≥i=exp((hi‚ä§WaEa))‚àëk=1nexp((hk‚ä§WaEa))(24)The Aggregated aspect-aware context vector is then:‚Ñéùëéùë†ùëùùëíùëêùë°=‚àëùëñ=1ùëõùõæùëñ‚Ñéùëñhaspect=‚àëi=1nŒ≥ihi(25)This is so as to ensure that the sentiment is directly conditioned on the target aspect, without misclassification due to irrelevant aspects in multi-aspect reviews.3.8.4. Final Hierarchical Attention RepresentationThe three-level outputs are added together to obtain the hierarchical context vector:‚Ñé‚àó=ùëäùëê[‚Ñéùë§ùëúùëüùëë;‚Ñéùëùùëúùë†;‚Ñéùëéùë†ùëùùëíùëêùë°]+ùëèùëêh*=Wc[hword;hpos;haspect]+bc(26)Now here(ùëäùëê)(Wc)and(ùëèùëê)(bc)are trainable parameters 3.9. Final Classification and OptimizationOnce the hierarchical context representation‚Ñé‚àóh*has been obtained by the Hierarchical Attention Mechanism, the model goes to the final sentiment prediction step. This step converts the multi-level representation, which is very rich, to a discrete sentiment label using a fully connected projection and a probabilistic classification layer.3.9.1. Linear Projection and Softmax ClassificationThe resultant attention-derived feature‚Ñé‚àóh*is concatenated and then subjected to a linear layer transformation that projects it into a subspace specific to the sentiment. This transformation captures the high-level abstractions in the word, position, and aspect-level attention.ùëß=ùëäùëú‚Ñé‚àó+ùëèùëúz=Woh*+bo(27)whereùëäùëúWoandùëèùëúboare the output weight matrix and bias vector, respectively. Logits z are then normalized by the SoftMax function to generate a probability distribution of the sentiment classes.ùë¶ÃÇ√óùëñ=exp(ùëßùëñ)‚àë√óùëò=1ùê∂exp(ùëßùëò),ùëñ=1,2,‚Ä¶,ùê∂y^√ói=exp(zi)‚àë√ók=1Cexp(zk),i=1,2,‚Ä¶,C(28)In this case,Crefers to the total amount of sentiment categories (e.g., 5 of fine-grained sentiment: very negative, negative, neutral, positive, and very positive). The probability of the most likely class is chosen as the predicted sentiment.3.9.2. Regularization and Focal Loss OptimizationTo reduce overfitting and increase generalization,(‚Ñé‚àó)(h*)dropout is regularized before classification. The dropout randomly kills neurons in training, making it resistant to noise, and it is not dependent on particular features. Since the example of the sentiment datasets usually has an imbalance in classes, such as more neutral reviews than extremely positive or negative ones, the model uses Focal Loss, rather than the conventional cross-entropy. Focal Loss is a dynamic weight, or scaling, loss that learns to place more emphasis on the learning of the harder, misclassified samples. It is defined as‚Ñíùëìùëúùëêùëéùëô=‚àíùõºùë°(1‚àíùë¶ÃÇùë°)ùõælog(ùë¶ÃÇùë°)Lfocal=‚àíŒ±t(1‚àíy^t)Œ≥log(y^t)(29)where(ùõºùë°)(Œ±t)is the weighting factor for class(ùë°)(t),(ùõæ)(Œ≥)is the focusing parameter controlling difficulty emphasis,(ùë¶ÃÇùë°)(y^t)is the predicted probability for the true class.This adaptive loss promotes equal learning among the categories of sentiment and guarantees better performance on underrepresented labels.3.9.3. Training StrategyThe Adam optimizer is applied with a learning rate to optimize the AGF-HAM model parameters, and with the help of adaptive moment estimation, it should converge steadily. Early stopping is used to avoid overtraining, with the basis of validation loss. The total training goal reduces the amount of focal loss of all samples:‚Ñí√óùë°ùëúùë°ùëéùëô=1ùëÅ‚àë√óùëñ=1ùëÅ‚Ñíùëìùëúùëêùëéùëô(ùë¶ÃÇùëñ,ùë¶ùëñ)L√ótotal=1N‚àë√ói=1NLfocal(y^i,yi)(30)In this step, the learned hierarchical and gated representations are incorporated into a small, interpretable, and accurate sentiment prediction that is both accurate and interpretable. 3.10. Explainability ModuleAn Explainability Module Layer is incorporated as the last step of the HAM framework in order to guarantee interpretability and transparency of the final model predictions. The layer makes the process of forming the sentiments‚Äô decisions globally and locally interpretable by showing how the model forms its conclusion based on linguistic and contextual knowledge. The explainability module is based on three complementary sub-layers‚ÄîAttention Heatmaps, Aspect-Level Explanations, and SHAP Integrated Gradients‚Äîthat have a specific analytical purpose to hold models accountable and explainable to humans.3.10.1. Attention HeadmapsThe Attention Headmaps sub-layer shows token-level scores of the importance of the attention heads in the Transformer encoder and BiLSTM layers. It puts emphasis on particular words, phrases, or clauses that have a strong impact on the sentiment or emotional polarity of a certain text. This visualization, in addition to showing the focus distribution of the model, also proves the interpretive reliability of the attention mechanism. With the help of these heatmaps, researchers and practitioners can confirm that the model focuses on semantically significant areas and not accidental associations.ùõºùëñ=exp(ùë£‚ä§tanh(ùëä‚Ñé‚Ñéùëñ+ùëäùëûùëû+ùëè))‚àëùëõùëó=1exp(ùë£‚ä§tanh(ùëä‚Ñé‚Ñéùëó+ùëäùëûùëû+ùëè))Œ±i=expv‚ä§tanh(Whhi+Wqq+b)‚àëj=1nexpv‚ä§tanh(Whhj+Wqq+b)(31)This calculates the attention weight of each tokent, and this is the degree to which that token is relevant to the query (aspect or sentence context). It involves a SoftMax normalization such that all weights of attention have a sum of 1.ùêªatt(ùë°ùëñ)=ùõºùëñforeachtokenùë°ùëñ.Hatt(ti)=Œ±iforeachtokenti.(32)This maps the attention weight to the underlying token, which compose the raw headmap of token importance in the text. It graphically shows the words that make the greatest contribution to sentiment decisions.Headmapnorm(ùë°ùëñ)=ùõºùëñ‚àíminùëóùõºùëómaxùëóùõºùëó‚àíminùëóùõºùëó.Headmapnorm(ti)=Œ±i‚àíminjŒ±jmaxjŒ±j‚àíminjŒ±j.(33)The weights between the attention and the visualization of the headmap are normalized to 0 or 1 to form a visual representation of the headmap. It guarantees a similar visualization of samples to make them easier to interpret.3.10.2. Aspect-Level ExplanationThe sub-layer of Aspect-Level Explanation gives a fine-grained interpretability, where sentiment weights are mapped to the identified aspects in the text. This sub-layer explains the roles of each product or emotional aspect in the final sentiment classification using dependency-based aspect extraction and context embeddings. It can be used to provide transparency in aspect-aware decision-making and bridge the semantic connection between aspect terms (battery, camera, service) and the contextual sentiments. This sub-layer is specifically useful when the task at hand demands domain interpretability, like reviewing or detecting emotions in social situations.ùõº(ùëé)ùëñ=exp(ùë£‚ä§ùëétanh(ùëä‚Ñé‚Ñéùëñ+ùëäùëéùëíùëé+ùëèùëé))‚àëùëõùëó=1exp(ùë£‚ä§ùëétanh(ùëä‚Ñé‚Ñéùëó+ùëäùëéùëíùëé+ùëèùëé))Œ±i(a)=expva‚ä§tanh(Whhi+Waea+ba)‚àëj=1nexpva‚ä§tanh(Whhj+Waea+ba)(34)This computes aspect-conditioned attention, in whichùëíùëéeais the embedding of a given aspect (e.g., ‚Äúbattery‚Äù or ‚Äúservice‚Äù) It helps the model to pay attention to the most relevant words to that aspect.ùëêùëé=‚àëùëñ=1ùëõùõº(ùëé)ùëñ,‚Ñéùëñ(aspect-awarecontextvector)ca=‚àëi=1nŒ±i(a),hi(aspect-awarecontextvector)(35)In this case, the hidden states are used to create a weighted context vector, which is a mixture of the hidden states with aspect-specific attention weights. It is a compressed feeling indicator of that specific element.ùë†(ùëò)ùëé=softmaxùëò(ùëä(ùëò)ùëúùëêùëé+ùëè(ùëò)ùëú)(classprobabilityforaspectùëé)sa(k)=softmaxk(Wo(k)ca+bo(k))(classprobabilityforaspecta)(36)This is to apply sentiment polarity prediction as an aspectawith all classes using a softmax layer. It produces probabilities of the form of positive, neutral, or negative sentiment.TopTokens√óùëé=argsort√óùëñ(ùõº(ùëé)ùëñ),top‚àíùëö.TopTokens√óa=argsort√óiŒ±i(a),top-m.(37)This is used to select the most influential tokens that assist in sentiment on aspecta. These tokens are used to produce aspect-level textual explanations.3.10.3. SHAP/Integrated GradientsThe SHAP Integrated Gradients sub-layer is a combination of SHapley Additive exPlanations (SHAP) with Integrated Gradients (IG) used to measure the contribution of each feature to the output of a model. This hybrid interpretability method combines model sensitivity and feature attribution consistency into unified importance maps that are complementary to the attention-based interpretations. This sub-layer is a combination of SHAP and IG and is able to provide a coherent and consistent explanation of decision boundaries, which strengthens the transparency and reliability of the model.ùúôùëñ=‚àëùëÜ‚äÜùëÅ‚àñùëñ|ùëÜ|!,(|ùëÅ|‚àí|ùëÜ|‚àí1)!|ùëÅ|!‚éõ‚éù‚éú‚éú‚éúùêπùëÜ‚à™ùëñ(ùë•ùëÜ‚à™ùëñ)‚àíùêπùëÜ(ùë•ùëÜ)‚éû‚é†‚éü‚éü‚éüœïi=‚àëS‚äÜN‚àñi|S|!,(|N|‚àí|S|‚àí1)!|N|!(FS‚à™i(xS‚à™i)‚àíFS(xS))(38)This is the Shapley value equation, which is used to measure the contribution of a given feature to the prediction of the model. It assesses the effect of the addition of a feature (word/token) on model output on all the subsets.‚àëùëñ=1ùëõùúôùëñ=ùêπ(ùë•)‚àíùêπ(ùë•baseline)‚àëi=1nœïi=F(x)‚àíF(xbaseline)(39)SHAP values can be summed up to form the difference between baseline and model prediction. This guarantees additive attribution of features, which ensures consistency of interpretability.IGùëñ(ùë•)=(ùë•ùëñ‚àíùë•‚Ä≤√óùëñ)‚à´√óùõº=01‚àÇùêπ(ùë•‚Ä≤+ùõº(ùë•‚àíùë•‚Ä≤))‚àÇùë•ùëñ,ùëëùõºIGi(x)=(xi‚àíx‚Ä≤√ói)‚à´√óŒ±=01‚àÇFx‚Ä≤+Œ±(x‚àíx‚Ä≤)‚àÇxi,dŒ±(40)Integrated Gradients calculate the contribution of each input feature to the output of a model by summing the gradients between the path taken between a baselineùë•‚Ä≤x‚Ä≤and the real input x. It offers path-sensitive interpretability that is smooth and has less noise than raw gradients.To illustrate the interpretability of the proposed explainability module, we provide examples of each of the sentiment categories: very positive, positive, neutral, negative, and very negative. A heatmap visualization inFigure 4is provided per sentence to show the distribution of the attention weights of the individual tokens, i.e., the focus of the model when inferring the sentiment. The darker the region of the heatmap, the greater the intensity of attention of the token is, and the greater its influence on the final prediction of the model will be.Figure 4.Heatmap visualization showing token-level attention distribution for sentences across five sentiment categories. Darker shades represent higher attention weights to the model‚Äôs sentiment prediction.Aspect-Level Summary:Detected Aspect:phone/performanceOpinion Tokens:awful, slow, crashes, uselessAspect-Sentiment Score:‚àí0.93‚àí0.93Aspect Polarity Label:Very NegativeFinal Interpretation:The model classifies this sentence asVery Negative. Attention, Aspect-level Polarity, and SHAP/IG all emphasize the same tokens:awful,useless, andcrashes‚Äîindicating they are the strongest contributors to the negative sentiment toward thephone‚Äôs performance.Besides visual interpretation, there is also elaborate explainability of one representative sentence inTable 2andTable 3. The table is a quantitative report on the attention weight, aspect-level polarity, SHAP value, and Integrated Gradient (IG) contribution of each token. These three measures are then normalized and averaged to obtain the combined importance score to reflect the contextual and causal influence on the decision of the model. This integrated analysis would allow not only the explainability framework to identify words that carry sentiment (e.g., awful, useless, excellent) but also clarify their relationship with certain aspects in context, allowing the transparent and understandable interpretation of how the model reaches its classification.Table 2.Explainability Table for Sentence: ‚ÄúThis phone is awful slow crashes often today useless‚Äù.Table 3.Definition and Formula of Each Explainability Metric.The combination of these three sub-layers gives HAM not only superior performance in predictive mode but also a high level of interpretive ability. This Explainability Module Layer converts the HAM framework to more of an intelligent system that can be trusted and interpreted by humans, having the ability to provide the answer to why and how decisions are made through complex sentiment and emotional environments.",
            "3.1. Input Layer": "The model takes two kinds of inputs; one of them is the review text, which contains the sentiment-bearing content, and the other one is the aspect term, which determines the point of analysis. This format is a two-input structure that is essential to Aspect-Based Sentiment Analysis (ABSA), which makes sure that no predictions are made on the entire review but on the aspect itself. We start with a review textRand the corresponding aspectA.ùëÖ=(ùë§1,‚Ä¶,ùë§ùëõ),ùê¥=(ùëé1,‚Ä¶,ùëéùëö).R=(w1,‚Ä¶,wn),A=(a1,‚Ä¶,am).(1)",
            "3.2. Preprocessing and Tokenization": "Raw review text and aspect terms are initially normalized with the removal of noise (punctuation, case sensitivity errors, and unnecessary spaces) and later tokenized with BERT and RoBERTa subword tokenizers. This will be done to ensure that it is compatible with existing contextual encoders and that semantic and syntactic data are retained to support aspect-sensitive representation. PLM tokenizers (WordPiece/BPE) generate subword tokens that are compatible with BERT/RoBERTa and allow the computation of spans that are correct to position them.ùëá=Tokenize√óBERT(ùëÖ)=(ùë°1,‚Ä¶,ùë°ùëá),ùëá‚Ä≤=Tokenize√óRoBERTa(ùëÖ).T=Tokenize√óBERT(R)=(t1,‚Ä¶,tT),T‚Ä≤=Tokenize√óRoBERTa(R).(2) Map aspect tokens to token span indices(‚Ñêùëé‚äÜ1,‚Ä¶,ùëá)(Ia‚äÜ1,‚Ä¶,T)(3)",
            "3.3. Aspect Extraction": "Aspects are mined in a hybrid fashion that integrates both rule-based linguistic techniques with dependence-based heuristics. This step is necessary to guarantee that the terms of aspects that were relevant (e.g., product features or service attributes) are clearly defined. The model aims to isolate the aspects of the entire review text in favor of the fine-grained opinion targets instead of having to use only coarse-grained sentiment indications. This layer selects aspect (A) and its token span. Correct span detection makes position-related attention and aspect pooling correct.(‚Ñêùëé=ùëñùë†,‚Ä¶,ùëñùëí)(Ia=is,‚Ä¶,ie)(4)",
            "3.4. Token and Position Embeddings": "After the review tokens and aspect tokens are received, all tokens are projected to a dense space of vectors with pretrained contextual encoders. Specifically,(ùëÖ=ùë§1,ùë§2,‚Ä¶,ùë§ùëõ)(R=w1,w2,‚Ä¶,wn)is a sequence of tokens, each of which is converted to a contextual embedding(ùë§ùëñ)(wi)using BERT and RoBERTa. In the same way, the aspect sequence(ùëÜ=ùëé1,ùëé2,‚Ä¶,ùëéùëö)(S=a1,a2,‚Ä¶,am)is expressed as aspect embeddings(ùê∏ùëéùëó)(Eaj). Positional embeddings are introduced in order to save word order information. After the Transformer formulation, the embedding of the input of the ith review token is defined as:ùëãùëñ=ùê∏ùë§ùëñ+ùëÉùëñ,forùëñ‚àà[1,ùëõ]Xi=Ewi+Pi,fori‚àà[1,n](5) (ùê∏ùë§ùëñ)(Ewi)is the embedding of the contextual token and(ùëÉùëñ)(Pi)is the embedding of sequential order. Likewise, position-aware embeddings are added to aspect tokensùëãùëéùëó=ùê∏ùëéùëó+ùëÉùëéùëó,forùëó‚àà[1,ùëö]Xaj=Eaj+Paj,forj‚àà[1,m](6) The embedding scheme guarantees that the model learns the semantic meaning of the token and their relative position with regard to the aspect, which is imperative in ABSA. The model is sensitive to what and where is said in the review by being an integration of position encodings and contextual embeddings.",
            "3.5. Linear Projection": "In order to have compatibility between contextual embeddings and aspect embeddings, we use a linear projection layer that transforms each of the representations to one common latent space of dimension d. Review embeddings with BERT and RoBERTa:ùêªùêµùê∏ùëÖùëá‚àóùëñ=ùëä‚àóùêµùê∏ùëÖùëáùê∏ùêµùê∏ùëÖùëá‚àóùë§ùëñ+ùëè‚àóùêµùê∏ùëÖùëá,ùëñ‚àà[1,ùëõ]HBERT‚àói=W‚àóBERTEBERT‚àówi+b‚àóBERT,i‚àà[1,n](7)ùêªùëÖùëúùêµùê∏ùëÖùëáùëé‚àóùëñ=ùëä‚àóùëÖùëúùêµùê∏ùëÖùëáùëéùê∏ùëÖùëúùêµùê∏ùëÖùëáùëé‚àóùë§ùëñ+ùëè‚àóùëÖùëúùêµùê∏ùëÖùëáùëé,ùëñ‚àà[1,ùëõ]HRoBERTa‚àói=W‚àóRoBERTaERoBERTa‚àówi+b‚àóRoBERTa,i‚àà[1,n](8)and to the aspect of aggregation adding.ùêªùëé=ùëäùëéùê∏ùëé+ùëèùëé.Ha=WaEa+ba.(9) In this case,(ùëäùêµùê∏ùëÖùëá,ùëäùëÖùëúùêµùê∏ùëÖùëáùëé,ùëäùëé‚àà‚Ñùùëë√óùëë‚Ä≤)(WBERT,WRoBERTa,Wa‚ààRd√ód‚Ä≤)are learnable projection matrices and b terms are bias vectors. This forecast is such that: It is assumed that all embeddings((ùêªùêµùê∏ùëÖùëá,ùêªùëÖùëúùêµùê∏ùëÖùëáùëé,ùêªùëé))((HBERT,HRoBERTa,Ha))are in the same semantic space of dimension d.The representations are then comparable directly and can be successfully fused in the next Gated Fusion Layer.All unnecessary differences between models BERT vs RoBERTa are averaged, and complementary features are kept. Therefore, a linear projection serves as a semantic alignment proxy, which progresses the embeddings towards adaptive combination in the gated fusion process.",
            "3.6. Gated Fusion Layer": "BERT and RoBERTa are complementary in offering contextual representations, but when they are simply concatenated, their results usually include redundancy and suboptimal integration. We propose solving this issue with a Gated Fusion Layer that actively regulates the input of each embedding stream (BERT, RoBERTa, and Aspect) through learning task-specific gating parameters. Formally, assume that the linearly projected embeddings are.ùêªùêµùê∏ùëÖùëá‚àà‚Ñùùëõ√óùëë,ùêªùëÖùëúùêµùê∏ùëÖùëáùëé‚àà‚Ñùùëõ√óùëë,ùêªùëé‚àà‚Ñùùëë.HBERT‚ààRn√ód,HRoBERTa‚ààRn√ód,Ha‚ààRd.(10) A gating mechanism is used to dynamically down-weight the significance of BERT and RoBERTa embeddings at each token position:ùê∫ùëñ=ùúé(ùëäùëî[ùêªùêµùê∏ùëÖùëáùëñ;ùêªùëÖùëúùêµùê∏ùëÖùëáùëéùëñ;ùêªùëé]+ùëèùëî),ùëñ‚àà[1,ùëõ],Gi=œÉ(Wg[HiBERT;HiRoBERTa;Ha]+bg),i‚àà[1,n],(11) And where(ùëäùëî‚àà‚Ñùùëë√ó3ùëë)ùëéùëõùëë(ùëèùëî‚àà‚Ñùùëë)(Wg‚ààRd√ó3d)and(bg‚ààRd)are trainable parameters,([¬∑])([¬∑])is concatenation, and(ùúé(¬∑))(œÉ(¬∑))is the sigmoid function. The fused representation is then calculated as:ùêªùëìùë¢ùë†ùëñùëúùëõùëñ=ùê∫ùëñ‚äôùêªùêµùê∏ùëÖùëáùëñ+(1‚àíùê∫ùëñ)‚äôùêªùëÖùëúùêµùê∏ùëÖùëáùëéùëñ,Hifusion=Gi‚äôHiBERT+(1‚àíGi)‚äôHiRoBERTa,(12)and lastly trained in the aspect embedding by:ùêªùëìùëñùëõùëéùëôùëñ=ùêªùëìùë¢ùë†ùëñùëúùëõùëñ‚äïùêªùëé,Hifinal=Hifusion‚äïHa,(13)where(‚äô)(‚äô)is defined as element-wise multiplication and(‚äï)(‚äï)is defined as concatenation of vectors. Here are some key features of Gated Fusion. NowFigure 2illustrates the Gated Fusion Model, where the Gated Fusion Module shows how to integratively introduce Transformer-based embeddings (e.g., BERT and RoBERTa) and explicit aspect embeddings. Contributions of each source are balanced dynamically by the gating layer by means of element-wise gating and attention weighting to ensure aspect-aware semantic representations are context-sensitive, robust, and optimally fused before sequence modeling. Figure 2.Gated Fusion Module with Transformer and aspect embeddings based on dynamic element-wise gating and aspect attention weighting to create semantic representations that are robust, aspect-aware, and sensitive to attention. Adaptive weighting: The model is trained to apply BERT or RoBERTa features based on the token and context.Aspect-conscious integration: The fusion is still biased towards the opinion target but not generic sentiment by incorporating the aspect embedding in the gating function.Redundancy minimization: Gating filters rather than concatenating both embeddings results in redundancy minimization.Interpretability: Gating scores are visualizable to learn which model had a more significant impact on a decision of a particular token. In this way, the Gated Fusion Layer is the semantic integration point, ensuring that the heterogenous embeddings are combined into one, aspect-sensitive representation, which is then sent to the downstream BiLSTM + Attention layer to reason sequentially.",
            "3.7. Bidirectional Long Short-Term Memory (BiLSTM)": "Following gated fusion, aspect-aware embeddings(ùêªùëìùëñùëõùëéùëô)(Hfinal)are then fed into a Bidirectional Long Short-Term Memory (BiLSTM) network to capture sequential interaction and syntax in the review text. In contrast to conventional RNNs, LSTMs have gating mechanisms, which reduce vanishing gradients, allowing them to learn long-term contextual dependencies. The two-way variant also implies that the past (left context) and future (right context) information were coded at the same time. In the abstract, provided the fused input sequence:ùêªùëìùëñùëõùëéùëô=ùêªùëìùëñùëõùëéùëô1,ùêªùëìùëñùëõùëéùëô2,‚Ä¶,ùêªùëìùëñùëõùëéùëôùëõ,ùêªùëìùëñùëõùëéùëôùëñ‚àà‚Ñùùëë,Hfinal=H1final,H2final,‚Ä¶,Hnfinal,Hifinal‚ààRd,(14) The forward and backward LSTMs calculate hidden states as:‚Ñé‚Üíùëñ=LSTMùëì(ùêªùëìùëñùëõùëéùëô√óùëñ,‚Ñé‚Üí√óùëñ‚àí1),h‚Üíi=LSTMf(Hfinal√ói,h‚Üí√ói‚àí1),(15)‚Ñé‚Üêùëñ=LSTMùëè(ùêªùëìùëñùëõùëéùëô√óùëñ,‚Ñé‚Üê√óùëñ+1),h‚Üêi=LSTMb(Hfinal√ói,h‚Üê√ói+1),(16) In which(‚Ñé‚Üíùëñ)(h‚Üíi)represents the encoding of information starting at the start until token i, and(‚Ñé‚Üêùëñ)(h‚Üêi)represents the encoding of information starting at the end, all the way back to token i. The two-directional hidden states are combined to obtain the BiLSTM representation at that time step.‚Ñéùëñ=[‚Ñé‚Üíùëñ;‚Ñé‚Üêùëñ],‚Ñéùëñ‚àà‚Ñù2ùëë.hi=[h‚Üíi;h‚Üêi],hi‚ààR2d.(17) In this way, the BiLSTM generates the sequence of outputs.ùêªùêµùëñùêøùëÜùëáùëÄ=‚Ñé1,‚Ñé2,‚Ä¶,‚Ñéùëõ,ùêªùêµùëñùêøùëÜùëáùëÄ‚àà‚Ñùùëõ√ó2ùëë.HBiLSTM=h1,h2,‚Ä¶,hn,HBiLSTM‚ààRn√ó2d.(18) The importance of BiLSTM in ABSA. Sequential reasoning: Reasoning between opinion words at different distances, for example ‚Äúnot good‚Äù.Aspect alignment: The model captures the aspect-conditioned sentiment by learning a contextual flow around the aspect that involves the aspect preceding context and the aspect succeeding context.Complementary to transformers: Transformers are more effective at contextualizing on a global scale, whereas BiLSTM strengthens local, position-sensitive dependencies, which prove particularly useful in aspect-based tasks. The structure of the architecture is based on a combination of deep contextual embeddings (BERT, RoBERTa) and sequential structure modeling (BiLSTM) to provide semantic richness and contextual accuracy to support strong sentiment classification.",
            "3.8. Hierarchical Attention Mechanism (HAM)": "The proposed model uses a HAM module to overcome the necessity of refining the BiLSTM outputs and allowing the aspect-based focus. In comparison to traditional single-layer attention, HAM incorporates three complementary views: word-level, position-level, and aspect-level attention, such that the sentiment decision does not simply rely on the token semantics but also the location of words and their connection to the target aspect. Therefore, here,Figure 3illustrates the Multi-level Attention Process HAM module, where word-level attention brings out aspect-relevant tokens, and sentence-level attention combines these representations into aspect-specific contextual vectors. Position-conscious weighting is an additional refinement of attention that gives higher emphasis on tokens that are close to the aspect term, which maximizes interpretability and sentiment discrimination. Figure 3.Multi-level attention process represented by the Hierarchical Attention Mechanism (HAM) module, which shows word-level and sentence-level attention with position-sensitive weighting to give stress on aspect-relevant tokens and create interpretable, aspect-specific contextual representations. 3.8.1. Word-Level AttentionAt this stage, the model allocates the weight of importance to each contextual hidden state(‚Ñéùëñ)(hi)of the BiLSTM, where examples of our sentiment-bearing words are given, and excellent, poor, and irrelevant tokens are suppressed.ùõºùëñ=exp(‚Ñé‚ä§ùëñùëäùë§ùë¢ùë§)‚àëùëõùëò=1exp(‚Ñé‚ä§ùëòùëäùë§ùë¢ùë§)Œ±i=exp(hi‚ä§Wwuw)‚àëk=1nexp(hk‚ä§Wwuw)(19)where(ùëäùë§)(Ww)is a trainable projection matrix and(ùë¢ùë§)(uw)is a word-level context vector. The summary representation is:‚Ñéùë§ùëúùëüùëë=‚àëùëñ=1ùëõùõºùëñ‚Ñéùëñhword=‚àëi=1nŒ±ihi(20)This makes sure that semantic salience is saved from the sequence of reviews. 3.8.2. Position-Level AttentionThe significance of words is not enough in ABSA, because in many cases, the sentiment polarity is determined by the relative closeness to the aspect. The position-level attention, therefore, brings in distance-conscious weighting, where the tokens nearer to the aspect have more weight. At position (i) and aspect centered at position (j):ùëùùëñ=11+|ùëñ‚àíùëó|pi=11+|i‚àíj|(21)Then, a normalized position attention score is used:ùõΩùëñ=exp(ùëùùëñ¬∑(‚Ñé‚ä§ùëñùëäùëùùë¢ùëù))‚àëùëõùëò=1exp(ùëùùëò¬∑(‚Ñé‚ä§ùëòùëäùëùùë¢ùëù))Œ≤i=exp(pi¬∑(hi‚ä§Wpup))‚àëk=1nexp(pk¬∑(hk‚ä§Wpup))(22)and the position-sensitive representation is:‚Ñéùëùùëúùë†=‚àëùëñ=1ùëõùõΩùëñ‚Ñéùëñhpos=‚àëi=1nŒ≤ihi(23)This level makes sure that the model puts tokens in close proximity to the aspect (e.g., in, ‚ÄúThe camera quality of this phone is outstanding,‚Äù the phrase ‚Äúoutstanding‚Äù must be more closely associated with ‚Äúcamera‚Äù than with words far away). 3.8.3. Aspect-Level AttentionThe last attention layer is used to align review tokens with the specific aspect representationùê∏ùëéEato yield aspect-conditioned sentiment.ùõæùëñ=exp((‚Ñé‚ä§ùëñùëäùëéùê∏ùëé))‚àëùëõùëò=1exp((‚Ñé‚ä§ùëòùëäùëéùê∏ùëé))Œ≥i=exp((hi‚ä§WaEa))‚àëk=1nexp((hk‚ä§WaEa))(24)The Aggregated aspect-aware context vector is then:‚Ñéùëéùë†ùëùùëíùëêùë°=‚àëùëñ=1ùëõùõæùëñ‚Ñéùëñhaspect=‚àëi=1nŒ≥ihi(25)This is so as to ensure that the sentiment is directly conditioned on the target aspect, without misclassification due to irrelevant aspects in multi-aspect reviews. 3.8.4. Final Hierarchical Attention RepresentationThe three-level outputs are added together to obtain the hierarchical context vector:‚Ñé‚àó=ùëäùëê[‚Ñéùë§ùëúùëüùëë;‚Ñéùëùùëúùë†;‚Ñéùëéùë†ùëùùëíùëêùë°]+ùëèùëêh*=Wc[hword;hpos;haspect]+bc(26)Now here(ùëäùëê)(Wc)and(ùëèùëê)(bc)are trainable parameters",
            "3.8.1. Word-Level Attention": "At this stage, the model allocates the weight of importance to each contextual hidden state(‚Ñéùëñ)(hi)of the BiLSTM, where examples of our sentiment-bearing words are given, and excellent, poor, and irrelevant tokens are suppressed.ùõºùëñ=exp(‚Ñé‚ä§ùëñùëäùë§ùë¢ùë§)‚àëùëõùëò=1exp(‚Ñé‚ä§ùëòùëäùë§ùë¢ùë§)Œ±i=exp(hi‚ä§Wwuw)‚àëk=1nexp(hk‚ä§Wwuw)(19)where(ùëäùë§)(Ww)is a trainable projection matrix and(ùë¢ùë§)(uw)is a word-level context vector. The summary representation is:‚Ñéùë§ùëúùëüùëë=‚àëùëñ=1ùëõùõºùëñ‚Ñéùëñhword=‚àëi=1nŒ±ihi(20) This makes sure that semantic salience is saved from the sequence of reviews.",
            "3.8.2. Position-Level Attention": "The significance of words is not enough in ABSA, because in many cases, the sentiment polarity is determined by the relative closeness to the aspect. The position-level attention, therefore, brings in distance-conscious weighting, where the tokens nearer to the aspect have more weight. At position (i) and aspect centered at position (j):ùëùùëñ=11+|ùëñ‚àíùëó|pi=11+|i‚àíj|(21) Then, a normalized position attention score is used:ùõΩùëñ=exp(ùëùùëñ¬∑(‚Ñé‚ä§ùëñùëäùëùùë¢ùëù))‚àëùëõùëò=1exp(ùëùùëò¬∑(‚Ñé‚ä§ùëòùëäùëùùë¢ùëù))Œ≤i=exp(pi¬∑(hi‚ä§Wpup))‚àëk=1nexp(pk¬∑(hk‚ä§Wpup))(22)and the position-sensitive representation is:‚Ñéùëùùëúùë†=‚àëùëñ=1ùëõùõΩùëñ‚Ñéùëñhpos=‚àëi=1nŒ≤ihi(23) This level makes sure that the model puts tokens in close proximity to the aspect (e.g., in, ‚ÄúThe camera quality of this phone is outstanding,‚Äù the phrase ‚Äúoutstanding‚Äù must be more closely associated with ‚Äúcamera‚Äù than with words far away).",
            "3.8.3. Aspect-Level Attention": "The last attention layer is used to align review tokens with the specific aspect representationùê∏ùëéEato yield aspect-conditioned sentiment.ùõæùëñ=exp((‚Ñé‚ä§ùëñùëäùëéùê∏ùëé))‚àëùëõùëò=1exp((‚Ñé‚ä§ùëòùëäùëéùê∏ùëé))Œ≥i=exp((hi‚ä§WaEa))‚àëk=1nexp((hk‚ä§WaEa))(24) The Aggregated aspect-aware context vector is then:‚Ñéùëéùë†ùëùùëíùëêùë°=‚àëùëñ=1ùëõùõæùëñ‚Ñéùëñhaspect=‚àëi=1nŒ≥ihi(25) This is so as to ensure that the sentiment is directly conditioned on the target aspect, without misclassification due to irrelevant aspects in multi-aspect reviews.",
            "3.8.4. Final Hierarchical Attention Representation": "The three-level outputs are added together to obtain the hierarchical context vector:‚Ñé‚àó=ùëäùëê[‚Ñéùë§ùëúùëüùëë;‚Ñéùëùùëúùë†;‚Ñéùëéùë†ùëùùëíùëêùë°]+ùëèùëêh*=Wc[hword;hpos;haspect]+bc(26) Now here(ùëäùëê)(Wc)and(ùëèùëê)(bc)are trainable parameters",
            "3.9. Final Classification and Optimization": "Once the hierarchical context representation‚Ñé‚àóh*has been obtained by the Hierarchical Attention Mechanism, the model goes to the final sentiment prediction step. This step converts the multi-level representation, which is very rich, to a discrete sentiment label using a fully connected projection and a probabilistic classification layer. 3.9.1. Linear Projection and Softmax ClassificationThe resultant attention-derived feature‚Ñé‚àóh*is concatenated and then subjected to a linear layer transformation that projects it into a subspace specific to the sentiment. This transformation captures the high-level abstractions in the word, position, and aspect-level attention.ùëß=ùëäùëú‚Ñé‚àó+ùëèùëúz=Woh*+bo(27)whereùëäùëúWoandùëèùëúboare the output weight matrix and bias vector, respectively. Logits z are then normalized by the SoftMax function to generate a probability distribution of the sentiment classes.ùë¶ÃÇ√óùëñ=exp(ùëßùëñ)‚àë√óùëò=1ùê∂exp(ùëßùëò),ùëñ=1,2,‚Ä¶,ùê∂y^√ói=exp(zi)‚àë√ók=1Cexp(zk),i=1,2,‚Ä¶,C(28)In this case,Crefers to the total amount of sentiment categories (e.g., 5 of fine-grained sentiment: very negative, negative, neutral, positive, and very positive). The probability of the most likely class is chosen as the predicted sentiment. 3.9.2. Regularization and Focal Loss OptimizationTo reduce overfitting and increase generalization,(‚Ñé‚àó)(h*)dropout is regularized before classification. The dropout randomly kills neurons in training, making it resistant to noise, and it is not dependent on particular features. Since the example of the sentiment datasets usually has an imbalance in classes, such as more neutral reviews than extremely positive or negative ones, the model uses Focal Loss, rather than the conventional cross-entropy. Focal Loss is a dynamic weight, or scaling, loss that learns to place more emphasis on the learning of the harder, misclassified samples. It is defined as‚Ñíùëìùëúùëêùëéùëô=‚àíùõºùë°(1‚àíùë¶ÃÇùë°)ùõælog(ùë¶ÃÇùë°)Lfocal=‚àíŒ±t(1‚àíy^t)Œ≥log(y^t)(29)where(ùõºùë°)(Œ±t)is the weighting factor for class(ùë°)(t),(ùõæ)(Œ≥)is the focusing parameter controlling difficulty emphasis,(ùë¶ÃÇùë°)(y^t)is the predicted probability for the true class.This adaptive loss promotes equal learning among the categories of sentiment and guarantees better performance on underrepresented labels. 3.9.3. Training StrategyThe Adam optimizer is applied with a learning rate to optimize the AGF-HAM model parameters, and with the help of adaptive moment estimation, it should converge steadily. Early stopping is used to avoid overtraining, with the basis of validation loss. The total training goal reduces the amount of focal loss of all samples:‚Ñí√óùë°ùëúùë°ùëéùëô=1ùëÅ‚àë√óùëñ=1ùëÅ‚Ñíùëìùëúùëêùëéùëô(ùë¶ÃÇùëñ,ùë¶ùëñ)L√ótotal=1N‚àë√ói=1NLfocal(y^i,yi)(30)In this step, the learned hierarchical and gated representations are incorporated into a small, interpretable, and accurate sentiment prediction that is both accurate and interpretable.",
            "3.9.1. Linear Projection and Softmax Classification": "The resultant attention-derived feature‚Ñé‚àóh*is concatenated and then subjected to a linear layer transformation that projects it into a subspace specific to the sentiment. This transformation captures the high-level abstractions in the word, position, and aspect-level attention.ùëß=ùëäùëú‚Ñé‚àó+ùëèùëúz=Woh*+bo(27)whereùëäùëúWoandùëèùëúboare the output weight matrix and bias vector, respectively. Logits z are then normalized by the SoftMax function to generate a probability distribution of the sentiment classes.ùë¶ÃÇ√óùëñ=exp(ùëßùëñ)‚àë√óùëò=1ùê∂exp(ùëßùëò),ùëñ=1,2,‚Ä¶,ùê∂y^√ói=exp(zi)‚àë√ók=1Cexp(zk),i=1,2,‚Ä¶,C(28) In this case,Crefers to the total amount of sentiment categories (e.g., 5 of fine-grained sentiment: very negative, negative, neutral, positive, and very positive). The probability of the most likely class is chosen as the predicted sentiment.",
            "3.9.2. Regularization and Focal Loss Optimization": "To reduce overfitting and increase generalization,(‚Ñé‚àó)(h*)dropout is regularized before classification. The dropout randomly kills neurons in training, making it resistant to noise, and it is not dependent on particular features. Since the example of the sentiment datasets usually has an imbalance in classes, such as more neutral reviews than extremely positive or negative ones, the model uses Focal Loss, rather than the conventional cross-entropy. Focal Loss is a dynamic weight, or scaling, loss that learns to place more emphasis on the learning of the harder, misclassified samples. It is defined as‚Ñíùëìùëúùëêùëéùëô=‚àíùõºùë°(1‚àíùë¶ÃÇùë°)ùõælog(ùë¶ÃÇùë°)Lfocal=‚àíŒ±t(1‚àíy^t)Œ≥log(y^t)(29)where (ùõºùë°)(Œ±t)is the weighting factor for class(ùë°)(t),(ùõæ)(Œ≥)is the focusing parameter controlling difficulty emphasis,(ùë¶ÃÇùë°)(y^t)is the predicted probability for the true class. This adaptive loss promotes equal learning among the categories of sentiment and guarantees better performance on underrepresented labels.",
            "3.9.3. Training Strategy": "The Adam optimizer is applied with a learning rate to optimize the AGF-HAM model parameters, and with the help of adaptive moment estimation, it should converge steadily. Early stopping is used to avoid overtraining, with the basis of validation loss. The total training goal reduces the amount of focal loss of all samples:‚Ñí√óùë°ùëúùë°ùëéùëô=1ùëÅ‚àë√óùëñ=1ùëÅ‚Ñíùëìùëúùëêùëéùëô(ùë¶ÃÇùëñ,ùë¶ùëñ)L√ótotal=1N‚àë√ói=1NLfocal(y^i,yi)(30) In this step, the learned hierarchical and gated representations are incorporated into a small, interpretable, and accurate sentiment prediction that is both accurate and interpretable.",
            "3.10. Explainability Module": "An Explainability Module Layer is incorporated as the last step of the HAM framework in order to guarantee interpretability and transparency of the final model predictions. The layer makes the process of forming the sentiments‚Äô decisions globally and locally interpretable by showing how the model forms its conclusion based on linguistic and contextual knowledge. The explainability module is based on three complementary sub-layers‚ÄîAttention Heatmaps, Aspect-Level Explanations, and SHAP Integrated Gradients‚Äîthat have a specific analytical purpose to hold models accountable and explainable to humans. 3.10.1. Attention HeadmapsThe Attention Headmaps sub-layer shows token-level scores of the importance of the attention heads in the Transformer encoder and BiLSTM layers. It puts emphasis on particular words, phrases, or clauses that have a strong impact on the sentiment or emotional polarity of a certain text. This visualization, in addition to showing the focus distribution of the model, also proves the interpretive reliability of the attention mechanism. With the help of these heatmaps, researchers and practitioners can confirm that the model focuses on semantically significant areas and not accidental associations.ùõºùëñ=exp(ùë£‚ä§tanh(ùëä‚Ñé‚Ñéùëñ+ùëäùëûùëû+ùëè))‚àëùëõùëó=1exp(ùë£‚ä§tanh(ùëä‚Ñé‚Ñéùëó+ùëäùëûùëû+ùëè))Œ±i=expv‚ä§tanh(Whhi+Wqq+b)‚àëj=1nexpv‚ä§tanh(Whhj+Wqq+b)(31)This calculates the attention weight of each tokent, and this is the degree to which that token is relevant to the query (aspect or sentence context). It involves a SoftMax normalization such that all weights of attention have a sum of 1.ùêªatt(ùë°ùëñ)=ùõºùëñforeachtokenùë°ùëñ.Hatt(ti)=Œ±iforeachtokenti.(32)This maps the attention weight to the underlying token, which compose the raw headmap of token importance in the text. It graphically shows the words that make the greatest contribution to sentiment decisions.Headmapnorm(ùë°ùëñ)=ùõºùëñ‚àíminùëóùõºùëómaxùëóùõºùëó‚àíminùëóùõºùëó.Headmapnorm(ti)=Œ±i‚àíminjŒ±jmaxjŒ±j‚àíminjŒ±j.(33)The weights between the attention and the visualization of the headmap are normalized to 0 or 1 to form a visual representation of the headmap. It guarantees a similar visualization of samples to make them easier to interpret. 3.10.2. Aspect-Level ExplanationThe sub-layer of Aspect-Level Explanation gives a fine-grained interpretability, where sentiment weights are mapped to the identified aspects in the text. This sub-layer explains the roles of each product or emotional aspect in the final sentiment classification using dependency-based aspect extraction and context embeddings. It can be used to provide transparency in aspect-aware decision-making and bridge the semantic connection between aspect terms (battery, camera, service) and the contextual sentiments. This sub-layer is specifically useful when the task at hand demands domain interpretability, like reviewing or detecting emotions in social situations.ùõº(ùëé)ùëñ=exp(ùë£‚ä§ùëétanh(ùëä‚Ñé‚Ñéùëñ+ùëäùëéùëíùëé+ùëèùëé))‚àëùëõùëó=1exp(ùë£‚ä§ùëétanh(ùëä‚Ñé‚Ñéùëó+ùëäùëéùëíùëé+ùëèùëé))Œ±i(a)=expva‚ä§tanh(Whhi+Waea+ba)‚àëj=1nexpva‚ä§tanh(Whhj+Waea+ba)(34)This computes aspect-conditioned attention, in whichùëíùëéeais the embedding of a given aspect (e.g., ‚Äúbattery‚Äù or ‚Äúservice‚Äù) It helps the model to pay attention to the most relevant words to that aspect.ùëêùëé=‚àëùëñ=1ùëõùõº(ùëé)ùëñ,‚Ñéùëñ(aspect-awarecontextvector)ca=‚àëi=1nŒ±i(a),hi(aspect-awarecontextvector)(35)In this case, the hidden states are used to create a weighted context vector, which is a mixture of the hidden states with aspect-specific attention weights. It is a compressed feeling indicator of that specific element.ùë†(ùëò)ùëé=softmaxùëò(ùëä(ùëò)ùëúùëêùëé+ùëè(ùëò)ùëú)(classprobabilityforaspectùëé)sa(k)=softmaxk(Wo(k)ca+bo(k))(classprobabilityforaspecta)(36)This is to apply sentiment polarity prediction as an aspectawith all classes using a softmax layer. It produces probabilities of the form of positive, neutral, or negative sentiment.TopTokens√óùëé=argsort√óùëñ(ùõº(ùëé)ùëñ),top‚àíùëö.TopTokens√óa=argsort√óiŒ±i(a),top-m.(37)This is used to select the most influential tokens that assist in sentiment on aspecta. These tokens are used to produce aspect-level textual explanations. 3.10.3. SHAP/Integrated GradientsThe SHAP Integrated Gradients sub-layer is a combination of SHapley Additive exPlanations (SHAP) with Integrated Gradients (IG) used to measure the contribution of each feature to the output of a model. This hybrid interpretability method combines model sensitivity and feature attribution consistency into unified importance maps that are complementary to the attention-based interpretations. This sub-layer is a combination of SHAP and IG and is able to provide a coherent and consistent explanation of decision boundaries, which strengthens the transparency and reliability of the model.ùúôùëñ=‚àëùëÜ‚äÜùëÅ‚àñùëñ|ùëÜ|!,(|ùëÅ|‚àí|ùëÜ|‚àí1)!|ùëÅ|!‚éõ‚éù‚éú‚éú‚éúùêπùëÜ‚à™ùëñ(ùë•ùëÜ‚à™ùëñ)‚àíùêπùëÜ(ùë•ùëÜ)‚éû‚é†‚éü‚éü‚éüœïi=‚àëS‚äÜN‚àñi|S|!,(|N|‚àí|S|‚àí1)!|N|!(FS‚à™i(xS‚à™i)‚àíFS(xS))(38)This is the Shapley value equation, which is used to measure the contribution of a given feature to the prediction of the model. It assesses the effect of the addition of a feature (word/token) on model output on all the subsets.‚àëùëñ=1ùëõùúôùëñ=ùêπ(ùë•)‚àíùêπ(ùë•baseline)‚àëi=1nœïi=F(x)‚àíF(xbaseline)(39)SHAP values can be summed up to form the difference between baseline and model prediction. This guarantees additive attribution of features, which ensures consistency of interpretability.IGùëñ(ùë•)=(ùë•ùëñ‚àíùë•‚Ä≤√óùëñ)‚à´√óùõº=01‚àÇùêπ(ùë•‚Ä≤+ùõº(ùë•‚àíùë•‚Ä≤))‚àÇùë•ùëñ,ùëëùõºIGi(x)=(xi‚àíx‚Ä≤√ói)‚à´√óŒ±=01‚àÇFx‚Ä≤+Œ±(x‚àíx‚Ä≤)‚àÇxi,dŒ±(40)Integrated Gradients calculate the contribution of each input feature to the output of a model by summing the gradients between the path taken between a baselineùë•‚Ä≤x‚Ä≤and the real input x. It offers path-sensitive interpretability that is smooth and has less noise than raw gradients.To illustrate the interpretability of the proposed explainability module, we provide examples of each of the sentiment categories: very positive, positive, neutral, negative, and very negative. A heatmap visualization inFigure 4is provided per sentence to show the distribution of the attention weights of the individual tokens, i.e., the focus of the model when inferring the sentiment. The darker the region of the heatmap, the greater the intensity of attention of the token is, and the greater its influence on the final prediction of the model will be.Figure 4.Heatmap visualization showing token-level attention distribution for sentences across five sentiment categories. Darker shades represent higher attention weights to the model‚Äôs sentiment prediction.Aspect-Level Summary:Detected Aspect:phone/performanceOpinion Tokens:awful, slow, crashes, uselessAspect-Sentiment Score:‚àí0.93‚àí0.93Aspect Polarity Label:Very NegativeFinal Interpretation:The model classifies this sentence asVery Negative. Attention, Aspect-level Polarity, and SHAP/IG all emphasize the same tokens:awful,useless, andcrashes‚Äîindicating they are the strongest contributors to the negative sentiment toward thephone‚Äôs performance.Besides visual interpretation, there is also elaborate explainability of one representative sentence inTable 2andTable 3. The table is a quantitative report on the attention weight, aspect-level polarity, SHAP value, and Integrated Gradient (IG) contribution of each token. These three measures are then normalized and averaged to obtain the combined importance score to reflect the contextual and causal influence on the decision of the model. This integrated analysis would allow not only the explainability framework to identify words that carry sentiment (e.g., awful, useless, excellent) but also clarify their relationship with certain aspects in context, allowing the transparent and understandable interpretation of how the model reaches its classification.Table 2.Explainability Table for Sentence: ‚ÄúThis phone is awful slow crashes often today useless‚Äù.Table 3.Definition and Formula of Each Explainability Metric.The combination of these three sub-layers gives HAM not only superior performance in predictive mode but also a high level of interpretive ability. This Explainability Module Layer converts the HAM framework to more of an intelligent system that can be trusted and interpreted by humans, having the ability to provide the answer to why and how decisions are made through complex sentiment and emotional environments.",
            "3.10.1. Attention Headmaps": "The Attention Headmaps sub-layer shows token-level scores of the importance of the attention heads in the Transformer encoder and BiLSTM layers. It puts emphasis on particular words, phrases, or clauses that have a strong impact on the sentiment or emotional polarity of a certain text. This visualization, in addition to showing the focus distribution of the model, also proves the interpretive reliability of the attention mechanism. With the help of these heatmaps, researchers and practitioners can confirm that the model focuses on semantically significant areas and not accidental associations.ùõºùëñ=exp(ùë£‚ä§tanh(ùëä‚Ñé‚Ñéùëñ+ùëäùëûùëû+ùëè))‚àëùëõùëó=1exp(ùë£‚ä§tanh(ùëä‚Ñé‚Ñéùëó+ùëäùëûùëû+ùëè))Œ±i=expv‚ä§tanh(Whhi+Wqq+b)‚àëj=1nexpv‚ä§tanh(Whhj+Wqq+b)(31) This calculates the attention weight of each tokent, and this is the degree to which that token is relevant to the query (aspect or sentence context). It involves a SoftMax normalization such that all weights of attention have a sum of 1.ùêªatt(ùë°ùëñ)=ùõºùëñforeachtokenùë°ùëñ.Hatt(ti)=Œ±iforeachtokenti.(32) This maps the attention weight to the underlying token, which compose the raw headmap of token importance in the text. It graphically shows the words that make the greatest contribution to sentiment decisions.Headmapnorm(ùë°ùëñ)=ùõºùëñ‚àíminùëóùõºùëómaxùëóùõºùëó‚àíminùëóùõºùëó.Headmapnorm(ti)=Œ±i‚àíminjŒ±jmaxjŒ±j‚àíminjŒ±j.(33) The weights between the attention and the visualization of the headmap are normalized to 0 or 1 to form a visual representation of the headmap. It guarantees a similar visualization of samples to make them easier to interpret.",
            "3.10.2. Aspect-Level Explanation": "The sub-layer of Aspect-Level Explanation gives a fine-grained interpretability, where sentiment weights are mapped to the identified aspects in the text. This sub-layer explains the roles of each product or emotional aspect in the final sentiment classification using dependency-based aspect extraction and context embeddings. It can be used to provide transparency in aspect-aware decision-making and bridge the semantic connection between aspect terms (battery, camera, service) and the contextual sentiments. This sub-layer is specifically useful when the task at hand demands domain interpretability, like reviewing or detecting emotions in social situations.ùõº(ùëé)ùëñ=exp(ùë£‚ä§ùëétanh(ùëä‚Ñé‚Ñéùëñ+ùëäùëéùëíùëé+ùëèùëé))‚àëùëõùëó=1exp(ùë£‚ä§ùëétanh(ùëä‚Ñé‚Ñéùëó+ùëäùëéùëíùëé+ùëèùëé))Œ±i(a)=expva‚ä§tanh(Whhi+Waea+ba)‚àëj=1nexpva‚ä§tanh(Whhj+Waea+ba)(34) This computes aspect-conditioned attention, in whichùëíùëéeais the embedding of a given aspect (e.g., ‚Äúbattery‚Äù or ‚Äúservice‚Äù) It helps the model to pay attention to the most relevant words to that aspect.ùëêùëé=‚àëùëñ=1ùëõùõº(ùëé)ùëñ,‚Ñéùëñ(aspect-awarecontextvector)ca=‚àëi=1nŒ±i(a),hi(aspect-awarecontextvector)(35) In this case, the hidden states are used to create a weighted context vector, which is a mixture of the hidden states with aspect-specific attention weights. It is a compressed feeling indicator of that specific element.ùë†(ùëò)ùëé=softmaxùëò(ùëä(ùëò)ùëúùëêùëé+ùëè(ùëò)ùëú)(classprobabilityforaspectùëé)sa(k)=softmaxk(Wo(k)ca+bo(k))(classprobabilityforaspecta)(36) This is to apply sentiment polarity prediction as an aspectawith all classes using a softmax layer. It produces probabilities of the form of positive, neutral, or negative sentiment.TopTokens√óùëé=argsort√óùëñ(ùõº(ùëé)ùëñ),top‚àíùëö.TopTokens√óa=argsort√óiŒ±i(a),top-m.(37) This is used to select the most influential tokens that assist in sentiment on aspecta. These tokens are used to produce aspect-level textual explanations.",
            "3.10.3. SHAP/Integrated Gradients": "The SHAP Integrated Gradients sub-layer is a combination of SHapley Additive exPlanations (SHAP) with Integrated Gradients (IG) used to measure the contribution of each feature to the output of a model. This hybrid interpretability method combines model sensitivity and feature attribution consistency into unified importance maps that are complementary to the attention-based interpretations. This sub-layer is a combination of SHAP and IG and is able to provide a coherent and consistent explanation of decision boundaries, which strengthens the transparency and reliability of the model.ùúôùëñ=‚àëùëÜ‚äÜùëÅ‚àñùëñ|ùëÜ|!,(|ùëÅ|‚àí|ùëÜ|‚àí1)!|ùëÅ|!‚éõ‚éù‚éú‚éú‚éúùêπùëÜ‚à™ùëñ(ùë•ùëÜ‚à™ùëñ)‚àíùêπùëÜ(ùë•ùëÜ)‚éû‚é†‚éü‚éü‚éüœïi=‚àëS‚äÜN‚àñi|S|!,(|N|‚àí|S|‚àí1)!|N|!(FS‚à™i(xS‚à™i)‚àíFS(xS))(38) This is the Shapley value equation, which is used to measure the contribution of a given feature to the prediction of the model. It assesses the effect of the addition of a feature (word/token) on model output on all the subsets.‚àëùëñ=1ùëõùúôùëñ=ùêπ(ùë•)‚àíùêπ(ùë•baseline)‚àëi=1nœïi=F(x)‚àíF(xbaseline)(39) SHAP values can be summed up to form the difference between baseline and model prediction. This guarantees additive attribution of features, which ensures consistency of interpretability.IGùëñ(ùë•)=(ùë•ùëñ‚àíùë•‚Ä≤√óùëñ)‚à´√óùõº=01‚àÇùêπ(ùë•‚Ä≤+ùõº(ùë•‚àíùë•‚Ä≤))‚àÇùë•ùëñ,ùëëùõºIGi(x)=(xi‚àíx‚Ä≤√ói)‚à´√óŒ±=01‚àÇFx‚Ä≤+Œ±(x‚àíx‚Ä≤)‚àÇxi,dŒ±(40) Integrated Gradients calculate the contribution of each input feature to the output of a model by summing the gradients between the path taken between a baselineùë•‚Ä≤x‚Ä≤and the real input x. It offers path-sensitive interpretability that is smooth and has less noise than raw gradients. To illustrate the interpretability of the proposed explainability module, we provide examples of each of the sentiment categories: very positive, positive, neutral, negative, and very negative. A heatmap visualization inFigure 4is provided per sentence to show the distribution of the attention weights of the individual tokens, i.e., the focus of the model when inferring the sentiment. The darker the region of the heatmap, the greater the intensity of attention of the token is, and the greater its influence on the final prediction of the model will be. Figure 4.Heatmap visualization showing token-level attention distribution for sentences across five sentiment categories. Darker shades represent higher attention weights to the model‚Äôs sentiment prediction. Aspect-Level Summary: Detected Aspect:phone/performanceOpinion Tokens:awful, slow, crashes, uselessAspect-Sentiment Score:‚àí0.93‚àí0.93Aspect Polarity Label:Very Negative Final Interpretation: The model classifies this sentence asVery Negative. Attention, Aspect-level Polarity, and SHAP/IG all emphasize the same tokens:awful,useless, andcrashes‚Äîindicating they are the strongest contributors to the negative sentiment toward thephone‚Äôs performance. Besides visual interpretation, there is also elaborate explainability of one representative sentence inTable 2andTable 3. The table is a quantitative report on the attention weight, aspect-level polarity, SHAP value, and Integrated Gradient (IG) contribution of each token. These three measures are then normalized and averaged to obtain the combined importance score to reflect the contextual and causal influence on the decision of the model. This integrated analysis would allow not only the explainability framework to identify words that carry sentiment (e.g., awful, useless, excellent) but also clarify their relationship with certain aspects in context, allowing the transparent and understandable interpretation of how the model reaches its classification. Table 2.Explainability Table for Sentence: ‚ÄúThis phone is awful slow crashes often today useless‚Äù. Table 3.Definition and Formula of Each Explainability Metric. The combination of these three sub-layers gives HAM not only superior performance in predictive mode but also a high level of interpretive ability. This Explainability Module Layer converts the HAM framework to more of an intelligent system that can be trusted and interpreted by humans, having the ability to provide the answer to why and how decisions are made through complex sentiment and emotional environments.",
            "4. Dataset Description and Hypermeters": "Three datasets were used to determine the effectiveness of the proposed AGF-HAM framework, its robustness, and its generalization ability; two publicly available benchmark corpora and one custom-made dataset were used. This multi-dataset approach guarantees thorough validation on domain-specific, product-driven, and emotion-driven settings, with internal and external experimental validity. 4.1. Amazon Cell Phones and Accessories ReviewsThe source of the Amazon Cell Phones and Accessories Reviews dataset is a reputable source, Kaggle, and the corpus is publicly available on (https://www.kaggle.com/datasets/grikomsn/amazon-cell-phones-reviews?select=20191226-reviews.csv, accessed on 2 December 2025) and widely used in sentiment analysis research. It consists of 67,986 customer reviews and has eight formatted attributes: ASIN, product name, rating, review date, verification status, review title, review body, and helpful votes. Each review is rated on a five-point scale (1‚Äì5), which directly translates to sentiment polarity categories of very negative, negative, neutral, positive, and very positive. The data set offers an equal and domain-specific benchmark that reasonably captures consumer opinion, buying patterns, and sentiment subtleties regarding cell phones and electronic accessories. It is especially well-suited to the aspect-based and Transformer-based sentiment models because it is structured and has a large sample size. 4.2. GoEmotions (5-Class Variant)The GoEmotions dataset is publicly available on kaggle.com (https://www.kaggle.com/datasets/debarshichanda/goemotions, accessed on 2 December 2025), which is a fine-grained emotion classification benchmark that was created by Google. It includes three sub-versions: GoEmotions1 (70,000 samples), GoEmotions2 (70,000 samples), and GoEmotions3 (70,226 samples) that together offer more than 210,000 labeled examples. A text instance is marked with one or more of 28 different categories of emotions that describe the full spectrum of expressions of feelings. The dataset can be used to perform single-label and multi-label emotion recognition tasks, but is especially useful in testing models on subtle emotional perception. Its scale, language diversity, and fine-grained annotations are a perfect addition to sentiment-oriented datasets that provide a deeper level of emotion to test the overall validity and strength of the suggested AGF-HAM framework.In order to provide methodological consistency and provide a fair comparative analysis between datasets with varying degrees of emotional granularity, the original 28 fine-grained emotion labels in the GoEmotions dataset were mapped inTable 4systematically into five sentiment-oriented categories, namely Very Positive, Positive, Neutral, Negative, and Very Negative. Such a hierarchical feeling category corresponds to the five-class sentiment category of the Amazon Cell Phones and Accessories Reviews dataset, which is consistent in evaluation metrics across domains. The aggregation does not remove the semantic variety and emotionality of the social media expression and offers a comparative benchmarking framework. Therefore, this mapping helps to make the performance evaluation of datasets with differences in their domain focus on social media discourse, as well as emotional expressiveness and linguistic variability fair.Table 4.Mapping of GoEmotions Fine-Grained Labels into Five Sentiment-Oriented Classes. 4.3. Summary of DatasetsTable 5summarizes the key characteristics of all datasets used in this study.Table 5.Summary of Datasets Used in AGF-HAM Evaluation.The combination of these three datasets,Table 5ensures that the AGF-HAM model is evaluated on multiple linguistic domains‚Äîtechnical, commercial, and social‚Äîcapturing a broad spectrum of sentiment expressions. This diverse evaluation framework not only validates the model‚Äôs effectiveness but also demonstrates its robustness and adaptability in handling domain-specific and cross-domain sentiment variations. 4.4. HyperMetersThe AGF-HAM model has various hyperparameters and optimized parameters to provide balanced learning, robust convergence, and a high ability to generalize them across datasets. All the model parts were optimized by systematic experimentation along grid and random search algorithms. The Transformer backbones (BERT and RoBERTa) were pretrained, and they were fine-tuned together with task-specific layers. Learning was stabilized by the use of Adam optimizer with an adaptive learning rate schedule, and regularization was carried out by dropout. The empirical choice of batch size, sequence length, and representational depth was through hidden dimensions and sequence length. Validation loss was used to avoid overfitting by early termination.Table 6summarizes the optimal configuration used in all experiments.Table 6.Hyperparameter Configuration for AGF-HAM Model.The balance in expressive capacity and training stability is represented in the above configuration. A learning rate of(2√ó10‚àí5)(2√ó10‚àí5)and a batch size of 32 were identified as being optimal in staying constant with Transformer backbones. The moderate dropout rate of 0.3 served as a good counter to overfitting without using model capacity in an underutilized way. The focal loss parameters((ùõæ=2.0,ùõºùë°adaptive))((Œ≥=2.0,Œ±tadaptive))were used to deal with the imbalance of the classes, where each sentiment category would make fair contributions. A hierarchical attention head of 8 and a BiLSTM hidden size of 256 was rich enough in its representational capability to capture the word, position, and aspect dependencies. Premature termination also helped in training efficiency and strong generalization between datasets.To achieve consistency in the baselines of all comparisons, all the baseline models (recent Transformer-only architecture and hybrid Transformer-BiLSTM models) were fine-tuned under the same experimental conditions. The training regimen, optimization scheme, and preprocessing procedures were applied consistently to all models. All the hyperparameters (learning rate, batch size, optimizer, dropout, hidden dimensions, focal loss parameters, early stopping rule, etc.) will be compiled toTable 6, so that these will be completely reproducible. The resulting single configuration makes a sound performance comparison possible and isolates the actual contribution of each architectural component of the proposed model.",
            "4.1. Amazon Cell Phones and Accessories Reviews": "The source of the Amazon Cell Phones and Accessories Reviews dataset is a reputable source, Kaggle, and the corpus is publicly available on (https://www.kaggle.com/datasets/grikomsn/amazon-cell-phones-reviews?select=20191226-reviews.csv, accessed on 2 December 2025) and widely used in sentiment analysis research. It consists of 67,986 customer reviews and has eight formatted attributes: ASIN, product name, rating, review date, verification status, review title, review body, and helpful votes. Each review is rated on a five-point scale (1‚Äì5), which directly translates to sentiment polarity categories of very negative, negative, neutral, positive, and very positive. The data set offers an equal and domain-specific benchmark that reasonably captures consumer opinion, buying patterns, and sentiment subtleties regarding cell phones and electronic accessories. It is especially well-suited to the aspect-based and Transformer-based sentiment models because it is structured and has a large sample size.",
            "4.2. GoEmotions (5-Class Variant)": "The GoEmotions dataset is publicly available on kaggle.com (https://www.kaggle.com/datasets/debarshichanda/goemotions, accessed on 2 December 2025), which is a fine-grained emotion classification benchmark that was created by Google. It includes three sub-versions: GoEmotions1 (70,000 samples), GoEmotions2 (70,000 samples), and GoEmotions3 (70,226 samples) that together offer more than 210,000 labeled examples. A text instance is marked with one or more of 28 different categories of emotions that describe the full spectrum of expressions of feelings. The dataset can be used to perform single-label and multi-label emotion recognition tasks, but is especially useful in testing models on subtle emotional perception. Its scale, language diversity, and fine-grained annotations are a perfect addition to sentiment-oriented datasets that provide a deeper level of emotion to test the overall validity and strength of the suggested AGF-HAM framework. In order to provide methodological consistency and provide a fair comparative analysis between datasets with varying degrees of emotional granularity, the original 28 fine-grained emotion labels in the GoEmotions dataset were mapped inTable 4systematically into five sentiment-oriented categories, namely Very Positive, Positive, Neutral, Negative, and Very Negative. Such a hierarchical feeling category corresponds to the five-class sentiment category of the Amazon Cell Phones and Accessories Reviews dataset, which is consistent in evaluation metrics across domains. The aggregation does not remove the semantic variety and emotionality of the social media expression and offers a comparative benchmarking framework. Therefore, this mapping helps to make the performance evaluation of datasets with differences in their domain focus on social media discourse, as well as emotional expressiveness and linguistic variability fair. Table 4.Mapping of GoEmotions Fine-Grained Labels into Five Sentiment-Oriented Classes.",
            "4.3. Summary of Datasets": "Table 5summarizes the key characteristics of all datasets used in this study. Table 5.Summary of Datasets Used in AGF-HAM Evaluation. The combination of these three datasets,Table 5ensures that the AGF-HAM model is evaluated on multiple linguistic domains‚Äîtechnical, commercial, and social‚Äîcapturing a broad spectrum of sentiment expressions. This diverse evaluation framework not only validates the model‚Äôs effectiveness but also demonstrates its robustness and adaptability in handling domain-specific and cross-domain sentiment variations.",
            "4.4. HyperMeters": "The AGF-HAM model has various hyperparameters and optimized parameters to provide balanced learning, robust convergence, and a high ability to generalize them across datasets. All the model parts were optimized by systematic experimentation along grid and random search algorithms. The Transformer backbones (BERT and RoBERTa) were pretrained, and they were fine-tuned together with task-specific layers. Learning was stabilized by the use of Adam optimizer with an adaptive learning rate schedule, and regularization was carried out by dropout. The empirical choice of batch size, sequence length, and representational depth was through hidden dimensions and sequence length. Validation loss was used to avoid overfitting by early termination.Table 6summarizes the optimal configuration used in all experiments. Table 6.Hyperparameter Configuration for AGF-HAM Model. The balance in expressive capacity and training stability is represented in the above configuration. A learning rate of(2√ó10‚àí5)(2√ó10‚àí5)and a batch size of 32 were identified as being optimal in staying constant with Transformer backbones. The moderate dropout rate of 0.3 served as a good counter to overfitting without using model capacity in an underutilized way. The focal loss parameters((ùõæ=2.0,ùõºùë°adaptive))((Œ≥=2.0,Œ±tadaptive))were used to deal with the imbalance of the classes, where each sentiment category would make fair contributions. A hierarchical attention head of 8 and a BiLSTM hidden size of 256 was rich enough in its representational capability to capture the word, position, and aspect dependencies. Premature termination also helped in training efficiency and strong generalization between datasets. To achieve consistency in the baselines of all comparisons, all the baseline models (recent Transformer-only architecture and hybrid Transformer-BiLSTM models) were fine-tuned under the same experimental conditions. The training regimen, optimization scheme, and preprocessing procedures were applied consistently to all models. All the hyperparameters (learning rate, batch size, optimizer, dropout, hidden dimensions, focal loss parameters, early stopping rule, etc.) will be compiled toTable 6, so that these will be completely reproducible. The resulting single configuration makes a sound performance comparison possible and isolates the actual contribution of each architectural component of the proposed model.",
            "5. Experiments and Results Discussions": "Comparison of the performance of the baseline models within the four datasets is summarized inTable 7. The conventional machine-learning algorithm SVM had an average score of about 76‚Äì78% across GoEmotions variants and 80.1% on the Amazon data, but was marginally less effective than Logistic Regression (LR), which had a score of about 77‚Äì79%. Naive Bayes (NB) had the relatively poorer performance with an accuracy level of 74‚Äì75. The shift to deep-learning models, LSTM, RCNN, GRU, and Bi-GRU, showed some performance improvements with an accuracy ranging between 81 and 85%; as they have the capacity to discover contextual dependencies and sequential characteristics in a better manner than the old models. It is interesting to note that Bi-GRU obtained the best accuracy between the recurrent architectures, with 84.2‚Äì85.4% on the GoEmotions datasets and 86.9% on the Amazon dataset. Table 7.Performance Comparison of Baseline Models across Datasets. In addition, the Transformer-based BERT model showed a significant improvement in the performance with 88.4, 89.1, and 89.9 accuracy with the three versions of GoEmotions, and 91.3 accuracy with the Amazon Cell Phones and Accessories Reviews dataset. It shows that BERT is greatly contextually sensitive and has a powerful ability of pretrained linguistic representation, which allows it to outperform classical and recurrent models. In general, although the trends of all models were similar to improve the performance with the Amazon data, this could be explained by the fact that the latter is more organized and focused on sentiments than the multi-emotional variability of GoEmotions. Comparative analysis that was conducted with the Transformer-based and the hybrid architectures, as illustrated inTable 8, indicates that there was a massive performance enhancement in comparison to the traditional and recurrent baselines. BERT, a typical Transformer model, had an accuracy of 88‚Äì89% with the variants of GoEmotions and 91.3% on the Amazon dataset, indicating a good understanding of the context. ALBERT and XLNet exhibited slightly better results, reaching approximately 89‚Äì91% accuracy, which was made possible by the sharing of parameters and permutation-based attention mechanisms, respectively. RoBERTa, through its good pretraining optimization, had better results compared to other baseline transformers, reaching 91.9 and 93.1 on GoEmotions-3 and the Amazon reviews, respectively. Table 8.Performance Comparison of Transformer-Based and Hybrid Models. The addition of sequential layers was another way of adding depth to the representation. RoBERTa+BiLSTM hybrid obtained 91.7‚Äì93.0% of the accuracy in GoEmotions and 94.0% in Amazon, and DistilBERT+Attention reached the same efficiency with a minor decrease in computational complexity. These findings highlight the usefulness of using a combination of contextual embeddings and sequence learning, and attention refining. It is important to note that the HAM model (Hybrid Attention Mechanism) had the highest overall performance, showing a 94.5, 95.1, and 95.6% accuracy on GoEmotions variants and a 96.4% accuracy on the Amazon dataset, and higher precision, recall, and F1 scores of over 93%. This illustrates the ability of HAM to focus the fine-grained emotional and sentiment representations by the interaction of high-order Transformer representations and the BiLSTM. 5.1. Ablation StudyAn ablation analysis was performed on both GoEmotions-3 and Amazon Cell Phones data to determine the role of each architectural component, as shown inTable 3. As a starting point, the RoBERTa baseline recorded an accuracy of 91.9 on GoEmotions and 93.1 on Amazon, which is a good contextual base. BiLSTM addition increased the performance by around 1‚Äì1.5, which suggests that sequential bidirectional encoding is an effective supplement to the Transformer based on its static contextual embeddings because it captures the temporal relationships and polarity flow in longer reviews.The incorporation of an attention mechanism also increased the accuracy to 94.3% and 95.2%, thus demonstrating the relevance of weighted feature emphasis in detecting sentiment-varying tokens and aspect-specific features. Another improvement from 0.76% to 0.85% was achieved when using Focal Loss, which meant that the imbalance in classes was overcome and the model‚Äôs sensitivity to the sentiment categories of minorities was improved. Lastly, the HAM configuration with RoBERTa, BiLSTM, hierarchical attention, and adaptive optimization had the best performance with 95.6 on GoEmotions and 96.4 on Amazon.The results of the ablation (Table 9) indicate clearly the incremental contribution of each element in the proposed architecture of HAM. BiLSTM enhancement of RoBERTa sequence modeling and attention, which adds greater value to aspect-sensitive feature weighting. An addition of the focal loss can help in terms of class imbalance, and the complete HAM in terms of fusing the fusion gate with GAT and position-aware attention shows the best results in the accuracy of both datasets. Such accumulating gains justify the need and usefulness of every module.Table 9.Ablation study on the proposed HAM Model (GoEmotions-3 and Amazon Datasets).These results highlight that all elements play a significant role: RoBERTa brings out the depth of the context, BiLSTM brings out the sequential comprehension, Attention brings out the interpretability and weighting of tokens, and Focal Loss reduces the bias caused by imbalance. The strength and complementary quality of the hybrid design are justified by the incremental benefits that it has brought in the process of predicting sentiment, which proves that the HAM framework is capable of successfully integrating global and local semantic information to make better predictions.In addition to the quantitative performance, interpretability analysis with the help of LIME and SHAP also supports the results of the ablation study. The analysis of the token-weighted attention and contribution showed that all structural modifications made in HAM directly increase the explainability and transparency of decisions made by the model. Due to hierarchical attention integration, as an example, the model can emphasize aspect-relevant words (e.g., battery life, camera quality) and deemphasize non-informative tokens to generate more human-congruent reasoning patterns. Likewise, Focal Loss not only enhances the precision of the underrepresented sentiment classes but also stabilizes the heatmaps of attention-ensuring that attention is consistently concentrated in varied samples.In comparative XAI visualizations, it is evident that RoBERTa and RoBERTa+BiLSTM are relatively useful in general sentiment polarity but are prone to misunderstanding the context of compound sentences or sarcasm. Conversely, the HAM framework offers more aspect-sensitive explanations that are more interpretable with a high fidelity of interpretation, all of which proves that every model improvement would lead not only to numerical improvements but also to qualitative knowledge. The given hybrid design is, therefore, the solution to two tasks: state-of-the-art performance and open decision-making, which is needed in the real-life application of sentiment and emotion analysis. 5.2. Comparative StudyThe relative analysis of the previous works highlights the ongoing innovations in sentiment and emotion classification techniques. According toTable 10, during the preliminary stage, Studies [1,3] used BERT with Contextualized Psychological Dimensions (CPD) on the GoEmotions dataset with F1-scores of 51.96% and 52.34%, respectively, reflecting the fact that it has 28 fine-grained emotion categories that are difficult to process. Likewise, Study [2], which used baseline BERT on the same dataset, reached a similar F1-score of 52%, justifying the need to investigate deeper contextual learning processes. In pursuit of better representation learning, Study [4] proposed Seq2Emo on a fused SemEval-18 and GoEmotions dataset, and the F1-score was improved to 59.57, which demonstrates the value of multi-dataset fusion and sequence-based emotion models.Table 10.Comparative Study of Sentiment Analysis Models on Various Datasets.Graph-enhanced Transformer architectures were also seen to improve further. In ref. [5], the authors used BERT with Gated Graph Attention (GAT) on the Rest14, Lap14, and Twitter datasets with F1-scores of 82.73, 79.49, and 74.93, respectively, and demonstrated significant improvements in the aspect-based sentiment and short context. Research [6] furthered this trend with 84.27% on sentiment benchmark datasets with Prompt-ConvBERT and Prompt-ConvRoBERTa models, and Research [7] with a hybrid CNN + BERT + RoBERTa model with a similar 84.58% on GoEmotions, indicating the effectiveness of convolutional-semantic fusion. The past traditional baselines, including Naive Bayes (NB) as tested in Study [8] in a variety of datasets (IMDB, Sentiment140, SemEval, STS-Gold), had an F1-score that varied between 73% and 86%, which demonstrated the weakness of non-contextual models. Conversely, Study [9], which used RoBERTa with Adapters and BiLSTM, achieved a better GoEmotions classification of 87, which is indicative of the power of sequential contextual encoding.Based on these developments, our Hybrid Attention Model (HAM) makes a drastic improvement in its performance by obtaining an F1 score of 94.9% on both the GoEmotions and Amazon Cell Phones and Accessories Reviews datasets. The improved performance can be attributed to the joint efforts of BERT/RoBERTa embeddings, GAT-based relational attention, and BiLSTM-based bidirectional context learning, which, together, allow understanding emotional states on a fine scale and cross-domain flexibility. This indicates that HAM is a successful tool to address the gap between the emotionally charged text in social media and the aspect-based product reviews, which is a new milestone in sentiment analysis studies.Figure 5fully illustrates the performance characteristics of the AGF-HAM model in terms of the various dataset mappings and domains of GoEmotions and Amazon Cell Phones. The accuracy and an equivalent training validation loss curve of the Amazon Cell Phones dataset, respectively, resulting inFigure 5a,b, indicate a consistent convergence of the model and the lack of overfitting. The accuracy of GoEmotions-1, GoEmotions-2, and GoEmotions-3 in 10 epochs is shown inFigure 5c,e,g, respectively, and shows a steady improvement and strength over various five-class mappings. In line with this,Figure 5d,f,h demonstrate the training and validation loss curves of the identical mappings, which show the smooth optimization behavior of the model and strong generalization in text domains of various emotions.Figure 5.Graph of Accuracy, Training loss, validation loss of Proposed model AGF-HAM. (a) Accuracy (Amazon Cell Phone); (b) Training and validtion loss (Amazon Cell Phone); (c) Accuracy (GoEmotions-1); (d) Training and validtion loss (GoEmotions-1); (e) Accuracy with GoEmotions-2; (f) Training and validtion loss (GoEmotions-2); (g) Accuracy (GoEmotions-3); (h) Training and validation loss (GoEmotions-3).The confusion matrices inFigure 6give a detailed illustration of the consistency of the AGF-HAM model in the classification of the two datasets.Figure 6a is a representation of the Amazon Cell Phones dataset, where it is evident that there is a separation between the classes and that there is very little misclassification between the levels of sentiments. The GoEmotions-1, GoEmotions-2, and GoEmotions-3 mappings are represented inFigure 6b‚Äìd, respectively. These matrices indicate hexahedra prediction patterns with very high accuracy for positive and very positive sentiment categories. All these findings together validate the statement that AGF-HAM has a high level of discriminative ability and domain flexibility and is capable of dealing with structured review data as well as non-structured, empathetic text.Figure 6.Confusion matrices of the proposed model AGF-HAM. (a) Confusion Matrix of Amazon Cell Phone; (b) Confusion Matrix of GoEmotions-1; (c) Confusion Matrix of GoEmotions-2; (d) Confusion Matrix of GoEmotions-3.",
            "5.1. Ablation Study": "An ablation analysis was performed on both GoEmotions-3 and Amazon Cell Phones data to determine the role of each architectural component, as shown inTable 3. As a starting point, the RoBERTa baseline recorded an accuracy of 91.9 on GoEmotions and 93.1 on Amazon, which is a good contextual base. BiLSTM addition increased the performance by around 1‚Äì1.5, which suggests that sequential bidirectional encoding is an effective supplement to the Transformer based on its static contextual embeddings because it captures the temporal relationships and polarity flow in longer reviews. The incorporation of an attention mechanism also increased the accuracy to 94.3% and 95.2%, thus demonstrating the relevance of weighted feature emphasis in detecting sentiment-varying tokens and aspect-specific features. Another improvement from 0.76% to 0.85% was achieved when using Focal Loss, which meant that the imbalance in classes was overcome and the model‚Äôs sensitivity to the sentiment categories of minorities was improved. Lastly, the HAM configuration with RoBERTa, BiLSTM, hierarchical attention, and adaptive optimization had the best performance with 95.6 on GoEmotions and 96.4 on Amazon. The results of the ablation (Table 9) indicate clearly the incremental contribution of each element in the proposed architecture of HAM. BiLSTM enhancement of RoBERTa sequence modeling and attention, which adds greater value to aspect-sensitive feature weighting. An addition of the focal loss can help in terms of class imbalance, and the complete HAM in terms of fusing the fusion gate with GAT and position-aware attention shows the best results in the accuracy of both datasets. Such accumulating gains justify the need and usefulness of every module. Table 9.Ablation study on the proposed HAM Model (GoEmotions-3 and Amazon Datasets). These results highlight that all elements play a significant role: RoBERTa brings out the depth of the context, BiLSTM brings out the sequential comprehension, Attention brings out the interpretability and weighting of tokens, and Focal Loss reduces the bias caused by imbalance. The strength and complementary quality of the hybrid design are justified by the incremental benefits that it has brought in the process of predicting sentiment, which proves that the HAM framework is capable of successfully integrating global and local semantic information to make better predictions. In addition to the quantitative performance, interpretability analysis with the help of LIME and SHAP also supports the results of the ablation study. The analysis of the token-weighted attention and contribution showed that all structural modifications made in HAM directly increase the explainability and transparency of decisions made by the model. Due to hierarchical attention integration, as an example, the model can emphasize aspect-relevant words (e.g., battery life, camera quality) and deemphasize non-informative tokens to generate more human-congruent reasoning patterns. Likewise, Focal Loss not only enhances the precision of the underrepresented sentiment classes but also stabilizes the heatmaps of attention-ensuring that attention is consistently concentrated in varied samples. In comparative XAI visualizations, it is evident that RoBERTa and RoBERTa+BiLSTM are relatively useful in general sentiment polarity but are prone to misunderstanding the context of compound sentences or sarcasm. Conversely, the HAM framework offers more aspect-sensitive explanations that are more interpretable with a high fidelity of interpretation, all of which proves that every model improvement would lead not only to numerical improvements but also to qualitative knowledge. The given hybrid design is, therefore, the solution to two tasks: state-of-the-art performance and open decision-making, which is needed in the real-life application of sentiment and emotion analysis.",
            "5.2. Comparative Study": "The relative analysis of the previous works highlights the ongoing innovations in sentiment and emotion classification techniques. According toTable 10, during the preliminary stage, Studies [1,3] used BERT with Contextualized Psychological Dimensions (CPD) on the GoEmotions dataset with F1-scores of 51.96% and 52.34%, respectively, reflecting the fact that it has 28 fine-grained emotion categories that are difficult to process. Likewise, Study [2], which used baseline BERT on the same dataset, reached a similar F1-score of 52%, justifying the need to investigate deeper contextual learning processes. In pursuit of better representation learning, Study [4] proposed Seq2Emo on a fused SemEval-18 and GoEmotions dataset, and the F1-score was improved to 59.57, which demonstrates the value of multi-dataset fusion and sequence-based emotion models. Table 10.Comparative Study of Sentiment Analysis Models on Various Datasets. Graph-enhanced Transformer architectures were also seen to improve further. In ref. [5], the authors used BERT with Gated Graph Attention (GAT) on the Rest14, Lap14, and Twitter datasets with F1-scores of 82.73, 79.49, and 74.93, respectively, and demonstrated significant improvements in the aspect-based sentiment and short context. Research [6] furthered this trend with 84.27% on sentiment benchmark datasets with Prompt-ConvBERT and Prompt-ConvRoBERTa models, and Research [7] with a hybrid CNN + BERT + RoBERTa model with a similar 84.58% on GoEmotions, indicating the effectiveness of convolutional-semantic fusion. The past traditional baselines, including Naive Bayes (NB) as tested in Study [8] in a variety of datasets (IMDB, Sentiment140, SemEval, STS-Gold), had an F1-score that varied between 73% and 86%, which demonstrated the weakness of non-contextual models. Conversely, Study [9], which used RoBERTa with Adapters and BiLSTM, achieved a better GoEmotions classification of 87, which is indicative of the power of sequential contextual encoding. Based on these developments, our Hybrid Attention Model (HAM) makes a drastic improvement in its performance by obtaining an F1 score of 94.9% on both the GoEmotions and Amazon Cell Phones and Accessories Reviews datasets. The improved performance can be attributed to the joint efforts of BERT/RoBERTa embeddings, GAT-based relational attention, and BiLSTM-based bidirectional context learning, which, together, allow understanding emotional states on a fine scale and cross-domain flexibility. This indicates that HAM is a successful tool to address the gap between the emotionally charged text in social media and the aspect-based product reviews, which is a new milestone in sentiment analysis studies. Figure 5fully illustrates the performance characteristics of the AGF-HAM model in terms of the various dataset mappings and domains of GoEmotions and Amazon Cell Phones. The accuracy and an equivalent training validation loss curve of the Amazon Cell Phones dataset, respectively, resulting inFigure 5a,b, indicate a consistent convergence of the model and the lack of overfitting. The accuracy of GoEmotions-1, GoEmotions-2, and GoEmotions-3 in 10 epochs is shown inFigure 5c,e,g, respectively, and shows a steady improvement and strength over various five-class mappings. In line with this,Figure 5d,f,h demonstrate the training and validation loss curves of the identical mappings, which show the smooth optimization behavior of the model and strong generalization in text domains of various emotions. Figure 5.Graph of Accuracy, Training loss, validation loss of Proposed model AGF-HAM. (a) Accuracy (Amazon Cell Phone); (b) Training and validtion loss (Amazon Cell Phone); (c) Accuracy (GoEmotions-1); (d) Training and validtion loss (GoEmotions-1); (e) Accuracy with GoEmotions-2; (f) Training and validtion loss (GoEmotions-2); (g) Accuracy (GoEmotions-3); (h) Training and validation loss (GoEmotions-3). The confusion matrices inFigure 6give a detailed illustration of the consistency of the AGF-HAM model in the classification of the two datasets.Figure 6a is a representation of the Amazon Cell Phones dataset, where it is evident that there is a separation between the classes and that there is very little misclassification between the levels of sentiments. The GoEmotions-1, GoEmotions-2, and GoEmotions-3 mappings are represented inFigure 6b‚Äìd, respectively. These matrices indicate hexahedra prediction patterns with very high accuracy for positive and very positive sentiment categories. All these findings together validate the statement that AGF-HAM has a high level of discriminative ability and domain flexibility and is capable of dealing with structured review data as well as non-structured, empathetic text. Figure 6.Confusion matrices of the proposed model AGF-HAM. (a) Confusion Matrix of Amazon Cell Phone; (b) Confusion Matrix of GoEmotions-1; (c) Confusion Matrix of GoEmotions-2; (d) Confusion Matrix of GoEmotions-3.",
            "6. Conclusions and Future Work": "The current research presented an overall hybrid of deep-learning architecture (HAM), which combines Transformer-based encoders, bidirectional sequential modeling, attention-based interpretability, and explainability to classify sentiment and emotion robustly. The methodology, which was structured into 13 systematically structured steps, starting with data preprocessing, embedding generation, and aspect extraction, to explainability using attention heatmaps, aspect-level interpretation, and SHAP/Integrated Gradients, guaranteed the predictive power as well as interpretative clarity. Evaluation: Experimental results on benchmark datasets, such as GoEmotions-1, GoEmotions-2, GoEmotions-3, and Amazon Cell Phones, showed the effectiveness of the proposed HAM model in comparison to the existing Transformer baselines, namely BERT, ALBERT, RoBERTa, XLNet, and hybrid models, including RoBERTa+BiLSTM and DistilBERT+Attention. Interestingly, HAM worked best with an accuracy of 96.4, a precision of 95.1, a recall of 94.7, and an F1-score of 94.9, which is far better than state-of-the-art counterparts. Such findings verify the ability of the model to effectively represent fine-grained contextual nuances, and at the same time, it is interpretable due to its explainability sub-layers. The transparency and trustworthiness of the predictions provided by the model are ensured by the addition of the Explainability Module, which includes attention heatmaps, aspect-level explanations, and SHAP/Integrated Gradients analysis, which introduces the element of transparency and trust to the performance of deep-learning models in relation to human comprehension. Such a combined methodology not only determines the polarity of sentiment but also reveals the reasons that certain tokens of textual data or factors guide model choices, allowing even more responsible AI behavior. Future studies can expand the suggested HAM architecture in a number of methodical and topic-focused directions. To begin with, as we have found out that fusion gating and hierarchical attention enhance aspect sensitivity, one way in which future research can complement this finding is by investigating the use of multimodal cues (especially text-aligned visual features) to further enhance aspect-sensitive representations. Second, since the model has been shown to successfully run on datasets with various linguistic properties, cross-lingual adaptation and domain transfer are promising extensions that can be employed technically consistently with the pipeline we have now. Third, the explainability modules (attention heatmaps and SHAP/IG) employed in this study can be extended to interactive, real-time explainability dashboards to facilitate transparency of decisions in the application areas. Lastly, a lightweight or adapter-based variant of HAM could be useful to scale to large-scale or privacy-considerate contexts, providing viable deployment advantages."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2227-7390/13/24/3892",
        "scraped_at": "2025-12-05 23:52:41"
    },
    {
        "title": "Measuring Behavioral Influence on Social Media: A Social Impact Theory Approach to Identifying Influential Users",
        "authors": "byTarirai ChaniandOludayo O. Olugbara",
        "journal": "Journal. Media2025,6(4), 205;https://doi.org/10.3390/journalmedia6040205- 5 Dec 2025",
        "abstract": "The rise of social media has democratized information sharing, allowing ordinary individuals to become influential voices in public discourse. However, traditional methods for identifying influential users rely primarily on network centrality measures that fail to capture the behavioral dynamics underlying actual influence capacity in digital environments. This study introduces the Social Influence Strength Index (SISI), a metric grounded in social impact theory that assesses influence through behavioral engagement indicators rather than network structure alone. The SISI combines three key elements: the average engagement rate, follower reach score, and mention prominence score, using a geometric mean to account for the multiplicative nature of social influence. This was developed and validated using a dataset of 1.2 million tweets from South African migration discussions, a context characterized by high emotional engagement and diverse participant types. SISI‚Äôs behavioral principles make it applicable for identifying influential voices across various social media contexts where authentic engagement matters. The results demonstrate substantial divergence between SISI and traditional centrality measures (Spearman œÅ = 0.34, 95% CI: 0.32‚Äì0.36 with eigenvector centrality; top-10 user overlap Jaccard index = 0.20), with the SISI consistently recognizing behaviorally influential users that network-based approaches overlook. Validation analyses confirm the SISI‚Äôs predictive validity (high-SISI users maintain 3.5√ó higher engagement rates in subsequent periods,p< 0.001), discriminant validity (distinguishing content creators from amplifiers, Cohen‚Äôs d = 1.32), and convergent validity with expert assessments (Spearman œÅ = 0.61 vs. œÅ = 0.28 for eigenvector centrality). The research reveals that digital influence stems from genuine audience engagement and community recognition rather than structural network positioning. By integrating social science theory with computational methods, this work presents a theoretically grounded framework for measuring digital influence, with potential applications in understanding information credibility, audience mobilization, and the evolving dynamics of social media-driven public discourse across diverse domains including marketing, policy communication, and digital information ecosystems.Keywords:digital influence;social media metrics;social impact theory;network centrality;behavioral engagement;social influencers",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "The rapid proliferation of social media platforms has fundamentally transformed how information spreads, opinions form, and behaviors change in contemporary society (Lazer et al., 2020). Digital environments create unprecedented opportunities for individuals to influence large audiences, shape public discourse, and mobilize collective action across geographical and temporal boundaries. This transformation has been particularly pronounced in journalism, where social media influencers and citizen journalists have emerged as alternative news sources, often bypassing traditional media gatekeepers to directly inform and engage audiences (Guest & Martin, 2021;Hurcombe, 2024). Understanding who wields influence in these spaces has become critical for diverse stakeholders, from marketers seeking authentic brand ambassadors (Joshi et al., 2023) to policymakers attempting to understand public sentiment (Ausat, 2023) and researchers investigating the mechanics of information diffusion (Luo, 2022). However, the measurement of social media influence remains dominated by computational approaches that prioritize network structural properties over behavioral indicators of actual influence capacity. Current methodologies rely heavily on centrality measures, such as degree centrality, betweenness centrality, closeness centrality, and PageRank. While these metrics provide valuable insights into connectivity patterns and potential reach, they operate under a fundamental assumption that structural positioning equates to influence capacity. This assumption proves problematic in digital environments where influence manifests through complex behavioral dynamics rather than mere network placement. This structural bias becomes increasingly problematic as algorithmic curation reshapes how information flows on social media platforms. Platform algorithms prioritize content based on engagement signals, likes, shares, and comments, rather than solely on structural network positioning. This means that influence increasingly depends on the behavioral ability to generate an audience response rather than follower count or network centrality (Bhandari & Bimo, 2022). Recent research shows that algorithmic amplification favors content resonance and engagement intensity over structural positioning, fundamentally changing how influence functions in digital environments (Dujeancourt & Garz, 2023;Corsi, 2024;Fern√°ndez et al., 2024). This algorithmic attention economy rewards demonstrated engagement capacity, i.e., the behavioral strength emphasized in social impact theory, rather than assumed influence based on network location, creating conditions where traditional centrality measures systematically misidentify influential actors. This limitation becomes especially critical in news and information contexts, where the ability to establish credibility and trust with audiences depends on behavioral engagement patterns rather than follower counts or network connections. This paper addresses three key research questions: (1) How can we operationalize influence beyond network positioning to incorporate behavioral engagement patterns? (2) To what extent do structural centrality measures align with multidimensional behavioral influence? (3) Can a composite index integrating audience engagement, follower strength, and media prominence better identify influential actors in political discourse? Our contribution is the Social Influence Strength Index (SISI), which combines three behavioral dimensions, audience engagement responsiveness, follower relationship strength, and media and political sphere mentions, to complement structural network measures in identifying social media influence. Guilbeault and Centola(2021) demonstrated that traditional centrality measures systematically fail for complex social contagions that require multiple peer confirmations, revealing that degree, betweenness, and PageRank centrality often misidentify influential actors in social media contexts. Their findings show that effective influence in digital environments requires accounting for reinforcing ties and behavioral confirmation processes that traditional metrics ignore completely. This finding aligns with broader critiques highlighting how centrality measures exhibit context-insensitivity, treating influence as a static, universal property rather than a dynamic phenomenon that varies across topics, audiences, and temporal contexts (Morrison et al., 2022;Spiller et al., 2020). The limitations of centrality-based approaches become apparent when examining real-world influence patterns on social media platforms. Users with extensive follower networks may generate minimal audience engagement, while others with modest followings consistently mobilize their audiences into meaningful action (Edelmann et al., 2020). Traditional metrics fail to capture these qualitative differences, focusing instead on quantitative indicators such as follower counts, connection numbers, or structural bridge positions. This structural bias overlooks the demonstrated capacity to generate authentic engagement, inspire content sharing, or stimulate meaningful dialogue. In this paper, we argue that these traits constitute genuine influence in digital spaces. These measurement challenges are particularly relevant for understanding the evolving landscape of digital journalism, where the boundaries between professional reporters, citizen journalists, and news influencers are continually blurring. The rise of influencer journalism, where individuals with significant online followings act as intermediaries of news and information, requires new approaches to assessing credibility and influence that account for behavioral engagement rather than institutional affiliation or network size alone. Therefore, the predominance of computational efficiency over theoretical validity in influence measurement reflects a broader disconnect between social science theory and digital analytics (Radford & Joseph, 2020;Schoch & Brandes, 2016). While computer scientists and data analysts have developed sophisticated algorithms for processing large-scale social media data, these approaches often lack grounding in established frameworks for understanding human social behavior. Social psychology, communication theory, and sociology offer rich insights into how influence operates in human interactions, yet these theoretical foundations remain largely untapped in computational social science applications (Edelmann et al., 2020). This theoretical gap is particularly problematic given that social media platforms, despite their digital nature, fundamentally facilitate human social interactions governed by the exact psychological and social mechanisms that operate in offline contexts. Decades of social science research have identified key factors that determine influence effectiveness, including source credibility, message resonance, audience characteristics, and contextual factors (Chalakudi et al., 2023). These insights could significantly enhance computational approaches to influence measurement, yet they remain underutilized due to the challenge of operationalizing behavioral concepts in computational frameworks. In this paper, we argue that the social impact theory, developed byBibb Latan√©(1981), provides a particularly promising framework for understanding influence dynamics. The theory states that social influence results from the interaction of three key factors: the strength of the influence source (their perceived power, authority, and social capital), the immediacy of the source to the target audience (spatial, temporal, and social proximity), and the number of sources present in the influence situation (Latan√© & Wolf, 1981). This framework has demonstrated robustness across diverse contexts and populations, yet its application to digital influence measurement remains limited, despite growing recognition of its relevance to social media analytics. The ‚Äústrength‚Äù component offers especially relevant insights for social media contexts, where users‚Äô ability to command attention, mobilize audiences, and sustain authority within digital networks varies dramatically (Harkins & Latan√©, 1998). Unlike structural positioning, strength encompasses demonstrated behavioral capacity that is the observable patterns of audience engagement, content resonance, and recognition within discourse communities that indicate actual rather than potential influence. Recent work on female Instagram creators demonstrates that influence often emerges from behavioral motivational drivers such as self-expression, empowerment, and aspiration, rather than mere structural positioning in networks (Mlangeni et al., 2025). This paper addresses the theoretical and methodological limitations of current influence measurement approaches by developing the Social Influence Strength Index (SISI), a novel metric that operationalizes social impact theory‚Äôs ‚Äústrength‚Äù component for digital environments. The SISI represents a fundamental departure from centrality-based approaches by measuring actualized influence through behavioral indicators rather than structural positioning. The metric integrates three complementary dimensions that capture different aspects of influence strength: the average engagement rate (measuring audience mobilization efficiency), follower reach score (assessing contextually normalized audience scale), and mention prominence score (evaluating discourse recognition and authority). Our primary contribution lies in demonstrating how established social science theory can enhance computational approaches to influence measurement. By grounding the SISI in social impact theory, we provide a theoretically justified framework for selecting and combining influence indicators, moving beyond ad hoc metric combinations toward principled measurement design. Empirically, we validate the SISI through a comprehensive analysis of 1.2 million tweets from South African migration discourse collected between 2021 and 2022. This dataset provides an ideal testing ground due to the topic‚Äôs high emotional engagement, diverse participant types, and authentic discourse patterns that reflect real-world influence dynamics (Tarisayi & Manik, 2020;Chiumbu & Moyo, 2018). Our findings reveal that the SISI consistently identifies influential users who are overlooked by traditional centrality measures, demonstrating that behavioral influence operates independently of structural positioning within networks. The implications extend beyond methodological innovation to practical applications across multiple domains where authentic influence identification provides a competitive advantage, from marketing strategy development (Zhou et al., 2024) to policy communication and social media research (Tang, 2023).",
            "2. The Social Influence Strength Index (SISI)": "The Social Influence Strength Index (SISI) operationalizes the strength component of social impact theory through a multidimensional framework that captures users‚Äô demonstrated capacity to influence others within digital social environments. Unlike traditional metrics that rely on structural network properties or simple engagement counts, the SISI measures actualized influence by examining behavioral manifestations of strength across three complementary dimensions. This approach reflects the theoretical understanding that influence strength emerges not from position alone but from the consistent ability to mobilize audiences, command attention, and maintain authority within discourse communities. The core premise underlying SISI design is that digital influence strength manifests through observable patterns of audience response and community recognition. In social media contexts, strength cannot be assumed from follower counts or network centrality but must be demonstrated through sustained ability to generate meaningful engagement (Wies et al., 2023), maintain audience attention, and earn recognition as a valuable contributor to ongoing conversations (Kubler, 2023). This behavioral focus aligns with social impact theory‚Äôs emphasis on actual rather than potential influence while protecting metric gaming through artificial follower inflation or engagement manipulation. Recent research supports the multidimensional approach to influence measurement. For instance,Zhuang et al.(2021) developed a multidimensional social influence (MSI) measurement approach analyzing structure-based, information-based, and action-based factors, demonstrating that influence in online social networks is a complex force determined by multiple attributes from different dimensions. Their experimental studies showed that multidimensional approaches outperformed traditional single-dimensional methods in identifying influential users across both topic-level and global-level networks. This empirical validation reinforces the theoretical rationale for SISI‚Äôs comprehensive framework. Empirical influence measurement has evolved significantly beyond follower counts.Cha et al.(2010) demonstrated that influence is multifaceted, showing weak correlations between indegree, retweets, and mentions on Twitter, establishing that no single metric captures influence comprehensively.Bakshy et al.(2011) further showed that influence depends on both reach and engagement probability, not merely network size. The SISI extends this tradition by integrating behavioral engagement (AER), relationship quality (FRS), and cross-platform visibility (MPS), whereas these earlier studies examined dimensions separately. Unlike Cha et al.‚Äôs descriptive comparison, the SISI provides a weighted composite specifically for contexts where confirmation bias and selective engagement dominate. The SISI also adopts a multidimensional approach that acknowledges the complex nature of influence strength in digital environments. Rather than relying on a single indicator that might capture only one aspect of influence capacity, the metric integrates three distinct but complementary dimensions that together provide a comprehensive assessment of strength. This design recognizes that users may demonstrate influence through different pathways. For example, some may demonstrate influence through exceptional engagement efficiency, others through broad reach within relevant communities, and others through recognition as thought leaders whose contributions shape ongoing discourse (Park & Lee, 2021). The SISI is made up of three components, which are the average engagement rate (AER), follower reach score (FRS), and mention prominence score (MPS). The AER component quantifies users‚Äô demonstrated ability to mobilize their audiences into active participation and response. Unlike cumulative engagement metrics that can be inflated by high posting frequency or large follower bases, the AER focuses on engagement efficiency, which is the consistent capacity to generate meaningful audience interaction relative to potential exposure. Recent benchmark data reveal significant variations in engagement rates across platforms and industries, with good engagement rates typically falling between 1% and 3% for most social media platforms in 2025 (ContentStudio, 2025). This component directly operationalizes social impact theory‚Äôs conceptualization of strength as the source‚Äôs ability to command attention and inspire behavioral responses from target audiences. The AER calculation normalizes total engagement by both follower count and number of posts, providing a measure of per-post engagement efficiency that remains comparable across users with different activity levels and audience sizes. The mathematical formulation incorporates weighted considerations for different interaction types:AER=1n√ó‚àë(EiDi√ó100)AER=1n√ó‚àëEiDi√ó100(1)where: EiEi= total engagements (likes, comments, shares, saves) for post i;DiDi= potential audience (preferably reach, otherwise estimated via followers √ó platform reach rate);nn= number of post analyzed. We weight interactions equally (likes = 1, replies = 1, retweets = 1, quotes = 1), treating all engagement actions as equivalent signals of audience response, consistent with platform algorithms that count all interactions toward visibility metrics. The FRS considers the context of audience size by adjusting users‚Äô follower counts relative to relevant comparison groups within their domains or platforms. While the total audience size impacts influence potential, a meaningful assessment requires understanding how users‚Äô reach compares to typical expectations within their specific contexts. A recent industry analysis shows significant variation in follower distributions across sectors, with higher education achieving engagement rates of 4.52% with 28 weekly posts on Instagram, while entertainment and media industries display different optimal posting frequencies and engagement patterns (Hootsuite, 2025).FRS=(FùíæFmedian)√ó100FRS=FiFmedian√ó100(2)where: FùíæFi= individual follower count;FmedianFmedian= median followers within the comparison group. Scores above 100 indicate above-average follower strength; scores below 100 indicate below-average strength; a score of 100 indicates median-level followers. Comparison groups were defined by topic hashtag cluster (identified via Louvain community detection on hashtag co-occurrence networks) and verified status, ensuring like-to-like comparisons within thematically similar discourse communities. This normalization prevents extreme outliers from distorting assessments while still recognizing exceptional reach when it occurs, providing intuitive interpretation where scores above 100 indicate above-average reach while scores below 100 suggest below-average audience size relative to contextual expectations. The MPS captures an actor‚Äôs visibility and prominence beyond their immediate network by measuring how frequently they are mentioned by media outlets, journalists, political figures, and verified accounts in the broader public discourse. Unlike follower-based metrics that reflect self-selected audiences, the MPS indicates recognition by elite information brokers and agenda-setters who amplify certain voices into wider political conversations. The MPS is calculated as:MPS=(MiMmedian)√ó100MPS=MiMmedian√ó100(3)where: MiMi= total mentions by verified account (direct @tags, replies, quote shares);MmedianMmedian= median mentions in the comparison group. Scores above 100 indicate above-average media visibility and elite recognition; scores below 100 indicate below-average visibility; a score of 100 represents median-level prominence. We identified media and political accounts using two criteria: (1) verified status combined with account bio keywords (e.g., ‚Äújournalist‚Äù, ‚Äúnews‚Äù, ‚Äúreporter‚Äù, ‚Äúpolitician‚Äù, ‚Äúsenator‚Äù); (2) manual validation of high-frequency mentioners to ensure classification accuracy. This approach captures mentions that signal influence beyond grassroots engagement, reflecting the user‚Äôs ability to penetrate elite discourse spaces and shape agenda-setting processes. The ratio-based formulation provides intuitive interpretation while maintaining comparability across users operating in different discourse communities. The SISI integrates its three component dimensions through a geometric mean calculation that reflects the multiplicative nature of influence processes described in social impact theory. The mathematical formulation is:SISI = ‚àõ(AER √ó FRS √ó MPS)(4) To ensure equal weighting in the geometric mean calculation, we apply min‚Äìmax scaling to normalize each component (AER, FRS, MPS) to the [0, 1] interval:ScaledComponent=RawComponent‚àíMinMax‚àíMinScaledComponent=RawComponent‚àíMinMax‚àíMin Throughout theSection 4, all reported component values represent scaled scores (0‚Äì1) unless explicitly noted. To ensure computational stability, we implement three preprocessing steps before calculating the geometric mean. First, we handle zeros by adding a small constant Œµ = 1 √ó 10‚àí6to each component (AER, FRS, MPS) before integration, preventing the geometric mean from collapsing to zero when any single dimension is zero. This approach is preferable to deletion (which would lose valuable partial-influence profiles) or substitution with median values (which would distort individual scores). Second, we apply min‚Äìmax scaling to normalize each component to the [0, 1] interval within the dataset, ensuring equal weighting in the geometric mean despite different raw scales. Third, we cap outliers at the 99th percentile for each component before scaling to prevent extreme values from distorting the distribution. These steps ensure the SISI captures multidimensional influence while maintaining robustness to edge cases and scale differences. The geometric mean calculation prevents metric gaming by requiring authentic performance across all influence dimensions rather than allowing users to achieve high scores through artificial inflation of single components. This approach aligns with the theoretical understanding that effective influence emerges from the interaction of multiple factors rather than their simple combination. The choice of the geometric mean for SISI integration reflects careful consideration of how influence components interact in real-world social systems. Social impact theory‚Äôs multiplicative formulation suggests that influence effectiveness depends on the simultaneous presence of multiple factors rather than their independent contribution. Mathematical properties of the geometric mean align closely with these theoretical expectations, as the multiplicative calculation means that each component contributes proportionally to the final score, with increases in any single component producing effects that depend on the values of other components. Users demonstrating a high AER, combined with a moderate FRS, typically represent niche experts or micro-influencers who have cultivated highly engaged communities around specialized content. These profiles indicate exceptional ability to create meaningful interactions with available audiences, suggesting strong content quality and audience resonance within specific domains.",
            "3. Methodology": "3.1. Research Context and Data SelectionTo evaluate the proposed metric, we selected the South African migration discourse on the social media platform X (formerly Twitter) as our primary data source, covering the period from 1 January 2021 to 31 December 2022. This choice was driven by several methodological considerations that align with the theoretical requirements for testing influence metrics grounded in social impact theory while addressing contemporary challenges in social media data collection.The migration discourse in South Africa represents an ideal context for influence measurement validation due to its inherently polarizing and emotionally engaging nature. The topic consistently generates high levels of public participation, creating rich datasets of authentic user interactions that are essential for validating metrics designed to measure behavioral influence (Hove, 2022). Recent research has demonstrated that xenophobic discourse in South Africa has shifted from physical confrontations to ongoing dialogue on public platforms, such as social media, with Twitter serving as a primary arena for these discussions (Makhura, 2022). Unlike artificially stimulated engagement or promotional content, migration discussions reflect genuine public sentiment, producing interaction patterns that accurately represent real-world influence dynamics.The diversity of participants in migration discourse provides another crucial advantage for validation purposes. These discussions attract politicians, activists, journalists, academics, civil society organizations, and ordinary citizens, creating a heterogeneous user population with varying influence mechanisms and audience relationships. This diversity enables testing of the SISI‚Äôs capacity to identify influence across different user types. The emergence of organized digital movements such as Operation Dudula and Put South Africans First during this period provides particularly valuable natural experiments for understanding influence mobilization through social media platforms (Tarisayi, 2024;Dratwa, 2023). 3.2. Data Collection Protocol3.2.1. Data CollectionThe data collection process employed a systematic, multi-phase approach designed to ensure we comprehensively captured the migration-related discourse while maintaining data quality and thematic relevance. The protocol balanced breadth of coverage with specificity to migration topics, employing iterative refinement processes that adapted to evolving discourse patterns throughout the collection period. Recent methodological innovations in social media research emphasize the importance of such adaptive approaches, particularly given the dynamic nature of online discourse and the emergence of new hashtags and terminologies (Kim et al., 2023;Chani et al., 2023).3.2.2. Keyword Development and ValidationThe initial phase established a set of keywords through triangulated input sources, including an academic literature review, empirical platform exploration, and expert consultation with migration researchers and civil society organizations. The academic literature review identified terminology commonly used in scholarly migration research, providing theoretical grounding for keyword selection. Preliminary platform exploration using broad search terms such as ‚Äúmigration SA‚Äù, ‚Äúforeign nationals‚Äù, and ‚Äúxenophobia‚Äù revealed frequently occurring hashtags and phrases in real-time user conversations, including emergent terms such as #OperationDudula and #ForeignersMustGo that became central to the discourse during the study period.Expert consultation with migration researchers and civil society organizations provided crucial validation of keyword relevance while identifying colloquial terms and emerging hashtags that might be overlooked through purely academic or algorithmic approaches. This consultation process ensured that the keyword selection process captured authentic discourse patterns while maintaining a focus on migration-related themes rather than tangentially related content. The resulting initial keyword set included both formal terminology (immigration, xenophobia, foreign nationals) and colloquial expressions (#PutSouthAfricansFirst, #ForeignersMustGo) that reflect actual user language patterns documented in recent research on South African digital xenophobia (Raborife et al., 2024).The final keyword set included 28 hashtags (#OperationDudula, #ForeignersMustGo, #PutSouthAfricansFirst, #XenophobiaInSA, #IllegalImmigrants, #BorderSecurity, among others) and 15 keywords (‚Äúillegal immigrants‚Äù, ‚Äúundocumented foreigners‚Äù, ‚Äúforeign nationals‚Äù, ‚Äúmigration policy‚Äù, ‚Äúxenophobia‚Äù, ‚Äúborder control‚Äù, etc.). An English language filter (lang:en) was applied via Twitter‚Äôs native detection. Tweets containing at least one hashtag OR keyword were included. Retweets were included in the network construction but excluded from AER calculations; quote tweets were treated as original content; replies were included in all metrics.3.2.3. Iterative Refinement and ExpansionThe keyword refinement process employed a hashtag co-occurrence analysis and event-driven expansion to remain responsive to evolving discourse patterns. Weekly reviews of collected data identified frequently co-occurring hashtags that indicated relevant content worth including. Event-driven analyses monitored discourse spikes around specific incidents such as policy announcements, protests, or outbreaks of violence, which often introduced new terminology or revived dormant hashtags. This adaptable approach was vital, particularly with the appearance of new movements and hashtags during the collection period, including the June 2021 launch of Operation Dudula‚Äôs ‚ÄúLet‚Äôs Clean Soweto‚Äù campaign, which generated notable social media activity. 3.3. Data ExtractionFollowing best practices for ethical social media research established by recent methodological guidance (Chani et al., 2023;Chen et al., 2024), the extraction process employed systematic sampling approaches that ensured representative coverage. At all stages, procedures were carefully aligned with the platform‚Äôs terms of service and strict user privacy protections. The extraction was carried out using Python 3.10, specifically leveraging the SNScrape library, which enabled the automated retrieval of publicly available posts while preserving the metadata integrity. Only publicly accessible content was collected, and no private or restricted data were accessed. To further safeguard our research ethics, protocols were established for the secure handling of user-related information, including anonymization of identifiers, minimization of sensitive data, and compliance with contemporary standards for social media research.The sampling process was continuous (not periodic) via SNScrape‚Äôs real-time collection over the entire period. Missing engagement metrics (<0.1% of tweets) were excluded from AER calculations but retained for the network analysis. The de-duplication process used tweet IDs as unique identifiers. The bot filtering process employed account-level heuristics; accounts with >50 tweets/day or >90% retweet ratios were flagged and excluded, affecting approximately 3.2% of collected accounts. This research received institutional ethics clearance for a secondary analysis of public social media data. All user IDs were hashed (SHA-256), no usernames appeared in outputs, and aggregate reporting maintained n ‚â• 10 group sizes to prevent re-identification. 3.4. Validation ProceduresBeyond comparing the SISI with traditional centrality measures, we conducted three validation tests to establish predictive, discriminant, and convergent validity.3.4.1. Predictive ValidityWe calculated SISI scores using data from January 2021 to September 2022, then tested whether high-SISI users maintained elevated engagement rates in Q4-2022. Users were divided into quartiles by SISI score. Only users posting ‚â•5 times in Q4-2022 were included (n = 12,847). We compared engagement rates across quartiles using Mann‚ÄìWhitney U tests due to non-normal distributions.3.4.2. Discriminant ValidityWe sampled 500 highly retweeted original tweets (‚â•100 retweets, April‚ÄìSeptember 2022) using stratified random sampling (n = 100 per month). Three trained coders classified each tweet as:Content Creator: Original analyses, firsthand reporting, novel arguments;Amplifier: Primarily retweets or quotes with minimal added value;Mixed: Combines substantial original content with amplification.Coders were trained on 50 pilot tweets until achieving Krippendorff‚Äôs Œ± ‚â• 0.80. The final sample inter-coder reliability Œ± was 0.83 (95% CI: 0.79‚Äì0.87). We compared SISI scores between content creators (n = 287) and amplifiers (n = 156) using independent samplest-tests, excluding mixed cases (n = 57).3.4.3. Convergent ValidityThree migration experts (PhDs with ‚â•5 years‚Äô of South African migration research, ‚â•3 publications) rated 50 randomly sampled users stratified across SISI quintiles (n = 10 per quintile). Experts received anonymized profiles containing three tweets and aggregate statistics (posts, engagement rate, followers) but no network position information. Experts rated influence on 7-point Likert scales; inter-rater reliability: ICC(2,3) = 0.74 (95% CI: 0.63‚Äì0.83). We computed Spearman correlations between averaged expert ratings and both SISI and eigenvector centrality.",
            "3.1. Research Context and Data Selection": "To evaluate the proposed metric, we selected the South African migration discourse on the social media platform X (formerly Twitter) as our primary data source, covering the period from 1 January 2021 to 31 December 2022. This choice was driven by several methodological considerations that align with the theoretical requirements for testing influence metrics grounded in social impact theory while addressing contemporary challenges in social media data collection. The migration discourse in South Africa represents an ideal context for influence measurement validation due to its inherently polarizing and emotionally engaging nature. The topic consistently generates high levels of public participation, creating rich datasets of authentic user interactions that are essential for validating metrics designed to measure behavioral influence (Hove, 2022). Recent research has demonstrated that xenophobic discourse in South Africa has shifted from physical confrontations to ongoing dialogue on public platforms, such as social media, with Twitter serving as a primary arena for these discussions (Makhura, 2022). Unlike artificially stimulated engagement or promotional content, migration discussions reflect genuine public sentiment, producing interaction patterns that accurately represent real-world influence dynamics. The diversity of participants in migration discourse provides another crucial advantage for validation purposes. These discussions attract politicians, activists, journalists, academics, civil society organizations, and ordinary citizens, creating a heterogeneous user population with varying influence mechanisms and audience relationships. This diversity enables testing of the SISI‚Äôs capacity to identify influence across different user types. The emergence of organized digital movements such as Operation Dudula and Put South Africans First during this period provides particularly valuable natural experiments for understanding influence mobilization through social media platforms (Tarisayi, 2024;Dratwa, 2023).",
            "3.2. Data Collection Protocol": "3.2.1. Data CollectionThe data collection process employed a systematic, multi-phase approach designed to ensure we comprehensively captured the migration-related discourse while maintaining data quality and thematic relevance. The protocol balanced breadth of coverage with specificity to migration topics, employing iterative refinement processes that adapted to evolving discourse patterns throughout the collection period. Recent methodological innovations in social media research emphasize the importance of such adaptive approaches, particularly given the dynamic nature of online discourse and the emergence of new hashtags and terminologies (Kim et al., 2023;Chani et al., 2023). 3.2.2. Keyword Development and ValidationThe initial phase established a set of keywords through triangulated input sources, including an academic literature review, empirical platform exploration, and expert consultation with migration researchers and civil society organizations. The academic literature review identified terminology commonly used in scholarly migration research, providing theoretical grounding for keyword selection. Preliminary platform exploration using broad search terms such as ‚Äúmigration SA‚Äù, ‚Äúforeign nationals‚Äù, and ‚Äúxenophobia‚Äù revealed frequently occurring hashtags and phrases in real-time user conversations, including emergent terms such as #OperationDudula and #ForeignersMustGo that became central to the discourse during the study period.Expert consultation with migration researchers and civil society organizations provided crucial validation of keyword relevance while identifying colloquial terms and emerging hashtags that might be overlooked through purely academic or algorithmic approaches. This consultation process ensured that the keyword selection process captured authentic discourse patterns while maintaining a focus on migration-related themes rather than tangentially related content. The resulting initial keyword set included both formal terminology (immigration, xenophobia, foreign nationals) and colloquial expressions (#PutSouthAfricansFirst, #ForeignersMustGo) that reflect actual user language patterns documented in recent research on South African digital xenophobia (Raborife et al., 2024).The final keyword set included 28 hashtags (#OperationDudula, #ForeignersMustGo, #PutSouthAfricansFirst, #XenophobiaInSA, #IllegalImmigrants, #BorderSecurity, among others) and 15 keywords (‚Äúillegal immigrants‚Äù, ‚Äúundocumented foreigners‚Äù, ‚Äúforeign nationals‚Äù, ‚Äúmigration policy‚Äù, ‚Äúxenophobia‚Äù, ‚Äúborder control‚Äù, etc.). An English language filter (lang:en) was applied via Twitter‚Äôs native detection. Tweets containing at least one hashtag OR keyword were included. Retweets were included in the network construction but excluded from AER calculations; quote tweets were treated as original content; replies were included in all metrics. 3.2.3. Iterative Refinement and ExpansionThe keyword refinement process employed a hashtag co-occurrence analysis and event-driven expansion to remain responsive to evolving discourse patterns. Weekly reviews of collected data identified frequently co-occurring hashtags that indicated relevant content worth including. Event-driven analyses monitored discourse spikes around specific incidents such as policy announcements, protests, or outbreaks of violence, which often introduced new terminology or revived dormant hashtags. This adaptable approach was vital, particularly with the appearance of new movements and hashtags during the collection period, including the June 2021 launch of Operation Dudula‚Äôs ‚ÄúLet‚Äôs Clean Soweto‚Äù campaign, which generated notable social media activity.",
            "3.2.1. Data Collection": "The data collection process employed a systematic, multi-phase approach designed to ensure we comprehensively captured the migration-related discourse while maintaining data quality and thematic relevance. The protocol balanced breadth of coverage with specificity to migration topics, employing iterative refinement processes that adapted to evolving discourse patterns throughout the collection period. Recent methodological innovations in social media research emphasize the importance of such adaptive approaches, particularly given the dynamic nature of online discourse and the emergence of new hashtags and terminologies (Kim et al., 2023;Chani et al., 2023).",
            "3.2.2. Keyword Development and Validation": "The initial phase established a set of keywords through triangulated input sources, including an academic literature review, empirical platform exploration, and expert consultation with migration researchers and civil society organizations. The academic literature review identified terminology commonly used in scholarly migration research, providing theoretical grounding for keyword selection. Preliminary platform exploration using broad search terms such as ‚Äúmigration SA‚Äù, ‚Äúforeign nationals‚Äù, and ‚Äúxenophobia‚Äù revealed frequently occurring hashtags and phrases in real-time user conversations, including emergent terms such as #OperationDudula and #ForeignersMustGo that became central to the discourse during the study period. Expert consultation with migration researchers and civil society organizations provided crucial validation of keyword relevance while identifying colloquial terms and emerging hashtags that might be overlooked through purely academic or algorithmic approaches. This consultation process ensured that the keyword selection process captured authentic discourse patterns while maintaining a focus on migration-related themes rather than tangentially related content. The resulting initial keyword set included both formal terminology (immigration, xenophobia, foreign nationals) and colloquial expressions (#PutSouthAfricansFirst, #ForeignersMustGo) that reflect actual user language patterns documented in recent research on South African digital xenophobia (Raborife et al., 2024). The final keyword set included 28 hashtags (#OperationDudula, #ForeignersMustGo, #PutSouthAfricansFirst, #XenophobiaInSA, #IllegalImmigrants, #BorderSecurity, among others) and 15 keywords (‚Äúillegal immigrants‚Äù, ‚Äúundocumented foreigners‚Äù, ‚Äúforeign nationals‚Äù, ‚Äúmigration policy‚Äù, ‚Äúxenophobia‚Äù, ‚Äúborder control‚Äù, etc.). An English language filter (lang:en) was applied via Twitter‚Äôs native detection. Tweets containing at least one hashtag OR keyword were included. Retweets were included in the network construction but excluded from AER calculations; quote tweets were treated as original content; replies were included in all metrics.",
            "3.2.3. Iterative Refinement and Expansion": "The keyword refinement process employed a hashtag co-occurrence analysis and event-driven expansion to remain responsive to evolving discourse patterns. Weekly reviews of collected data identified frequently co-occurring hashtags that indicated relevant content worth including. Event-driven analyses monitored discourse spikes around specific incidents such as policy announcements, protests, or outbreaks of violence, which often introduced new terminology or revived dormant hashtags. This adaptable approach was vital, particularly with the appearance of new movements and hashtags during the collection period, including the June 2021 launch of Operation Dudula‚Äôs ‚ÄúLet‚Äôs Clean Soweto‚Äù campaign, which generated notable social media activity.",
            "3.3. Data Extraction": "Following best practices for ethical social media research established by recent methodological guidance (Chani et al., 2023;Chen et al., 2024), the extraction process employed systematic sampling approaches that ensured representative coverage. At all stages, procedures were carefully aligned with the platform‚Äôs terms of service and strict user privacy protections. The extraction was carried out using Python 3.10, specifically leveraging the SNScrape library, which enabled the automated retrieval of publicly available posts while preserving the metadata integrity. Only publicly accessible content was collected, and no private or restricted data were accessed. To further safeguard our research ethics, protocols were established for the secure handling of user-related information, including anonymization of identifiers, minimization of sensitive data, and compliance with contemporary standards for social media research. The sampling process was continuous (not periodic) via SNScrape‚Äôs real-time collection over the entire period. Missing engagement metrics (<0.1% of tweets) were excluded from AER calculations but retained for the network analysis. The de-duplication process used tweet IDs as unique identifiers. The bot filtering process employed account-level heuristics; accounts with >50 tweets/day or >90% retweet ratios were flagged and excluded, affecting approximately 3.2% of collected accounts. This research received institutional ethics clearance for a secondary analysis of public social media data. All user IDs were hashed (SHA-256), no usernames appeared in outputs, and aggregate reporting maintained n ‚â• 10 group sizes to prevent re-identification.",
            "3.4. Validation Procedures": "Beyond comparing the SISI with traditional centrality measures, we conducted three validation tests to establish predictive, discriminant, and convergent validity. 3.4.1. Predictive ValidityWe calculated SISI scores using data from January 2021 to September 2022, then tested whether high-SISI users maintained elevated engagement rates in Q4-2022. Users were divided into quartiles by SISI score. Only users posting ‚â•5 times in Q4-2022 were included (n = 12,847). We compared engagement rates across quartiles using Mann‚ÄìWhitney U tests due to non-normal distributions. 3.4.2. Discriminant ValidityWe sampled 500 highly retweeted original tweets (‚â•100 retweets, April‚ÄìSeptember 2022) using stratified random sampling (n = 100 per month). Three trained coders classified each tweet as:Content Creator: Original analyses, firsthand reporting, novel arguments;Amplifier: Primarily retweets or quotes with minimal added value;Mixed: Combines substantial original content with amplification.Coders were trained on 50 pilot tweets until achieving Krippendorff‚Äôs Œ± ‚â• 0.80. The final sample inter-coder reliability Œ± was 0.83 (95% CI: 0.79‚Äì0.87). We compared SISI scores between content creators (n = 287) and amplifiers (n = 156) using independent samplest-tests, excluding mixed cases (n = 57). 3.4.3. Convergent ValidityThree migration experts (PhDs with ‚â•5 years‚Äô of South African migration research, ‚â•3 publications) rated 50 randomly sampled users stratified across SISI quintiles (n = 10 per quintile). Experts received anonymized profiles containing three tweets and aggregate statistics (posts, engagement rate, followers) but no network position information. Experts rated influence on 7-point Likert scales; inter-rater reliability: ICC(2,3) = 0.74 (95% CI: 0.63‚Äì0.83). We computed Spearman correlations between averaged expert ratings and both SISI and eigenvector centrality.",
            "3.4.1. Predictive Validity": "We calculated SISI scores using data from January 2021 to September 2022, then tested whether high-SISI users maintained elevated engagement rates in Q4-2022. Users were divided into quartiles by SISI score. Only users posting ‚â•5 times in Q4-2022 were included (n = 12,847). We compared engagement rates across quartiles using Mann‚ÄìWhitney U tests due to non-normal distributions.",
            "3.4.2. Discriminant Validity": "We sampled 500 highly retweeted original tweets (‚â•100 retweets, April‚ÄìSeptember 2022) using stratified random sampling (n = 100 per month). Three trained coders classified each tweet as: Content Creator: Original analyses, firsthand reporting, novel arguments;Amplifier: Primarily retweets or quotes with minimal added value;Mixed: Combines substantial original content with amplification. Coders were trained on 50 pilot tweets until achieving Krippendorff‚Äôs Œ± ‚â• 0.80. The final sample inter-coder reliability Œ± was 0.83 (95% CI: 0.79‚Äì0.87). We compared SISI scores between content creators (n = 287) and amplifiers (n = 156) using independent samplest-tests, excluding mixed cases (n = 57).",
            "3.4.3. Convergent Validity": "Three migration experts (PhDs with ‚â•5 years‚Äô of South African migration research, ‚â•3 publications) rated 50 randomly sampled users stratified across SISI quintiles (n = 10 per quintile). Experts received anonymized profiles containing three tweets and aggregate statistics (posts, engagement rate, followers) but no network position information. Experts rated influence on 7-point Likert scales; inter-rater reliability: ICC(2,3) = 0.74 (95% CI: 0.63‚Äì0.83). We computed Spearman correlations between averaged expert ratings and both SISI and eigenvector centrality.",
            "4. Results": "4.1. Dataset Overview and Descriptive StatisticsThe data collection process produced a comprehensive dataset of 1.2 million tweets from 47,892 unique users involved in South African migration discussions between 1 January 2021 and 31 December 2022. This extensive dataset provides a solid foundation for validating the SISI‚Äôs effectiveness in identifying influential users and allows for detailed comparisons with traditional centrality measures across various user types and engagement patterns. The temporal distribution analysis shows significant variation in discourse activity over the collection period, with notable spikes corresponding to major migration-related events such as the Zimbabwe Exemption Permit policy debates in mid-2021, xenophobic incidents across various provinces, and the rise of digital movements such as Operation Dudula. These activity peaks provided natural experiments for influence measurement, creating periods where users‚Äô ability to shape discourse and mobilize audiences became especially evident. The average daily tweet volume ranged from 1200 tweets during baseline periods to over 15,000 during peak events, demonstrating the dataset‚Äôs capacity to capture both routine discourse and crisis-driven engagement patterns.User participation patterns demonstrate the diverse nature of migration discourse, with participants ranging from highly active political commentators who post hundreds of times each month to occasional contributors who tweet only during major events. This variety creates ideal conditions for testing the SISI‚Äôs ability to identify influence across different engagement strategies and user types. The dataset includes verified accounts representing politicians, journalists, and organizations alongside unverified individual users, allowing an analysis of how formal authority markers relate to actual influence capacity measured through behavioral indicators.An analysis of engagement distribution reveals typical heavy-tailed patterns common in social media interactions, with approximately 5% of tweets generating 70% of total engagement, while most receive little interaction. However, preliminary findings indicate that high-engagement content does not directly correlate with traditional centrality measures, offering initial evidence for the need for behavioral influence metrics that measure audience mobilization capacity rather than structural positioning. The median engagement rate across all users was 1.2%, with notable variation from users achieving rates below 0.1% to outstanding performers exceeding 10% engagement.To provide a concise overview of the dataset and to support the descriptive statistics reported above,Table 1summarizes the key characteristics of the collected corpus.Table 1.Dataset Overview. 4.2. SISI Component Analysis4.2.1. Average Engagement Rate (AER)The analysis of average engagement rates reveals significant variation in users‚Äô capacity to mobilize their audiences, with the highest-performing users demonstrating engagement rates that exceed typical levels by several orders of magnitude. The top-ranking user achieved an exceptionally high AER, indicating a strong capacity to generate audience interaction relative to follower count and posting frequency. This performance stands in sharp contrast to the user‚Äôs more moderate centrality rankings (4th in betweenness centrality, 21st in closeness centrality, and 15th in eigenvector centrality), highlighting a clear divergence between structural positioning within the network and behavioral influence capacity.Contemporary social media benchmarks provide important context for these findings. A recent industry analysis showed that good engagement rates typically fall between 1% and 3% for most social media platforms in 2025, with rates above 3% considered excellent (Social Insider, 2025). Within our dataset, the top 10% of users by AER achieved rates consistently above 5%, placing them in the exceptional performer category according to current industry standards. This exceptional performance occurred despite many of these users having moderate follower counts, supporting the SISI‚Äôs theoretical foundation in measuring actualized rather than potential influence.A detailed examination of high-AER users revealed consistent patterns of content that resonates deeply with audiences, generating substantial retweet and reply activity that extends well beyond passive consumption. The top-performing user generated 23,113 retweets and 90,133 likes across 1093 posts, indicating sustained ability to create content that audiences find sufficiently valuable to actively amplify and engage with. This level of engagement reflects genuine influence capacity that traditional metrics fail to capture adequately, as demonstrated by the weak correlation (r = 0.31) between AER scores and combined centrality rankings.The comparative analysis between high-AER and high-centrality users reveals systematic differences in audience mobilization patterns. Users achieving high centrality scores often demonstrate substantial follower counts and network connectivity but generate proportionally lower engagement rates, suggesting that structural positioning does not automatically translate to audience activation capacity. This finding supports the SISI‚Äôs theoretical foundation in measuring actualized rather than potential influence while validating recent research emphasizing the importance of engagement quality over quantity metrics.4.2.2. Follower Reach Score (FRS) Distribution and ContextThe follower reach score analysis reveals the importance of contextual normalization for meaningful influence assessments across diverse user types and domains. The highest-scoring user achieved an FRS of 0.48, indicating an audience size substantially above average for their relevant comparison group. The highest-scoring user achieved a scaled FRS of 0.48 (raw FRS = 148), indicating an audience size 48% above the median follower count in their comparison groupHowever, this user‚Äôs performance in traditional centrality measures (273rd in betweenness, 30th in closeness, 45th in eigenvector) demonstrates a limited correlation between audience reach and network structural positioning, supporting the theoretical rationale for contextual normalization in influence measurement.The contextual approach embedded in FRS design has proved essential for meaningful comparisons across different user types within the migration discourse dataset. Political commentators, institutional accounts, activists, and ordinary citizens operate within distinct ecosystems where typical follower counts vary dramatically. Recent benchmarking research confirms this variation, showing that follower growth rates differ significantly across industries and account types, with smaller accounts often achieving proportionally higher engagement rates despite lower absolute follower numbers (Wies et al., 2023).The analysis of FRS distribution patterns reveals substantial variation even among users with similar absolute follower counts, reflecting differences in domain contexts and audience development strategies. Users specializing in migration discourse typically maintained smaller but more engaged audiences compared to general political commentators, resulting in higher FRS scores that reflect their specialized influence within relevant communities. This pattern validates the theoretical rationale for contextual normalization while demonstrating the FRS‚Äôs capacity to identify users whose audience development exceeds expectations for their particular contexts and domains.Particularly revealing are cases where users with substantial follower counts relative to their domains fail to achieve correspondingly high centrality rankings. One prominent example involves a verified institutional account with a broad audience reach but limited connectivity across network pathways, suggesting concentrated influence within specific follower communities rather than broader network influence. This pattern highlights the distinction between audience potential and network structural importance that FRS normalization helps clarify, supporting the multidimensional approach embedded in SISI design.4.2.3. Mention Prominence Score (MPS) Results and Community RecognitionThe mention prominence score analysis identifies users who have achieved recognition as key discourse participants regardless of their follower counts or network structural positions. The highest-scoring user received 3936 mentions throughout the analysis period, indicating substantial recognition within migration discourse communities. Notably, this user achieved moderate centrality rankings (169th in betweenness, 3rd in closeness, and 1st eigenvector centrality) that do not fully reflect their discourse prominence and community recognition, demonstrating the independent value of mention-based influence assessment.The relationship between mention prominence and traditional centrality measures proves complex and inconsistent across users. While some high-MPS users also achieve strong centrality scores, others demonstrate significant discourse recognition despite limited structural network positioning. This pattern suggests that thought leadership and community recognition operate through mechanisms distinct from formal network connections, supporting the SISI‚Äôs multidimensional approach to influence measurement and aligning with recent research on the complexity of social media influence dynamics (Han & Balabanis, 2024).The diversity analysis within MPS calculations reveals that high-scoring users receive mentions from broad ranges of community participants rather than concentrated attention from small follower groups. This broad recognition pattern indicates the genuine community standing rather than artificial prominence generated through coordinated campaigns or narrow follower engagement. The diversity component effectively distinguishes between authentic discourse leadership and manufactured visibility, providing protection against manipulation that has become increasingly important given documented concerns about artificial influence inflation (Annaki et al., 2025;Okoronkwo, 2024).The temporal analysis of mention patterns demonstrates that high-MPS users maintain consistent recognition throughout the analysis period rather than achieving temporary prominence during isolated events. This sustained pattern indicates the established community standing and ongoing thought leadership rather than situational visibility, providing evidence of genuine influence capacity that extends beyond momentary attention. The temporal consistency validates the MPS as a measure of stable influence characteristics rather than temporary phenomena, supporting its utility for practical influence identification applications. 4.3. Integrated SISI Performance and Validation4.3.1. Overall SISI Rankings and Centrality DivergenceThe comprehensive SISI analysis reveals systematic divergence from traditional centrality measures in identifying influential users within the migration discourse. The top five SISI-ranked users demonstrate influence patterns that traditional metrics fail to capture adequately, with several achieving high SISI scores despite moderate or low centrality rankings across multiple measures. This divergence provides strong empirical support for the theoretical argument that influence operates through behavioral mechanisms distinct from network structure properties.To further illustrate the divergence between behavioral influence and structural network positioning,Table 2presents the top 10 users ranked by SISI scores alongside their AER, FRS, and MPS components and corresponding centrality rankings. The table highlights the limited overlap between high SISI scorers and traditional network-based influence indicators.Table 2.Top 10 users by Social Influence Strength Index (SISI) score.The highest SISI-scoring user exemplifies this divergence pattern, ranking 4th in betweenness centrality, 21st in closeness centrality, and 15th in eigenvector centrality. Despite these moderate structural positions, this user demonstrates exceptional capacity for audience mobilization through sustained high engagement rates and substantial content amplification. With over 20,000 users retweeting their content and nearly 100,000 total reactions, this user achieves a clear behavioral influence that centrality measures systematically underestimate.The ranking comparison analysis reveals that only two of the top five SISI users appear in the top ten of any traditional centrality measure, indicating substantial non-overlap between structural and behavioral influence identification. This finding supports the theoretical argument that influence operates through behavioral mechanisms distinct from network structural properties, validating the SISI‚Äôs focus on demonstrated rather than assumed influence capacity. The divergence aligns with recent meta-analytic research demonstrating that social media influence effectiveness depends more on content quality and audience engagement than on structural network positioning (Han & Balabanis, 2024).The second-ranked SISI scorer had a notably strong centrality performance (169th in betweenness, 3rd in closeness, 1st in eigenvector), representing a case where behavioral and structural influence align. However, this user‚Äôs exceptional performance stems from achieving remarkable engagement efficiency, with only 83 posts generating over 20,000 reactions and nearly 7000 retweets. This efficiency demonstrates influence capacity that extends beyond structural positioning to encompass content quality and audience resonance, supporting the behavioral focus embedded in SISI design.To visualize the relationship between behavioral influence and structural positioning,Figure 1displays a scatterplot of SISI scores against eigenvector centrality with the corresponding Pearson and Spearman correlations, confidence intervals, and sample size. This supports the claim of substantial non-overlap between behavioral and structural influence measures. As both variables exhibit skewed distributions, we report Spearman‚Äôs œÅ as our primary metric (with Pearson‚Äôs r included for comparability), along with 95% bootstrap confidence intervals (1000 resamples),p-values, and the sample size (n = 47,892).Figure 1.Scatterplot of SISI scores vs. eigenvector centrality.4.3.2. Behavioral Influence Evidence and Quality‚ÄìQuantity PatternsThe SISI analysis reveals consistent patterns where influence stems from content quality and audience resonance rather than posting volume or follower accumulation. Multiple high-scoring users achieve substantial influence with modest posting frequencies, while others with extensive posting activity generate proportionally lower audience mobilization rates. These patterns validate the SISI‚Äôs theoretical foundation in measuring actualized influence through behavioral indicators rather than activity metrics, aligning with recent research that emphasizes engagement quality over posting frequency (Mufadhol et al., 2024).Particularly compelling evidence emerges from a comparison between users with similar follower counts but dramatically different SISI scores. Users achieving high behavioral influence demonstrate consistent ability to generate meaningful audience interaction that extends beyond passive consumption to active engagement and content amplification. This pattern indicates genuine influence capacity that motivates audiences to invest effort in sharing, commenting, and extending conversations around influential users‚Äô content, supporting the behavioral grounding embedded in the SISI‚Äôs theoretical foundation.The content amplification analysis reveals that high-SISI users achieve substantially greater retweet rates relative to their follower bases, indicating content that resonates sufficiently to motivate an audience-driven distribution. This organic amplification represents particularly strong evidence of influence, since it reflects the audience‚Äôs choice to actively promote content rather than passive consumption. Users achieving high amplification rates demonstrate the capacity to create content that audiences find valuable enough to associate with their own online identities through sharing, validating the engagement quality focus embedded in AER calculations.The quality versus quantity distinction proves especially important for understanding influence mechanisms within the migration discourse. Users who focus on creating thoughtful, substantive content consistently outperform those who prioritize high-frequency posting, suggesting that audience value perceptions drive influence more effectively than mere visibility or activity. This finding has significant implications for influence strategy and validates the SISI‚Äôs emphasis on engagement quality rather than volume metrics, supporting recent industry research emphasizing authentic engagement over superficial activity measures (Nwaiwu et al., 2024). 4.4. Model ValidationTo validate the SISI‚Äôs effectiveness, we conducted three validation tests following procedures detailed inSection 4.4. First, for predictive validity, we examined whether high-SISI users‚Äô subsequent posts (in the final quarter of 2022) maintained elevated engagement rates. Users in the top SISI quartile maintained median engagement rates of 3.8% in subsequent periods, compared to 1.1% for the overall population (Mann‚ÄìWhitney U = 18,432,p< 0.001), demonstrating temporal stability. Second, for discriminant validity, we compared the SISI‚Äôs ability to distinguish between users who generated discourse-shaping content (identified through qualitative coding of highly retweeted original tweets, n = 500) versus those who primarily amplified others‚Äô content. The SISI scores were significantly higher for content creators (M = 0.42, SD = 0.18) than amplifiers (M = 0.19, SD = 0.12; t(498) = 12.7,p< 0.001), while centrality measures showed no significant difference (eigenvector: t(498) = 1.3,p= 0.19). Third, for convergent validity, expert assessments from three migration researchers rating the influence of 50 randomly selected users correlated significantly with the SISI (Spearman‚Äôs œÅ = 0.61,p< 0.001) but weakly with eigenvector centrality (œÅ = 0.28,p< 0.05). These validation tests demonstrate that the SISI captures meaningful influence dimensions beyond structural positioning.",
            "4.1. Dataset Overview and Descriptive Statistics": "The data collection process produced a comprehensive dataset of 1.2 million tweets from 47,892 unique users involved in South African migration discussions between 1 January 2021 and 31 December 2022. This extensive dataset provides a solid foundation for validating the SISI‚Äôs effectiveness in identifying influential users and allows for detailed comparisons with traditional centrality measures across various user types and engagement patterns. The temporal distribution analysis shows significant variation in discourse activity over the collection period, with notable spikes corresponding to major migration-related events such as the Zimbabwe Exemption Permit policy debates in mid-2021, xenophobic incidents across various provinces, and the rise of digital movements such as Operation Dudula. These activity peaks provided natural experiments for influence measurement, creating periods where users‚Äô ability to shape discourse and mobilize audiences became especially evident. The average daily tweet volume ranged from 1200 tweets during baseline periods to over 15,000 during peak events, demonstrating the dataset‚Äôs capacity to capture both routine discourse and crisis-driven engagement patterns. User participation patterns demonstrate the diverse nature of migration discourse, with participants ranging from highly active political commentators who post hundreds of times each month to occasional contributors who tweet only during major events. This variety creates ideal conditions for testing the SISI‚Äôs ability to identify influence across different engagement strategies and user types. The dataset includes verified accounts representing politicians, journalists, and organizations alongside unverified individual users, allowing an analysis of how formal authority markers relate to actual influence capacity measured through behavioral indicators. An analysis of engagement distribution reveals typical heavy-tailed patterns common in social media interactions, with approximately 5% of tweets generating 70% of total engagement, while most receive little interaction. However, preliminary findings indicate that high-engagement content does not directly correlate with traditional centrality measures, offering initial evidence for the need for behavioral influence metrics that measure audience mobilization capacity rather than structural positioning. The median engagement rate across all users was 1.2%, with notable variation from users achieving rates below 0.1% to outstanding performers exceeding 10% engagement. To provide a concise overview of the dataset and to support the descriptive statistics reported above,Table 1summarizes the key characteristics of the collected corpus. Table 1.Dataset Overview.",
            "4.2. SISI Component Analysis": "4.2.1. Average Engagement Rate (AER)The analysis of average engagement rates reveals significant variation in users‚Äô capacity to mobilize their audiences, with the highest-performing users demonstrating engagement rates that exceed typical levels by several orders of magnitude. The top-ranking user achieved an exceptionally high AER, indicating a strong capacity to generate audience interaction relative to follower count and posting frequency. This performance stands in sharp contrast to the user‚Äôs more moderate centrality rankings (4th in betweenness centrality, 21st in closeness centrality, and 15th in eigenvector centrality), highlighting a clear divergence between structural positioning within the network and behavioral influence capacity.Contemporary social media benchmarks provide important context for these findings. A recent industry analysis showed that good engagement rates typically fall between 1% and 3% for most social media platforms in 2025, with rates above 3% considered excellent (Social Insider, 2025). Within our dataset, the top 10% of users by AER achieved rates consistently above 5%, placing them in the exceptional performer category according to current industry standards. This exceptional performance occurred despite many of these users having moderate follower counts, supporting the SISI‚Äôs theoretical foundation in measuring actualized rather than potential influence.A detailed examination of high-AER users revealed consistent patterns of content that resonates deeply with audiences, generating substantial retweet and reply activity that extends well beyond passive consumption. The top-performing user generated 23,113 retweets and 90,133 likes across 1093 posts, indicating sustained ability to create content that audiences find sufficiently valuable to actively amplify and engage with. This level of engagement reflects genuine influence capacity that traditional metrics fail to capture adequately, as demonstrated by the weak correlation (r = 0.31) between AER scores and combined centrality rankings.The comparative analysis between high-AER and high-centrality users reveals systematic differences in audience mobilization patterns. Users achieving high centrality scores often demonstrate substantial follower counts and network connectivity but generate proportionally lower engagement rates, suggesting that structural positioning does not automatically translate to audience activation capacity. This finding supports the SISI‚Äôs theoretical foundation in measuring actualized rather than potential influence while validating recent research emphasizing the importance of engagement quality over quantity metrics. 4.2.2. Follower Reach Score (FRS) Distribution and ContextThe follower reach score analysis reveals the importance of contextual normalization for meaningful influence assessments across diverse user types and domains. The highest-scoring user achieved an FRS of 0.48, indicating an audience size substantially above average for their relevant comparison group. The highest-scoring user achieved a scaled FRS of 0.48 (raw FRS = 148), indicating an audience size 48% above the median follower count in their comparison groupHowever, this user‚Äôs performance in traditional centrality measures (273rd in betweenness, 30th in closeness, 45th in eigenvector) demonstrates a limited correlation between audience reach and network structural positioning, supporting the theoretical rationale for contextual normalization in influence measurement.The contextual approach embedded in FRS design has proved essential for meaningful comparisons across different user types within the migration discourse dataset. Political commentators, institutional accounts, activists, and ordinary citizens operate within distinct ecosystems where typical follower counts vary dramatically. Recent benchmarking research confirms this variation, showing that follower growth rates differ significantly across industries and account types, with smaller accounts often achieving proportionally higher engagement rates despite lower absolute follower numbers (Wies et al., 2023).The analysis of FRS distribution patterns reveals substantial variation even among users with similar absolute follower counts, reflecting differences in domain contexts and audience development strategies. Users specializing in migration discourse typically maintained smaller but more engaged audiences compared to general political commentators, resulting in higher FRS scores that reflect their specialized influence within relevant communities. This pattern validates the theoretical rationale for contextual normalization while demonstrating the FRS‚Äôs capacity to identify users whose audience development exceeds expectations for their particular contexts and domains.Particularly revealing are cases where users with substantial follower counts relative to their domains fail to achieve correspondingly high centrality rankings. One prominent example involves a verified institutional account with a broad audience reach but limited connectivity across network pathways, suggesting concentrated influence within specific follower communities rather than broader network influence. This pattern highlights the distinction between audience potential and network structural importance that FRS normalization helps clarify, supporting the multidimensional approach embedded in SISI design. 4.2.3. Mention Prominence Score (MPS) Results and Community RecognitionThe mention prominence score analysis identifies users who have achieved recognition as key discourse participants regardless of their follower counts or network structural positions. The highest-scoring user received 3936 mentions throughout the analysis period, indicating substantial recognition within migration discourse communities. Notably, this user achieved moderate centrality rankings (169th in betweenness, 3rd in closeness, and 1st eigenvector centrality) that do not fully reflect their discourse prominence and community recognition, demonstrating the independent value of mention-based influence assessment.The relationship between mention prominence and traditional centrality measures proves complex and inconsistent across users. While some high-MPS users also achieve strong centrality scores, others demonstrate significant discourse recognition despite limited structural network positioning. This pattern suggests that thought leadership and community recognition operate through mechanisms distinct from formal network connections, supporting the SISI‚Äôs multidimensional approach to influence measurement and aligning with recent research on the complexity of social media influence dynamics (Han & Balabanis, 2024).The diversity analysis within MPS calculations reveals that high-scoring users receive mentions from broad ranges of community participants rather than concentrated attention from small follower groups. This broad recognition pattern indicates the genuine community standing rather than artificial prominence generated through coordinated campaigns or narrow follower engagement. The diversity component effectively distinguishes between authentic discourse leadership and manufactured visibility, providing protection against manipulation that has become increasingly important given documented concerns about artificial influence inflation (Annaki et al., 2025;Okoronkwo, 2024).The temporal analysis of mention patterns demonstrates that high-MPS users maintain consistent recognition throughout the analysis period rather than achieving temporary prominence during isolated events. This sustained pattern indicates the established community standing and ongoing thought leadership rather than situational visibility, providing evidence of genuine influence capacity that extends beyond momentary attention. The temporal consistency validates the MPS as a measure of stable influence characteristics rather than temporary phenomena, supporting its utility for practical influence identification applications.",
            "4.2.1. Average Engagement Rate (AER)": "The analysis of average engagement rates reveals significant variation in users‚Äô capacity to mobilize their audiences, with the highest-performing users demonstrating engagement rates that exceed typical levels by several orders of magnitude. The top-ranking user achieved an exceptionally high AER, indicating a strong capacity to generate audience interaction relative to follower count and posting frequency. This performance stands in sharp contrast to the user‚Äôs more moderate centrality rankings (4th in betweenness centrality, 21st in closeness centrality, and 15th in eigenvector centrality), highlighting a clear divergence between structural positioning within the network and behavioral influence capacity. Contemporary social media benchmarks provide important context for these findings. A recent industry analysis showed that good engagement rates typically fall between 1% and 3% for most social media platforms in 2025, with rates above 3% considered excellent (Social Insider, 2025). Within our dataset, the top 10% of users by AER achieved rates consistently above 5%, placing them in the exceptional performer category according to current industry standards. This exceptional performance occurred despite many of these users having moderate follower counts, supporting the SISI‚Äôs theoretical foundation in measuring actualized rather than potential influence. A detailed examination of high-AER users revealed consistent patterns of content that resonates deeply with audiences, generating substantial retweet and reply activity that extends well beyond passive consumption. The top-performing user generated 23,113 retweets and 90,133 likes across 1093 posts, indicating sustained ability to create content that audiences find sufficiently valuable to actively amplify and engage with. This level of engagement reflects genuine influence capacity that traditional metrics fail to capture adequately, as demonstrated by the weak correlation (r = 0.31) between AER scores and combined centrality rankings. The comparative analysis between high-AER and high-centrality users reveals systematic differences in audience mobilization patterns. Users achieving high centrality scores often demonstrate substantial follower counts and network connectivity but generate proportionally lower engagement rates, suggesting that structural positioning does not automatically translate to audience activation capacity. This finding supports the SISI‚Äôs theoretical foundation in measuring actualized rather than potential influence while validating recent research emphasizing the importance of engagement quality over quantity metrics.",
            "4.2.2. Follower Reach Score (FRS) Distribution and Context": "The follower reach score analysis reveals the importance of contextual normalization for meaningful influence assessments across diverse user types and domains. The highest-scoring user achieved an FRS of 0.48, indicating an audience size substantially above average for their relevant comparison group. The highest-scoring user achieved a scaled FRS of 0.48 (raw FRS = 148), indicating an audience size 48% above the median follower count in their comparison group However, this user‚Äôs performance in traditional centrality measures (273rd in betweenness, 30th in closeness, 45th in eigenvector) demonstrates a limited correlation between audience reach and network structural positioning, supporting the theoretical rationale for contextual normalization in influence measurement. The contextual approach embedded in FRS design has proved essential for meaningful comparisons across different user types within the migration discourse dataset. Political commentators, institutional accounts, activists, and ordinary citizens operate within distinct ecosystems where typical follower counts vary dramatically. Recent benchmarking research confirms this variation, showing that follower growth rates differ significantly across industries and account types, with smaller accounts often achieving proportionally higher engagement rates despite lower absolute follower numbers (Wies et al., 2023). The analysis of FRS distribution patterns reveals substantial variation even among users with similar absolute follower counts, reflecting differences in domain contexts and audience development strategies. Users specializing in migration discourse typically maintained smaller but more engaged audiences compared to general political commentators, resulting in higher FRS scores that reflect their specialized influence within relevant communities. This pattern validates the theoretical rationale for contextual normalization while demonstrating the FRS‚Äôs capacity to identify users whose audience development exceeds expectations for their particular contexts and domains. Particularly revealing are cases where users with substantial follower counts relative to their domains fail to achieve correspondingly high centrality rankings. One prominent example involves a verified institutional account with a broad audience reach but limited connectivity across network pathways, suggesting concentrated influence within specific follower communities rather than broader network influence. This pattern highlights the distinction between audience potential and network structural importance that FRS normalization helps clarify, supporting the multidimensional approach embedded in SISI design.",
            "4.2.3. Mention Prominence Score (MPS) Results and Community Recognition": "The mention prominence score analysis identifies users who have achieved recognition as key discourse participants regardless of their follower counts or network structural positions. The highest-scoring user received 3936 mentions throughout the analysis period, indicating substantial recognition within migration discourse communities. Notably, this user achieved moderate centrality rankings (169th in betweenness, 3rd in closeness, and 1st eigenvector centrality) that do not fully reflect their discourse prominence and community recognition, demonstrating the independent value of mention-based influence assessment. The relationship between mention prominence and traditional centrality measures proves complex and inconsistent across users. While some high-MPS users also achieve strong centrality scores, others demonstrate significant discourse recognition despite limited structural network positioning. This pattern suggests that thought leadership and community recognition operate through mechanisms distinct from formal network connections, supporting the SISI‚Äôs multidimensional approach to influence measurement and aligning with recent research on the complexity of social media influence dynamics (Han & Balabanis, 2024). The diversity analysis within MPS calculations reveals that high-scoring users receive mentions from broad ranges of community participants rather than concentrated attention from small follower groups. This broad recognition pattern indicates the genuine community standing rather than artificial prominence generated through coordinated campaigns or narrow follower engagement. The diversity component effectively distinguishes between authentic discourse leadership and manufactured visibility, providing protection against manipulation that has become increasingly important given documented concerns about artificial influence inflation (Annaki et al., 2025;Okoronkwo, 2024). The temporal analysis of mention patterns demonstrates that high-MPS users maintain consistent recognition throughout the analysis period rather than achieving temporary prominence during isolated events. This sustained pattern indicates the established community standing and ongoing thought leadership rather than situational visibility, providing evidence of genuine influence capacity that extends beyond momentary attention. The temporal consistency validates the MPS as a measure of stable influence characteristics rather than temporary phenomena, supporting its utility for practical influence identification applications.",
            "4.3. Integrated SISI Performance and Validation": "4.3.1. Overall SISI Rankings and Centrality DivergenceThe comprehensive SISI analysis reveals systematic divergence from traditional centrality measures in identifying influential users within the migration discourse. The top five SISI-ranked users demonstrate influence patterns that traditional metrics fail to capture adequately, with several achieving high SISI scores despite moderate or low centrality rankings across multiple measures. This divergence provides strong empirical support for the theoretical argument that influence operates through behavioral mechanisms distinct from network structure properties.To further illustrate the divergence between behavioral influence and structural network positioning,Table 2presents the top 10 users ranked by SISI scores alongside their AER, FRS, and MPS components and corresponding centrality rankings. The table highlights the limited overlap between high SISI scorers and traditional network-based influence indicators.Table 2.Top 10 users by Social Influence Strength Index (SISI) score.The highest SISI-scoring user exemplifies this divergence pattern, ranking 4th in betweenness centrality, 21st in closeness centrality, and 15th in eigenvector centrality. Despite these moderate structural positions, this user demonstrates exceptional capacity for audience mobilization through sustained high engagement rates and substantial content amplification. With over 20,000 users retweeting their content and nearly 100,000 total reactions, this user achieves a clear behavioral influence that centrality measures systematically underestimate.The ranking comparison analysis reveals that only two of the top five SISI users appear in the top ten of any traditional centrality measure, indicating substantial non-overlap between structural and behavioral influence identification. This finding supports the theoretical argument that influence operates through behavioral mechanisms distinct from network structural properties, validating the SISI‚Äôs focus on demonstrated rather than assumed influence capacity. The divergence aligns with recent meta-analytic research demonstrating that social media influence effectiveness depends more on content quality and audience engagement than on structural network positioning (Han & Balabanis, 2024).The second-ranked SISI scorer had a notably strong centrality performance (169th in betweenness, 3rd in closeness, 1st in eigenvector), representing a case where behavioral and structural influence align. However, this user‚Äôs exceptional performance stems from achieving remarkable engagement efficiency, with only 83 posts generating over 20,000 reactions and nearly 7000 retweets. This efficiency demonstrates influence capacity that extends beyond structural positioning to encompass content quality and audience resonance, supporting the behavioral focus embedded in SISI design.To visualize the relationship between behavioral influence and structural positioning,Figure 1displays a scatterplot of SISI scores against eigenvector centrality with the corresponding Pearson and Spearman correlations, confidence intervals, and sample size. This supports the claim of substantial non-overlap between behavioral and structural influence measures. As both variables exhibit skewed distributions, we report Spearman‚Äôs œÅ as our primary metric (with Pearson‚Äôs r included for comparability), along with 95% bootstrap confidence intervals (1000 resamples),p-values, and the sample size (n = 47,892).Figure 1.Scatterplot of SISI scores vs. eigenvector centrality. 4.3.2. Behavioral Influence Evidence and Quality‚ÄìQuantity PatternsThe SISI analysis reveals consistent patterns where influence stems from content quality and audience resonance rather than posting volume or follower accumulation. Multiple high-scoring users achieve substantial influence with modest posting frequencies, while others with extensive posting activity generate proportionally lower audience mobilization rates. These patterns validate the SISI‚Äôs theoretical foundation in measuring actualized influence through behavioral indicators rather than activity metrics, aligning with recent research that emphasizes engagement quality over posting frequency (Mufadhol et al., 2024).Particularly compelling evidence emerges from a comparison between users with similar follower counts but dramatically different SISI scores. Users achieving high behavioral influence demonstrate consistent ability to generate meaningful audience interaction that extends beyond passive consumption to active engagement and content amplification. This pattern indicates genuine influence capacity that motivates audiences to invest effort in sharing, commenting, and extending conversations around influential users‚Äô content, supporting the behavioral grounding embedded in the SISI‚Äôs theoretical foundation.The content amplification analysis reveals that high-SISI users achieve substantially greater retweet rates relative to their follower bases, indicating content that resonates sufficiently to motivate an audience-driven distribution. This organic amplification represents particularly strong evidence of influence, since it reflects the audience‚Äôs choice to actively promote content rather than passive consumption. Users achieving high amplification rates demonstrate the capacity to create content that audiences find valuable enough to associate with their own online identities through sharing, validating the engagement quality focus embedded in AER calculations.The quality versus quantity distinction proves especially important for understanding influence mechanisms within the migration discourse. Users who focus on creating thoughtful, substantive content consistently outperform those who prioritize high-frequency posting, suggesting that audience value perceptions drive influence more effectively than mere visibility or activity. This finding has significant implications for influence strategy and validates the SISI‚Äôs emphasis on engagement quality rather than volume metrics, supporting recent industry research emphasizing authentic engagement over superficial activity measures (Nwaiwu et al., 2024).",
            "4.3.1. Overall SISI Rankings and Centrality Divergence": "The comprehensive SISI analysis reveals systematic divergence from traditional centrality measures in identifying influential users within the migration discourse. The top five SISI-ranked users demonstrate influence patterns that traditional metrics fail to capture adequately, with several achieving high SISI scores despite moderate or low centrality rankings across multiple measures. This divergence provides strong empirical support for the theoretical argument that influence operates through behavioral mechanisms distinct from network structure properties. To further illustrate the divergence between behavioral influence and structural network positioning,Table 2presents the top 10 users ranked by SISI scores alongside their AER, FRS, and MPS components and corresponding centrality rankings. The table highlights the limited overlap between high SISI scorers and traditional network-based influence indicators. Table 2.Top 10 users by Social Influence Strength Index (SISI) score. The highest SISI-scoring user exemplifies this divergence pattern, ranking 4th in betweenness centrality, 21st in closeness centrality, and 15th in eigenvector centrality. Despite these moderate structural positions, this user demonstrates exceptional capacity for audience mobilization through sustained high engagement rates and substantial content amplification. With over 20,000 users retweeting their content and nearly 100,000 total reactions, this user achieves a clear behavioral influence that centrality measures systematically underestimate. The ranking comparison analysis reveals that only two of the top five SISI users appear in the top ten of any traditional centrality measure, indicating substantial non-overlap between structural and behavioral influence identification. This finding supports the theoretical argument that influence operates through behavioral mechanisms distinct from network structural properties, validating the SISI‚Äôs focus on demonstrated rather than assumed influence capacity. The divergence aligns with recent meta-analytic research demonstrating that social media influence effectiveness depends more on content quality and audience engagement than on structural network positioning (Han & Balabanis, 2024). The second-ranked SISI scorer had a notably strong centrality performance (169th in betweenness, 3rd in closeness, 1st in eigenvector), representing a case where behavioral and structural influence align. However, this user‚Äôs exceptional performance stems from achieving remarkable engagement efficiency, with only 83 posts generating over 20,000 reactions and nearly 7000 retweets. This efficiency demonstrates influence capacity that extends beyond structural positioning to encompass content quality and audience resonance, supporting the behavioral focus embedded in SISI design. To visualize the relationship between behavioral influence and structural positioning,Figure 1displays a scatterplot of SISI scores against eigenvector centrality with the corresponding Pearson and Spearman correlations, confidence intervals, and sample size. This supports the claim of substantial non-overlap between behavioral and structural influence measures. As both variables exhibit skewed distributions, we report Spearman‚Äôs œÅ as our primary metric (with Pearson‚Äôs r included for comparability), along with 95% bootstrap confidence intervals (1000 resamples),p-values, and the sample size (n = 47,892). Figure 1.Scatterplot of SISI scores vs. eigenvector centrality.",
            "4.3.2. Behavioral Influence Evidence and Quality‚ÄìQuantity Patterns": "The SISI analysis reveals consistent patterns where influence stems from content quality and audience resonance rather than posting volume or follower accumulation. Multiple high-scoring users achieve substantial influence with modest posting frequencies, while others with extensive posting activity generate proportionally lower audience mobilization rates. These patterns validate the SISI‚Äôs theoretical foundation in measuring actualized influence through behavioral indicators rather than activity metrics, aligning with recent research that emphasizes engagement quality over posting frequency (Mufadhol et al., 2024). Particularly compelling evidence emerges from a comparison between users with similar follower counts but dramatically different SISI scores. Users achieving high behavioral influence demonstrate consistent ability to generate meaningful audience interaction that extends beyond passive consumption to active engagement and content amplification. This pattern indicates genuine influence capacity that motivates audiences to invest effort in sharing, commenting, and extending conversations around influential users‚Äô content, supporting the behavioral grounding embedded in the SISI‚Äôs theoretical foundation. The content amplification analysis reveals that high-SISI users achieve substantially greater retweet rates relative to their follower bases, indicating content that resonates sufficiently to motivate an audience-driven distribution. This organic amplification represents particularly strong evidence of influence, since it reflects the audience‚Äôs choice to actively promote content rather than passive consumption. Users achieving high amplification rates demonstrate the capacity to create content that audiences find valuable enough to associate with their own online identities through sharing, validating the engagement quality focus embedded in AER calculations. The quality versus quantity distinction proves especially important for understanding influence mechanisms within the migration discourse. Users who focus on creating thoughtful, substantive content consistently outperform those who prioritize high-frequency posting, suggesting that audience value perceptions drive influence more effectively than mere visibility or activity. This finding has significant implications for influence strategy and validates the SISI‚Äôs emphasis on engagement quality rather than volume metrics, supporting recent industry research emphasizing authentic engagement over superficial activity measures (Nwaiwu et al., 2024).",
            "4.4. Model Validation": "To validate the SISI‚Äôs effectiveness, we conducted three validation tests following procedures detailed inSection 4.4. First, for predictive validity, we examined whether high-SISI users‚Äô subsequent posts (in the final quarter of 2022) maintained elevated engagement rates. Users in the top SISI quartile maintained median engagement rates of 3.8% in subsequent periods, compared to 1.1% for the overall population (Mann‚ÄìWhitney U = 18,432,p< 0.001), demonstrating temporal stability. Second, for discriminant validity, we compared the SISI‚Äôs ability to distinguish between users who generated discourse-shaping content (identified through qualitative coding of highly retweeted original tweets, n = 500) versus those who primarily amplified others‚Äô content. The SISI scores were significantly higher for content creators (M = 0.42, SD = 0.18) than amplifiers (M = 0.19, SD = 0.12; t(498) = 12.7,p< 0.001), while centrality measures showed no significant difference (eigenvector: t(498) = 1.3,p= 0.19). Third, for convergent validity, expert assessments from three migration researchers rating the influence of 50 randomly selected users correlated significantly with the SISI (Spearman‚Äôs œÅ = 0.61,p< 0.001) but weakly with eigenvector centrality (œÅ = 0.28,p< 0.05). These validation tests demonstrate that the SISI captures meaningful influence dimensions beyond structural positioning.",
            "5. Discussion": "5.1. Theoretical Contributions to Computational Social ScienceThe Social Influence Strength Index addresses a persistent problem in computational social science‚Äîthe gap between sophisticated algorithms and theoretical understanding of human behavior (Lazer et al., 2020). Most influence metrics emerged from network science traditions that prioritize computational efficiency over psychological validity. The SISI demonstrates how Latan√©‚Äôs social impact theory can guide metric design, moving beyond ad hoc metric combinations toward theoretically grounded assessment tools.This theoretical grounding matters because social media platforms, despite their digital nature, operate through recognizable social psychological mechanisms. The theory‚Äôs emphasis on ‚Äústrength‚Äù‚Äînamely the demonstrated capacity rather than structural position‚Äîproves especially relevant for digital environments where influence manifests through audience mobilization rather than network connectivity (Theocharis & Jungherr, 2020). Our analysis of the South African migration discourse supports this theoretical prediction, revealing influential users who generated substantial engagement despite modest follower counts.Contemporary developments in computational social science emphasize the growing need for theoretical grounding in digital analysis methods (Engel, 2023). The SISI‚Äôs emphasis on behavioral influence over structural positioning challenges prevailing assumptions in network science that equate centrality with influence capacity, providing empirical evidence that influence operates through demonstrated audience mobilization rather than network positioning alone.The divergence between the SISI and centrality suggests context-dependent utility. Centrality measures excel in broadcast scenarios where information flows unidirectionally through structural hubs, such as breaking news dissemination where exposure potential matters more than engagement depth or public health announcements requiring maximum reach through well-connected nodes. However, in contexts characterized by high confirmation bias and selective engagement such as the polarized migration discourse examined here, the SISI better captures influence because behavioral resonance and relationship strength drive information adoption, not merely structural exposure (Guilbeault & Centola, 2021). Their demonstration that complex contagions requiring peer confirmation systematically undermine centrality-based predictions directly supports the SISI‚Äôs behavioral focus; when audiences selectively engage based on content alignment rather than source position, influence manifests through demonstrated mobilization capacity rather than network location. Future research should systematically test the SISI across contexts varying in polarization intensity and network clustering density to map these boundary conditions. 5.2. Implications for Journalism and Digital News EcosystemsThe SISI‚Äôs behavioral approach offers particular value for understanding influence in news and information contexts, where traditional metrics often mislead by favoring institutional accounts with large followings over citizen journalists who generate authentic engagement around important issues. This misalignment becomes problematic as audiences increasingly turn to individual voices rather than institutional sources for news.In our migration discourse analysis, the SISI identified users who shaped the direction of conversation through compelling narratives and community engagement, while centrality measures favored accounts that accumulated followers without generating meaningful dialogue. This pattern mirrors broader trends in digital journalism, where authenticity and relatability often outweigh reach in determining influence (Mlambo et al., 2025).The framework‚Äôs capacity to identify diverse influence pathways proves valuable for understanding the contemporary news ecosystem. Some users excel at detailed analyses that generate deep engagement, while others succeed through broad contextual reach that amplifies key messages. Both patterns represent legitimate forms of journalistic influence that traditional metrics miss, addressing key challenges in understanding how citizen journalists and news influencers build credibility in digital environments. 5.3. Methodological AdvancesThe SISI introduces three methodological innovations that extend beyond this specific application. First, the contextual normalization approach addresses a persistent limitation in influence measurement‚Äîthe assumption that performance standards are universal rather than domain-specific. By normalizing metrics relative to relevant comparison groups, the SISI enables meaningful assessment across contexts while maintaining sensitivity to exceptional performance within specific domains.Second, the geometric mean integration reflects careful consideration of how influence components interact in real social systems. Social impact theory‚Äôs multiplicative formulation suggests that influence effectiveness depends on the simultaneous presence of multiple factors rather than their independent contributions. Our results support this theoretical expectation in that users who excel in one dimension while underperforming in others rarely achieve high overall SISI scores, indicating that authentic influence requires balanced strength across multiple dimensions.Third, the behavioral focus provides inherent protection against common forms of metric gaming. Users cannot achieve high SISI scores through artificial follower inflation alone, as the engagement efficiency component requires demonstrated audience mobilization. Similarly, purchased mentions from narrow groups fail to generate high MPS scores due to the diversity weighting. This resistance to manipulation proves increasingly important as concerns about inauthentic influence escalate (Okoronkwo, 2024;Annaki et al., 2025). 5.4. Practical ApplicationsThe practical implications of the SISI extend across multiple domains where authentic influence identification provides a competitive advantage. For marketing practitioners, the SISI offers a more sophisticated approach to influencer selection that prioritizes genuine audience engagement over superficial popularity metrics (van der Harst & Angelopoulos, 2024). Industry research shows that 66.4% of marketers found AI-improved influencer marketing campaign performance, yet traditional metrics often fail to capture the quality of influence that drives actual consumer behavior (Enberg, 2025).For policymakers and advocacy organizations, the SISI provides tools for understanding public discourse dynamics and identifying key voices in policy-relevant conversations (Margetts & Dorobantu, 2023). Users who excel in engagement efficiency may be effective for detailed policy communication, while those with broad contextual reach may be valuable for general awareness campaigns. The framework‚Äôs applications extend to academic researchers studying social media behavior and digital influence. A recent analysis of social media marketing research showed exponential growth in academic interest, with emerging themes requiring sophisticated measurement approaches (Shaheen, 2025). The SISI‚Äôs theoretical foundation and behavioral focus open new research questions about influence development in journalism contexts while providing consistent measurement principles for comparative analyses. 5.5. Limitations and Future Research DirectionsSeveral limitations constrain our findings and suggest necessary extensions of this work, and the validation scope remains narrow. Our validation is primarily internal, demonstrating that the SISI differs systematically from centrality measures, without establishing external validity. We have not shown whether high-SISI users actually change opinions, mobilize offline action, or achieve influence outcomes beyond engagement metrics. Future studies should correlate SISI scores with behavioral outcomes (petition signing, event attendance, purchasing decisions) or expert assessments of actual influence to establish predictive validity.Context specificity limits generalizability: Our findings derive from a single platform (Twitter/X), examining the emotionally charged migration discourse in South Africa. This topic‚Äôs polarizing nature may favor behavioral metrics over structural measures. The SISI requires testing across diverse contexts including low-engagement topics, professional discussions, breaking news coverage, and platforms with different engagement norms (Instagram, TikTok, LinkedIn) to establish broader applicability. Journalism-specific contexts particularly warrant dedicated investigation, as news influence may involve distinct behavioral patterns such as breaking news dissemination, fact-checking activities, and investigative reporting that require domain-specific SISI adaptations.The temporal dynamics remain unexplored: Our analysis treated influence as static across a two-year period without examining how influence develops or fluctuates. Longitudinal studies could reveal whether the SISI captures stable user characteristics or context-dependent phenomena, and whether influence trajectories follow predictable patterns as users develop audiences and refine strategies.Methodological choices lack empirical justification: While theoretically grounded, our geometric mean integration has not been compared against alternative aggregation methods (arithmetic mean, weighted combinations, machine learning approaches). Sensitivity analyses examining how component weights and integration methods affect rankings would strengthen confidence in our design choices.Gaming resistance remains untested: Although the SISI‚Äôs design provides some protection against manipulation through engagement diversity requirements, we did not systematically test its robustness against coordinated inauthentic behavior, bot networks, or sophisticated engagement manipulation. Explicit adversarial testing is needed to establish the SISI‚Äôs reliability in environments with strategic gaming.The comparative assessment is incomplete: We compared the SISI only against traditional centrality measures, not against other behavioral metrics or industry influence scores (Klout-style approaches, platform-native metrics). A systematic comparison across diverse influence measurement approaches would clarify the SISI‚Äôs relative performance and identify conditions where different metrics prove most suitable.These limitations do not invalidate our core finding that behavioral influence operates through mechanisms distinct from the network structure but they constrain claims about the SISI‚Äôs broader applicability and superiority. Addressing these gaps represents an essential next step in establishing the SISI as a robust, generalizable framework for measuring influence.Ethical considerations warrant explicit attention: First, while our analysis used publicly available data and employs anonymization protocols, the behavioral profiling inherent in the SISI raises consent questions. Users posting publicly may not anticipate systematic influence assessments, particularly when such assessments might inform targeting strategies by marketers, political campaigns, or platform moderators. Although legal frameworks typically exempt public data from consent requirements, ethical best practice increasingly demands transparency about how behavioral data enables influence profiling. Second, the SISI may perpetuate algorithmic bias despite its behavioral focus. Platform algorithms already privilege certain engagement types and user characteristics, and the SISI‚Äôs reliance on engagement metrics risks amplifying these existing biases, such as favoring users who conform to platform-rewarded content styles or systematically undervaluing influence in marginalized communities with different engagement norms. The metric might also create feedback loops where high-SISI users receive disproportionate attention, further concentrating influence regardless of content quality. Third, the SISI‚Äôs practical applications raise dual-use concerns; the same framework identifying authentic influencers for public health campaigns could target manipulation-susceptible users for misinformation or surveillance. Future implementations require careful consideration of these ethical dimensions, including transparent disclosure of influence assessment practices and systematic bias auditing across demographic groups.Future research should explore (1) dynamic SISI tracking to detect influence emergence and decay; (2) causal tests via natural experiments (e.g., suspensions, virality events); (3) cross-platform SISI validation combining X, Facebook, and Reddit data; (4) machine learning integration to predict influence trajectories from SISI components; and (5) adversarial robustness testing against coordinated manipulation strategies.",
            "5.1. Theoretical Contributions to Computational Social Science": "The Social Influence Strength Index addresses a persistent problem in computational social science‚Äîthe gap between sophisticated algorithms and theoretical understanding of human behavior (Lazer et al., 2020). Most influence metrics emerged from network science traditions that prioritize computational efficiency over psychological validity. The SISI demonstrates how Latan√©‚Äôs social impact theory can guide metric design, moving beyond ad hoc metric combinations toward theoretically grounded assessment tools. This theoretical grounding matters because social media platforms, despite their digital nature, operate through recognizable social psychological mechanisms. The theory‚Äôs emphasis on ‚Äústrength‚Äù‚Äînamely the demonstrated capacity rather than structural position‚Äîproves especially relevant for digital environments where influence manifests through audience mobilization rather than network connectivity (Theocharis & Jungherr, 2020). Our analysis of the South African migration discourse supports this theoretical prediction, revealing influential users who generated substantial engagement despite modest follower counts. Contemporary developments in computational social science emphasize the growing need for theoretical grounding in digital analysis methods (Engel, 2023). The SISI‚Äôs emphasis on behavioral influence over structural positioning challenges prevailing assumptions in network science that equate centrality with influence capacity, providing empirical evidence that influence operates through demonstrated audience mobilization rather than network positioning alone. The divergence between the SISI and centrality suggests context-dependent utility. Centrality measures excel in broadcast scenarios where information flows unidirectionally through structural hubs, such as breaking news dissemination where exposure potential matters more than engagement depth or public health announcements requiring maximum reach through well-connected nodes. However, in contexts characterized by high confirmation bias and selective engagement such as the polarized migration discourse examined here, the SISI better captures influence because behavioral resonance and relationship strength drive information adoption, not merely structural exposure (Guilbeault & Centola, 2021). Their demonstration that complex contagions requiring peer confirmation systematically undermine centrality-based predictions directly supports the SISI‚Äôs behavioral focus; when audiences selectively engage based on content alignment rather than source position, influence manifests through demonstrated mobilization capacity rather than network location. Future research should systematically test the SISI across contexts varying in polarization intensity and network clustering density to map these boundary conditions.",
            "5.2. Implications for Journalism and Digital News Ecosystems": "The SISI‚Äôs behavioral approach offers particular value for understanding influence in news and information contexts, where traditional metrics often mislead by favoring institutional accounts with large followings over citizen journalists who generate authentic engagement around important issues. This misalignment becomes problematic as audiences increasingly turn to individual voices rather than institutional sources for news. In our migration discourse analysis, the SISI identified users who shaped the direction of conversation through compelling narratives and community engagement, while centrality measures favored accounts that accumulated followers without generating meaningful dialogue. This pattern mirrors broader trends in digital journalism, where authenticity and relatability often outweigh reach in determining influence (Mlambo et al., 2025). The framework‚Äôs capacity to identify diverse influence pathways proves valuable for understanding the contemporary news ecosystem. Some users excel at detailed analyses that generate deep engagement, while others succeed through broad contextual reach that amplifies key messages. Both patterns represent legitimate forms of journalistic influence that traditional metrics miss, addressing key challenges in understanding how citizen journalists and news influencers build credibility in digital environments.",
            "5.3. Methodological Advances": "The SISI introduces three methodological innovations that extend beyond this specific application. First, the contextual normalization approach addresses a persistent limitation in influence measurement‚Äîthe assumption that performance standards are universal rather than domain-specific. By normalizing metrics relative to relevant comparison groups, the SISI enables meaningful assessment across contexts while maintaining sensitivity to exceptional performance within specific domains. Second, the geometric mean integration reflects careful consideration of how influence components interact in real social systems. Social impact theory‚Äôs multiplicative formulation suggests that influence effectiveness depends on the simultaneous presence of multiple factors rather than their independent contributions. Our results support this theoretical expectation in that users who excel in one dimension while underperforming in others rarely achieve high overall SISI scores, indicating that authentic influence requires balanced strength across multiple dimensions. Third, the behavioral focus provides inherent protection against common forms of metric gaming. Users cannot achieve high SISI scores through artificial follower inflation alone, as the engagement efficiency component requires demonstrated audience mobilization. Similarly, purchased mentions from narrow groups fail to generate high MPS scores due to the diversity weighting. This resistance to manipulation proves increasingly important as concerns about inauthentic influence escalate (Okoronkwo, 2024;Annaki et al., 2025).",
            "5.4. Practical Applications": "The practical implications of the SISI extend across multiple domains where authentic influence identification provides a competitive advantage. For marketing practitioners, the SISI offers a more sophisticated approach to influencer selection that prioritizes genuine audience engagement over superficial popularity metrics (van der Harst & Angelopoulos, 2024). Industry research shows that 66.4% of marketers found AI-improved influencer marketing campaign performance, yet traditional metrics often fail to capture the quality of influence that drives actual consumer behavior (Enberg, 2025). For policymakers and advocacy organizations, the SISI provides tools for understanding public discourse dynamics and identifying key voices in policy-relevant conversations (Margetts & Dorobantu, 2023). Users who excel in engagement efficiency may be effective for detailed policy communication, while those with broad contextual reach may be valuable for general awareness campaigns. The framework‚Äôs applications extend to academic researchers studying social media behavior and digital influence. A recent analysis of social media marketing research showed exponential growth in academic interest, with emerging themes requiring sophisticated measurement approaches (Shaheen, 2025). The SISI‚Äôs theoretical foundation and behavioral focus open new research questions about influence development in journalism contexts while providing consistent measurement principles for comparative analyses.",
            "5.5. Limitations and Future Research Directions": "Several limitations constrain our findings and suggest necessary extensions of this work, and the validation scope remains narrow. Our validation is primarily internal, demonstrating that the SISI differs systematically from centrality measures, without establishing external validity. We have not shown whether high-SISI users actually change opinions, mobilize offline action, or achieve influence outcomes beyond engagement metrics. Future studies should correlate SISI scores with behavioral outcomes (petition signing, event attendance, purchasing decisions) or expert assessments of actual influence to establish predictive validity. Context specificity limits generalizability: Our findings derive from a single platform (Twitter/X), examining the emotionally charged migration discourse in South Africa. This topic‚Äôs polarizing nature may favor behavioral metrics over structural measures. The SISI requires testing across diverse contexts including low-engagement topics, professional discussions, breaking news coverage, and platforms with different engagement norms (Instagram, TikTok, LinkedIn) to establish broader applicability. Journalism-specific contexts particularly warrant dedicated investigation, as news influence may involve distinct behavioral patterns such as breaking news dissemination, fact-checking activities, and investigative reporting that require domain-specific SISI adaptations. The temporal dynamics remain unexplored: Our analysis treated influence as static across a two-year period without examining how influence develops or fluctuates. Longitudinal studies could reveal whether the SISI captures stable user characteristics or context-dependent phenomena, and whether influence trajectories follow predictable patterns as users develop audiences and refine strategies. Methodological choices lack empirical justification: While theoretically grounded, our geometric mean integration has not been compared against alternative aggregation methods (arithmetic mean, weighted combinations, machine learning approaches). Sensitivity analyses examining how component weights and integration methods affect rankings would strengthen confidence in our design choices. Gaming resistance remains untested: Although the SISI‚Äôs design provides some protection against manipulation through engagement diversity requirements, we did not systematically test its robustness against coordinated inauthentic behavior, bot networks, or sophisticated engagement manipulation. Explicit adversarial testing is needed to establish the SISI‚Äôs reliability in environments with strategic gaming. The comparative assessment is incomplete: We compared the SISI only against traditional centrality measures, not against other behavioral metrics or industry influence scores (Klout-style approaches, platform-native metrics). A systematic comparison across diverse influence measurement approaches would clarify the SISI‚Äôs relative performance and identify conditions where different metrics prove most suitable. These limitations do not invalidate our core finding that behavioral influence operates through mechanisms distinct from the network structure but they constrain claims about the SISI‚Äôs broader applicability and superiority. Addressing these gaps represents an essential next step in establishing the SISI as a robust, generalizable framework for measuring influence. Ethical considerations warrant explicit attention: First, while our analysis used publicly available data and employs anonymization protocols, the behavioral profiling inherent in the SISI raises consent questions. Users posting publicly may not anticipate systematic influence assessments, particularly when such assessments might inform targeting strategies by marketers, political campaigns, or platform moderators. Although legal frameworks typically exempt public data from consent requirements, ethical best practice increasingly demands transparency about how behavioral data enables influence profiling. Second, the SISI may perpetuate algorithmic bias despite its behavioral focus. Platform algorithms already privilege certain engagement types and user characteristics, and the SISI‚Äôs reliance on engagement metrics risks amplifying these existing biases, such as favoring users who conform to platform-rewarded content styles or systematically undervaluing influence in marginalized communities with different engagement norms. The metric might also create feedback loops where high-SISI users receive disproportionate attention, further concentrating influence regardless of content quality. Third, the SISI‚Äôs practical applications raise dual-use concerns; the same framework identifying authentic influencers for public health campaigns could target manipulation-susceptible users for misinformation or surveillance. Future implementations require careful consideration of these ethical dimensions, including transparent disclosure of influence assessment practices and systematic bias auditing across demographic groups. Future research should explore (1) dynamic SISI tracking to detect influence emergence and decay; (2) causal tests via natural experiments (e.g., suspensions, virality events); (3) cross-platform SISI validation combining X, Facebook, and Reddit data; (4) machine learning integration to predict influence trajectories from SISI components; and (5) adversarial robustness testing against coordinated manipulation strategies."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2673-5172/6/4/205",
        "scraped_at": "2025-12-05 23:52:48"
    },
    {
        "title": "Correction: Zeng et al. High-Performance Silicon Nanowire Array Biosensor for Combined Detection of Colorectal Cancer Biomarkers.Micromachines2025,16, 1089",
        "authors": "byJiaye Zeng,Mingbin Liu,Xin Chen,Jintao Yi,Wenhe Liu,Xinjian Qu,Chaoran Liu,Serestina Viriri,Guangguang Yang,Xun YangandWeichao Yang",
        "journal": "Micromachines2025,16(12), 1381;https://doi.org/10.3390/mi16121381- 5 Dec 2025",
        "abstract": "",
        "keywords": "Keywords not found",
        "full_content": {
            "full_content": "Following publication, we noted that the sequence of the corresponding authors and their email addresses was incorrectly listed in the original publication, and a correction is required. With this correction, the Editorial Office and the authors have made the following amendments to the published article [1]:This correction changes ‚ÄúWeichao Yang1,* and Xun Yang1,*‚Äù to ‚ÄúXun Yang1,* and Weichao Yang1,*‚Äù, and ‚ÄúCorrespondence: yangwc@cwnu.edu.cn (W.Y.); yangxun@cwnu.edu.cn (X.Y.)‚Äù to ‚ÄúCorrespondence: yangxun@cwnu.edu.cn (X.Y.); yangwc@cwnu.edu.cn (W.Y.)‚Äù.The authors state that the scientific conclusions are unaffected. This correction was approved by the Academic Editor. The original publication has also been updated."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2072-666X/16/12/1381",
        "scraped_at": "2025-12-05 23:52:55"
    },
    {
        "title": "Image Captioning with Object Detection and Facial Expression Recognition for Smart Industry",
        "authors": "byAbdul Saboor Khan,Abdul Haseeb Khan,Muhammad Jamshed AbbassandImran Shafi",
        "journal": "Bioengineering2025,12(12), 1325;https://doi.org/10.3390/bioengineering12121325- 5 Dec 2025",
        "abstract": "This paper presents a new image captioning system which contains facial expression recognition as a way to provide better emotional and contextual comprehension of the captions generated. A combination of affective cues and visual features is made, which enables semantically full and emotionally conscious descriptions. Experiments were carried out on two created datasets, FlickrFace11k and COCOFace15k, with standard benchmarks such as BLEU, METEOR, ROUGE-L, CIDEr, and SPICE to analyze their effectiveness. The suggested model produced better results in all metrics as compared to baselines, like Show-Attend-Tell and Up-Down, remaining consistently better on all the scores. Remarkably, it has reached gains of 2.5 points on CIDEr and 1.0 on SPICE, which means a closer correlation to the prompt captions made by people. A 5-fold cross-validation confirmed the model‚Äôs robustness, with minimal standard deviation across folds (<¬±0.2). Qualitative results further demonstrated its ability to capture fine-grained emotional expressions often missed by conventional models. These findings underscore the model‚Äôs potential in affective computing, assistive technologies, and human-centric AI applications. The pipeline is designed for on-prem/edge deployment with lightweight interfaces to IoT middleware (MQTT/OPC UA), enabling smart-factory integration. These characteristics align the method with Industry 4.0 sensor networks and human-centric analytics.Keywords:facial expression recognition;image captioning;Vision-Language Pre-Training (VLP);deep learning;multimodal deep learning;Convolutional Neural Networks (CNN);object detection;Industry 4.0;IoT;Edge AI;human‚Äìrobot collaboration;predictive maintenance;HSE",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Image captioning is a famous computer vision and natural language processing unit that focuses on generating a coherent textual description of an image using visual semantics. It is a simultaneous combination of objects, spatial relations, and verbal nuances, and a grammatical and significant translation of what we would call the visual knowledge-based conventional image captioning. In these approaches, a classic encoder‚Äìdecoder model is generally applied, such that visual description information is drawn out by convolutional neural networks (CNNs) and then described by recurrent neural networks (RNNs), generally a long short-term memory (LSTM) network, to deliver a text message [1]. Improvement in deep learning has made it possible to improve the quality of generated captions through the incorporation of the attribute of attention, upon which the model is able to focus upon different areas of an image as it generates each word in the generated caption. As an example, Li et al. [2] came up with the so-called Oscar model that combined object-level semantics representation with cross-modal transformers and more successfully promoted the semantic matching between image regions and textual tokens, thereby increasing the accuracy and descriptiveness of the captions. On the same note, another model which uses an object-level feature is the bottom-up and top-down approach developed by Anderson et al. [3], focusing on utilizing Faster R-CNN to enhance the reasoning about salient regions in the model. However, these models largely emphasize the visual information at object or grid level with much less attention paid to emotional cues by human subjects in pictures. Facial expression recognition (FER) has become an auxiliary method to realize the emotional state based on human facial features and allows further semantic interpretation of scenes with people in them. Incorporation of FER into captioning models holds the promise of creating not only factually correct but empathetically aligned captioning models based on the directional affective surfaces approach. At the same time, object detecting systems like YOLO and Faster R-CNN have empowered the granularity and accuracy of visual features to extract, so the model could identify and recognize several objects in a real-time scenario [4,5]. The inclusion of FER and object detection within a single captioning framework implies a considerable level of enrichment of the descriptive and affective aspect of captions. Integration can be helpful towards seeing images more holistically, especially where the concept of human interaction and emotion, as well as contextual behavior, is involved. Such multimodal fusion, however, is less analyzed in existing studies, which tend to look at each of the visual, emotional, and linguistic elements independently, or even in loosely coupled systems [6]. Despite notable progress, several limitations remain in existing image captioning systems. First, conventional models that rely on global CNN features frequently overlook subtle spatial relationships and fail to detect small or contextually important objects. Second, region-based attention mechanisms are constrained by the accuracy of object proposals and may neglect interactions or emotional expressions, especially in crowded or dynamic scenes. Third, although FER has been effectively used in individual cases, its systematic use in the generation of captions is scanty, hence resulting in the lack of emotional depth in captions. Furthermore, the benchmark datasets are usually not annotated parallel in terms of object classes and facial emotions, which is problematic in terms of effective multimodal training and assessment. These shortcomings obstruct the capability of the existing systems in generating semantically and affectively informative captions [7,8]. In Industry 4.0 environments, camera sensors and edge AI enable real-time understanding of human‚Äìmachine contexts. Our FER-guided captioning can (i) monitor operator affect and task context for safety/HSE dashboards, (ii) enrich event logs with human-centric semantics for root-cause analysis and predictive maintenance narratives, and (iii) support human‚Äìrobot collaboration by turning raw video into structured, explainable summaries. The pipeline is deployable on edge devices and integrates with plant IoT middleware (e.g., MQTT/OPC UA), aligning with the Special Issue‚Äôs focus on sensor- and IoT-enabled smart manufacturing. In order to cope with these difficulties, the proposed study suggests a hybrid version of facial expression recognition and object detection based on the YOLOv5 architecture and deep CNN encoder. The proposed method will invoke the combination of the grid features, object-level semantics, and emotional cues, generating the image caption that will be not only contextually accurate but will also touch the emotions. The effectiveness of the method in both descriptive accuracy and expressive power in terms of emotions is proven using the empirical benchmark assessments on the benchmark datasets. This paper is structured as follows. The related works in image captioning, FER, and object detection review are presented inSection 2.Section 3gives a description of the proposed process comprising feature extraction and caption generation. InSection 4, experimental configuration, datasets, and baseline models are described. InSection 5, the outcomes and the comparisons of the performance are reported.Section 6talks about limitations, and theSection 7writes the conclusion of this study and how future research could be conducted.",
            "2. Related Works": "2.1. Overview of Image Captioning MethodsImage captioning is a quickly growing research challenge at the nexus of both computer vision and natural language processing and its basic concept is the translation of visual information into descriptive textual stories. The initial approaches were based on template systems, which are, by nature, too rigid and are not flexible [9]. An important change happened when deep learning approaches were introduced, especially with convolutional neural networks (CNNs) and recurrent neural networks (RNNs), through which the process of image captioning was fundamentally altered [10].A very important encoder‚Äìdecoder scheme is called Show and Tell and was proposed by Oriol Vinyals et al. [11]. The approach taken, in their case, was basing the model on CNNs to obtain visual features, which were then sequentially addressed by Long Short-Term Memory (LSTM) networks to create captions. Although this method represented a vital innovation since it produced more human-like descriptions, it usually failed to capture vital spatial information and failed to identify complicated associations among components of the image.Afterward, Kelvin Xu et al. presented the model of ‚ÄúShow, Attend and Tell‚Äù, which added attention mechanism to the conventional CNN-RNN model [12]. The model dynamically attaches greater weight to particular areas of images when generating a caption, thereby enhancing relevance of captions and laying much emphasis on those aspects of visual information which are relevant. Nonetheless, even this method had its drawbacks in literally predicting complex object interactions and hidden emotions context.Addressing these shortcomings, Peter Anderson et al. developed a ‚ÄúBottom-up and Top-down‚Äù attention mechanism [3]. In contrast to earlier approaches that depended solely on grid-based CNN features, recent advancements have introduced region-specific feature extraction using object detection frameworks like EfficientDet [13]. By leveraging a compound scaling method to balance network depth, width, and resolution, EfficientDet enables accurate and computationally efficient object localization, facilitating richer and more structured visual representations. These region-aware features significantly enhance the descriptiveness of generated captions by providing finer details of salient objects within a scene. However, such methods still fall short in capturing emotional subtleties, particularly facial expressions, which are essential for generating semantically complete and context-aware captions, especially in images involving human interactions. 2.2. Object Detection Methods in Image CaptioningAccurate and efficient object detection significantly impacts captioning quality, especially in scenarios requiring detailed contextual understanding. Faster R-CNN by Shaoqing Ren et al. represents a state-of-the-art two-stage detection model utilizing region proposal networks, achieving high precision in object localization [14]. Despite its accuracy, Faster R-CNN is computationally demanding and less suitable for real-time captioning applications.In contrast, YOLO (You Only Look Once) variants, particularly YOLOv5 developed by Glenn Jocher and colleagues [15], offer efficient single-stage detection solutions. YOLOv5 partitions the input image into grids and predicts bounding boxes and class probabilities simultaneously, enabling rapid inference speeds and reduced computational overhead. In this work, we have focused on YOLOv5 because it offers an optimal balance between detection accuracy and real-time processing efficiency, making it highly suitable for applications that require fast and responsive image analysis. Its lightweight architecture and streamlined inference pipeline allow for the simultaneous detection of multiple objects with minimal latency, which aligns with the performance requirements of our proposed image captioning framework. 2.3. Incorporating Facial Expressions into Image CaptioningFacial expressions convey crucial affective information that can significantly enhance the contextual and emotional accuracy of image captions. Traditional image captioning models primarily focus on object detection and scene understanding while overlooking emotional cues, resulting in descriptions that are often semantically correct but emotionally sterile. The integration of facial expression analysis offers a promising direction to bridge this gap, particularly in human-centric images where emotions play a critical role in narrative interpretation.One of the notable projects in this direction is the Face-Cap model provided by Nezami et al. [7], which directly applies facial expression elements to the captioning pipeline. It is based on the model that extracts facial regions with any conventional detection methods and classifies those regions into specific emotional states with deep convolutional networks. These emotion vectors are subsequently embedded in the architecture of the decoder, which will affect word generation according to the feeling level of the characters in the picture. As indicated by the results, Face-Cap produced more emotional captions pertinent to people, which were more contextually relevant in captions of people. Nevertheless, the model does not enjoy full utilization of spatial relationships between objects, thus having a limited descriptive power in more complex scenes.Further than standalone models, such as Face-Cap, recent advancements in facial expression recognition (FER) have allowed the use of emotion features to be used as auxiliary data into multimodal captioning tasks. Often, pretrained CNNs like VGG-Face [16], ResNet50 [17], or more specific emotion architectures trained on a dataset of feelings, e.g., FER2013 [18], are used in FER systems. These nets learn high-dimensional emotion representations of faces which in turn can be merged into the visual object features and global image embedding in united feature space. In emerging FER-guided captioning pipelines, facial emotion vectors are used either to initialize the decoder‚Äôs LSTM hidden states or as part of an attention mechanism that guides caption generation. Building on these foundations, Zhou et al. [19] recently proposed ESCNet, an emotional stimuli-aware captioning network that generates affective captions via a fine-tuned LLM to dramatically improve image emotion classification performance, achieving new state-of-the-art results on multiple benchmarks. 2.4. Contextual and Spatial Relationship Modeling in CaptioningThe key issue in the current image captioning frameworks is effective modeling of spatial and contextual relations between objects. In recent studies, the contextual relationships are characterized as playing the determinant role in attaining a realistic description of images. This approach achieved better descriptive quality with spatial and semantic descriptions clearly defined, yet failed at considering emotional properties during caption generation.Moreover, the latest developments suggest the use of hybrid designs combined to use several feature extraction techniques to enhance the accuracy of captions. As shown by Wang et al., the strategy of integrating local object-related features and global features of context on producing a more detailed description of the image turned out to be useful [20]. This end-to-end feature combination was effective compared to those based on deciding on a single level of features (object-level or global-level). 2.5. Evaluation MetricsStandard image captioning evaluation metrics were employed to assess performance, including BLEU [21], METEOR [22], ROUGE-L [23], CIDEr [24], and SPICE [25]. While BLEU and ROUGE measure n-gram overlap and fluency, METEOR aligns closer to human judgment by considering synonyms. Crucially, CIDEr and SPICE are prioritized in this study as they specifically evaluate the semantic consensus and propositional content of captions, which is vital for verifying the integration of emotional context. 2.6. Research Gap and ContributionIn spite of enormous progress, modern technologies have significant drawbacks. First of all, approaches based on a missed perspective can be associated only with object detection, and related context can easily miss the essential information related to emotion that is presented in the form of facial expressions and is essential to the description histories that include human-related interactions [7,17]. On the other hand, the approaches that specifically aim at the analysis of emotions seldom have established strong object detection schemes that subsequently lower the resolution of a complex image scene description [17]. In addition, the existing object detection models, such as Faster R-CNN, are precise but very computationally demanding, restricting their real-time usage [14].It is these gaps that point to the need for a single model able to pinpoint all the other aspects of the research (efficient object detection, detailed spatial relationships, and robust emotional expression analysis) in exactly the same direction as the one being studied in the current research.Table 1, presented below, provides a comparative overview of selected state-of-the-art image captioning methodologies, highlighting their contributions, strengths, and limitations:Table 1.Summary of key methods in image captioning.This paper clearly identifies a distinct research gap: the absence of comprehensive integration of efficient object detection, facial expression analysis, and explicit spatial-contextual modeling. Addressing these limitations, the proposed method integrates YOLOv5 for object detection efficiency, robust facial expression recognition, and contextual modeling to significantly enhance caption accuracy and relevance.",
            "2.1. Overview of Image Captioning Methods": "Image captioning is a quickly growing research challenge at the nexus of both computer vision and natural language processing and its basic concept is the translation of visual information into descriptive textual stories. The initial approaches were based on template systems, which are, by nature, too rigid and are not flexible [9]. An important change happened when deep learning approaches were introduced, especially with convolutional neural networks (CNNs) and recurrent neural networks (RNNs), through which the process of image captioning was fundamentally altered [10]. A very important encoder‚Äìdecoder scheme is called Show and Tell and was proposed by Oriol Vinyals et al. [11]. The approach taken, in their case, was basing the model on CNNs to obtain visual features, which were then sequentially addressed by Long Short-Term Memory (LSTM) networks to create captions. Although this method represented a vital innovation since it produced more human-like descriptions, it usually failed to capture vital spatial information and failed to identify complicated associations among components of the image. Afterward, Kelvin Xu et al. presented the model of ‚ÄúShow, Attend and Tell‚Äù, which added attention mechanism to the conventional CNN-RNN model [12]. The model dynamically attaches greater weight to particular areas of images when generating a caption, thereby enhancing relevance of captions and laying much emphasis on those aspects of visual information which are relevant. Nonetheless, even this method had its drawbacks in literally predicting complex object interactions and hidden emotions context. Addressing these shortcomings, Peter Anderson et al. developed a ‚ÄúBottom-up and Top-down‚Äù attention mechanism [3]. In contrast to earlier approaches that depended solely on grid-based CNN features, recent advancements have introduced region-specific feature extraction using object detection frameworks like EfficientDet [13]. By leveraging a compound scaling method to balance network depth, width, and resolution, EfficientDet enables accurate and computationally efficient object localization, facilitating richer and more structured visual representations. These region-aware features significantly enhance the descriptiveness of generated captions by providing finer details of salient objects within a scene. However, such methods still fall short in capturing emotional subtleties, particularly facial expressions, which are essential for generating semantically complete and context-aware captions, especially in images involving human interactions.",
            "2.2. Object Detection Methods in Image Captioning": "Accurate and efficient object detection significantly impacts captioning quality, especially in scenarios requiring detailed contextual understanding. Faster R-CNN by Shaoqing Ren et al. represents a state-of-the-art two-stage detection model utilizing region proposal networks, achieving high precision in object localization [14]. Despite its accuracy, Faster R-CNN is computationally demanding and less suitable for real-time captioning applications. In contrast, YOLO (You Only Look Once) variants, particularly YOLOv5 developed by Glenn Jocher and colleagues [15], offer efficient single-stage detection solutions. YOLOv5 partitions the input image into grids and predicts bounding boxes and class probabilities simultaneously, enabling rapid inference speeds and reduced computational overhead. In this work, we have focused on YOLOv5 because it offers an optimal balance between detection accuracy and real-time processing efficiency, making it highly suitable for applications that require fast and responsive image analysis. Its lightweight architecture and streamlined inference pipeline allow for the simultaneous detection of multiple objects with minimal latency, which aligns with the performance requirements of our proposed image captioning framework.",
            "2.3. Incorporating Facial Expressions into Image Captioning": "Facial expressions convey crucial affective information that can significantly enhance the contextual and emotional accuracy of image captions. Traditional image captioning models primarily focus on object detection and scene understanding while overlooking emotional cues, resulting in descriptions that are often semantically correct but emotionally sterile. The integration of facial expression analysis offers a promising direction to bridge this gap, particularly in human-centric images where emotions play a critical role in narrative interpretation. One of the notable projects in this direction is the Face-Cap model provided by Nezami et al. [7], which directly applies facial expression elements to the captioning pipeline. It is based on the model that extracts facial regions with any conventional detection methods and classifies those regions into specific emotional states with deep convolutional networks. These emotion vectors are subsequently embedded in the architecture of the decoder, which will affect word generation according to the feeling level of the characters in the picture. As indicated by the results, Face-Cap produced more emotional captions pertinent to people, which were more contextually relevant in captions of people. Nevertheless, the model does not enjoy full utilization of spatial relationships between objects, thus having a limited descriptive power in more complex scenes. Further than standalone models, such as Face-Cap, recent advancements in facial expression recognition (FER) have allowed the use of emotion features to be used as auxiliary data into multimodal captioning tasks. Often, pretrained CNNs like VGG-Face [16], ResNet50 [17], or more specific emotion architectures trained on a dataset of feelings, e.g., FER2013 [18], are used in FER systems. These nets learn high-dimensional emotion representations of faces which in turn can be merged into the visual object features and global image embedding in united feature space. In emerging FER-guided captioning pipelines, facial emotion vectors are used either to initialize the decoder‚Äôs LSTM hidden states or as part of an attention mechanism that guides caption generation. Building on these foundations, Zhou et al. [19] recently proposed ESCNet, an emotional stimuli-aware captioning network that generates affective captions via a fine-tuned LLM to dramatically improve image emotion classification performance, achieving new state-of-the-art results on multiple benchmarks.",
            "2.4. Contextual and Spatial Relationship Modeling in Captioning": "The key issue in the current image captioning frameworks is effective modeling of spatial and contextual relations between objects. In recent studies, the contextual relationships are characterized as playing the determinant role in attaining a realistic description of images. This approach achieved better descriptive quality with spatial and semantic descriptions clearly defined, yet failed at considering emotional properties during caption generation. Moreover, the latest developments suggest the use of hybrid designs combined to use several feature extraction techniques to enhance the accuracy of captions. As shown by Wang et al., the strategy of integrating local object-related features and global features of context on producing a more detailed description of the image turned out to be useful [20]. This end-to-end feature combination was effective compared to those based on deciding on a single level of features (object-level or global-level).",
            "2.5. Evaluation Metrics": "Standard image captioning evaluation metrics were employed to assess performance, including BLEU [21], METEOR [22], ROUGE-L [23], CIDEr [24], and SPICE [25]. While BLEU and ROUGE measure n-gram overlap and fluency, METEOR aligns closer to human judgment by considering synonyms. Crucially, CIDEr and SPICE are prioritized in this study as they specifically evaluate the semantic consensus and propositional content of captions, which is vital for verifying the integration of emotional context.",
            "2.6. Research Gap and Contribution": "In spite of enormous progress, modern technologies have significant drawbacks. First of all, approaches based on a missed perspective can be associated only with object detection, and related context can easily miss the essential information related to emotion that is presented in the form of facial expressions and is essential to the description histories that include human-related interactions [7,17]. On the other hand, the approaches that specifically aim at the analysis of emotions seldom have established strong object detection schemes that subsequently lower the resolution of a complex image scene description [17]. In addition, the existing object detection models, such as Faster R-CNN, are precise but very computationally demanding, restricting their real-time usage [14]. It is these gaps that point to the need for a single model able to pinpoint all the other aspects of the research (efficient object detection, detailed spatial relationships, and robust emotional expression analysis) in exactly the same direction as the one being studied in the current research. Table 1, presented below, provides a comparative overview of selected state-of-the-art image captioning methodologies, highlighting their contributions, strengths, and limitations: Table 1.Summary of key methods in image captioning. This paper clearly identifies a distinct research gap: the absence of comprehensive integration of efficient object detection, facial expression analysis, and explicit spatial-contextual modeling. Addressing these limitations, the proposed method integrates YOLOv5 for object detection efficiency, robust facial expression recognition, and contextual modeling to significantly enhance caption accuracy and relevance.",
            "3. Proposed Method": "The proposed architecture, as illustrated inFigure 1, integrates facial sentiment analysis, object detection, and global scene understanding to generate emotionally rich image captions. The input image is processed through three parallel modules: (i) a YOLOv5-based object detector with CSP and PANet layers for region-level features; (ii) a FER module (e.g., ResNet50) to extract emotional cues from detected faces; and (iii) a pretrained CNN (e.g., ResNet50) for capturing global context. These features are encoded and passed to an attention-based LSTM decoder, which fuses the information at each time step to produce context-aware and affective captions. Figure 1.Overview of the proposed model combining object, facial expression, and scene features for emotion-aware image captioning. 3.1. Encoder Architecture and Feature ExtractionThe encoder module is responsible for extracting heterogeneous visual representations from the input image, namely (i) region-based object features, (ii) facial expression features, and (iii) global scene features. These are subsequently fused and fed into the decoder for caption generation. This section elaborates each extraction process.3.1.1. Region-Based Object Detection Using YOLOv5Object-level semantics are obtained using the YOLOv5 object detector [15], a single-stage detection network well-suited for real-time processing. YOLOv5 divides the input image into anùëÜ√óùëÜS√óSgrid. Each cell predicts bounding boxes, objectness confidence, and class probabilities. Specifically, for each detected object, the feature vector includes four bounding box coordinates(ùë•,ùë¶,ùë§,‚Ñé)x,y,w,h, one objectness score, and 80 class probabilities. Thus, the per-object feature vector has a dimensionality ofùëëùëúùëèùëóùëíùëêùë°=4+1+80=85dobject=4+1+80=85(1)We constrain the model to detect a maximum of 10 objects per image, resulting in a fixed object feature matrix ofùêπùëúùëèùëóùëíùëêùë°‚ààR10√ó85Fobject‚ààR10√ó85(2)This matrix is flattened to form a vector of length 850. The class probabilities span 80 standard COCO categories, including ‚Äúperson,‚Äù ‚Äúdog,‚Äù ‚Äúcar,‚Äù ‚Äúchair,‚Äù etc., which are crucial for semantic grounding [26].The object detection module, depicted inFigure 2, functions as the encoder in the proposed framework. It adopts YOLOv5, leveraging a CSP-based backbone with Bottleneck CSP modules for efficient feature extraction and an SPP layer to capture multi-scale contextual information. A PANet structure further refines these features through1√ó11√ó1convolutions, upsampling, and concatenation. The final output generates object-level representations, including bounding box coordinates, confidence scores, and class probabilities for up to ten objects. These semantically rich features provide essential visual cues that are passed to the captioning decoder to enhance the relevance and detail of the generated image descriptions. Algorithm 1 shows the pseudocode of the object detection process.Algorithm 1:Object detection feature extractionInput: Image I (H √ó W √ó 3)Output: Object feature matrix F_object ‚àà ‚Ñù10√ó851.Initialize:2.‚ÄÉ‚ÄÉ‚ÄÉLoad pretrained YOLOv5 model3.‚ÄÉ‚ÄÉ‚ÄÉSet S ‚Üê grid_size4.‚ÄÉ‚ÄÉ‚ÄÉFobject‚Üê zeros(10, 85)5.‚ÄÉDetect Objects:6.‚ÄÉ‚ÄÉ‚ÄÉdetections ‚Üê YOLOv5.forward(I)7.fori = 1 to min(|detections|, 10)do8.‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉExtract bounding box: (x, y, w, h)9.‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉExtract objectness score: conf10. ¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉExtract class probabilities: p_class ‚àà ‚Ñù8011. ¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉFobject[i] ‚Üê concatenate[(x, y, w, h), conf, p_class]12.end for13.¬†¬†Return FobjectFigure 2.Encoder architecture for object feature extraction using BottleNeck CSP and PANet for multi-scale feature fusion.3.1.2. Facial Expression Recognition and Emotion Feature ExtractionOnce the object is detected, the model will then apply recognition of facial emotions that will be used to establish the affective context, helping the model come up with the caption. The Multi-task Cascaded Convolutional Network (MTCNN) [9] is used to pre-localize facial parts, which is robust on obtaining faces in unconstrained scenarios. These identified regions are then input into a facial expression recognition (FER) network, i.e., either VGG-Face or ResNet-50, which have been found to perform well in such emotion classification studies [16,17]. The system codes three faces in an image into a 2048-dimensional feature vectors. In case there are less than three found, the zero-padding is performed to ensure the same size of inputs is used. Such sentiment embeddings are valuable additions to the captioning procedure to give it an emotional context. Each detected face is passed through a convolutional neural network (CNN), yielding a high-level embedding of shape49√ó204849√ó2048, where 49 represents the spatial grid(7√ó7)(7√ó7)and 2048 is the feature depth. The architecture supports up to three faces per image, resulting in an aggregated feature tensor of size147√ó2048147√ó2048. This is denoted asùêπùëíùëöùëúùë°ùëñùëúùëõ‚ààR147√ó2048Femotion‚ààR147√ó2048(3)In scenarios with fewer than three faces, zero-padding is applied to preserve fixed input dimensionality. These emotion vectors are used not only for enriching semantic content but also for initializing the decoder LSTM‚Äôs hidden and cell states, thereby influencing the syntactic generation process from the outset [7]. The image shows the process of extracting facial sentiments from detected face regions using an FER model. Detected faces are passed through a CNN-based FER pipeline to generate emotional feature vectors. These vectors encode affective cues that are later used to enrich caption generation as illustrated inFigure 3, this step enables the model to incorporate emotional context by identifying facial expressions from up to three individuals per image. Also, a pseudo code is presented to clear the concept in Algorithm 2.Algorithm 2:Facial expression recognitionInput:Image I (H √ó W √ó 3)Output:Emotion feature tensor Femotion‚àà ‚Ñù147√ó20481.Initialize:2. ‚ÄÉ‚ÄÉ‚ÄÉLoad MTCNN face detector3. ‚ÄÉ‚ÄÉ‚ÄÉLoad pretrained FER model (ResNet50 or VGG-Face)4. ‚ÄÉ‚ÄÉ‚ÄÉFemotion‚Üê zeros(147, 2048)5. ‚ÄÉ‚ÄÉ‚ÄÉmax_faces ‚Üê 36.Detect Faces:7. ‚ÄÉ‚ÄÉ‚ÄÉface_regions ‚Üê MTCNN.detect(I)8. ‚ÄÉ‚ÄÉ‚ÄÉnum_faces ‚Üê min(|face_regions|, max_faces)9.fori = 1 to num_facesdo10.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉface ‚Üê crop_face(I, face_regions[i])11.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉface ‚Üê resize(face, (224, 224))12.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉemotion_features ‚Üê FER_model.extract(face) ‚ñ∑ Shape: (7 √ó 7 √ó 2048)13.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉemotion_vec ‚Üê flatten_spatial(emotion_features) ‚ñ∑ Shape: (49 √ó 2048)14.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉstart_idx ‚Üê i √ó 4915.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉFemotion[start_idx : start_idx + 49] ‚Üê emotion_vec16.end for17.ReturnFemotion‚ñ∑ Zero-padded if faces < 3Figure 3.Face sentiment extraction using a face detector and FER model to generate emotional features.3.1.3. Global Visual Feature ExtractionGlobal scene understanding is incorporated by extracting grid-based features using ResNet50 [17]. The network processes the entire input image with original dimensions224√ó224√ó3224√ó224√ó3, producing an output of7√ó7√ó20487√ó7√ó2048. This is flattened and compressed to a tensor of shape14√ó204814√ó2048through average pooling and dimensionality reduction, resulting inùêπùëîùëüùëñùëë‚ààR14√ó2048Fgrid‚ààR14√ó2048(4)These features offer a macro-level view of the scene, capturing contextual relationships that may not be represented in object-specific streams. Algorithm 3 shows the pseudocode for grid features extraction.Algorithm 3:Global feature extractionInput:Image I (224 √ó 224 √ó 3)Output: Global feature matrix Fgrid‚àà ‚Ñù14√ó20481.Initialize:2. ‚ÄÉ‚ÄÉ‚ÄÉLoad pretrained ResNet503. ‚ÄÉ‚ÄÉ‚ÄÉI ‚Üê resize(I, (224, 224))4.Extract Features:5. ‚ÄÉ‚ÄÉ‚ÄÉfeatures ‚Üê ResNet50.forward(I) ‚ñ∑ Output: (7 √ó 7 √ó 2048)6. ‚ÄÉ‚ÄÉ‚ÄÉFgrid‚Üê adaptive_avg_pool(features, (14, 2048))7.ReturnFgrid 3.2. Decoder Architecture and Caption GenerationThe decoder, illustrated inFigure 4, employs a Long Short-Term Memory (LSTM) architecture to sequentially generate captions. Captioning begins with an initial embedding from the <START> token or an emotion-informed embedding. At each time step, the LSTM unit processes the previous word, integrates an attention-weighted context vector derived from object, global, and emotional features, and updates its internal states accordingly. This attention mechanism helps the decoder focus on the most relevant visual information at each step. The resulting hidden state is passed through a softmax layer to predict the next word in the sequence, continuing until the <END> token is produced.Figure 4.Decoder architecture for caption generation using attention-driven LSTM network.3.2.1. Feature Integration and InitializationThe object and global features are concatenated along the channel axis to form the composite attention matrix:ùêπùëéùë°ùë°ùëõ=[ùêπùëúùëèùëóùëíùëêùë°,ùêπùëîùëüùëñùëë]‚ààR(10+14)√ó2048Fattn=Fobject,Fgrid‚ààR10+14√ó2048(5)This corresponds to 24 spatial tokens, each of 2048 dimensions, and forms the basis for computing attention. The total vector length is850+28,672=29,522850+28,672=29,522, which is retained for reference. The facial expression features are projected using a linear transformation and reshaped into a vector to initialize the LSTM‚Äôs hidden(‚Ñéùëú)hoand cell states(ùëêùëú)coas follows:‚Ñéùëú,ùëêùëú=ùêøùëÜùëáùëÄ(ùëäùëì.ùë£ùëíùëê(ùêπùëíùëöùëúùë°ùëñùëúùëõ))ho,co=LSTMWf.vecFemotion(6)Here,ùëäùëìWfis a trainable weight matrix andùë£ùëíùëê(.)vec(.)denotes flattening of the tensor.3.2.2. Attention MechanismThe attention module dynamically assigns weights to the integrated features based on the decoder‚Äôs previous hidden state. At each time stepùë°t, the attention energyùëíùë°ùëñeitfor locationùëñiis computed by:ùëíùë°ùëñ=ùë£ùë°ùëáùëéùëõ‚Ñé(ùëä‚Ñé‚Ñéùë°‚àí1+ùëäùëéùëéùëñ)eit=vtTanhWhht‚àí1+Waai(7)‚àùùë°ùëñ=exp(ùëíùë°ùëñ)‚àëùêøùëó=1exp(ùëíùë°ùëó)‚àùit=expeit‚àëj=1Lexpejt(8)ùëêùë°=‚àëùêøùëñ=1‚àùùë°ùëñùëéùëñct=‚àëi=1L‚àùitai(9)where the terms are defined as follows:‚Ñéùë°‚àí1ht‚àí1is the previous hidden state;ùëéùëñaiis theùëñùë°‚Ñéithvisual feature vector fromùêπùëéùë°ùë°ùëõFattn;ùêø=24L=24is the number of attention locations;ùë£ùë°,ùëä‚Ñé,ùëäùëévt,Wh,Waare trainable parameters;‚àùùë°ùëñ‚àùitare the attention weights;ùëêùë°ctis the resulting context vector.This formulation is adapted from the standard Bahdanau attention mechanism [12] but extended to multi-source visual embedding.3.2.3. Sequential Caption GenerationAt each decoding step, the LSTM receives the previous word embeddingùë¶ùë°‚àí1yt‚àí1, the previous hidden state, and the context vectorùëêùë°ctto generate the next hidden state and output:‚Ñéùë°,ùëêùë°=ùêøùëÜùëáùëÄ(ùë¶ùë°‚àí1,‚Ñéùë°‚àí1,ùëêùë°‚àí1)ht,ct=LSTMyt‚àí1,ht‚àí1,ct‚àí1(10)The probability distribution over the vocabulary is computed via a softmax function:ùëÉ(ùë¶ùë°)=ùëÜùëúùëìùë°ùëöùëéùë•(ùëäùëú.‚Ñéùë°)Pyt=SoftmaxWo.ht(11)whereùëäùëúWois a learned projection matrix. The word with the highest probability is selected as the output token for timeùë°t. 3.3. Implementation WorkflowThe methodology was implemented in Python 3.8 using the PyTorch library. The model training and inference were executed on an NVIDIA RTX 4090 GPU with 24 GB VRAM. The implementation followed the sequence below:Dataset Annotation:The COCO and FER2013 datasets were used to label bounding boxes and emotional labels.Model Initialization:Pretrained weights were used by YOLOv5, ResNet50, and VGG-Face.Fine-tuning:The model was trained on the target dataset using the Adam optimizer with a learning rate of1√ó10‚àí41√ó10‚àí4Evaluation:CIDEr, ROUGE-L, METEOR, and SPICE were used to validate performance, and they include in-depth linguistic plus semantic evaluation [21,22,23,24,25]. 3.4. Sensor/IoT Deployment ConsiderationsSensing and compute.Standard RGB industrial cameras stream 1080p@30 FPS to an edge box (e.g., Jetson Orin/x86 + RTX 4090).Latency.YOLOv5 + FER + LSTM captioning achieves 600 ms on our hardware; captions are emitted at 1‚Äì2 Hz per stream.Connectivity.Captions/alerts are published via MQTT/OPC UA to the MES/SCADA data bus; payloads include object IDs, emotion tags, and timestamps.Privacy.On-prem processing; no raw video leaves the factory LAN; only structured events are logged.Use cases.Operator fatigue/strain cues, unsafe posture alerts, human‚Äìrobot handover summaries, and incident narration for PdM/HSE",
            "3.1. Encoder Architecture and Feature Extraction": "The encoder module is responsible for extracting heterogeneous visual representations from the input image, namely (i) region-based object features, (ii) facial expression features, and (iii) global scene features. These are subsequently fused and fed into the decoder for caption generation. This section elaborates each extraction process. 3.1.1. Region-Based Object Detection Using YOLOv5Object-level semantics are obtained using the YOLOv5 object detector [15], a single-stage detection network well-suited for real-time processing. YOLOv5 divides the input image into anùëÜ√óùëÜS√óSgrid. Each cell predicts bounding boxes, objectness confidence, and class probabilities. Specifically, for each detected object, the feature vector includes four bounding box coordinates(ùë•,ùë¶,ùë§,‚Ñé)x,y,w,h, one objectness score, and 80 class probabilities. Thus, the per-object feature vector has a dimensionality ofùëëùëúùëèùëóùëíùëêùë°=4+1+80=85dobject=4+1+80=85(1)We constrain the model to detect a maximum of 10 objects per image, resulting in a fixed object feature matrix ofùêπùëúùëèùëóùëíùëêùë°‚ààR10√ó85Fobject‚ààR10√ó85(2)This matrix is flattened to form a vector of length 850. The class probabilities span 80 standard COCO categories, including ‚Äúperson,‚Äù ‚Äúdog,‚Äù ‚Äúcar,‚Äù ‚Äúchair,‚Äù etc., which are crucial for semantic grounding [26].The object detection module, depicted inFigure 2, functions as the encoder in the proposed framework. It adopts YOLOv5, leveraging a CSP-based backbone with Bottleneck CSP modules for efficient feature extraction and an SPP layer to capture multi-scale contextual information. A PANet structure further refines these features through1√ó11√ó1convolutions, upsampling, and concatenation. The final output generates object-level representations, including bounding box coordinates, confidence scores, and class probabilities for up to ten objects. These semantically rich features provide essential visual cues that are passed to the captioning decoder to enhance the relevance and detail of the generated image descriptions. Algorithm 1 shows the pseudocode of the object detection process.Algorithm 1:Object detection feature extractionInput: Image I (H √ó W √ó 3)Output: Object feature matrix F_object ‚àà ‚Ñù10√ó851.Initialize:2.‚ÄÉ‚ÄÉ‚ÄÉLoad pretrained YOLOv5 model3.‚ÄÉ‚ÄÉ‚ÄÉSet S ‚Üê grid_size4.‚ÄÉ‚ÄÉ‚ÄÉFobject‚Üê zeros(10, 85)5.‚ÄÉDetect Objects:6.‚ÄÉ‚ÄÉ‚ÄÉdetections ‚Üê YOLOv5.forward(I)7.fori = 1 to min(|detections|, 10)do8.‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉExtract bounding box: (x, y, w, h)9.‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉExtract objectness score: conf10. ¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉExtract class probabilities: p_class ‚àà ‚Ñù8011. ¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉFobject[i] ‚Üê concatenate[(x, y, w, h), conf, p_class]12.end for13.¬†¬†Return FobjectFigure 2.Encoder architecture for object feature extraction using BottleNeck CSP and PANet for multi-scale feature fusion. 3.1.2. Facial Expression Recognition and Emotion Feature ExtractionOnce the object is detected, the model will then apply recognition of facial emotions that will be used to establish the affective context, helping the model come up with the caption. The Multi-task Cascaded Convolutional Network (MTCNN) [9] is used to pre-localize facial parts, which is robust on obtaining faces in unconstrained scenarios. These identified regions are then input into a facial expression recognition (FER) network, i.e., either VGG-Face or ResNet-50, which have been found to perform well in such emotion classification studies [16,17]. The system codes three faces in an image into a 2048-dimensional feature vectors. In case there are less than three found, the zero-padding is performed to ensure the same size of inputs is used. Such sentiment embeddings are valuable additions to the captioning procedure to give it an emotional context. Each detected face is passed through a convolutional neural network (CNN), yielding a high-level embedding of shape49√ó204849√ó2048, where 49 represents the spatial grid(7√ó7)(7√ó7)and 2048 is the feature depth. The architecture supports up to three faces per image, resulting in an aggregated feature tensor of size147√ó2048147√ó2048. This is denoted asùêπùëíùëöùëúùë°ùëñùëúùëõ‚ààR147√ó2048Femotion‚ààR147√ó2048(3)In scenarios with fewer than three faces, zero-padding is applied to preserve fixed input dimensionality. These emotion vectors are used not only for enriching semantic content but also for initializing the decoder LSTM‚Äôs hidden and cell states, thereby influencing the syntactic generation process from the outset [7]. The image shows the process of extracting facial sentiments from detected face regions using an FER model. Detected faces are passed through a CNN-based FER pipeline to generate emotional feature vectors. These vectors encode affective cues that are later used to enrich caption generation as illustrated inFigure 3, this step enables the model to incorporate emotional context by identifying facial expressions from up to three individuals per image. Also, a pseudo code is presented to clear the concept in Algorithm 2.Algorithm 2:Facial expression recognitionInput:Image I (H √ó W √ó 3)Output:Emotion feature tensor Femotion‚àà ‚Ñù147√ó20481.Initialize:2. ‚ÄÉ‚ÄÉ‚ÄÉLoad MTCNN face detector3. ‚ÄÉ‚ÄÉ‚ÄÉLoad pretrained FER model (ResNet50 or VGG-Face)4. ‚ÄÉ‚ÄÉ‚ÄÉFemotion‚Üê zeros(147, 2048)5. ‚ÄÉ‚ÄÉ‚ÄÉmax_faces ‚Üê 36.Detect Faces:7. ‚ÄÉ‚ÄÉ‚ÄÉface_regions ‚Üê MTCNN.detect(I)8. ‚ÄÉ‚ÄÉ‚ÄÉnum_faces ‚Üê min(|face_regions|, max_faces)9.fori = 1 to num_facesdo10.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉface ‚Üê crop_face(I, face_regions[i])11.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉface ‚Üê resize(face, (224, 224))12.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉemotion_features ‚Üê FER_model.extract(face) ‚ñ∑ Shape: (7 √ó 7 √ó 2048)13.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉemotion_vec ‚Üê flatten_spatial(emotion_features) ‚ñ∑ Shape: (49 √ó 2048)14.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉstart_idx ‚Üê i √ó 4915.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉFemotion[start_idx : start_idx + 49] ‚Üê emotion_vec16.end for17.ReturnFemotion‚ñ∑ Zero-padded if faces < 3Figure 3.Face sentiment extraction using a face detector and FER model to generate emotional features. 3.1.3. Global Visual Feature ExtractionGlobal scene understanding is incorporated by extracting grid-based features using ResNet50 [17]. The network processes the entire input image with original dimensions224√ó224√ó3224√ó224√ó3, producing an output of7√ó7√ó20487√ó7√ó2048. This is flattened and compressed to a tensor of shape14√ó204814√ó2048through average pooling and dimensionality reduction, resulting inùêπùëîùëüùëñùëë‚ààR14√ó2048Fgrid‚ààR14√ó2048(4)These features offer a macro-level view of the scene, capturing contextual relationships that may not be represented in object-specific streams. Algorithm 3 shows the pseudocode for grid features extraction.Algorithm 3:Global feature extractionInput:Image I (224 √ó 224 √ó 3)Output: Global feature matrix Fgrid‚àà ‚Ñù14√ó20481.Initialize:2. ‚ÄÉ‚ÄÉ‚ÄÉLoad pretrained ResNet503. ‚ÄÉ‚ÄÉ‚ÄÉI ‚Üê resize(I, (224, 224))4.Extract Features:5. ‚ÄÉ‚ÄÉ‚ÄÉfeatures ‚Üê ResNet50.forward(I) ‚ñ∑ Output: (7 √ó 7 √ó 2048)6. ‚ÄÉ‚ÄÉ‚ÄÉFgrid‚Üê adaptive_avg_pool(features, (14, 2048))7.ReturnFgrid",
            "3.1.1. Region-Based Object Detection Using YOLOv5": "Object-level semantics are obtained using the YOLOv5 object detector [15], a single-stage detection network well-suited for real-time processing. YOLOv5 divides the input image into anùëÜ√óùëÜS√óSgrid. Each cell predicts bounding boxes, objectness confidence, and class probabilities. Specifically, for each detected object, the feature vector includes four bounding box coordinates(ùë•,ùë¶,ùë§,‚Ñé)x,y,w,h, one objectness score, and 80 class probabilities. Thus, the per-object feature vector has a dimensionality ofùëëùëúùëèùëóùëíùëêùë°=4+1+80=85dobject=4+1+80=85(1) We constrain the model to detect a maximum of 10 objects per image, resulting in a fixed object feature matrix ofùêπùëúùëèùëóùëíùëêùë°‚ààR10√ó85Fobject‚ààR10√ó85(2) This matrix is flattened to form a vector of length 850. The class probabilities span 80 standard COCO categories, including ‚Äúperson,‚Äù ‚Äúdog,‚Äù ‚Äúcar,‚Äù ‚Äúchair,‚Äù etc., which are crucial for semantic grounding [26]. The object detection module, depicted inFigure 2, functions as the encoder in the proposed framework. It adopts YOLOv5, leveraging a CSP-based backbone with Bottleneck CSP modules for efficient feature extraction and an SPP layer to capture multi-scale contextual information. A PANet structure further refines these features through1√ó11√ó1convolutions, upsampling, and concatenation. The final output generates object-level representations, including bounding box coordinates, confidence scores, and class probabilities for up to ten objects. These semantically rich features provide essential visual cues that are passed to the captioning decoder to enhance the relevance and detail of the generated image descriptions. Algorithm 1 shows the pseudocode of the object detection process.Algorithm 1:Object detection feature extractionInput: Image I (H √ó W √ó 3)Output: Object feature matrix F_object ‚àà ‚Ñù10√ó851.Initialize:2.‚ÄÉ‚ÄÉ‚ÄÉLoad pretrained YOLOv5 model3.‚ÄÉ‚ÄÉ‚ÄÉSet S ‚Üê grid_size4.‚ÄÉ‚ÄÉ‚ÄÉFobject‚Üê zeros(10, 85)5.‚ÄÉDetect Objects:6.‚ÄÉ‚ÄÉ‚ÄÉdetections ‚Üê YOLOv5.forward(I)7.fori = 1 to min(|detections|, 10)do8.‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉExtract bounding box: (x, y, w, h)9.‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉExtract objectness score: conf10. ¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉExtract class probabilities: p_class ‚àà ‚Ñù8011. ¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉFobject[i] ‚Üê concatenate[(x, y, w, h), conf, p_class]12.end for13.¬†¬†Return Fobject Figure 2.Encoder architecture for object feature extraction using BottleNeck CSP and PANet for multi-scale feature fusion.",
            "3.1.2. Facial Expression Recognition and Emotion Feature Extraction": "Once the object is detected, the model will then apply recognition of facial emotions that will be used to establish the affective context, helping the model come up with the caption. The Multi-task Cascaded Convolutional Network (MTCNN) [9] is used to pre-localize facial parts, which is robust on obtaining faces in unconstrained scenarios. These identified regions are then input into a facial expression recognition (FER) network, i.e., either VGG-Face or ResNet-50, which have been found to perform well in such emotion classification studies [16,17]. The system codes three faces in an image into a 2048-dimensional feature vectors. In case there are less than three found, the zero-padding is performed to ensure the same size of inputs is used. Such sentiment embeddings are valuable additions to the captioning procedure to give it an emotional context. Each detected face is passed through a convolutional neural network (CNN), yielding a high-level embedding of shape49√ó204849√ó2048, where 49 represents the spatial grid(7√ó7)(7√ó7)and 2048 is the feature depth. The architecture supports up to three faces per image, resulting in an aggregated feature tensor of size147√ó2048147√ó2048. This is denoted asùêπùëíùëöùëúùë°ùëñùëúùëõ‚ààR147√ó2048Femotion‚ààR147√ó2048(3) In scenarios with fewer than three faces, zero-padding is applied to preserve fixed input dimensionality. These emotion vectors are used not only for enriching semantic content but also for initializing the decoder LSTM‚Äôs hidden and cell states, thereby influencing the syntactic generation process from the outset [7]. The image shows the process of extracting facial sentiments from detected face regions using an FER model. Detected faces are passed through a CNN-based FER pipeline to generate emotional feature vectors. These vectors encode affective cues that are later used to enrich caption generation as illustrated inFigure 3, this step enables the model to incorporate emotional context by identifying facial expressions from up to three individuals per image. Also, a pseudo code is presented to clear the concept in Algorithm 2.Algorithm 2:Facial expression recognitionInput:Image I (H √ó W √ó 3)Output:Emotion feature tensor Femotion‚àà ‚Ñù147√ó20481.Initialize:2. ‚ÄÉ‚ÄÉ‚ÄÉLoad MTCNN face detector3. ‚ÄÉ‚ÄÉ‚ÄÉLoad pretrained FER model (ResNet50 or VGG-Face)4. ‚ÄÉ‚ÄÉ‚ÄÉFemotion‚Üê zeros(147, 2048)5. ‚ÄÉ‚ÄÉ‚ÄÉmax_faces ‚Üê 36.Detect Faces:7. ‚ÄÉ‚ÄÉ‚ÄÉface_regions ‚Üê MTCNN.detect(I)8. ‚ÄÉ‚ÄÉ‚ÄÉnum_faces ‚Üê min(|face_regions|, max_faces)9.fori = 1 to num_facesdo10.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉface ‚Üê crop_face(I, face_regions[i])11.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉface ‚Üê resize(face, (224, 224))12.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉemotion_features ‚Üê FER_model.extract(face) ‚ñ∑ Shape: (7 √ó 7 √ó 2048)13.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉemotion_vec ‚Üê flatten_spatial(emotion_features) ‚ñ∑ Shape: (49 √ó 2048)14.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉstart_idx ‚Üê i √ó 4915.¬†¬†‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉFemotion[start_idx : start_idx + 49] ‚Üê emotion_vec16.end for17.ReturnFemotion‚ñ∑ Zero-padded if faces < 3 Figure 3.Face sentiment extraction using a face detector and FER model to generate emotional features.",
            "3.1.3. Global Visual Feature Extraction": "Global scene understanding is incorporated by extracting grid-based features using ResNet50 [17]. The network processes the entire input image with original dimensions224√ó224√ó3224√ó224√ó3, producing an output of7√ó7√ó20487√ó7√ó2048. This is flattened and compressed to a tensor of shape14√ó204814√ó2048through average pooling and dimensionality reduction, resulting inùêπùëîùëüùëñùëë‚ààR14√ó2048Fgrid‚ààR14√ó2048(4) These features offer a macro-level view of the scene, capturing contextual relationships that may not be represented in object-specific streams. Algorithm 3 shows the pseudocode for grid features extraction.Algorithm 3:Global feature extractionInput:Image I (224 √ó 224 √ó 3)Output: Global feature matrix Fgrid‚àà ‚Ñù14√ó20481.Initialize:2. ‚ÄÉ‚ÄÉ‚ÄÉLoad pretrained ResNet503. ‚ÄÉ‚ÄÉ‚ÄÉI ‚Üê resize(I, (224, 224))4.Extract Features:5. ‚ÄÉ‚ÄÉ‚ÄÉfeatures ‚Üê ResNet50.forward(I) ‚ñ∑ Output: (7 √ó 7 √ó 2048)6. ‚ÄÉ‚ÄÉ‚ÄÉFgrid‚Üê adaptive_avg_pool(features, (14, 2048))7.ReturnFgrid",
            "3.2. Decoder Architecture and Caption Generation": "The decoder, illustrated inFigure 4, employs a Long Short-Term Memory (LSTM) architecture to sequentially generate captions. Captioning begins with an initial embedding from the <START> token or an emotion-informed embedding. At each time step, the LSTM unit processes the previous word, integrates an attention-weighted context vector derived from object, global, and emotional features, and updates its internal states accordingly. This attention mechanism helps the decoder focus on the most relevant visual information at each step. The resulting hidden state is passed through a softmax layer to predict the next word in the sequence, continuing until the <END> token is produced. Figure 4.Decoder architecture for caption generation using attention-driven LSTM network. 3.2.1. Feature Integration and InitializationThe object and global features are concatenated along the channel axis to form the composite attention matrix:ùêπùëéùë°ùë°ùëõ=[ùêπùëúùëèùëóùëíùëêùë°,ùêπùëîùëüùëñùëë]‚ààR(10+14)√ó2048Fattn=Fobject,Fgrid‚ààR10+14√ó2048(5)This corresponds to 24 spatial tokens, each of 2048 dimensions, and forms the basis for computing attention. The total vector length is850+28,672=29,522850+28,672=29,522, which is retained for reference. The facial expression features are projected using a linear transformation and reshaped into a vector to initialize the LSTM‚Äôs hidden(‚Ñéùëú)hoand cell states(ùëêùëú)coas follows:‚Ñéùëú,ùëêùëú=ùêøùëÜùëáùëÄ(ùëäùëì.ùë£ùëíùëê(ùêπùëíùëöùëúùë°ùëñùëúùëõ))ho,co=LSTMWf.vecFemotion(6)Here,ùëäùëìWfis a trainable weight matrix andùë£ùëíùëê(.)vec(.)denotes flattening of the tensor. 3.2.2. Attention MechanismThe attention module dynamically assigns weights to the integrated features based on the decoder‚Äôs previous hidden state. At each time stepùë°t, the attention energyùëíùë°ùëñeitfor locationùëñiis computed by:ùëíùë°ùëñ=ùë£ùë°ùëáùëéùëõ‚Ñé(ùëä‚Ñé‚Ñéùë°‚àí1+ùëäùëéùëéùëñ)eit=vtTanhWhht‚àí1+Waai(7)‚àùùë°ùëñ=exp(ùëíùë°ùëñ)‚àëùêøùëó=1exp(ùëíùë°ùëó)‚àùit=expeit‚àëj=1Lexpejt(8)ùëêùë°=‚àëùêøùëñ=1‚àùùë°ùëñùëéùëñct=‚àëi=1L‚àùitai(9)where the terms are defined as follows:‚Ñéùë°‚àí1ht‚àí1is the previous hidden state;ùëéùëñaiis theùëñùë°‚Ñéithvisual feature vector fromùêπùëéùë°ùë°ùëõFattn;ùêø=24L=24is the number of attention locations;ùë£ùë°,ùëä‚Ñé,ùëäùëévt,Wh,Waare trainable parameters;‚àùùë°ùëñ‚àùitare the attention weights;ùëêùë°ctis the resulting context vector.This formulation is adapted from the standard Bahdanau attention mechanism [12] but extended to multi-source visual embedding. 3.2.3. Sequential Caption GenerationAt each decoding step, the LSTM receives the previous word embeddingùë¶ùë°‚àí1yt‚àí1, the previous hidden state, and the context vectorùëêùë°ctto generate the next hidden state and output:‚Ñéùë°,ùëêùë°=ùêøùëÜùëáùëÄ(ùë¶ùë°‚àí1,‚Ñéùë°‚àí1,ùëêùë°‚àí1)ht,ct=LSTMyt‚àí1,ht‚àí1,ct‚àí1(10)The probability distribution over the vocabulary is computed via a softmax function:ùëÉ(ùë¶ùë°)=ùëÜùëúùëìùë°ùëöùëéùë•(ùëäùëú.‚Ñéùë°)Pyt=SoftmaxWo.ht(11)whereùëäùëúWois a learned projection matrix. The word with the highest probability is selected as the output token for timeùë°t.",
            "3.2.1. Feature Integration and Initialization": "The object and global features are concatenated along the channel axis to form the composite attention matrix:ùêπùëéùë°ùë°ùëõ=[ùêπùëúùëèùëóùëíùëêùë°,ùêπùëîùëüùëñùëë]‚ààR(10+14)√ó2048Fattn=Fobject,Fgrid‚ààR10+14√ó2048(5) This corresponds to 24 spatial tokens, each of 2048 dimensions, and forms the basis for computing attention. The total vector length is850+28,672=29,522850+28,672=29,522, which is retained for reference. The facial expression features are projected using a linear transformation and reshaped into a vector to initialize the LSTM‚Äôs hidden(‚Ñéùëú)hoand cell states(ùëêùëú)coas follows:‚Ñéùëú,ùëêùëú=ùêøùëÜùëáùëÄ(ùëäùëì.ùë£ùëíùëê(ùêπùëíùëöùëúùë°ùëñùëúùëõ))ho,co=LSTMWf.vecFemotion(6) Here,ùëäùëìWfis a trainable weight matrix andùë£ùëíùëê(.)vec(.)denotes flattening of the tensor.",
            "3.2.2. Attention Mechanism": "The attention module dynamically assigns weights to the integrated features based on the decoder‚Äôs previous hidden state. At each time stepùë°t, the attention energyùëíùë°ùëñeitfor locationùëñiis computed by:ùëíùë°ùëñ=ùë£ùë°ùëáùëéùëõ‚Ñé(ùëä‚Ñé‚Ñéùë°‚àí1+ùëäùëéùëéùëñ)eit=vtTanhWhht‚àí1+Waai(7)‚àùùë°ùëñ=exp(ùëíùë°ùëñ)‚àëùêøùëó=1exp(ùëíùë°ùëó)‚àùit=expeit‚àëj=1Lexpejt(8)ùëêùë°=‚àëùêøùëñ=1‚àùùë°ùëñùëéùëñct=‚àëi=1L‚àùitai(9)where the terms are defined as follows: ‚Ñéùë°‚àí1ht‚àí1is the previous hidden state;ùëéùëñaiis theùëñùë°‚Ñéithvisual feature vector fromùêπùëéùë°ùë°ùëõFattn;ùêø=24L=24is the number of attention locations;ùë£ùë°,ùëä‚Ñé,ùëäùëévt,Wh,Waare trainable parameters;‚àùùë°ùëñ‚àùitare the attention weights;ùëêùë°ctis the resulting context vector. This formulation is adapted from the standard Bahdanau attention mechanism [12] but extended to multi-source visual embedding.",
            "3.2.3. Sequential Caption Generation": "At each decoding step, the LSTM receives the previous word embeddingùë¶ùë°‚àí1yt‚àí1, the previous hidden state, and the context vectorùëêùë°ctto generate the next hidden state and output:‚Ñéùë°,ùëêùë°=ùêøùëÜùëáùëÄ(ùë¶ùë°‚àí1,‚Ñéùë°‚àí1,ùëêùë°‚àí1)ht,ct=LSTMyt‚àí1,ht‚àí1,ct‚àí1(10) The probability distribution over the vocabulary is computed via a softmax function:ùëÉ(ùë¶ùë°)=ùëÜùëúùëìùë°ùëöùëéùë•(ùëäùëú.‚Ñéùë°)Pyt=SoftmaxWo.ht(11)whereùëäùëúWois a learned projection matrix. The word with the highest probability is selected as the output token for timeùë°t.",
            "3.3. Implementation Workflow": "The methodology was implemented in Python 3.8 using the PyTorch library. The model training and inference were executed on an NVIDIA RTX 4090 GPU with 24 GB VRAM. The implementation followed the sequence below: Dataset Annotation:The COCO and FER2013 datasets were used to label bounding boxes and emotional labels.Model Initialization:Pretrained weights were used by YOLOv5, ResNet50, and VGG-Face.Fine-tuning:The model was trained on the target dataset using the Adam optimizer with a learning rate of1√ó10‚àí41√ó10‚àí4Evaluation:CIDEr, ROUGE-L, METEOR, and SPICE were used to validate performance, and they include in-depth linguistic plus semantic evaluation [21,22,23,24,25].",
            "3.4. Sensor/IoT Deployment Considerations": "Sensing and compute.Standard RGB industrial cameras stream 1080p@30 FPS to an edge box (e.g., Jetson Orin/x86 + RTX 4090).Latency.YOLOv5 + FER + LSTM captioning achieves 600 ms on our hardware; captions are emitted at 1‚Äì2 Hz per stream.Connectivity.Captions/alerts are published via MQTT/OPC UA to the MES/SCADA data bus; payloads include object IDs, emotion tags, and timestamps.Privacy.On-prem processing; no raw video leaves the factory LAN; only structured events are logged.Use cases.Operator fatigue/strain cues, unsafe posture alerts, human‚Äìrobot handover summaries, and incident narration for PdM/HSE",
            "4. Experiments": "This subsection describes the experimental environment with which the usefulness of the suggestive facial-expression-augmented image captioning model should be assessed. It contains the description of the datasets, baseline models, the specifications of the implementation, evaluation protocols, and training approaches. Experiments have been performed in a controlled environment, where each experiment can be replicated to establish a fair comparison of results to the state-of-the-art techniques. 4.1. DatasetsTo fully test the proposed approach, three publicly available datasets were employed as FlickrFace 11k [7], COCOFace 15k [26], and FER2013 [18]. These data have been chosen in order to represent the diversity of the object instances, facial emotions, and human-oriented scenes in reality.4.1.1. FlickrFace 11kFlickrFace 11k means a narrow selection of the Flickr30k picture captioning corpus, editorialized to prefer pictures of human faces and expressions. It includes 11,000 high-resolution visuals, all of which are assigned five captions written by a human being that relate to various human acts and expressions. The dataset is split accordingly with 9000 training images, 1000 validation ones, and the same of the testing ones in accordance with the Karpathy split protocol [7]. The screening has been made with regard to diversity in terms of demography and emotions.Figure 5a,b show sample images capturing expressions such as joy, curiosity, and contemplation. Its emphasis on facial visibility makes it well-suited for training emotion-aware captioning models.Figure 5.(a) A group of people smiling and posing together at what appears to be a social gathering or party. (b) A musical performance on stage with multiple performers singing and playing instruments under concert lighting.4.1.2. COCOFace 15kThe COCOFace 15k dataset is a filtered subset of the COCO 2017 dataset [26], consisting of 15,000 images with visible human faces. It is split into 13,000 training and 2000 validation/test samples. Each image includes multiple captions and bounding boxes, supporting joint learning of object semantics and visual localization. The dataset offers wide variability in facial orientation, lighting, and occlusion, making it ideal for training facial expression models and evaluating captioning systems in real-world scenarios.Figure 6a,b showcases its contextual and emotional diversity, positioning COCOFace 15k as a strong benchmark for multimodal captioning research.Figure 6.(a) A young child sitting on a couch and holding a colorful umbrella, smiling toward the camera. (b) Two adults dressed formally, engaging in a handshake or exchange during what appears to be an outdoor event.4.1.3. FER2013The FER2013 dataset is a well-established benchmark in facial expression recognition research and is extensively used in affective computing [18]. It consists of 35,887 grayscale images with a resolution of 48 √ó 48 pixels. Each image represents a cropped frontal face, labeled with one of seven emotion classes: anger, disgust, fear, happiness, sadness, surprise, or neutral. The dataset is organized into 28,709 training images, 3589 for validation, and 3589 for testing. Due to its wide usage and consistent labeling, FER2013 serves as a reliable source for pretraining the facial expression recognition component of the proposed model.Figure 7a‚Äìc depict the emotional variety in FER2013, underscoring its value for facial expression recognition in vision-language captioning.Table 2presents a short statistics of the datasets used.Figure 7.Sample images from the FER2013 dataset displaying distinct emotional expressions: (a) surprise, (b) fear, and (c) happiness [18].Table 2.Summary of datasets used in experiments. 4.2. Baseline ModelsTo benchmark the proposed model, we compare it against two key attention-based image captioning frameworks: Show, Attend and Tell by Xu et al. [12] and the Bottom-Up and Top-Down Attention model by Anderson et al. [3]. These baselines serve as strong references to assess the impact of incorporating facial expression features and real-time object detection into caption generation.The Show, Attend and Tell model is especially relevant to our approach, as it pioneered the integration of soft attention mechanisms within a CNN-LSTM architecture. By allowing the decoder to focus dynamically on different spatial regions of an image, the model improved contextual alignment between visual content and generated words. We adopt this structure as a foundational benchmark, as our model extends its architecture by introducing emotion-based initialization through facial expression features and enhancing object localization using YOLOv5. Comparison with this model helps quantify the direct contribution of emotional cues to caption fluency and relevance.The Bottom-Up and Top-Down Attention model, while more complex, serves as a comparison for evaluating object-level attention. It uses Faster R-CNN to extract detailed region proposals and applies top-down attention to guide captioning. Though accurate, it is computationally slower than our YOLOv5-based approach. This comparison highlights how our method maintains semantic richness while improving inference efficiency.Both baselines were retrained on FlickrFace 11k and COCOFace 15k using identical splits and preprocessing to ensure objective performance evaluation. 4.3. Implementation DetailsThe proposed model was implemented using Python 3.8 and the PyTorch 1.8.1 deep learning framework (version 1.13). The experiments were conducted on a system equipped with the hardware and software specifications detailed inTable 3.Table 3.System specifications used for training and evaluating the proposed model.For image preprocessing, the OpenCV and Albumentations libraries were employed. Dataset management and augmentation were handled via custom PyTorch dataset and DataLoader classes. The implementation supports modular training, allowing for separate pretraining of object detection and emotion recognition components before joint optimization. All models were trained with reproducible random seeds, and results were averaged over three runs to reduce variance. 4.4. Evaluation MetricsWe evaluated caption quality using five standard metrics: BLEU, METEOR, ROUGE-L, CIDEr, and SPICE. BLEU@N measures n-gram precision [21], while METEOR captures word alignment, synonyms, and ordering [22]. ROUGE-L assesses fluency through longest common subsequence matching [23]. CIDEr evaluates consensus with multiple references [24], and SPICE focuses on semantic content using scene graph comparisons [25]. These metrics collectively assess both linguistic accuracy and semantic relevance. 4.5. Training ProcedureThe training strategy is divided into two stages to leverage pretrained knowledge while allowing end-to-end optimization.4.5.1. Stage 1: Pretraining of Feature ExtractorsIn the first stage, pretrained models are used for individual components:YOLOv5 is initialized with weights trained on the COCO dataset.ResNet50 and VGG-Face are used for extracting grid and facial features, respectively.FER-CNN is pretrained on FER2013 to classify facial expressions.These pretrained models are frozen during initial training to prevent overfitting and to ensure stability in feature representations.4.5.2. Stage 2: End-to-End Captioning Model TrainingIn the second stage, the complete architecture, including the LSTM decoder and attention mechanism, is fine-tuned in an end-to-end manner. The word embedding layer is uniformly initialized with a dimensionality of 300, while the LSTM‚Äôs hidden and cell states are configured with 512 units each. Optimization is carried out using the Adam optimizer, beginning with an initial learning rate of 0.001. To encourage convergence, the learning rate is reduced to 0.0001 if the METEOR score does not improve over two consecutive validation epochs. The model is trained for a maximum of 30 epochs using a mini-batch size of 64. To mitigate overfitting and stabilize training, dropout is applied at a rate of 0.5, and gradient clipping is enforced with a maximum norm of 5. Initially, the model is trained using Cross-Entropy Loss to maximize likelihood of ground-truth tokens. Subsequently, CIDEr optimization is applied using a reinforcement learning objective to better align model predictions with human consensus. 4.6. Training with Cross-Entropy LossIn the initial training phase, the model is optimized using a cross-entropy loss function, which encourages the decoder to generate word sequences that closely match the ground truth captions. The loss is formally defined as follows:ùêøùê∂ùê∏=‚àí‚àëùëáùë°=1ùëôùëúùëîùëù(ùë¶ùë°|ùë¶1:ùë°‚àí1,ùëé)LCE=‚àí‚àët=1Tlogpyt|y1:t‚àí1,a(12)whereùë¶ùë°ytis the ground truth word at time stepùë°t, andùëù(ùë¶ùë°|ùë¶1:ùë°‚àí1,ùëé)pyt|y1:t‚àí1,ais the predicted probability conditioned on the previous words and attention-based visual contextùëéa. 4.7. Optimization with CIDEr Score (Reinforcement Learning)To further refine the caption generation toward human-like quality, the model is fine-tuned using reinforcement learning with the CIDEr metric as a reward signal. The reinforcement learning loss is defined as follows:ùêøùëÖùêø=‚àí‚àëùëáùë°=1(ùê∂ùêºùê∑ùê∏ùëü(ùë¶,ùë¶ÃÇ)‚àíùëè)ùëôùëúùëîùëù(ùë¶ùë°|ùë¶1:ùë°‚àí1,ùëé)LRL=‚àí‚àët=1TCIDEry,y^‚àíblogpyt|y1:t‚àí1,a(13)whereùë¶ÃÇy^is the generated caption,ùë¶yis the reference caption, andùëèbrepresents the baseline reward to reduce variance. This will bring consistency between the training goal and the evaluation measurement adopted in testing, so as to make the captions more relevant and fluent.",
            "4.1. Datasets": "To fully test the proposed approach, three publicly available datasets were employed as FlickrFace 11k [7], COCOFace 15k [26], and FER2013 [18]. These data have been chosen in order to represent the diversity of the object instances, facial emotions, and human-oriented scenes in reality. 4.1.1. FlickrFace 11kFlickrFace 11k means a narrow selection of the Flickr30k picture captioning corpus, editorialized to prefer pictures of human faces and expressions. It includes 11,000 high-resolution visuals, all of which are assigned five captions written by a human being that relate to various human acts and expressions. The dataset is split accordingly with 9000 training images, 1000 validation ones, and the same of the testing ones in accordance with the Karpathy split protocol [7]. The screening has been made with regard to diversity in terms of demography and emotions.Figure 5a,b show sample images capturing expressions such as joy, curiosity, and contemplation. Its emphasis on facial visibility makes it well-suited for training emotion-aware captioning models.Figure 5.(a) A group of people smiling and posing together at what appears to be a social gathering or party. (b) A musical performance on stage with multiple performers singing and playing instruments under concert lighting. 4.1.2. COCOFace 15kThe COCOFace 15k dataset is a filtered subset of the COCO 2017 dataset [26], consisting of 15,000 images with visible human faces. It is split into 13,000 training and 2000 validation/test samples. Each image includes multiple captions and bounding boxes, supporting joint learning of object semantics and visual localization. The dataset offers wide variability in facial orientation, lighting, and occlusion, making it ideal for training facial expression models and evaluating captioning systems in real-world scenarios.Figure 6a,b showcases its contextual and emotional diversity, positioning COCOFace 15k as a strong benchmark for multimodal captioning research.Figure 6.(a) A young child sitting on a couch and holding a colorful umbrella, smiling toward the camera. (b) Two adults dressed formally, engaging in a handshake or exchange during what appears to be an outdoor event. 4.1.3. FER2013The FER2013 dataset is a well-established benchmark in facial expression recognition research and is extensively used in affective computing [18]. It consists of 35,887 grayscale images with a resolution of 48 √ó 48 pixels. Each image represents a cropped frontal face, labeled with one of seven emotion classes: anger, disgust, fear, happiness, sadness, surprise, or neutral. The dataset is organized into 28,709 training images, 3589 for validation, and 3589 for testing. Due to its wide usage and consistent labeling, FER2013 serves as a reliable source for pretraining the facial expression recognition component of the proposed model.Figure 7a‚Äìc depict the emotional variety in FER2013, underscoring its value for facial expression recognition in vision-language captioning.Table 2presents a short statistics of the datasets used.Figure 7.Sample images from the FER2013 dataset displaying distinct emotional expressions: (a) surprise, (b) fear, and (c) happiness [18].Table 2.Summary of datasets used in experiments.",
            "4.1.1. FlickrFace 11k": "FlickrFace 11k means a narrow selection of the Flickr30k picture captioning corpus, editorialized to prefer pictures of human faces and expressions. It includes 11,000 high-resolution visuals, all of which are assigned five captions written by a human being that relate to various human acts and expressions. The dataset is split accordingly with 9000 training images, 1000 validation ones, and the same of the testing ones in accordance with the Karpathy split protocol [7]. The screening has been made with regard to diversity in terms of demography and emotions.Figure 5a,b show sample images capturing expressions such as joy, curiosity, and contemplation. Its emphasis on facial visibility makes it well-suited for training emotion-aware captioning models. Figure 5.(a) A group of people smiling and posing together at what appears to be a social gathering or party. (b) A musical performance on stage with multiple performers singing and playing instruments under concert lighting.",
            "4.1.2. COCOFace 15k": "The COCOFace 15k dataset is a filtered subset of the COCO 2017 dataset [26], consisting of 15,000 images with visible human faces. It is split into 13,000 training and 2000 validation/test samples. Each image includes multiple captions and bounding boxes, supporting joint learning of object semantics and visual localization. The dataset offers wide variability in facial orientation, lighting, and occlusion, making it ideal for training facial expression models and evaluating captioning systems in real-world scenarios.Figure 6a,b showcases its contextual and emotional diversity, positioning COCOFace 15k as a strong benchmark for multimodal captioning research. Figure 6.(a) A young child sitting on a couch and holding a colorful umbrella, smiling toward the camera. (b) Two adults dressed formally, engaging in a handshake or exchange during what appears to be an outdoor event.",
            "4.1.3. FER2013": "The FER2013 dataset is a well-established benchmark in facial expression recognition research and is extensively used in affective computing [18]. It consists of 35,887 grayscale images with a resolution of 48 √ó 48 pixels. Each image represents a cropped frontal face, labeled with one of seven emotion classes: anger, disgust, fear, happiness, sadness, surprise, or neutral. The dataset is organized into 28,709 training images, 3589 for validation, and 3589 for testing. Due to its wide usage and consistent labeling, FER2013 serves as a reliable source for pretraining the facial expression recognition component of the proposed model.Figure 7a‚Äìc depict the emotional variety in FER2013, underscoring its value for facial expression recognition in vision-language captioning.Table 2presents a short statistics of the datasets used. Figure 7.Sample images from the FER2013 dataset displaying distinct emotional expressions: (a) surprise, (b) fear, and (c) happiness [18]. Table 2.Summary of datasets used in experiments.",
            "4.2. Baseline Models": "To benchmark the proposed model, we compare it against two key attention-based image captioning frameworks: Show, Attend and Tell by Xu et al. [12] and the Bottom-Up and Top-Down Attention model by Anderson et al. [3]. These baselines serve as strong references to assess the impact of incorporating facial expression features and real-time object detection into caption generation. The Show, Attend and Tell model is especially relevant to our approach, as it pioneered the integration of soft attention mechanisms within a CNN-LSTM architecture. By allowing the decoder to focus dynamically on different spatial regions of an image, the model improved contextual alignment between visual content and generated words. We adopt this structure as a foundational benchmark, as our model extends its architecture by introducing emotion-based initialization through facial expression features and enhancing object localization using YOLOv5. Comparison with this model helps quantify the direct contribution of emotional cues to caption fluency and relevance. The Bottom-Up and Top-Down Attention model, while more complex, serves as a comparison for evaluating object-level attention. It uses Faster R-CNN to extract detailed region proposals and applies top-down attention to guide captioning. Though accurate, it is computationally slower than our YOLOv5-based approach. This comparison highlights how our method maintains semantic richness while improving inference efficiency. Both baselines were retrained on FlickrFace 11k and COCOFace 15k using identical splits and preprocessing to ensure objective performance evaluation.",
            "4.3. Implementation Details": "The proposed model was implemented using Python 3.8 and the PyTorch 1.8.1 deep learning framework (version 1.13). The experiments were conducted on a system equipped with the hardware and software specifications detailed inTable 3. Table 3.System specifications used for training and evaluating the proposed model. For image preprocessing, the OpenCV and Albumentations libraries were employed. Dataset management and augmentation were handled via custom PyTorch dataset and DataLoader classes. The implementation supports modular training, allowing for separate pretraining of object detection and emotion recognition components before joint optimization. All models were trained with reproducible random seeds, and results were averaged over three runs to reduce variance.",
            "4.4. Evaluation Metrics": "We evaluated caption quality using five standard metrics: BLEU, METEOR, ROUGE-L, CIDEr, and SPICE. BLEU@N measures n-gram precision [21], while METEOR captures word alignment, synonyms, and ordering [22]. ROUGE-L assesses fluency through longest common subsequence matching [23]. CIDEr evaluates consensus with multiple references [24], and SPICE focuses on semantic content using scene graph comparisons [25]. These metrics collectively assess both linguistic accuracy and semantic relevance.",
            "4.5. Training Procedure": "The training strategy is divided into two stages to leverage pretrained knowledge while allowing end-to-end optimization. 4.5.1. Stage 1: Pretraining of Feature ExtractorsIn the first stage, pretrained models are used for individual components:YOLOv5 is initialized with weights trained on the COCO dataset.ResNet50 and VGG-Face are used for extracting grid and facial features, respectively.FER-CNN is pretrained on FER2013 to classify facial expressions.These pretrained models are frozen during initial training to prevent overfitting and to ensure stability in feature representations. 4.5.2. Stage 2: End-to-End Captioning Model TrainingIn the second stage, the complete architecture, including the LSTM decoder and attention mechanism, is fine-tuned in an end-to-end manner. The word embedding layer is uniformly initialized with a dimensionality of 300, while the LSTM‚Äôs hidden and cell states are configured with 512 units each. Optimization is carried out using the Adam optimizer, beginning with an initial learning rate of 0.001. To encourage convergence, the learning rate is reduced to 0.0001 if the METEOR score does not improve over two consecutive validation epochs. The model is trained for a maximum of 30 epochs using a mini-batch size of 64. To mitigate overfitting and stabilize training, dropout is applied at a rate of 0.5, and gradient clipping is enforced with a maximum norm of 5. Initially, the model is trained using Cross-Entropy Loss to maximize likelihood of ground-truth tokens. Subsequently, CIDEr optimization is applied using a reinforcement learning objective to better align model predictions with human consensus.",
            "4.5.1. Stage 1: Pretraining of Feature Extractors": "In the first stage, pretrained models are used for individual components: YOLOv5 is initialized with weights trained on the COCO dataset.ResNet50 and VGG-Face are used for extracting grid and facial features, respectively.FER-CNN is pretrained on FER2013 to classify facial expressions. These pretrained models are frozen during initial training to prevent overfitting and to ensure stability in feature representations.",
            "4.5.2. Stage 2: End-to-End Captioning Model Training": "In the second stage, the complete architecture, including the LSTM decoder and attention mechanism, is fine-tuned in an end-to-end manner. The word embedding layer is uniformly initialized with a dimensionality of 300, while the LSTM‚Äôs hidden and cell states are configured with 512 units each. Optimization is carried out using the Adam optimizer, beginning with an initial learning rate of 0.001. To encourage convergence, the learning rate is reduced to 0.0001 if the METEOR score does not improve over two consecutive validation epochs. The model is trained for a maximum of 30 epochs using a mini-batch size of 64. To mitigate overfitting and stabilize training, dropout is applied at a rate of 0.5, and gradient clipping is enforced with a maximum norm of 5. Initially, the model is trained using Cross-Entropy Loss to maximize likelihood of ground-truth tokens. Subsequently, CIDEr optimization is applied using a reinforcement learning objective to better align model predictions with human consensus.",
            "4.6. Training with Cross-Entropy Loss": "In the initial training phase, the model is optimized using a cross-entropy loss function, which encourages the decoder to generate word sequences that closely match the ground truth captions. The loss is formally defined as follows:ùêøùê∂ùê∏=‚àí‚àëùëáùë°=1ùëôùëúùëîùëù(ùë¶ùë°|ùë¶1:ùë°‚àí1,ùëé)LCE=‚àí‚àët=1Tlogpyt|y1:t‚àí1,a(12)whereùë¶ùë°ytis the ground truth word at time stepùë°t, andùëù(ùë¶ùë°|ùë¶1:ùë°‚àí1,ùëé)pyt|y1:t‚àí1,ais the predicted probability conditioned on the previous words and attention-based visual contextùëéa.",
            "4.7. Optimization with CIDEr Score (Reinforcement Learning)": "To further refine the caption generation toward human-like quality, the model is fine-tuned using reinforcement learning with the CIDEr metric as a reward signal. The reinforcement learning loss is defined as follows:ùêøùëÖùêø=‚àí‚àëùëáùë°=1(ùê∂ùêºùê∑ùê∏ùëü(ùë¶,ùë¶ÃÇ)‚àíùëè)ùëôùëúùëîùëù(ùë¶ùë°|ùë¶1:ùë°‚àí1,ùëé)LRL=‚àí‚àët=1TCIDEry,y^‚àíblogpyt|y1:t‚àí1,a(13)whereùë¶ÃÇy^is the generated caption,ùë¶yis the reference caption, andùëèbrepresents the baseline reward to reduce variance. This will bring consistency between the training goal and the evaluation measurement adopted in testing, so as to make the captions more relevant and fluent.",
            "5. Results": "The current section contributes and discusses the results of the suggested image captioning system, examined based on quantitative measurements (metrics), as well as qualitative measurements. The outcomes are contrasted to two baselines Show-Attend-Tell [12] and Bottom-Up and Top-Down (BUTD) Attention [3] over two typical sets of data, FlickrFace11k and COCOFace15k. 5.1. Quantitative ResultsA comparative analysis between three people image captioning models: Show-Attend-Tell model, Up-Down model, and proposed model is given inTable 4on two benchmark datasets, FlickrFace11k and COCOFace15k. Assessment of the models is provided with an extensive range of evaluation measures, such as BLEU@1-4, METEOR, ROUGE-L, CIDEr, and SPICE that encompass the n-gram fidelity and surface measure with the human-written captions, as well as the semantic overlap with human-written captions. The proposed model outperforms any of the baselines in terms of all metrics and datasets, with much higher improvement on CIDEr and SPICE, the two metrics that excellently evaluate the semantic richness and conformity with reference captions of the model. These findings support the effectiveness of the presented method of producing more representatively correct image descriptions as well as that of a more expressive description semantically.Table 4.Performance comparison on FlickrFace11k and COCOFace15k test splits.Based on the table, it is clear that the specified model has an advantage over both baselines, which is especially prominent in CIDEr and SPICE scores, which serve as good predictors of semantic richness and image-caption relevance. The proposed model matches the performance of the FlickrFace11k dataset with CIDEr of 25.4, which is +1.0 higher than BUTD and +3.5 higher than Show-Attend-Tell. SPICE, associated with semantic propositional content of captions, exhibits significant improvements as well, its results demonstrating a better grasp of human expression and interactions. Similarly, the model achieves 30.3 CIDEr and 13.2 SPICE on the COCOFace15k dataset, demonstrating strong generalization to a variety of real-world use-cases. 5.2. K-Fold Cross Validation ResultsA 5-fold cross-validation approach was considered to critically assess the generalizability of the suggested model over the FlickrFace11k and COCOFace15k datasets. Every fold was kept at the 80-10-10 split into training, validation, and testing, respectively. The results indicate a high stability of the model performance with all the standard deviations being very low, having a range of ‚àí0.02 to 0.18 in all the evaluation metrics as reported inTable 5. This low variation indicates the stability and trustworthiness of the suggested methodology, and the model does not overfit and can produce consistent and semantically valuable descriptions on out-of-sample information. Moreover, semantic-rich measures (CIDEr and SPICE) also indicated high consistency in the scores, further proving that the model could capture both contextual as well as emotional fidelity across the variety of data drop partitions.Table 5.Performance of the proposed model across 5-Fold cross-validation on FlickrFace11k and COCOFace15k datasets using standard evaluation metrics. 5.3. Graphical ResultsA graphical representation of the BLEU, METEOR, CIDEr, and SPICE scores between the three models was created to visually complement the results provided in the figures, as illustrated below.As illustrated inFigure 8andFigure 9, the presented model achieves significant improvement over the fixed baselines Show-Attend-Tell and Up-Down in all of the assessment measures in both the FlickrFace11k and COCOFace15k datasets. The gains are especially significant in CIDEr and SPICE, which are considered powerful metrics of semantic accuracy and adherence to the human-powered captions. Such improvements highlight that the model can produce descriptions going beyond lexical correctness to being semantically rich, describing visual images in a more human-like way. Moreover, the results depicted inFigure 10andFigure 11show the strong generalization capability of the model since it has a low variance over five cross-validation folds. This consistency confirms that the model has consistent performance with or without training subset variation, which is an essential requirement during deployment in real-life environments. The statistical reliability of the proposed approach is further confirmed by relatively low standard deviations across the metrics like BLEU, METEOR, and ROUGE-L.Figure 8.Performance metrics on FlickrFace11k dataset for Show-Attend-Tell, Up-Down, and proposed model.Figure 9.Performance metrics on COCOFace15k dataset for Show-Attend-Tell, Up-Down, and proposed model.Figure 10.Average performance metrics across 5 folds‚ÄîFlickrFace11k dataset.Figure 11.Average performance metrics across 5 folds‚ÄîCOCOFace15k dataset. 5.4. Qualitative ResultsQualitative evaluation further substantiates the effectiveness of the proposed model. As depicted inFigure 3, two representative images and their associated captions are provided. For each image, two ground truth captions (GT-1 and GT-2) and outputs from Show-Attend-Tell, BUTD, and the proposed model are compared.The qualitative analysis presented inFigure 12offers deeper insight into the expressive strength of the proposed captioning framework. Unlike baseline models, which often overlook emotional nuance, the proposed model effectively integrates facial expression recognition to capture affective and contextual details within images. For instance, the model‚Äôs ability to describe a child as ‚Äúcrying with tears on his face‚Äù illustrates its capacity to infer and articulate subtle emotional cues, an advancement that is critical for applications in assistive technology, human‚Äìcomputer interaction, and emotionally intelligent systems. In addition to quantitative scoring measures, cross-validation robustness, visual performance, and qualitative quality on all assessment scales, the proposed model shows significant gains against established status quos. The steady improvements in SPICE and CIDEr validate its semantic insights, and qualitative samples validate its contextual sensitivity. These findings confirm the model‚Äôs suitability in solving the problem of human-centered, emotion-sensitive description of images.Figure 12.Qualitative results of our proposed method with emotions as compared to Show, Attend and Tell and BUTD on the FlickrFace 11k (Right) and COCOFace15k (Left) Test Images.Collectively, these results confirm that the suggested model not only enhances quantitative performance across standard benchmarks but also brings qualitative improvements that are more aligned with human imperative capabilities.",
            "5.1. Quantitative Results": "A comparative analysis between three people image captioning models: Show-Attend-Tell model, Up-Down model, and proposed model is given inTable 4on two benchmark datasets, FlickrFace11k and COCOFace15k. Assessment of the models is provided with an extensive range of evaluation measures, such as BLEU@1-4, METEOR, ROUGE-L, CIDEr, and SPICE that encompass the n-gram fidelity and surface measure with the human-written captions, as well as the semantic overlap with human-written captions. The proposed model outperforms any of the baselines in terms of all metrics and datasets, with much higher improvement on CIDEr and SPICE, the two metrics that excellently evaluate the semantic richness and conformity with reference captions of the model. These findings support the effectiveness of the presented method of producing more representatively correct image descriptions as well as that of a more expressive description semantically. Table 4.Performance comparison on FlickrFace11k and COCOFace15k test splits. Based on the table, it is clear that the specified model has an advantage over both baselines, which is especially prominent in CIDEr and SPICE scores, which serve as good predictors of semantic richness and image-caption relevance. The proposed model matches the performance of the FlickrFace11k dataset with CIDEr of 25.4, which is +1.0 higher than BUTD and +3.5 higher than Show-Attend-Tell. SPICE, associated with semantic propositional content of captions, exhibits significant improvements as well, its results demonstrating a better grasp of human expression and interactions. Similarly, the model achieves 30.3 CIDEr and 13.2 SPICE on the COCOFace15k dataset, demonstrating strong generalization to a variety of real-world use-cases.",
            "5.2. K-Fold Cross Validation Results": "A 5-fold cross-validation approach was considered to critically assess the generalizability of the suggested model over the FlickrFace11k and COCOFace15k datasets. Every fold was kept at the 80-10-10 split into training, validation, and testing, respectively. The results indicate a high stability of the model performance with all the standard deviations being very low, having a range of ‚àí0.02 to 0.18 in all the evaluation metrics as reported inTable 5. This low variation indicates the stability and trustworthiness of the suggested methodology, and the model does not overfit and can produce consistent and semantically valuable descriptions on out-of-sample information. Moreover, semantic-rich measures (CIDEr and SPICE) also indicated high consistency in the scores, further proving that the model could capture both contextual as well as emotional fidelity across the variety of data drop partitions. Table 5.Performance of the proposed model across 5-Fold cross-validation on FlickrFace11k and COCOFace15k datasets using standard evaluation metrics.",
            "5.3. Graphical Results": "A graphical representation of the BLEU, METEOR, CIDEr, and SPICE scores between the three models was created to visually complement the results provided in the figures, as illustrated below. As illustrated inFigure 8andFigure 9, the presented model achieves significant improvement over the fixed baselines Show-Attend-Tell and Up-Down in all of the assessment measures in both the FlickrFace11k and COCOFace15k datasets. The gains are especially significant in CIDEr and SPICE, which are considered powerful metrics of semantic accuracy and adherence to the human-powered captions. Such improvements highlight that the model can produce descriptions going beyond lexical correctness to being semantically rich, describing visual images in a more human-like way. Moreover, the results depicted inFigure 10andFigure 11show the strong generalization capability of the model since it has a low variance over five cross-validation folds. This consistency confirms that the model has consistent performance with or without training subset variation, which is an essential requirement during deployment in real-life environments. The statistical reliability of the proposed approach is further confirmed by relatively low standard deviations across the metrics like BLEU, METEOR, and ROUGE-L. Figure 8.Performance metrics on FlickrFace11k dataset for Show-Attend-Tell, Up-Down, and proposed model. Figure 9.Performance metrics on COCOFace15k dataset for Show-Attend-Tell, Up-Down, and proposed model. Figure 10.Average performance metrics across 5 folds‚ÄîFlickrFace11k dataset. Figure 11.Average performance metrics across 5 folds‚ÄîCOCOFace15k dataset.",
            "5.4. Qualitative Results": "Qualitative evaluation further substantiates the effectiveness of the proposed model. As depicted inFigure 3, two representative images and their associated captions are provided. For each image, two ground truth captions (GT-1 and GT-2) and outputs from Show-Attend-Tell, BUTD, and the proposed model are compared. The qualitative analysis presented inFigure 12offers deeper insight into the expressive strength of the proposed captioning framework. Unlike baseline models, which often overlook emotional nuance, the proposed model effectively integrates facial expression recognition to capture affective and contextual details within images. For instance, the model‚Äôs ability to describe a child as ‚Äúcrying with tears on his face‚Äù illustrates its capacity to infer and articulate subtle emotional cues, an advancement that is critical for applications in assistive technology, human‚Äìcomputer interaction, and emotionally intelligent systems. In addition to quantitative scoring measures, cross-validation robustness, visual performance, and qualitative quality on all assessment scales, the proposed model shows significant gains against established status quos. The steady improvements in SPICE and CIDEr validate its semantic insights, and qualitative samples validate its contextual sensitivity. These findings confirm the model‚Äôs suitability in solving the problem of human-centered, emotion-sensitive description of images. Figure 12.Qualitative results of our proposed method with emotions as compared to Show, Attend and Tell and BUTD on the FlickrFace 11k (Right) and COCOFace15k (Left) Test Images. Collectively, these results confirm that the suggested model not only enhances quantitative performance across standard benchmarks but also brings qualitative improvements that are more aligned with human imperative capabilities.",
            "6. Discussion": "The findings of this study provide strong support for the hypothesis that incorporating facial expression recognition (FER) into image captioning systems significantly enhances both the semantic and emotional fidelity of generated captions. Unlike traditional frameworks that rely primarily on object detection and global visual features, the proposed model succeeds in bridging the semantic gap between mere object presence and affective context by embedding emotion vectors derived from facial cues. This integration enables the generation of captions that are not only contextually accurate but also emotionally resonant, advancing toward a more human-centric narrative generation framework. Performance gains observed in CIDEr and SPICE scores validate this enhancement. These metrics are specifically designed to capture the richness and propositional quality of captions, and the achieved improvements (+2.5 CIDEr and +1.0 SPICE) demonstrate that captions generated by the model align more closely with human expectations for interpretative depth and sentiment [18,19]. These results align with previous research such as Face-Cap [17], which demonstrated that the use of emotional cues led to captions perceived as more relatable. However, the current model extends this by combining emotional vectors with efficient object detection and global context modeling, thereby addressing a key limitation in earlier work, which often lacked detailed spatial grounding [13,23]. The implementation of YOLOv5 as the backbone for object detection contributed to real-time inference capability without compromising detection precision [27]. This is a notable improvement over systems like Faster R-CNN [14], which, although accurate, are computationally expensive and less suited for applications requiring rapid response, such as assistive tools or autonomous systems. Meanwhile, facial expression features extracted via VGG-Face and ResNet50 [16,17] successfully supported the hypothesis that multimodal fusion enhances caption expressiveness. The model‚Äôs ability to encode emotional context during decoding via initialization of LSTM states proved essential in producing emotionally aware language sequences. These findings echo the goals of affective computing, which advocates for machines capable of understanding and responding to human emotions in nuanced ways. Moreover, qualitative results underscore the system‚Äôs ability to interpret emotional subtleties often overlooked in conventional captioning. In comparison to baseline models, the proposed method more accurately articulated expressions such as sadness, surprise, or joy. For instance, captions describing a ‚Äúchild crying with tears on his face‚Äù illustrate how the system captures visual-emotional cues and translates them into meaningful textual outputs. This capability is particularly valuable in fields like human‚Äìcomputer interaction and visual accessibility, where emotionally enriched descriptions can enhance user comprehension and engagement. The stability of the model, as reflected in 5-fold cross-validation experiments with standard deviations consistently below ¬±0.2, also demonstrates robust generalization. The consistent performance across two diverse datasets, FlickrFace11k and COCOFace15k, suggests that the model maintains effectiveness across different visual domains, facial orientations, and illumination conditions. This indicates its potential for deployment in various real-world contexts, where diverse and noisy data are the norm. From a broader perspective, these results support the growing consensus in the field that affective signals are indispensable for generating naturalistic, human-like descriptions. As image captioning systems evolve beyond object enumeration toward interpretive and empathetic storytelling, models capable of incorporating emotion, context, and relational reasoning will play a central role. Future research could explore more fine-grained affective states, cultural variance in emotional expression, or even real-time multilingual caption generation using this architecture as a foundation. Recent domain-specific vision-language models have further validated the industrial relevance of multimodal approaches: MaViLa [28] demonstrates strong gains in manufacturing scene understanding, quality control, and human‚Äìmachine interaction [29], while specialized surveys highlight the transformative role of VLMs in human‚Äìrobot collaboration and smart-factory deployment [30].",
            "7. Conclusions": "This study proposed an emotionally enhanced image captioning model which used the recognition of facial expression to improve the vision generation pipeline. With the traditional attention mechanism integrated along with the affective cues, the model generates captions that are not only descriptively but also emotionally sophisticated. Significant experiments conducted on the FlickrFace11k and COCOFace15k datasets in this study showed that the proposed method has a better performance than existing baselines, Show-Attend-Tell and Up-Down, on multiple evaluation metrics, such as BLEU, METEOR, CIDEr, and SPICE. The model also demonstrated high generalization power as it had stable 5-fold cross-validation and minimal variability. Based on the qualitative analysis schemes, the model has also been shown to produce captions that are similar to how a person would interpret. The results of the experiments indicate that the appending of the emotional context is capable of greatly increasing semantic richness and the levels of human-likeness of image captions. Future research should include more varied and comprehensive datasets, multilingual captioning, as well as adaptive emotion modeling that would allow extending the range of applicability of this approach. Future work will target multi-camera synchronization on the shop floor, KPI-driven alerting via plant IoT buses, and validation in real manufacturing cells, furthering the scope on IoT-enabled smart industry. These extensions will accelerate the transition from isolated proof-of-concept demonstrations to resilient, scalable cyber-physical production systems that fully exploit real-time IoT data streams for autonomous and adaptive manufacturing."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2306-5354/12/12/1325",
        "scraped_at": "2025-12-05 23:53:04"
    },
    {
        "title": "Printed Circuit Board Defect Detection Based on Lightweight Deep Learning Fusion Model",
        "authors": "byYuling Wang,Zhicheng Chen,Jie Wang,Peng Shang,Arcot SowmyaandChangming Sun",
        "journal": "Sensors2025,25(24), 7403; https://doi.org/10.3390/s25247403 (registering¬†DOI) - 5 Dec 2025",
        "abstract": "Printed circuit boards (PCBs) are ubiquitous and essential electronic components. Tiny targets and high precision are the focus of PCB defect detection. This paper proposes an improved model focusing on tiny defect detection and model compression to achieve better performance in PCB defect detection. The improved model has a compact structure based on MobileNet v3 Small-CA and an image-cutting layer. Moreover, it applies an improved multi-scale fusion step with a location weighted mechanism to enhance representation performance. The proposed model outperforms state-of-the-art detection algorithms such as Faster R-CNN, EfficientDet, SSD, and YOLO v7, based on experimental results on a public synthetic PCB dataset. The proposed tiny object detection model has better performances on speed and detection accuracy, thereby benefitting the manufacturing industries associated with PCBs.Keywords:defect detection;PCB;lightweight deep learning model;feature fusion",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Printed circuit boards (PCB) are the core element of electronic components, and during production they are vulnerable to defects such as missing holes, open circuits, spurs, mouse bites, short circuits, and spurious copper, presenting a variety of defect types, variable defect sizes and shapes, small defect areas, and defects that are similar to the background. Therefore, PCB defect detection is an important problem to address in the field. Two popular PCB defect detection techniques used currently include radiographic test techniques and conventional image processing-based techniques. Radiographic tests (RT) use x-rays or Œ≥-rays that can penetrate an object to detect defects in the internal structure. The process can reveal the nature, size and location of the defects, among other information. For real-time radiographic inspection of weld defects in spiral tubes, Zou et al. [1] suggested a Kalman filter-based automatic detection approach that offers some robustness in the case of unsteady detection speed. The current challenges that need to be addressed are, however, increasing the detection accuracy and efficiency, and decreasing the detection cost. Certain detection approaches based on conventional image processing to solve the aforementioned issues have also been developed. They primarily classify images using histograms of oriented gradients (HOG) [2] and scale invariant feature transform (SIFT) [3]. Although the defect detection method using HOG features has high accuracy and speed, HOG features are not scale invariant, and the detection framework is essentially a global rigid template model, which requires global matching of the entire object and has a high computational cost. Khan et al. [4] summarised the performance of SIFT on several classification datasets and proposed three shorter SIFT descriptors. The computational cost can be successfully reduced using the suggested methods. Ong et al. [5] proposed the SEConv-YOLO model, which incorporates a lightweight SEConv feature extraction module, a WRSPP feature fusion neck, and an N-CIoU loss function to significantly improve the accuracy and efficiency of defect detection in PCBs and PCB wiring. Wang et al. [6] proposed the HSA-RTDETR method for PCB defect detection, incorporating a new R18-Faster-EMA backbone, an improved AIFI module, a HS-PAN, and an MPDIoU + NWD loss function. This approach significantly improves detection accuracy for small defects and speeds up model convergence. Despite this, there is still much space for improvement in detection efficiency. Especially for complex and diverse PCB defects, the detection accuracy needs to be much improved. In recent years, with the advancement of deep learning, defect detection based on deep learning technology is one of the current research foci [7,8,9]. Faster R-CNN [10,11] for two-stage defect detection and YOLO [10] for one-stage defect detection are the two primary algorithms used by current deep learning-based PCB defect detection approaches. Among them, Faster R-CNN has a high detection accuracy and a fast detection speed, because it uses anchor frames corresponding to the original image for obtaining the feature map [12]. Nevertheless, the anchor frames are subjected to several downsampling procedures and cover a larger portion of the original image, which makes it difficult for Faster R-CNN to effectively detect small PCB defects. The single-stage detection algorithm YOLO v7 can detect defects at different scales [13] and has good performance in detecting tiny defects, which helps overcome the issue of misdetection and missed detection on tiny defects. The YOLO v7 network involves a large number of parameters and calculations, making it challenging to deploy the network on lightweight devices. In order to further decrease the number of parameters and computational cost of the network to facilitate deployment on lightweight devices [14], improve detection accuracy, and avoid misdetection of minor defects, this paper proposes a PCB defect detection strategy based on a lightweight deep learning fusion model (LDLFModel). The model uses the YOLO v7 architecture as a reference and a lightweight network with the location attention mechanism called MobileNet v3 Small-CA as the backbone network, which has fewer parameters and lower computational cost. The location attention mechanism is applied in the multi-scale feature fusion module, and weighted feature fusion is performed on the feature map to improve the defect detection performance. Meanwhile, EIoU-loss is employed as the target frame regression loss to make the localisation on the prediction box more accurate. Finally, the detection module has been upgraded to include an image-cutting layer and a restoration layer to address the issue of minor defect misdetection [15]. The result of defect detection on PCBs using the LDLFModel is significantly better than using YOLO v7 alone.",
            "2. YOLO v7 Network Model": "YOLO is currently a detection method that has advantages in detection speed, and it has been applied in many areas and can meet real-time requirements. The YOLOs have evolved from the first to the tenth versions, and YOLO v7 is more widely used in practical projects. The network structure of YOLO v7 includes the following components: input, backbone, and head. An input image needs to be preprocessed, and data augmentation is adopted. Four random images are read from the input dataset, parts of the four images are taken for splicing, and the spliced images are used for data augmentation via random scaling, random flipping, random clipping, random perspective projection, and other operations so as to enrich the background of the defects and indirectly improve batch size, which can reduce the challenges of training. Meanwhile, YOLO v7 uses an adaptive anchor box to obtain the best anchor box parameter in the training set during training. Then, the input images are adjusted to an appropriate size according to the input model size, and they are sent to the detection network. In the backbone, CSPDarkNet, which adds the E-ELAN structure and the MP structure, is employed to extract the image features. Among them, the function of the E-ELAN structure is to improve the learning ability of the model, and the role of the MP structure is to improve the feature fusion ability of the model. Based on an input image with 960 √ó 960 pixels to be sent to CSPDarkNet, the three RGB channels undergo a Focus operation to obtain a feature map of size 480 √ó 480 √ó 12. The feature maps are used for a series of convolution operations, and then three effective feature layers of size 120 √ó 120, 60 √ó 60, and 30 √ó 30 are obtained through the E-ELAN network and the MP structure. Subsequently, these three effective feature layers are input into the head part, which utilises a feature pyramid network (FPN) and a path aggregation network (PAN) to fuse features. FPN, with a top-down approach, fuses the features extracted from the backbone network to convey deep semantic information. PAN conveys the location information using a bottom-up approach. In addition, the SPPCSPC structure, the E-ELAN structure, the MP structure, and RepConv are added to the head section. Among them, the role of SPPCSPC is to further expand the receptive field to improve the detection accuracy of the model. The role of RepConv is to improve the inference speed of the model. Then, three enhanced feature layers are obtained. Finally, the prediction component, namely the YOLO head, adjusts the channels of these enhanced feature layers to obtain three feature-cubes of sizes 120 √ó 120 √ó 33, 60 √ó 60 √ó 33, and 30 √ó 30 √ó 33. Then, the position of the centre point, the width and height of the target box, the confidence values, and the target categories of the feature layers are predicted.",
            "3. Proposed LDLFModel": "In order to better meet the deployment requirement on lightweight devices and improve detection performance in PCB defect detection, this paper proposes a lightweight deep learning fusion model (LDLFModel). While our approach uses the YOLO v7 framework as a reference, our primary contribution lies in the novel integration and customized optimization of its components to address the specific challenges of PCB defect detection, namely model lightness, tiny defect misdetection, and feature fusion efficiency. Firstly, beyond merely adopting a lightweight backbone, our technical contribution involves the design of a specialized backbone network, MobileNet v3 Small-CA. This is achieved by strategically embedding the coordinate attention (CA) mechanism into the MobileNet v3 Small architecture. We conducted extensive ablation studies to determine the optimal number and placement of CA modules, creating a backbone that is not only lightweight but also possesses enhanced location-aware capabilities crucial for pinpointing small defects. Secondly, we introduce a customized multi-scale feature fusion scheme. Instead of a simple feature addition in the head component, we implement a learnable weighted fusion mechanism and strategically insert CA modules into the fusion paths. This design, which assigns adaptive weights to features of different resolutions and reinforces spatial information during fusion, represents a key enhancement aimed at significantly boosting feature representation for multi-scale defects. Thirdly, to tackle the persistent problem of tiny defect misdetection, we propose a dedicated image-cutting layer. This module systematically divides high-resolution input images into overlapping patches, processes them independently, and then seamlessly reassembles the results. This pre-detection strategy serves as our focused solution to prevent the loss of fine-grained details, ensuring that even the smallest defects are captured effectively. Finally, the target box regression loss is replaced with EIoU-loss to improve localization accuracy. Together, these key innovations‚Äîthe optimized MobileNet v3 Small-CA backbone, the weighted and attention-enhanced feature fusion, and the innovative image-cutting layer‚Äîconstitute the core of the proposed LDLFModel, enabling high precision in detecting PCB defects of all sizes while maintaining a compact and efficient model suitable for lightweight devices. The overall LDLFModel network architecture is shown inFigure 1. Figure 1.The architecture of the LDLFModel network. 3.1. Backbone Adopts MobileNet v3 Small-CAThe backbone adopted by YOLO v7 is the Improved CSPDark-Net. Although the backbone network has good performance on feature extraction, there are problems of excessive parameters and computation, which makes it difficult to deploy the overall model on low performance devices. Designing efficient and smaller lightweight models is undoubtedly a great approach. Therefore, the LDLFModel adopts the lightweight network MobileNet v3 Small as the backbone to construct a feature extraction network based on the location attention mechanism.MobileNet v3 Small inherits the deep separable convolutions [16] and the inverted residual structure with linear bottlenecks [17], and adds a network search structure, a squeeze and excitation (SE) module [18], and activation functions Hard-sigmoid and Hard-swish which are composed of Conv2D, Inverted-bottleneck, Inverted-bottleneck-shortcut, and pooling. The Inverted-bottleneck consists of two 1 √ó 1 convolutions and a 3 √ó 3 deep separable convolution. In the latter, the feature map of each layer is convolved by using the appropriate convolution kernel separately, considerably reducing the number of parameters and computational complexity, while also enhancing the convolution speed. The Inverted-bottleneck-shortcut contains a shortcut in addition to Inverted-bottleneck. The shortcut can fuse the output feature map of the convolution layer and the input information so as to prevent gradient explosion.MobileNet v3 Small also contains SE attention modules that include two operations: ‚ÄòSqueeze‚Äô and ‚ÄòExcitation‚Äô (SE), which are used for embedding global information and weighting relationships of adaptive channels, respectively. By means of global average pooling, ‚ÄòSqueeze‚Äô compresses W √ó H features of the feature map in each channel into a group of real numbers for obtaining global information. ‚ÄòExcitation‚Äô performs dimension reduction, dimension addition, and activation processing on the group of real numbers. As a result, a group of new real numbers between 0 and 1 is obtained. Finally, these numbers are multiplied by the value of each pixel of the feature map in the corresponding channel to realise the attention weighting of the feature map and complete the capture of the weight of each channel. MobileNet v3-SE is shown inFigure 2. A CA attention module is additionally integrated into the last layer of the LDLFModel backbone network to improve the capability of the backbone network feature description, which is sensitive to location and orientation, and to make up for the SE module‚Äôs lack of location information. The CA attention module is displayed inFigure 3.Figure 2.MobileNet v3-SE.Figure 3.The CA attention mechanism in the network structure. 3.2. Changing the Target Box Regression Loss Function to EIoU-LossThe YOLO v7 network employs theCIoU-losstarget box loss function, and the correlation between the prediction box and the real box is shown inFigure 4. The aspect ratio, overlapped area, and centre point distance of the boundary box regression are taken into account by the target box loss function. Nevertheless, the formula only reflects the difference in aspect ratio, and does not reveal the real differences between widths, heights, and their confidences. Therefore, it hinders the effective optimisation of model similarity. For this problem, LDLFModel uses a better target box loss function, EIoU-loss, on the basis ofCIoU-loss, which utilises the real differences in height and width instead of aspect ratio, and introduces focal loss to optimise the sample out-of-balance issue in the bounding box regression tasks to obtain better anchor frames, thereby improving the accuracy of defect detection.Figure 4.Relationship between the prediction box and the real box.According toCIoU-loss, the centre point distance of the prediction box and the real box is denoted asœÅ(b,bgt). The difference of their widths is denoted asœÅ(w,wgt). The difference of their heights is denoted asœÅ(h,hgt). The diagonal length of the smallest bounding rectangle, including the prediction box and the real box, is denoted asc. The width and height of the smallest bounding rectangle are denoted asCwandChrespectively. The intersection ratio of prediction box A and real box B is:ùêºùëúùëà=|ùê¥‚à©ùêµ||ùê¥‚à™ùêµ|IoU=A‚à©BA‚à™B(1)The final target box loss function is:ùúÅùê∏ùêºùëúùëà=1‚àíùêºùëúùëà+ùúå2(ùëè,ùëèùëîùë°)ùëê2+ùúå2(ùë§,ùë§ùëîùë°)ùê∂2ùë§+ùúå2(‚Ñé,‚Ñéùëîùë°)ùê∂2‚ÑéŒ∂EIoU=1‚àíIoU+œÅ2(b,bgt)c2+œÅ2(w,wgt)Cw2+œÅ2(h,hgt)Ch2(2) 3.3. Weighted Feature Fusion OptimizationThe FPN of the head component of YOLO v7 adopts the top-down approach to fuse with the features extracted from the backbone network to transfer the deep semantic information, while the PAN uses the bottom-up approach to transfer position information to construct the feature fusion network. However, in many models, features from different levels or sources are often fused through simple concatenation. To address this limitation, our LDLFModel introduces a custom fusion module named BiconCat (as shown inFigure 1). Inspired by BiFPN, BiconCat moves beyond simple concatenation by integrating a fast normalized fusion mechanism. This mechanism adaptively weights multi-scale features from bidirectional paths, forming a crucial part of our enhanced feature fusion strategy. Although different input features are fused, they are just simply added. However, due to the different resolutions of these various input features, their contributions to the fused output features are also different. In order to further enhance the effect of feature fusion via the Head of YOLO v7, the LDLFModel adds weights to each feature fusion layer and quickly adopts a normalised fusion [19] to realise the weighted fusion of features. Considering that each 1 √ó 1 convolutional layer in the head section can obtain a greater degree of global information, the CA attention mechanism is added after the convolution layers in the head component. The results O of the fast normalised fusion process are as follows:ùëÇ=‚àëùëñùë§ùëñùëôùëü+‚àëùëóùë§ùëó‚ãÖùêºùëñ,andùëñ,ùëó‚àà{1,2,‚Ä¶,13}O=‚àëiwilr+‚àëjwj‚ãÖIi,andi,j‚àà1,2,‚Ä¶,13(3)wherewirepresents the learning weight corresponding to thei-th input featureIiandlrequals 0.0001 to ensure that the denominator is not 0. The fusion resultPxoutin thex-level feature layer can be expressed by the following formula:ùëÉùëúùë¢ùë°ùë•=ùê∂ùëúùëõùë£‚éõ‚éù‚éú‚éúùë§1¬∑ùëÉùëñùëõùë•+ùë§2¬∑ùëÖùëíùë†ùëñùëßùëí(ùëÉùëñùëõùë•+1)ùë§1+ùë§2+‚àà‚éû‚é†‚éü‚éüPxout=Convw1¬∑Pxin+w2¬∑Resize(Px+1in)w1+w2+‚àà(4)where ‚ÄòResize‚Äô refers to the up or down sampling according to the size of the feature map, ‚ÄòConv‚Äô refers to the convolution operation on features,w1andw2represent the weights,Pxoutrepresents the output feature after the fusion at thex-level feature layer, andPxinrepresents the input feature at the x-level feature layer. A small constant œµ= 0.0001 is introduced to prevent division by zero in the denominator.By integrating fast normalized feature fusion into the head of the LDLFModel, this approach effectively addresses the challenge of poor fusion among feature layers of different resolutions, enhancing the overall feature fusion performance. Compared with the simple eigenvalue connection, the accuracy of the fast normalised fusion method is improved by 2.4%, the recall rate is improved by 7.3%, and the mAP is improved by 4.3%. The advantages of fast normalised fusion are obvious. 3.4. Adding a Image-Cutting Layer to the Detection ModuleHigh resolution images obtained by modern imaging technologies can ensure that even minor defects in PCB images are also informative enough, which is more conducive to the detection of minor defects. However, if a high resolution image is directly input to the network for detection, the subtle image details may be lost due to the resizing operations, and then the information about the minor defect features will be insufficient, resulting in misdetection of minor defects. The input image size in the LDLFModel without the image-cutting layer is 960 √ó 960 pixels. By down-sampling, the output sizes of feature maps are 120 √ó 120, 60 √ó 60, and 30 √ó 30. The 120 √ó 120 feature map is mainly responsible for detecting tiny defects. The receptive field of each grid is 960/120 = 8. Assuming that the high resolution PCB image has a resolution ofM√óN, then if the width and height of tiny defects in the original image are less than 120 pixels, the tiny defects will become too small and may be missed. Certainly, if the input image resolution is high, a device with a higher computation power is required.In this study, small defects are defined as those with bounding box dimensions between 30 √ó 30 and 60 √ó 60 pixels in the original high-resolution PCB images (2544 √ó 2156 pixels). This criterion is consistent with common definitions of small objects in detection tasks such as traffic monitoring. To solve the issue of missing tiny defects, the LDLFModel adds an image-cutting layer to the detection network. The cutting process is shown inFigure 5. First, a cutting factormulis set, andmulis used to control the number of small images to be obtained. Withmul= 1 the image-cutting layer can be turned off, andmul> 1 indicates that the image-cutting layer is turned on. The relationship between the number of small images obtained andmulis defined as follows:ùëõ=ùë§ùë¢ùëô2n=wul2(5)Figure 5.The cutting process.Next, the widthxsizeand heightysizeof the minimum cutting frame is obtained. Then the widthxsmocand heightysmocof the minimum cutting frame is calculated:ùë•ùë†ùëöùëúùëê=ùë•ùë†ùëñùëßùëíùëöùë¢ùëô√ó1.2xsmoc=xsizemul√ó1.2(6)ùë¶ùë†ùëöùëúùëê=ùë¶ùë†ùëñùëßùëíùëöùë¢ùëô√ó1.2ysmoc=ysizemul√ó1.2(7)To ensure that any PCB tiny defects to be detected are not cut in half, a 20% overlap area between adjacent cut images is set. Then the cutting step is set to 64 pixels. Afterwards, the starting coordinate of each minimum division frame (xstartpoint,ystartpoint) is calculated:ùë•ùë†ùë°ùëéùëüùëùùëúùëñùëõùë°=ùëñ√óùë•ùë†ùëñùëßùëíùëöùë¢ùëô(ùëñùëìùëñ<ùëöùë¢ùëô)xstarpoint=i√óxsizemul(ifi<mul)(8)ùë¶ùë†ùë°ùëéùëüùëùùëúùëñùëõùë°=ùëó√óùë¶ùë†ùëñùëßùëíùëöùë¢ùëô(ùëñùëìùëó<ùëöùë¢ùëô)ystarpoint=j√óysizemul(ifj<mul)(9)whereiandjare natural numbers. Then the relative coordinates of each minimum division frame (xreal,yreal) are calculated:ùë•ùëüùëíùëéùëô=ùëöùëñùëõ(ùë•ùë†ùë°ùëéùëüùëùùëúùëñùëõùë°+ùë•ùë†ùëöùëúùëê,ùë•ùë†ùëñùëßùëí)xreal=min(xstarpoint+xsmoc,xsize)(10)ùë¶ùëüùëíùëéùëô=ùëöùëñùëõ(ùë¶ùë†ùë°ùëéùëüùëùùëúùëñùëõùë°+ùë¶ùë†ùëöùëúùëê,ùë¶ùë†ùëñùëßùëí)yreal=min(ystarpoint+ysmoc,ysize)(11)According to the relationship among the starting coordinates of each minimum division box, each relative coordinate, and the step size, it can be determined whether the image is completely cut at the current moving step size. If it cannot be completed, the previous cutting box is aligned with the right or bottom border of the image. Finally, the obtained small image is sent to the detection module for detection. Visualisation of the detection results can be carried out. After all the small images are obtained, the restoration layer will receive the small images with the results of the detection frames. The small images will be reassembled according to the coordinates of the lower right corner (xsmoc,ysmoc) of each minimum cut frame to restore the original large images. To adapt the input size for detecting small defects, the original high-resolution image (2544 √ó 2156 pixels) is divided into a 3 √ó 3 grid of sub-images. A 20% overlap, relative to the original image‚Äôs width and height, is applied between adjacent sub-images to avoid missing defects at the boundaries. Each resulting sub-image is then center-padded and resized to a fixed size of 960 √ó 960 pixels, ensuring a consistent and distortion-free input for the detection network. Finally, non-maximum suppression is performed, and any redundant frames are removed on the restored PCB image with the detection results.",
            "3.1. Backbone Adopts MobileNet v3 Small-CA": "The backbone adopted by YOLO v7 is the Improved CSPDark-Net. Although the backbone network has good performance on feature extraction, there are problems of excessive parameters and computation, which makes it difficult to deploy the overall model on low performance devices. Designing efficient and smaller lightweight models is undoubtedly a great approach. Therefore, the LDLFModel adopts the lightweight network MobileNet v3 Small as the backbone to construct a feature extraction network based on the location attention mechanism. MobileNet v3 Small inherits the deep separable convolutions [16] and the inverted residual structure with linear bottlenecks [17], and adds a network search structure, a squeeze and excitation (SE) module [18], and activation functions Hard-sigmoid and Hard-swish which are composed of Conv2D, Inverted-bottleneck, Inverted-bottleneck-shortcut, and pooling. The Inverted-bottleneck consists of two 1 √ó 1 convolutions and a 3 √ó 3 deep separable convolution. In the latter, the feature map of each layer is convolved by using the appropriate convolution kernel separately, considerably reducing the number of parameters and computational complexity, while also enhancing the convolution speed. The Inverted-bottleneck-shortcut contains a shortcut in addition to Inverted-bottleneck. The shortcut can fuse the output feature map of the convolution layer and the input information so as to prevent gradient explosion. MobileNet v3 Small also contains SE attention modules that include two operations: ‚ÄòSqueeze‚Äô and ‚ÄòExcitation‚Äô (SE), which are used for embedding global information and weighting relationships of adaptive channels, respectively. By means of global average pooling, ‚ÄòSqueeze‚Äô compresses W √ó H features of the feature map in each channel into a group of real numbers for obtaining global information. ‚ÄòExcitation‚Äô performs dimension reduction, dimension addition, and activation processing on the group of real numbers. As a result, a group of new real numbers between 0 and 1 is obtained. Finally, these numbers are multiplied by the value of each pixel of the feature map in the corresponding channel to realise the attention weighting of the feature map and complete the capture of the weight of each channel. MobileNet v3-SE is shown inFigure 2. A CA attention module is additionally integrated into the last layer of the LDLFModel backbone network to improve the capability of the backbone network feature description, which is sensitive to location and orientation, and to make up for the SE module‚Äôs lack of location information. The CA attention module is displayed inFigure 3. Figure 2.MobileNet v3-SE. Figure 3.The CA attention mechanism in the network structure.",
            "3.2. Changing the Target Box Regression Loss Function to EIoU-Loss": "The YOLO v7 network employs theCIoU-losstarget box loss function, and the correlation between the prediction box and the real box is shown inFigure 4. The aspect ratio, overlapped area, and centre point distance of the boundary box regression are taken into account by the target box loss function. Nevertheless, the formula only reflects the difference in aspect ratio, and does not reveal the real differences between widths, heights, and their confidences. Therefore, it hinders the effective optimisation of model similarity. For this problem, LDLFModel uses a better target box loss function, EIoU-loss, on the basis ofCIoU-loss, which utilises the real differences in height and width instead of aspect ratio, and introduces focal loss to optimise the sample out-of-balance issue in the bounding box regression tasks to obtain better anchor frames, thereby improving the accuracy of defect detection. Figure 4.Relationship between the prediction box and the real box. According toCIoU-loss, the centre point distance of the prediction box and the real box is denoted asœÅ(b,bgt). The difference of their widths is denoted asœÅ(w,wgt). The difference of their heights is denoted asœÅ(h,hgt). The diagonal length of the smallest bounding rectangle, including the prediction box and the real box, is denoted asc. The width and height of the smallest bounding rectangle are denoted asCwandChrespectively. The intersection ratio of prediction box A and real box B is:ùêºùëúùëà=|ùê¥‚à©ùêµ||ùê¥‚à™ùêµ|IoU=A‚à©BA‚à™B(1) The final target box loss function is:ùúÅùê∏ùêºùëúùëà=1‚àíùêºùëúùëà+ùúå2(ùëè,ùëèùëîùë°)ùëê2+ùúå2(ùë§,ùë§ùëîùë°)ùê∂2ùë§+ùúå2(‚Ñé,‚Ñéùëîùë°)ùê∂2‚ÑéŒ∂EIoU=1‚àíIoU+œÅ2(b,bgt)c2+œÅ2(w,wgt)Cw2+œÅ2(h,hgt)Ch2(2)",
            "3.3. Weighted Feature Fusion Optimization": "The FPN of the head component of YOLO v7 adopts the top-down approach to fuse with the features extracted from the backbone network to transfer the deep semantic information, while the PAN uses the bottom-up approach to transfer position information to construct the feature fusion network. However, in many models, features from different levels or sources are often fused through simple concatenation. To address this limitation, our LDLFModel introduces a custom fusion module named BiconCat (as shown inFigure 1). Inspired by BiFPN, BiconCat moves beyond simple concatenation by integrating a fast normalized fusion mechanism. This mechanism adaptively weights multi-scale features from bidirectional paths, forming a crucial part of our enhanced feature fusion strategy. Although different input features are fused, they are just simply added. However, due to the different resolutions of these various input features, their contributions to the fused output features are also different. In order to further enhance the effect of feature fusion via the Head of YOLO v7, the LDLFModel adds weights to each feature fusion layer and quickly adopts a normalised fusion [19] to realise the weighted fusion of features. Considering that each 1 √ó 1 convolutional layer in the head section can obtain a greater degree of global information, the CA attention mechanism is added after the convolution layers in the head component. The results O of the fast normalised fusion process are as follows:ùëÇ=‚àëùëñùë§ùëñùëôùëü+‚àëùëóùë§ùëó‚ãÖùêºùëñ,andùëñ,ùëó‚àà{1,2,‚Ä¶,13}O=‚àëiwilr+‚àëjwj‚ãÖIi,andi,j‚àà1,2,‚Ä¶,13(3)wherewirepresents the learning weight corresponding to thei-th input featureIiandlrequals 0.0001 to ensure that the denominator is not 0. The fusion resultPxoutin thex-level feature layer can be expressed by the following formula:ùëÉùëúùë¢ùë°ùë•=ùê∂ùëúùëõùë£‚éõ‚éù‚éú‚éúùë§1¬∑ùëÉùëñùëõùë•+ùë§2¬∑ùëÖùëíùë†ùëñùëßùëí(ùëÉùëñùëõùë•+1)ùë§1+ùë§2+‚àà‚éû‚é†‚éü‚éüPxout=Convw1¬∑Pxin+w2¬∑Resize(Px+1in)w1+w2+‚àà(4)where ‚ÄòResize‚Äô refers to the up or down sampling according to the size of the feature map, ‚ÄòConv‚Äô refers to the convolution operation on features,w1andw2represent the weights,Pxoutrepresents the output feature after the fusion at thex-level feature layer, andPxinrepresents the input feature at the x-level feature layer. A small constant œµ= 0.0001 is introduced to prevent division by zero in the denominator. By integrating fast normalized feature fusion into the head of the LDLFModel, this approach effectively addresses the challenge of poor fusion among feature layers of different resolutions, enhancing the overall feature fusion performance. Compared with the simple eigenvalue connection, the accuracy of the fast normalised fusion method is improved by 2.4%, the recall rate is improved by 7.3%, and the mAP is improved by 4.3%. The advantages of fast normalised fusion are obvious.",
            "3.4. Adding a Image-Cutting Layer to the Detection Module": "High resolution images obtained by modern imaging technologies can ensure that even minor defects in PCB images are also informative enough, which is more conducive to the detection of minor defects. However, if a high resolution image is directly input to the network for detection, the subtle image details may be lost due to the resizing operations, and then the information about the minor defect features will be insufficient, resulting in misdetection of minor defects. The input image size in the LDLFModel without the image-cutting layer is 960 √ó 960 pixels. By down-sampling, the output sizes of feature maps are 120 √ó 120, 60 √ó 60, and 30 √ó 30. The 120 √ó 120 feature map is mainly responsible for detecting tiny defects. The receptive field of each grid is 960/120 = 8. Assuming that the high resolution PCB image has a resolution ofM√óN, then if the width and height of tiny defects in the original image are less than 120 pixels, the tiny defects will become too small and may be missed. Certainly, if the input image resolution is high, a device with a higher computation power is required. In this study, small defects are defined as those with bounding box dimensions between 30 √ó 30 and 60 √ó 60 pixels in the original high-resolution PCB images (2544 √ó 2156 pixels). This criterion is consistent with common definitions of small objects in detection tasks such as traffic monitoring. To solve the issue of missing tiny defects, the LDLFModel adds an image-cutting layer to the detection network. The cutting process is shown inFigure 5. First, a cutting factormulis set, andmulis used to control the number of small images to be obtained. Withmul= 1 the image-cutting layer can be turned off, andmul> 1 indicates that the image-cutting layer is turned on. The relationship between the number of small images obtained andmulis defined as follows:ùëõ=ùë§ùë¢ùëô2n=wul2(5) Figure 5.The cutting process. Next, the widthxsizeand heightysizeof the minimum cutting frame is obtained. Then the widthxsmocand heightysmocof the minimum cutting frame is calculated:ùë•ùë†ùëöùëúùëê=ùë•ùë†ùëñùëßùëíùëöùë¢ùëô√ó1.2xsmoc=xsizemul√ó1.2(6)ùë¶ùë†ùëöùëúùëê=ùë¶ùë†ùëñùëßùëíùëöùë¢ùëô√ó1.2ysmoc=ysizemul√ó1.2(7) To ensure that any PCB tiny defects to be detected are not cut in half, a 20% overlap area between adjacent cut images is set. Then the cutting step is set to 64 pixels. Afterwards, the starting coordinate of each minimum division frame (xstartpoint,ystartpoint) is calculated:ùë•ùë†ùë°ùëéùëüùëùùëúùëñùëõùë°=ùëñ√óùë•ùë†ùëñùëßùëíùëöùë¢ùëô(ùëñùëìùëñ<ùëöùë¢ùëô)xstarpoint=i√óxsizemul(ifi<mul)(8)ùë¶ùë†ùë°ùëéùëüùëùùëúùëñùëõùë°=ùëó√óùë¶ùë†ùëñùëßùëíùëöùë¢ùëô(ùëñùëìùëó<ùëöùë¢ùëô)ystarpoint=j√óysizemul(ifj<mul)(9)whereiandjare natural numbers. Then the relative coordinates of each minimum division frame (xreal,yreal) are calculated:ùë•ùëüùëíùëéùëô=ùëöùëñùëõ(ùë•ùë†ùë°ùëéùëüùëùùëúùëñùëõùë°+ùë•ùë†ùëöùëúùëê,ùë•ùë†ùëñùëßùëí)xreal=min(xstarpoint+xsmoc,xsize)(10)ùë¶ùëüùëíùëéùëô=ùëöùëñùëõ(ùë¶ùë†ùë°ùëéùëüùëùùëúùëñùëõùë°+ùë¶ùë†ùëöùëúùëê,ùë¶ùë†ùëñùëßùëí)yreal=min(ystarpoint+ysmoc,ysize)(11) According to the relationship among the starting coordinates of each minimum division box, each relative coordinate, and the step size, it can be determined whether the image is completely cut at the current moving step size. If it cannot be completed, the previous cutting box is aligned with the right or bottom border of the image. Finally, the obtained small image is sent to the detection module for detection. Visualisation of the detection results can be carried out. After all the small images are obtained, the restoration layer will receive the small images with the results of the detection frames. The small images will be reassembled according to the coordinates of the lower right corner (xsmoc,ysmoc) of each minimum cut frame to restore the original large images. To adapt the input size for detecting small defects, the original high-resolution image (2544 √ó 2156 pixels) is divided into a 3 √ó 3 grid of sub-images. A 20% overlap, relative to the original image‚Äôs width and height, is applied between adjacent sub-images to avoid missing defects at the boundaries. Each resulting sub-image is then center-padded and resized to a fixed size of 960 √ó 960 pixels, ensuring a consistent and distortion-free input for the detection network. Finally, non-maximum suppression is performed, and any redundant frames are removed on the restored PCB image with the detection results.",
            "4. Experiments and Analysis": "This work used a Linux system and the PyTorch deep learning framework. The lectotype of GPU was provided by AutoDL (https://www.autodl.com). The configuration of the server processor was as follows: the CPU is 9-core Intel (R) Xeon (R) Silver 4210R CPU at 2.40 GHz and the memory is 256 GB. The model of the graphics processor is NVIDIA GeForce RTX 3080 and the graphics memory is 10 GB. The software environment is PyTorch 1.9.0, Python 3.8, and CUDA 11.1. 4.1. Evaluation MetricsIn defect detection, the two tasks, namely positioning and classification, need to be completed, which are measured by mean average precision (mAP). This is the average value of average precision (AP) on various defects. AP is determined by precision and recall in the classification results. Precision refers to the ratio of real positive samples for prediction. Recall indicates the proportion of positive samples with correct prediction in the actual positive samples.ùëöùê¥ùëÉ=‚àëùëõùëñ=1ùê¥ùëÉ(ùëñ)ùëõmAP=‚àëi=1nAP(i)n(12)ùê¥ùëÉ=‚à´10ùëÉ(ùëÖ)ùëëùëÖ=‚àëùëò=1ùëÅùëÉ(ùëò)ŒîùëÖ(ùëò)AP=‚à´01P(R)dR=‚àëk=1NP(k)ŒîR(k)(13)ùëÉùëüùëíùë†ùëñùëúùëõ=ùëáùëÉùëáùëÉ+ùêπùëÉPresion=TPTP+FP(14)ùëÖùëíùëêùëéùëôùëô=ùëáùëÉùëáùëÉ+ùêπùëÅRecall=TPTP+FN(15)whereTP,FPandFNrepresent true positive (both detection and ground truth are positive), false positive (detection is positive and ground truth is negative) and false negative (detection is negative and ground truth is positive), respectively.nindicates the number of categories of the defects to be detected.Nis defined as the number of all images in the test set. At the same time, to further demonstrate the advantages of the LDLFModel using a lightweight backbone network, the LDLFModel was compared to other models in terms of model size, number of parameters, and computational cost. 4.2. PCB Dataset and Data AugmentationThe PCB dataset used in this paper is derived from the Open Laboratory of Intelligent Robotics at Peking University, China. The dataset contains six types of defects, i.e., missing hole, open circuit, mouse bite, spur, short circuit, and spurious copper. Data augmentation is performed to expand the sample size in this work by random flipping, random brightness change, and random scaling of the PCB images in the original dataset. The augmented dataset contains 10,800 PCB images with defects. The information on the augmented dataset is shown inTable 1. The dataset is randomly divided into training set and test sets according to a ratio of 9:1. Examples of PCB images are depictedFigure 6.Figure 6.Samples of each type in the dataset.Table 1.The augmented dataset.Using data augmentation, the proposed model is enhanced for robustness. When the numbers of parameters are the same, the changes in precision, recall, and mAP are compared through experiments on the datasets before and after data augmentation. We observe a significant (about 5%) increase in recall, and an obvious increase (about 2%) in precision and mAP. Therefore, data augmentation is an important factor in the model‚Äôs performance. 4.3. Lightweight Backbone Network and EpochsIn the case of only changing the backbone network, where the model is trained using the data augmented dataset, the precision, recall, mAP, the number of parameters, computational cost, and the size of the trained model are compared when PPLCNet, ShuffleNet-v2, and Efficient lite are used as the lightweight backbone. The results inTable 2show that MobileNet v3 Small has the best overall performance compared to PP-LCNet, ShuffleNet-v2, and Efficient lite, and is more suitable as the backbone of the LDLFModel.Table 2.Comparison of precision, recall, mAP, computational cost, etc.The number of iterations will greatly affect the recognition results of the model. We conducted comparative experiments on different numbers of iterations on the augmentation dataset. The samples are randomly divided into training set and test set at a ratio of 9:1. The results inTable 3show that precision, recall and mAP have been improved to a certain extent. When the number of iterations is set to 300, the precision, recall, and mAP reach the maximum upper limit, and then the final number of iterations is set to 300 epochs in the following experiments.Table 3.Comparison on different epochs. 4.4. Comparative Experiment and Result AnalysisThe LDLFModel proposed in this paper is compared with various models in terms of the number of parameters, model size, and inference time, as shown inTable 4. The baseline model YOLO v7 has a model size of 74.8 MB, 36.49 million parameters, and an inference time of 18.3 ms. Simply replacing its backbone CSPDarkNet with the lightweight MobileNet v3 Small reduces the model size to 10.4 MB, the number of parameters to 7.2 million, and the inference time to 13.5 ms. Compared with other advanced models, C3F-SimAM-YOLO has a model size of 11.4 MB and 5.5 million parameters; SCF-YOLO has 9.7 MB and 6.1 million parameters; and YOLOv8-PCB has 5.6 MB and 5.9 million parameters. The proposed LDLFModel further reduces the model size to 5.4 MB and has 5.6 million parameters, achieving the best performance with an inference time of 12.7 ms while maintaining competitiveness.Table 4.Comparison of several network models.The comparative experiment used the augmented dataset described inSection 4.1, and the comparison of the model‚Äôs computational cost and detection performance is shown inTable 5. The benchmark model YOLO v7 has a computational cost of 16.4 GFLOPs, with an accuracy, recall, and mAP of 93.6%, 93.2%, and 94.2%, respectively. After replacing its backbone network with MobileNet v3 Small, the computational cost dropped to 6.3 GFLOPs. Compared with other advanced models, C3F-SimAM-YOLO achieves an accuracy of 98.0%, recall of 95.4%, and mAP of 97.5% at a computational cost of 12.6 GFLOPs; SCF-YOLO achieves an accuracy of 98.7%, recall of 96.3%, and mAP of 98.3% at 3.4 GFLOPs; and YOLOv8-PCB achieves an accuracy of 94.7%, recall of 94.0%, and mAP of 96.1% at 7.1 GFLOPs. Experimental results show that the computational cost of the LDLFModel in this study is significantly reduced to 1.6 GFLOPs. Compared with existing state-of-the-art models, it is 11.0G lower than C3F-SimAM-YOLO, 1.8G lower than SCF-YOLO, 5.5G lower than YOLOv8-PCB, 14.8 GFLOPs lower than YOLO v7, and 4.7 GFLOPs lower than MobileNet v3 Small-YOLO v7. In terms of detection performance, the LDLFModel achieves an accuracy, recall, and mAP of 99.4%, 97.6%, and 99.0%, respectively, demonstrating comprehensive advantages. Compared with SCF-YOLO, they increase by 0.7%, 1.3%, and 0.7%, respectively; compared with C3F-SimAM-YOLO, the three metrics increase by 1.4%, 2.2%, and 1.5%, respectively; compared with YOLOv8-PCB, the improvements are more significant, reaching 4.7%, 3.6%, and 2.9%, respectively. Additionally, compared with YOLO v7, the metrics increase by 5.8%, 4.4%, and 4.8%, and compared with MobileNet v3 Small-YOLO v7, they improve by 0.7%, 2.2%, and 1.6%, respectively. Notably, while the LDLFModel significantly reduces computational cost, it outperforms all other comparison models across all performance metrics, excelling not only in computational efficiency but also leading comprehensively in detection accuracy, demonstrating outstanding overall performance and achieving an optimal balance between accuracy and efficiency.Table 5.Comparison of computational cost, precision, recall, and mAP.In addition, in order to further demonstrate that the image-cutting layer of the LDLFModel has a certain effect on the problem of misdetection and missed detection of tiny defects, this paper compares the detection results of YOLO v7 and the LDLFModel with a 3 √ó 3 image-cutting layer. The comparisons are shown inFigure 7. It can be seen that the LDLFModel can detect tiny defects, which can effectively solve the problems of misdetection of small PCB defects. In conclusion, the LDLFModel greatly reduces the number of parameters and computational cost while maintaining high detection accuracy. It also effectively solves the problem of tiny defect misdetection and missed detection, making it more suited for industrial PCB defect detection requirements.Figure 7.Comparison of detection effect between uncut and 3 √ó 3 cut.For verifying the performance of the LDLFModel, the proposed model was compared to several other mainstream algorithms. The classic two-stage algorithms are Faster R-CNN (VGG16) and Faster R-CNN (ResNet101), which use VGG16 and ResNet101 [23] as the backbone networks, respectively. The classic one-stage algorithms are Efficient-Det (D0) [24], single shot multibox detector (SSD) [25], YOLO v7, and LDLFModel. The frames per second (FPS) and mAP of various algorithms and the AP of each type of PCB defect are shown inFigure 8,Figure 9andFigure 10.Figure 8.FPS comparison.Figure 9.mAP comparisons.Figure 10.AP comparison of the detection of six types of defects.As shown inFigure 8, the LDLFModel without the image-cutting layer achieves 78.73 FPS, outperforming Faster R-CNN (VGG16) at 77.68 FPS, Faster R-CNN (ResNet101) at 78.14 FPS, EfficientDet (D0) at 53.11 FPS, and SSD at 25.31 FPS. Its speed is also comparable to that of YOLO v7, indicating the LDLFModel‚Äôs superior detection speed.Figure 9demonstrates that the LDLFModel reaches a maximum mAP of 99.0%, exceeding the performance of Faster R-CNN (VGG16), EfficientDet (D0), and SSD by 27.45%, 12.1%, and 7.72%, respectively. It also outperforms YOLO v7 and Faster R-CNN (ResNet101) with mAP improvements of 4.8% and 3.7%. The results demonstrate that the LDLFModel delivers both high detection speed and strong accuracy, making it a reliable choice for defect detection.For six types of PCB defects, the APs of various algorithms are shown inFigure 10. It can be seen that compared to the Faster R-CNN (VGG16), Faster R-CNN (ResNet101), and EfficientDet (D0) algorithms, the APs of the LDLFModel on various defects are much higher than that of the other algorithms. Compared to the APs from the Faster R-CNN (VGG16) algorithm on the stray and spurious copper defects, the APs from the LDLFModel is 45.47% and 39.05% higher respectively. Compared to the APs on the six types of defects from YOLO v7, the proposed model has improved results. The above results show that the LDLFModel can achieve the detection requirements of multiple types of defects simultaneously, and high accuracy for six types of defects of PCBs. 4.5. Ablation Studies4.5.1. Regression Loss FunctionsBhattacharya and Cloutier [26] compared the performance of CIoU, DIoU, GIoU, and IoU. They found that CIoU outperformed the other loss functions. He et al. [27] presented a generalization Œ±-IoU loss that remarkably surpasses the IoU-based losses, and pointed out that better results will be achieved with Œ± = 3. We conducted experiments to compare the precision, recall, and mAP under CIoU, Œ±-IoU (Œ± = 3), and EIoU loss functions. The results inTable 6show that the EIoU loss has better overall performance than CIoU and Œ±-IoU (Œ± = 3) losses.Table 6.Comparison of precision, recall, and mAP.For balancing resource allocation and training time, the relationship among several losses, various learning rates, and the number of iterations was compared through the experiments. Some results are shown inFigure 11,Figure 12andFigure 13. Classification loss, marked as Cls loss, was used to calculate whether an anchor frame correctly corresponded to the label classification. Position loss, marked as Box loss, was used to calculate the error between the prediction box and calibration box. Confidence loss, marked as Obj_loss, was used to calculate the degree of confidence in a network.Figure 11.The relationship curve between Cls loss and epoch.Figure 12.The relationship curve between Box loss and epoch.Figure 13.The relationship curve between Obj loss and epoch.Learning rate is an important hyper parameter in a deep learning network. The batch size affects the generalisation performance of networks. Generally, when the batch size is increased, the learning rate should also be increased according to the linear scaling rule to ensure that the updated weights are equal when using the same sample. In a certain range, increasing the batch size is conducive to the stability of convergence, and a similar effect of learning rate attenuation can be achieved by increasing batch size [28]. Furthermore, with the same number of epochs, the larger the batch size, the fewer the batches required, and the training time is reduced [29]. Therefore, the learning rate in this experiment was set to 0.02 with a decay rate of 0.3, and the final learning rate was 0.006. According to the change of the losses with the number of iterations, the number of iterations was set to 300. The batch size was set to 33. In order to avoid missing small defects as much as possible, the resolution of the input image was adjusted to 960 √ó 960.4.5.2. Attention MechanismTo assess the effectiveness of applying the CA attention mechanism in the LDLFModel, we conducted experimental comparisons with the SE and CBAM attention mechanisms, applying each to the same position within the model. The results inTable 7indicate that the CA attention mechanism has better comprehensive performance than the SE and CBAM attention mechanisms.Table 7.Comparison of different attention mechanisms.Additionally, CA modules were applied to different positions of the LDLFModel, as shown inFigure 1, to evaluate their impact on precision, recall and mAP. The experimental results inTable 8display that positions 1, 2, 3, 4, and 5 achieved the highest precision, recall, and mAP. These findings demonstrate that incorporating the CA attention mechanism at these positions significantly enhances detection performance in the LDLFModel.Table 8.Comparison of different positions of the CA model.4.5.3. The Image-Cutting LayerTo assess the effect of the image-cutting layer, we compared detection performance before and after its application.Table 9shows that precision remained consistently high, with minimal variation, though certain defect types showed marked improvement. For instance, the ‚Äúmouse bites‚Äù defect saw a 0.7% increase in precision. More notably, recall improved visibly post-cutting, particularly for defects such as missing holes, open circuits, short circuits, and spurious copper, with increases of 2.4%, 2.3%, 0.7%, and 9%, respectively. Additionally, the mAP for spurious copper increased by 2.9%. These results indicate that the image-cutting layer positively impacts performance, particularly in improving recall for specific defect types.Table 9.Comparison of precision, recall, and mAP before and after cutting (%).Figure 14illustrates the comparison of detection performance for six types of defects on the same image, both before and after cutting. The left side of each arrow represents the detection results before cutting, while the right side shows the results after cutting. As can be seen in the figure, the detection accuracy for each defect type improves to some extent following the cutting process. These results confirm that data cutting enhances the detection of high-resolution PCB defects.Figure 14.Comparison of six types of defect detection results before and after cutting.",
            "4.1. Evaluation Metrics": "In defect detection, the two tasks, namely positioning and classification, need to be completed, which are measured by mean average precision (mAP). This is the average value of average precision (AP) on various defects. AP is determined by precision and recall in the classification results. Precision refers to the ratio of real positive samples for prediction. Recall indicates the proportion of positive samples with correct prediction in the actual positive samples.ùëöùê¥ùëÉ=‚àëùëõùëñ=1ùê¥ùëÉ(ùëñ)ùëõmAP=‚àëi=1nAP(i)n(12)ùê¥ùëÉ=‚à´10ùëÉ(ùëÖ)ùëëùëÖ=‚àëùëò=1ùëÅùëÉ(ùëò)ŒîùëÖ(ùëò)AP=‚à´01P(R)dR=‚àëk=1NP(k)ŒîR(k)(13)ùëÉùëüùëíùë†ùëñùëúùëõ=ùëáùëÉùëáùëÉ+ùêπùëÉPresion=TPTP+FP(14)ùëÖùëíùëêùëéùëôùëô=ùëáùëÉùëáùëÉ+ùêπùëÅRecall=TPTP+FN(15)whereTP,FPandFNrepresent true positive (both detection and ground truth are positive), false positive (detection is positive and ground truth is negative) and false negative (detection is negative and ground truth is positive), respectively.nindicates the number of categories of the defects to be detected.Nis defined as the number of all images in the test set. At the same time, to further demonstrate the advantages of the LDLFModel using a lightweight backbone network, the LDLFModel was compared to other models in terms of model size, number of parameters, and computational cost.",
            "4.2. PCB Dataset and Data Augmentation": "The PCB dataset used in this paper is derived from the Open Laboratory of Intelligent Robotics at Peking University, China. The dataset contains six types of defects, i.e., missing hole, open circuit, mouse bite, spur, short circuit, and spurious copper. Data augmentation is performed to expand the sample size in this work by random flipping, random brightness change, and random scaling of the PCB images in the original dataset. The augmented dataset contains 10,800 PCB images with defects. The information on the augmented dataset is shown inTable 1. The dataset is randomly divided into training set and test sets according to a ratio of 9:1. Examples of PCB images are depictedFigure 6. Figure 6.Samples of each type in the dataset. Table 1.The augmented dataset. Using data augmentation, the proposed model is enhanced for robustness. When the numbers of parameters are the same, the changes in precision, recall, and mAP are compared through experiments on the datasets before and after data augmentation. We observe a significant (about 5%) increase in recall, and an obvious increase (about 2%) in precision and mAP. Therefore, data augmentation is an important factor in the model‚Äôs performance.",
            "4.3. Lightweight Backbone Network and Epochs": "In the case of only changing the backbone network, where the model is trained using the data augmented dataset, the precision, recall, mAP, the number of parameters, computational cost, and the size of the trained model are compared when PPLCNet, ShuffleNet-v2, and Efficient lite are used as the lightweight backbone. The results inTable 2show that MobileNet v3 Small has the best overall performance compared to PP-LCNet, ShuffleNet-v2, and Efficient lite, and is more suitable as the backbone of the LDLFModel. Table 2.Comparison of precision, recall, mAP, computational cost, etc. The number of iterations will greatly affect the recognition results of the model. We conducted comparative experiments on different numbers of iterations on the augmentation dataset. The samples are randomly divided into training set and test set at a ratio of 9:1. The results inTable 3show that precision, recall and mAP have been improved to a certain extent. When the number of iterations is set to 300, the precision, recall, and mAP reach the maximum upper limit, and then the final number of iterations is set to 300 epochs in the following experiments. Table 3.Comparison on different epochs.",
            "4.4. Comparative Experiment and Result Analysis": "The LDLFModel proposed in this paper is compared with various models in terms of the number of parameters, model size, and inference time, as shown inTable 4. The baseline model YOLO v7 has a model size of 74.8 MB, 36.49 million parameters, and an inference time of 18.3 ms. Simply replacing its backbone CSPDarkNet with the lightweight MobileNet v3 Small reduces the model size to 10.4 MB, the number of parameters to 7.2 million, and the inference time to 13.5 ms. Compared with other advanced models, C3F-SimAM-YOLO has a model size of 11.4 MB and 5.5 million parameters; SCF-YOLO has 9.7 MB and 6.1 million parameters; and YOLOv8-PCB has 5.6 MB and 5.9 million parameters. The proposed LDLFModel further reduces the model size to 5.4 MB and has 5.6 million parameters, achieving the best performance with an inference time of 12.7 ms while maintaining competitiveness. Table 4.Comparison of several network models. The comparative experiment used the augmented dataset described inSection 4.1, and the comparison of the model‚Äôs computational cost and detection performance is shown inTable 5. The benchmark model YOLO v7 has a computational cost of 16.4 GFLOPs, with an accuracy, recall, and mAP of 93.6%, 93.2%, and 94.2%, respectively. After replacing its backbone network with MobileNet v3 Small, the computational cost dropped to 6.3 GFLOPs. Compared with other advanced models, C3F-SimAM-YOLO achieves an accuracy of 98.0%, recall of 95.4%, and mAP of 97.5% at a computational cost of 12.6 GFLOPs; SCF-YOLO achieves an accuracy of 98.7%, recall of 96.3%, and mAP of 98.3% at 3.4 GFLOPs; and YOLOv8-PCB achieves an accuracy of 94.7%, recall of 94.0%, and mAP of 96.1% at 7.1 GFLOPs. Experimental results show that the computational cost of the LDLFModel in this study is significantly reduced to 1.6 GFLOPs. Compared with existing state-of-the-art models, it is 11.0G lower than C3F-SimAM-YOLO, 1.8G lower than SCF-YOLO, 5.5G lower than YOLOv8-PCB, 14.8 GFLOPs lower than YOLO v7, and 4.7 GFLOPs lower than MobileNet v3 Small-YOLO v7. In terms of detection performance, the LDLFModel achieves an accuracy, recall, and mAP of 99.4%, 97.6%, and 99.0%, respectively, demonstrating comprehensive advantages. Compared with SCF-YOLO, they increase by 0.7%, 1.3%, and 0.7%, respectively; compared with C3F-SimAM-YOLO, the three metrics increase by 1.4%, 2.2%, and 1.5%, respectively; compared with YOLOv8-PCB, the improvements are more significant, reaching 4.7%, 3.6%, and 2.9%, respectively. Additionally, compared with YOLO v7, the metrics increase by 5.8%, 4.4%, and 4.8%, and compared with MobileNet v3 Small-YOLO v7, they improve by 0.7%, 2.2%, and 1.6%, respectively. Notably, while the LDLFModel significantly reduces computational cost, it outperforms all other comparison models across all performance metrics, excelling not only in computational efficiency but also leading comprehensively in detection accuracy, demonstrating outstanding overall performance and achieving an optimal balance between accuracy and efficiency. Table 5.Comparison of computational cost, precision, recall, and mAP. In addition, in order to further demonstrate that the image-cutting layer of the LDLFModel has a certain effect on the problem of misdetection and missed detection of tiny defects, this paper compares the detection results of YOLO v7 and the LDLFModel with a 3 √ó 3 image-cutting layer. The comparisons are shown inFigure 7. It can be seen that the LDLFModel can detect tiny defects, which can effectively solve the problems of misdetection of small PCB defects. In conclusion, the LDLFModel greatly reduces the number of parameters and computational cost while maintaining high detection accuracy. It also effectively solves the problem of tiny defect misdetection and missed detection, making it more suited for industrial PCB defect detection requirements. Figure 7.Comparison of detection effect between uncut and 3 √ó 3 cut. For verifying the performance of the LDLFModel, the proposed model was compared to several other mainstream algorithms. The classic two-stage algorithms are Faster R-CNN (VGG16) and Faster R-CNN (ResNet101), which use VGG16 and ResNet101 [23] as the backbone networks, respectively. The classic one-stage algorithms are Efficient-Det (D0) [24], single shot multibox detector (SSD) [25], YOLO v7, and LDLFModel. The frames per second (FPS) and mAP of various algorithms and the AP of each type of PCB defect are shown inFigure 8,Figure 9andFigure 10. Figure 8.FPS comparison. Figure 9.mAP comparisons. Figure 10.AP comparison of the detection of six types of defects. As shown inFigure 8, the LDLFModel without the image-cutting layer achieves 78.73 FPS, outperforming Faster R-CNN (VGG16) at 77.68 FPS, Faster R-CNN (ResNet101) at 78.14 FPS, EfficientDet (D0) at 53.11 FPS, and SSD at 25.31 FPS. Its speed is also comparable to that of YOLO v7, indicating the LDLFModel‚Äôs superior detection speed.Figure 9demonstrates that the LDLFModel reaches a maximum mAP of 99.0%, exceeding the performance of Faster R-CNN (VGG16), EfficientDet (D0), and SSD by 27.45%, 12.1%, and 7.72%, respectively. It also outperforms YOLO v7 and Faster R-CNN (ResNet101) with mAP improvements of 4.8% and 3.7%. The results demonstrate that the LDLFModel delivers both high detection speed and strong accuracy, making it a reliable choice for defect detection. For six types of PCB defects, the APs of various algorithms are shown inFigure 10. It can be seen that compared to the Faster R-CNN (VGG16), Faster R-CNN (ResNet101), and EfficientDet (D0) algorithms, the APs of the LDLFModel on various defects are much higher than that of the other algorithms. Compared to the APs from the Faster R-CNN (VGG16) algorithm on the stray and spurious copper defects, the APs from the LDLFModel is 45.47% and 39.05% higher respectively. Compared to the APs on the six types of defects from YOLO v7, the proposed model has improved results. The above results show that the LDLFModel can achieve the detection requirements of multiple types of defects simultaneously, and high accuracy for six types of defects of PCBs.",
            "4.5. Ablation Studies": "4.5.1. Regression Loss FunctionsBhattacharya and Cloutier [26] compared the performance of CIoU, DIoU, GIoU, and IoU. They found that CIoU outperformed the other loss functions. He et al. [27] presented a generalization Œ±-IoU loss that remarkably surpasses the IoU-based losses, and pointed out that better results will be achieved with Œ± = 3. We conducted experiments to compare the precision, recall, and mAP under CIoU, Œ±-IoU (Œ± = 3), and EIoU loss functions. The results inTable 6show that the EIoU loss has better overall performance than CIoU and Œ±-IoU (Œ± = 3) losses.Table 6.Comparison of precision, recall, and mAP.For balancing resource allocation and training time, the relationship among several losses, various learning rates, and the number of iterations was compared through the experiments. Some results are shown inFigure 11,Figure 12andFigure 13. Classification loss, marked as Cls loss, was used to calculate whether an anchor frame correctly corresponded to the label classification. Position loss, marked as Box loss, was used to calculate the error between the prediction box and calibration box. Confidence loss, marked as Obj_loss, was used to calculate the degree of confidence in a network.Figure 11.The relationship curve between Cls loss and epoch.Figure 12.The relationship curve between Box loss and epoch.Figure 13.The relationship curve between Obj loss and epoch.Learning rate is an important hyper parameter in a deep learning network. The batch size affects the generalisation performance of networks. Generally, when the batch size is increased, the learning rate should also be increased according to the linear scaling rule to ensure that the updated weights are equal when using the same sample. In a certain range, increasing the batch size is conducive to the stability of convergence, and a similar effect of learning rate attenuation can be achieved by increasing batch size [28]. Furthermore, with the same number of epochs, the larger the batch size, the fewer the batches required, and the training time is reduced [29]. Therefore, the learning rate in this experiment was set to 0.02 with a decay rate of 0.3, and the final learning rate was 0.006. According to the change of the losses with the number of iterations, the number of iterations was set to 300. The batch size was set to 33. In order to avoid missing small defects as much as possible, the resolution of the input image was adjusted to 960 √ó 960. 4.5.2. Attention MechanismTo assess the effectiveness of applying the CA attention mechanism in the LDLFModel, we conducted experimental comparisons with the SE and CBAM attention mechanisms, applying each to the same position within the model. The results inTable 7indicate that the CA attention mechanism has better comprehensive performance than the SE and CBAM attention mechanisms.Table 7.Comparison of different attention mechanisms.Additionally, CA modules were applied to different positions of the LDLFModel, as shown inFigure 1, to evaluate their impact on precision, recall and mAP. The experimental results inTable 8display that positions 1, 2, 3, 4, and 5 achieved the highest precision, recall, and mAP. These findings demonstrate that incorporating the CA attention mechanism at these positions significantly enhances detection performance in the LDLFModel.Table 8.Comparison of different positions of the CA model. 4.5.3. The Image-Cutting LayerTo assess the effect of the image-cutting layer, we compared detection performance before and after its application.Table 9shows that precision remained consistently high, with minimal variation, though certain defect types showed marked improvement. For instance, the ‚Äúmouse bites‚Äù defect saw a 0.7% increase in precision. More notably, recall improved visibly post-cutting, particularly for defects such as missing holes, open circuits, short circuits, and spurious copper, with increases of 2.4%, 2.3%, 0.7%, and 9%, respectively. Additionally, the mAP for spurious copper increased by 2.9%. These results indicate that the image-cutting layer positively impacts performance, particularly in improving recall for specific defect types.Table 9.Comparison of precision, recall, and mAP before and after cutting (%).Figure 14illustrates the comparison of detection performance for six types of defects on the same image, both before and after cutting. The left side of each arrow represents the detection results before cutting, while the right side shows the results after cutting. As can be seen in the figure, the detection accuracy for each defect type improves to some extent following the cutting process. These results confirm that data cutting enhances the detection of high-resolution PCB defects.Figure 14.Comparison of six types of defect detection results before and after cutting.",
            "4.5.1. Regression Loss Functions": "Bhattacharya and Cloutier [26] compared the performance of CIoU, DIoU, GIoU, and IoU. They found that CIoU outperformed the other loss functions. He et al. [27] presented a generalization Œ±-IoU loss that remarkably surpasses the IoU-based losses, and pointed out that better results will be achieved with Œ± = 3. We conducted experiments to compare the precision, recall, and mAP under CIoU, Œ±-IoU (Œ± = 3), and EIoU loss functions. The results inTable 6show that the EIoU loss has better overall performance than CIoU and Œ±-IoU (Œ± = 3) losses. Table 6.Comparison of precision, recall, and mAP. For balancing resource allocation and training time, the relationship among several losses, various learning rates, and the number of iterations was compared through the experiments. Some results are shown inFigure 11,Figure 12andFigure 13. Classification loss, marked as Cls loss, was used to calculate whether an anchor frame correctly corresponded to the label classification. Position loss, marked as Box loss, was used to calculate the error between the prediction box and calibration box. Confidence loss, marked as Obj_loss, was used to calculate the degree of confidence in a network. Figure 11.The relationship curve between Cls loss and epoch. Figure 12.The relationship curve between Box loss and epoch. Figure 13.The relationship curve between Obj loss and epoch. Learning rate is an important hyper parameter in a deep learning network. The batch size affects the generalisation performance of networks. Generally, when the batch size is increased, the learning rate should also be increased according to the linear scaling rule to ensure that the updated weights are equal when using the same sample. In a certain range, increasing the batch size is conducive to the stability of convergence, and a similar effect of learning rate attenuation can be achieved by increasing batch size [28]. Furthermore, with the same number of epochs, the larger the batch size, the fewer the batches required, and the training time is reduced [29]. Therefore, the learning rate in this experiment was set to 0.02 with a decay rate of 0.3, and the final learning rate was 0.006. According to the change of the losses with the number of iterations, the number of iterations was set to 300. The batch size was set to 33. In order to avoid missing small defects as much as possible, the resolution of the input image was adjusted to 960 √ó 960.",
            "4.5.2. Attention Mechanism": "To assess the effectiveness of applying the CA attention mechanism in the LDLFModel, we conducted experimental comparisons with the SE and CBAM attention mechanisms, applying each to the same position within the model. The results inTable 7indicate that the CA attention mechanism has better comprehensive performance than the SE and CBAM attention mechanisms. Table 7.Comparison of different attention mechanisms. Additionally, CA modules were applied to different positions of the LDLFModel, as shown inFigure 1, to evaluate their impact on precision, recall and mAP. The experimental results inTable 8display that positions 1, 2, 3, 4, and 5 achieved the highest precision, recall, and mAP. These findings demonstrate that incorporating the CA attention mechanism at these positions significantly enhances detection performance in the LDLFModel. Table 8.Comparison of different positions of the CA model.",
            "4.5.3. The Image-Cutting Layer": "To assess the effect of the image-cutting layer, we compared detection performance before and after its application.Table 9shows that precision remained consistently high, with minimal variation, though certain defect types showed marked improvement. For instance, the ‚Äúmouse bites‚Äù defect saw a 0.7% increase in precision. More notably, recall improved visibly post-cutting, particularly for defects such as missing holes, open circuits, short circuits, and spurious copper, with increases of 2.4%, 2.3%, 0.7%, and 9%, respectively. Additionally, the mAP for spurious copper increased by 2.9%. These results indicate that the image-cutting layer positively impacts performance, particularly in improving recall for specific defect types. Table 9.Comparison of precision, recall, and mAP before and after cutting (%). Figure 14illustrates the comparison of detection performance for six types of defects on the same image, both before and after cutting. The left side of each arrow represents the detection results before cutting, while the right side shows the results after cutting. As can be seen in the figure, the detection accuracy for each defect type improves to some extent following the cutting process. These results confirm that data cutting enhances the detection of high-resolution PCB defects. Figure 14.Comparison of six types of defect detection results before and after cutting.",
            "5. Conclusions": "To address the issues of missed detection in small PCB defects while reducing computational complexity and the number of parameters, the LDLFModel is proposed for PCB defect detection. To begin with, the LDLFModel employs the lightweight MobileNet v3 Small-CA with a location attention mechanism as the backbone network. Then, the feature maps are weighted and the location attention mechanism is added to the multi-scale feature fusion module. Moreover, the target frame regression loss function adopts EIoU-loss, which makes the prediction on frame localisation more accurate. Finally, adding a image-cutting layer in the detection module avoids the issue of missed small defect detection on PCB images. A thorough comparative test was conducted using the LDLFModel, Faster R-CNN, EfficientDet, SSD, and YOLO v7 on a publicly available PCB dataset. Based on the experimental results, the LDLFModel increases mAP by 27.45%, 3.7%, 12.1%, and 7.72% compared to Faster R-CNN, EfficientDet, SSD, and YOLO v7, respectively. The model size, computational cost, and the number of parameters of the LDLFModel are much smaller than those of YOLO v7. At the same time, the LDLFModel has a high accuracy on all six types of defects, and the additional image-cutting layer can effectively reduce the issue of misdetection and missed detection of tiny defects. These results suggest that the LDLFModel effectively reduces the model size, the number of parameters, and computational cost compared to the YOLO v7 model, while maintaining strong detection accuracy. Additionally, it shows promise in improving the detection of tiny defects. These improvements suggest that the LDLFModel offers potential advantages for PCB defect detection. In industrial inspection, it is essential to detect not only surface defects on PCBs but also potential internal issues. The method proposed in this paper is limited to surface defect detection and would need to be integrated with other techniques for identifying internal defects. Additionally, the sliding window approach used to cut the training dataset is conducted offline, which is not conducive to achieving end-to-end integrated processing. As the PCB industry advances, the requirements for defect detection are becoming more stringent. Therefore, ongoing research is necessary to optimize detection methods and improve overall applicability."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/1424-8220/25/24/7403",
        "scraped_at": "2025-12-05 23:53:14"
    },
    {
        "title": "Bagging-PiFormer: An Ensemble Transformer Framework with Cross-Channel Attention for Lithium-Ion Battery State-of-Health Estimation",
        "authors": "byShaofang Wu,Jifei Zhao,Weihong Tang,Xuhui LiuandYuqian Fan",
        "journal": "Batteries2025,11(12), 447;https://doi.org/10.3390/batteries11120447- 5 Dec 2025",
        "abstract": "Accurate estimation of lithium-ion battery (LIB) state of health (SOH) is critical for prolonging battery life and ensuring safe operation. To address the limitations of existing data-driven models in robustness and feature coupling, this paper presents a new Bagging-PiFormer framework for SOH estimation. The framework integrates ensemble learning with an improved Transformer architecture to achieve accurate and stable performance across various degradation conditions. Specifically, multiple PiFormer base models are trained independently under the Bagging strategy to enhance generalization. Each PiFormer consists of a stack of PiFormer layers, which combines a cross-channel attention mechanism to model voltage‚Äìcurrent interactions and a local convolutional feed-forward network (LocalConvFFN) to extract local degradation patterns from charging curves. Residual connections and layer normalization stabilize gradient propagation in deep layers, while a purely linear output head enables precise regression of the continuous SOH values. Experimental results on three datasets demonstrate that the proposed method achieves the lowest MAE, RMSE, and MAXE values among all compared models, reducing overall error by 10‚Äì33% relative to mainstream deep-learning methods such as Transformer, CNN-LSTM, and GCN-BiLSTM. These results confirm that the Bagging-PiFormer framework significantly improves both the accuracy and robustness of battery SOH estimation.Keywords:lithium-ion battery;state-of-health estimation;Bagging-PiFormer;cross-channel attention;local convolutional feed-forward network",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Lithium-ion batteries (LIBs) are extensively utilized in numerous fields due to their advantages of high energy density, long lifespan, and less environmental impact [1,2]. As these applications increasingly depend on battery capacity, safety, and reliability, accurately predicting the performance of batteries becomes particularly important [3]. Among many performance indicators, state of health (SOH) is a critical index for evaluating cell aging, capacity fading, and power degradation. Hence, accurate and timely SOH prediction is crucial for operational maintenance, safety optimization, and lifespan extension of LIBs [4]. Recent studies increasingly highlight the importance of ensemble learning and hybrid architectures in addressing the instability of single models. Garse et al. [5] demonstrated that ensemble methods reduce RMSE by >15% compared to standalone models, while Figgener et al. [6] highlighted the necessity of real-world validation for industrial deployment. Additionally, feature engineering innovations, such as the coefficient of variation (COV) proposed by Liao et al. [7], simplify health indicator extraction, and cross-channel fusion techniques [8,9] have become essential for modeling complex degradation coupling. Generally, SOH estimation methods primarily fall into two categories: model-based and data-driven. Model-based approaches assess battery degradation performance by tracking battery parameters via simple relationships based on experimental data or by estimating battery states via interpreting internal electrochemical characteristics [10,11]. For instance, a versatile equivalent circuit model (ECM) framework was proposed by integrating statistical analysis, principal component analysis (PCA), and clustering techniques to develop an ECM that is capable of representing various battery cell types [12]. A lithium-ion battery calendar-life prognostic model was presented in the literature [13], integrating traditional electrochemical models, mechanistic models, hybrid empirical-mechanistic models, and machine learning-based models to predict the calendar life of LIBs. While these approaches offer significant advantages in terms of adaptability and accuracy for handling complex battery behaviors and diverse application demands, they also face many challenges, for instance, high data requirements, high computational complexity, and limited model interpretability. During the recent decade, the advent of data-driven techniques has revolutionized the landscape of industrial system diagnosis as well as battery state estimation [14,15,16,17]. These approaches predominantly leverage machine learning algorithms, including support vector machines (SVMs), random forests (RFs), and deep learning architectures [15], to discern underlying patterns and trends from operational data. Unlike conventional methods, data-driven techniques uncover latent relationships and dynamic influences within the input data, thereby exhibiting significant adaptability and resilience when confronted with nonlinear challenges and fluctuating operational environments. Recent research [18] confirms that deep learning architectures must integrate both global dependencies and local degradation characteristics. Xu et al. [19] further highlighted the limitations of conventional BP networks in dynamic scenarios, underscoring the need for mechanisms like adaptive attention [20] or temporal convolution [21]. Rout et al. [22] developed a data-driven method for evaluating the health status of LIBs, employing a variety of machine learning algorithms such as AdaBoost and Xgboost, ridge regression, random forest, artificial neural network (ANN), and long-short term memory network (LSTM). Among these approaches, LSTM outperforms in capturing underlying relationships within battery data sequences, showing obvious advantages in reducing error and improving accuracy compared with the other models. Poh et al. [23] propose advanced data-driven methods for estimating SOH of LIBs using health indicators and machine learning models, achieving high accuracy (average RMSE of 0.94%) and computational efficiency (15% CPU usage) in automotive applications. Gui et al. [24] design a hybrid architecture for SOH estimation for LIBs, integrating a CNN and a Transformer in parallel, so that local features and global dependencies in multimodal data can be learned cooperatively. By combining local and global feature extraction with a multi-information alignment strategy, the model achieved an RMSE below 0.017 and an R2 score of about 1.0 on multiple datasets. Zheng et al. [25] propose a data-driven joint estimation method for the SOH of LIBs using SVM, CNN, and LSTM models, achieving high accuracy with an SOH estimation RMSE below 0.81% and an MAE below 0.65%. And Su et al. [26] propose a hybrid method combining an equivalent circuit model, deep learning, and transfer learning for battery capacity estimation, achieving a maximum test error of 0.0941. Li et al. [27] design a CNN-based transfer learning method, using accelerated aging data for pre-training and 15% of normal-speed data for fine-tuning, achieving an RMSE below 0.32% and accuracy over 99.7% on SONY and FST batteries. Overall, deep learning technology has achieved remarkable results in extracting complex battery behavior from massive data. However, one of the existing limitations of these methods is their insufficient efficiency in learning key features. To reliably estimate the SOH of LIBs under practical constraints, data-driven methods that leverage partial operational data have demonstrated significant potential. Recent advances in lithium-ion battery materials have shown that distinct cathode chemistries exhibit different aging pathways and degradation rates, which directly influence SOH evolution patterns [28]. The integration of advanced neural architectures, particularly those employing attention mechanisms to optimize the weight distribution of input health features, has been key to enhancing the characterization of battery degradation and achieving high estimation accuracy [29]. Arbaoui et al. [30] further advance the field by developing E-LSTM and CNN-LSTM models, emphasizing model explainability. They adopt Shapley additive explanations (SHAP) to interpret predictions, integrate pattern mining for anomaly detection on the MIT dataset, and realize a mean absolute error (MAE) of lower than 1%. For enhanced temporal modeling, Chu et al. [31] constructed a temporal convolutional network (TCN) integrated with an attention mechanism and refined it using Bayesian optimization. Their model, validated on a proprietary dataset, yielded a mean absolute percentage error (MAPE) below 5.00%, demonstrating robust prediction capability across both high and low cycle numbers. Xiao et al. [32] propose a hybrid model that effectively learns the spatial information and long-term correlation of data by integrating CNN and a mask multi-head attention mechanism. The model performs best on the NASA and CALCE datasets, and its lowest RMSE and SDE values confirm its generalization and robustness. Zhao et al. [33] implemented a standard Transformer network, employing a rigorous feature selection process based on Pearson correlation coefficients. This model, tested on the NASA dataset with selected charging features, attained an RMSE of 0.0290 and an MAE of 0.0258, underscoring the efficacy of attention-based architectures for sequence modeling. Rajasekaran et al. [34] propose a GAN-based model with a CNN-LSTM generator and triple-attention ESN discriminator. By testing on NASA datasets, the model achieves an MAE as low as 0.0036 and an RMSE below 0.0213. Mchara et al. [35] develop a CNN-BiLSTM model for SOH estimation, using LSTM layers and a convolutional layer enhanced with an attention mechanism. Evaluated on NASA batteries, the model achieves an MAPE as low as 0.0031 and an RMSE below 0.0041. Geng et al. [36] design an interpretable deep learning framework with a multi-head attention LSTM, optimized with the Sparrow Search Algorithm and DeepSHAP for feature analysis. The model achieves an RMSE below 5% and an MAPE under 3% in NASA, CALCE, and PolyU datasets. Wang et al. [37] present a joint SOH estimation method using CNN-GRU for SOH. By testing under 1C cycling at 10 ¬∞C and 25 ¬∞C, the model achieves an RMSE below 1% for SOH. However, Chen et al. [38] reveal that neglecting cross-channel interactions in Transformers underutilizes sensor data. Du et al. [39] also emphasize that battery inconsistency in series-connected packs introduces overfitting risks, aligning with our dual-channel attention design. Collectively, these studies underscore a clear trend: the integration of attention mechanisms and specialized network designs is pivotal for achieving precise, robust, and interpretable battery SOH estimation. Despite these advances, few architectures jointly address (i) multi-model robustness via bagging [40], (ii) cross-channel coupling [41], and (iii) lightweight deployment [42]. This gap motivates our Bagging-PiFormer framework. Although significant progress has been made in the accuracy of battery health status prediction in existing research, these achievements have not been fully validated on extensive battery data. The charging battery system is affected by multiple factors, such as charging rates, voltage changes, cycle times, material properties, and chemical composition. These factors often have complex temporal correlations that have a comprehensive effect on the health status of the battery. The current methods still lack sufficient understanding of the global structural features of data, making it difficult to comprehensively analyze these complex correlations. Deep learning has shown great potential in state-of-health (SOH) estimation for lithium-ion batteries. However, its performance is still constrained by the limitations of model architecture, temporal dependency modeling, and feature fusion capability. In general, current data-driven SOH estimation methods still face the following four major challenges: (1) Insufficient robustness of single-model architectures. Most existing approaches rely on a single network structure, which tends to suffer from performance fluctuations and overfitting when facing diverse degradation stages, battery types, or noisy data, leading to unstable estimation results. (2) Lack of effective utilization of channel coupling in conventional Transformer structures. During the charging process, voltage and current signals exhibit strong interdependence. However, standard Transformer models primarily capture sequential correlations while neglecting cross-channel interactions, resulting in incomplete extraction of degradation features. (3) Limited sensitivity of attention mechanisms to local temporal features. Although attention mechanisms are effective for capturing long-range dependencies, they often fail to recognize local short-term patterns, such as voltage spikes or current pulses, which are critical indicators of degradation behavior in charging curves. (4) Gradient attenuation and improper activation design in deep layers. Transformer-based architectures are prone to gradient vanishing or feature distribution drift when deeply stacked. Moreover, since SOH is a continuous regression variable, the use of nonlinear activation functions in the output layer may constrain the output range and reduce estimation accuracy. To address the above challenges, we propose a Bagging-PiFormer framework, an SOH estimation model based on ensemble learning, and an improved Transformer architecture. The proposed method integrates multi-model bagging, cross-channel attention fusion, and local convolutional feature extraction to achieve accurate and robust SOH estimation from both global and local perspectives. The main contributions of this study are summarized as follows: (1) Bagging-PiFormer ensemble framework: A bagging-based ensemble architecture is developed, where multiple independent PiFormer models are trained in parallel, and their results are averaged to enhance robustness and generalization while reducing overfitting risk. (2) Cross-channel attention mechanism: A dual-channel attention module is designed within each PiFormer layer to model the bidirectional dependencies between voltage and current features, enabling dynamic interaction and effective fusion of cross-channel information. (3) Local convolutional feed-forward network (LocalConvFFN): A local convolutional module is introduced after the attention layer to extract short-term temporal degradation patterns through deep convolution and SiLU activation, complementing the attention mechanism‚Äôs ability to model long-range dependencies. (4) Deep normalization and linear output design: Residual connections and layer normalization are employed to stabilize feature distribution in deep layers, while a purely linear output head is used to regress the continuous SOH value, avoiding unnecessary activation constraints and improving estimation precision. The remainder of this paper is organized as follows.Section 2describes the dataset and the multi-channel physical feature extension technique.Section 3details the cross-channel attention network.Section 4presents experiments conducted to validate the effectiveness of the proposed methods, andSection 5concludes the paper.",
            "2. Dataset and Multi-Channel Physical Feature Extension": "To simulate the real operating conditions of LIBs, two groups of experiments were conducted following the CLTC-P protocol, covering different charging conditions and battery aging status, resulting in the creation of Datasets A and B, respectively. To test the generalization performance of the model, this study also introduces Dataset C, which includes 124 A123 APR18650M1A lithium-ion cells tested within a forced convection chamber. These three datasets were then used to comprehensively evaluate the proposed method. All cycling experiments for Datasets A and B were performed using a Neware BTS-5V12A battery testing system (Neware Technology Limited Company, Shenzhen, Guangdong, China), ensuring a voltage and current measurement accuracy of 0.05%. All measurements and tester control for Datasets A and B were carried out using the BTSDA software (version 7.6.0, Neware Technology Limited Company). Dataset C is a publicly available dataset and therefore does not involve in-house testing equipment or software [43]. 2.1. Battery DatasetsDataset A: The test process of the INR 18650 battery is shown inFigure 1a. The process starts with a constant current charging: initially charging to 4.2 V at 6 C current, followed by a short rest, then charging to 4.2 V at 3 C current. After another one-minute rest, the standard 0.5 C constant-current constant-voltage (CC-CV) charging is performed, with a cut-off current of 0.05 C and a cut-off voltage of 4.2 V. Following a one-hour rest, the battery enters the discharge phase, continuously running the China light-duty test cycle for passenger cars (CLTC-P) cycles until the voltage drops to 2.7 V. The CLTC-P cycle is built based on big traffic data that includes low, medium, and high-speed intervals, lasting over 1800 s, and can effectively reflect real-world battery conditions. The capacity is measured every 50 cycles.Figure 1.Charging and discharging curves for (a) Dataset A, (b) Dataset B, and (c) Dataset C.Dataset B: The steps of each aging cycle for the used Prospower ICR 18650 P battery are shown inFigure 1b. The process begins with a constant current charge to 4.2 V, followed by a 4.2 V constant voltage charge until the current drops to 0.1 C. Then, the battery discharges at a constant current rate of 3 C until reaching the cut-off voltage of 2.5 V. A one-hour resting interval is included between each cycle.Dataset C: This dataset consists of 124 A123 APR18650M1A lithium-ion cells tested within a forced convection chamber maintained at 30 ¬∞C. Each cell underwent rapid-charging experiments employing either a one-step or two-step constant-current (CC) charging strategy, denoted as ‚ÄúC1(Q1)‚ÄìC2,‚Äù where C1 and C2 correspond to the first and second constant-current phases, respectively, and Q1 represents the state of charge (SOC, %) at the transition between them. The first charging phase was conducted at rates between 5.5 C and 6.1 C, followed by a second phase at 4.4 C‚Äì4.7 C until the SOC reached 80%. Subsequently, the cells were charged under a 1 C CC‚ÄìCV protocol with cutoff voltages of 3.6 V (upper) and 2.0 V (lower), according to manufacturer specifications.In this study, the three datasets were obtained from independent aging experiments on commercial 18650 lithium-ion batteries with different chemistries. Dataset A corresponds to an INR 18650 cell (NMC-based), Dataset B to a Prospower ICR 18650P cell (LCO-based), and Dataset C to an A123 APR18650M1A cell (LFP-based). These battery types differ in nominal voltage, rated capacity, and expected degradation characteristics, providing a diverse evaluation benchmark for SOH estimation. For each dataset, multi-channel physical measurements‚Äîincluding voltage, current, temperature, and internal resistance‚Äîwere recorded throughout the cycling process.Table 1summarizes the specifications of three datasets. The aging tests were conducted independently, and a pulse discharge method was used to emulate discharge patterns. All experiments were performed under temperature-controlled conditions. Partial constant current charging rates were 4 C for Dataset A and 5.5‚Äì6.1 C for Dataset B. SOH decay occurred over different time scales: Dataset A reached 80% capacity within one month, Dataset B over three months, and Dataset C over six months.Table 1.Related battery dataset specifications.The SOH is defined as the actual capacity of the lithium-ion cell divided by its initial capacity, as shown in Equation (1):SOH=ùê∂ùëêùê∂ùëñSOH=CcCi(1)whereùê∂ùëêCcrepresents the current maximum discharge capacity (remaining capacity) andùê∂ùëñCirepresents the initial capacity of the battery. 2.2. Health IndicatorsBattery cycle life can be characterized by changes in related parameters such as terminal voltage, internal resistance, and capacity, which reflect the degree of degradation and health status. The key to achieving accurate SOH estimation is to effectively identify features from these parameters that reliably reflect the battery aging process.Time-related features provide the most direct information about SOH variations. Deep learning models can learn complex nonlinear relationships between these temporal features and battery degradation from large datasets. However, for fast-charging batteries, the decay period is short. In Dataset B, SOH declines to 80% over six months, whereas in Datasets A and C, the same decay occurs in one month and three months, respectively. The number of cycles and the dataset size can limit the learning of the model, leading to the problem of vanishing gradients. This occurs when gradients of network parameters diminish toward zero during training, causing shallow layers to update faster while deeper layers learn slowly or stagnate, which impairs effective feature representation.2.2.1. Feature Extraction ProcessIn the feature extraction process, we process two constant-current phases and one constant-current constant-voltage phase to implement feature extraction. Each phase is divided into a series of equally spaced voltage intervals, and samples are collected within these intervals. The sampling calculation is given as follows [44]:Œîùëâ=Vùëöùëéùë•‚àíVùëöùëñùëõùëìùë†,ŒîV=Vmax‚àíVminfs,(2)where‚ñ≥ùëâ‚ñ≥Vdefines the voltage interval,Vùëöùëéùë•‚àíVùëöùëñùëõVmax‚àíVminis the maximum voltage difference within the current voltage segment, andùëìùë†fsis the sampling frequency. It is noted that the calculation of the sampling frequency directly impacts the subsequent feature extraction process.Equations (3) and (4) determine the voltage range of the interval [44]:ùëâùë†ùë°ùëéùëüùë°ùëñ=Vùëöùëéùë•‚àí‚ñ≥ùëâ‚àóùëñ,Vistart=Vmax‚àí‚ñ≥V‚àói,(3)ùëâùëíùëõùëëùëñ=Vùëöùëéùë•‚àí‚ñ≥ùëâ‚àó(ùëñ+1),Viend=Vmax‚àí‚ñ≥V‚àói+1,(4)whereùëâùë†ùë°ùëéùëüùë°ùëñVistartandùëâùëíùëõùëëùëñViendrepresent the starting and ending voltages of thei-th voltage interval, respectively.2.2.2. Time Variations Within Equal Voltage IntervalsThe temporal differential during terminal charge/discharge cycles was disproportionately greater than that of initial cycles. Consequently, the inferable time difference within uniform voltage intervals serves as a direct indicator of inherent cell decay characteristics. Calculating these temporal variations enables finer-grained resolution of non-uniform aging patterns. To establish temporal boundaries for each interval, we derived the maximum and minimum voltage values cycle alongside their sampling frequencies. Subsequent feature extraction was then performed within these voltage-partitioned temporal blocks. The computation defining time variations across constant voltage intervals is expressed by the following formula:‚àÜùëáùëñ=ùë°ùëíùëõùëëùëñ‚àíùë°ùë†ùë°ùëéùëüùë°ùëñ,‚àÜTi=tiend‚àítistart,(5)whereùë°ùëíùëõùëëùëñtiendandùë°ùë†ùë°ùëéùëüùë°ùëñtistartrepresent the starting and ending times within the voltage interval, respectively, and‚àÜùëáùëñ‚àÜTiis the time difference. This feature focuses on the temporal dynamics that occur during the charging period.2.2.3. Cumulative Integral of the Voltage VariationThe voltage curve exhibits distinct upward and leftward displacements accompanied by altered slope characteristics. During initial charging phases, voltage demonstrates a gradual elevation that attenuates near process completion. Crucially, instantaneous voltage readings alone provide insufficient battery state characterization.The cumulative integral methodology addresses this limitation by aggregating temporal voltage differentials, thereby amplifying minor fluctuations into discernible patterns that more accurately capture cell behavior. Comparative analysis shows that the terminal cycle‚Äôs cumulative integral is positioned superiorly and sinistrally to the initial cycle‚Äôs profile with marked magnitude deviation. Within this framework,Eirepresents the energy input during charging, defined as the cumulative integral of voltage across constant-voltage intervals between start and end times, minus the product of instantaneous voltage and temporal differential:ùê∏ùëñ=‚à´ùë°ùëíùëõùëëùëñùë°ùë†ùë°ùëéùëüùë°ùëñùëâ(ùë°)ùëëùë°‚àíùëâùë†ùë°ùëéùëüùë°ùëñ¬∑‚àÜùëáùëñ,Ei=‚à´tistarttiendV(t)dt‚àíVistart¬∑‚àÜTi,(6)whereùëâ(ùë°)V(t)is the voltage, and the initial pointùë°ùë†ùë°ùëéùëüùë°ùëñtistartand endpointùë°ùëíùëõùëëùëñtiendare determined by the time interval blocks of the first cycle.2.2.4. Horizontal Value of the Slope PeakDuring deep learning training, the accumulation of errors and noise amplification caused by integration operations can lead to gradient explosion, manifested as a sharp increase in network parameter gradients, uncontrolled weight updates, and ineffective representation. To address this issue, it is noticed that the peak position of the charging process continues to shift to the right with cyclic aging. This stable trend suggests that peak voltage (i.e., horizontal position) is a more reliable alternative feature than problematic area or peak height data. The specific calculation is as follows:Vùëùùëíùëéùëò=ùëéùëüùëîùëöùëéùë•(ùëëùëÑùëëùëâ)‚âàùëéùëüùëîùëöùëéùë•(ùëÑùëò+1‚àíùëÑùëòùëâùëò+1‚àíùëâùëò),Vpeak=argmaxdQdV‚âàargmaxQk+1‚àíQkVk+1‚àíVk,(7)whereùëÑQandVare the capacity and voltage, respectively.kandk+ 1 are time steps, and the variableùëâùëùùëíùëéùëòVpeakis used to quantify the horizontal value of the maximum slope of the battery voltage change curve, representing the maximum voltage increase rate during the charging process.2.2.5. Second-Order Differential Integral FeatureThe second-order differential integral feature is extracted to quantitatively characterize the subtle curvature changes in the voltage‚Äìcapacity curve. This feature serves as a highly sensitive indicator of degradation mechanisms such as lithium plating and solid electrolyte interphase growth. By integrating the second-order derivative, the feature captures the cumulative effect of these electrochemical shifts, providing a robust and noise-resistant metric that correlates strongly with capacity fade and internal resistance increase, making it a powerful predictor for state-of-health estimation. The specific calculation is as follows:Œìi=‚à´ùëâùëíùëõùëëùëñùëâùë†ùë°ùëéùëüùë°ùëñùëë2ùëâùëëùëÑ2dùëâ‚âà‚à´ùëâùëíùëõùëëùëñùëâùë†ùë°ùëéùëüùë°ùëñ(ùëëùëâùëëùëÑ)ùëò+1‚àí(ùëëùëâùëëùëÑ)ùëòùëÑk+1‚àíùëÑùëòdùëâ,Œìi=‚à´VistartViendd2VdQ2dV‚âà‚à´VistartVienddVdQk+1‚àídVdQkQk+1‚àíQkdV,(8)2.2.6. Total Charge Change Through the CircuitIn the constant current CC-CV charging process, under constant current charging or fast charging strategies, the charging capacity curve shows significant differences due to differences in current rate. These differences reflect the variation in the battery‚Äôs available capacity over time and capture the actual change in its ability to store charge during charging and discharging. As the number of batteries used increases and the degree of aging intensifies, their effective capacity gradually declines, amplifying differences in charging capacity. By multiplying the difference in charging capacity at each stage by the corresponding current, the change in charge within a specific voltage range can be quantified, providing deeper insights into the battery‚Äôs charging behavior. The specific calculation formula is as follows:ùëÉùëñ=(ùëÑùëíùëõùëëùëñ‚àíùëÑùë†ùë°ùëéùëüùë°ùëñ)*ùêºùëñ,Pi=(Qiend‚àíQistart)*Ii,(9)The product of the current and capacity difference in this stage defines the energy increment obtained by the battery during the charging interval, corresponding to the total charge passing through the circuit during this process.Data from the target lithium-ion battery is acquired for feature extraction, transforming the raw measurements into a learnable tensor. For the time-sequential triplet sequence recorded from the processed data, a physically inspired feature expansion is performed based on the three original channels. To ensure that these heterogeneous channels share the same dimension and possess differentiable learning properties in subsequent deep networks, a trainable linear mapping is introduced. Each sampling point is transformed and subjected to layer normalization to obtain a token representation.The final feature vectorùëÖùë°Rtincludes temporal features, voltage integral features, capacity-current features, second-order derivative integral features, and peak voltage features:ùëÖùë°=[‚àÜùëáùëñ,ùê∏ùëñ,ùëÉi,ùõ§ùëñ,ùëâùëùùëíùëéùëò],Rt=‚àÜTi,Ei,Pi,Œìi,Vpeak,(10)and the input tensorùêºùëõùëùùë¢ùë°ùë°Inputtis designed consideringùëâùëùùëíùëéùëò,‚àëùê∏ùëñ,‚àëùëÉùëñùëñ,ùõ§ùëöùëéùëñùëõVpeak,‚àëEi,‚àëPii,Œìmain:ùêºùëõùëùùë¢ùë°ùë°=[ùëâùëùùëíùëéùëò,‚àëùê∏ùëñ,‚àëùëÉùëñùëñ,ùõ§ùëöùëéùëñùëõ],Inputt=Vpeak,‚àëEi,‚àëPii,Œìmain,(11)The feature extraction procedure used in this work follows widely validated practices in battery SOH modeling. Prior studies have shown that degradation-sensitive indicators, such as voltage‚Äìcurrent progression patterns, transition-region behaviors, curve-shape descriptors, and cumulative aging trends, provide strong predictive value for SOH estimation. Representative works include impedance-guided feature selection [45], optimization-based or statistical feature screening strategies [46,47], SHAP-based explainable feature ranking [48], and noise-robust feature refinement methods [49]. Moreover, our previous publications have applied SHAP-based interpretability analysis [50] and developed feature-selection algorithms that were peer-reviewed and experimentally validated [44]. Together, these works support the empirical and mechanistic validity of the feature set adopted in this study. 2.3. Data Preprocessing and NormalizationTo ensure stable network training and consistent metric computation, all input features were subjected to Min‚ÄìMax normalization during preprocessing. For each battery cell, a separate scaler was fitted using only its training cycles, and all corresponding validation and test cycles were transformed using the same scaler to avoid data leakage. Each feature valueùúíœáwas mapped into the [0, 1] range as follows:ùúí‚àó=ùúí‚àíùúíùëöùëñùëõùúíùëöùëéùë•‚àíùúíùëöùëñùëõ,œá*=œá‚àíœáminœámax‚àíœámin,(12)This scaling removed the influence of heterogeneous physical units among voltage-, current-, and time-related indicators, thereby improving model convergence and robustness.The SOH labels were retained in their original physical range (typically 0.80‚Äì1.00), but underwent minor smoothing via a moving-average operation to suppress measurement noise. No normalization was applied to SOH during training; the model directly regressed the physical SOH value. For visualization and comparison, all predicted SOH values were multiplied by 100 during evaluation to express the results in percentages, following common practice in battery health studies. Importantly, metrics such as MAE, RMSE, and R2were calculated strictly on the restored physical-scale SOH values.",
            "2.1. Battery Datasets": "Dataset A: The test process of the INR 18650 battery is shown inFigure 1a. The process starts with a constant current charging: initially charging to 4.2 V at 6 C current, followed by a short rest, then charging to 4.2 V at 3 C current. After another one-minute rest, the standard 0.5 C constant-current constant-voltage (CC-CV) charging is performed, with a cut-off current of 0.05 C and a cut-off voltage of 4.2 V. Following a one-hour rest, the battery enters the discharge phase, continuously running the China light-duty test cycle for passenger cars (CLTC-P) cycles until the voltage drops to 2.7 V. The CLTC-P cycle is built based on big traffic data that includes low, medium, and high-speed intervals, lasting over 1800 s, and can effectively reflect real-world battery conditions. The capacity is measured every 50 cycles. Figure 1.Charging and discharging curves for (a) Dataset A, (b) Dataset B, and (c) Dataset C. Dataset B: The steps of each aging cycle for the used Prospower ICR 18650 P battery are shown inFigure 1b. The process begins with a constant current charge to 4.2 V, followed by a 4.2 V constant voltage charge until the current drops to 0.1 C. Then, the battery discharges at a constant current rate of 3 C until reaching the cut-off voltage of 2.5 V. A one-hour resting interval is included between each cycle. Dataset C: This dataset consists of 124 A123 APR18650M1A lithium-ion cells tested within a forced convection chamber maintained at 30 ¬∞C. Each cell underwent rapid-charging experiments employing either a one-step or two-step constant-current (CC) charging strategy, denoted as ‚ÄúC1(Q1)‚ÄìC2,‚Äù where C1 and C2 correspond to the first and second constant-current phases, respectively, and Q1 represents the state of charge (SOC, %) at the transition between them. The first charging phase was conducted at rates between 5.5 C and 6.1 C, followed by a second phase at 4.4 C‚Äì4.7 C until the SOC reached 80%. Subsequently, the cells were charged under a 1 C CC‚ÄìCV protocol with cutoff voltages of 3.6 V (upper) and 2.0 V (lower), according to manufacturer specifications. In this study, the three datasets were obtained from independent aging experiments on commercial 18650 lithium-ion batteries with different chemistries. Dataset A corresponds to an INR 18650 cell (NMC-based), Dataset B to a Prospower ICR 18650P cell (LCO-based), and Dataset C to an A123 APR18650M1A cell (LFP-based). These battery types differ in nominal voltage, rated capacity, and expected degradation characteristics, providing a diverse evaluation benchmark for SOH estimation. For each dataset, multi-channel physical measurements‚Äîincluding voltage, current, temperature, and internal resistance‚Äîwere recorded throughout the cycling process. Table 1summarizes the specifications of three datasets. The aging tests were conducted independently, and a pulse discharge method was used to emulate discharge patterns. All experiments were performed under temperature-controlled conditions. Partial constant current charging rates were 4 C for Dataset A and 5.5‚Äì6.1 C for Dataset B. SOH decay occurred over different time scales: Dataset A reached 80% capacity within one month, Dataset B over three months, and Dataset C over six months. Table 1.Related battery dataset specifications. The SOH is defined as the actual capacity of the lithium-ion cell divided by its initial capacity, as shown in Equation (1):SOH=ùê∂ùëêùê∂ùëñSOH=CcCi(1)whereùê∂ùëêCcrepresents the current maximum discharge capacity (remaining capacity) andùê∂ùëñCirepresents the initial capacity of the battery.",
            "2.2. Health Indicators": "Battery cycle life can be characterized by changes in related parameters such as terminal voltage, internal resistance, and capacity, which reflect the degree of degradation and health status. The key to achieving accurate SOH estimation is to effectively identify features from these parameters that reliably reflect the battery aging process. Time-related features provide the most direct information about SOH variations. Deep learning models can learn complex nonlinear relationships between these temporal features and battery degradation from large datasets. However, for fast-charging batteries, the decay period is short. In Dataset B, SOH declines to 80% over six months, whereas in Datasets A and C, the same decay occurs in one month and three months, respectively. The number of cycles and the dataset size can limit the learning of the model, leading to the problem of vanishing gradients. This occurs when gradients of network parameters diminish toward zero during training, causing shallow layers to update faster while deeper layers learn slowly or stagnate, which impairs effective feature representation. 2.2.1. Feature Extraction ProcessIn the feature extraction process, we process two constant-current phases and one constant-current constant-voltage phase to implement feature extraction. Each phase is divided into a series of equally spaced voltage intervals, and samples are collected within these intervals. The sampling calculation is given as follows [44]:Œîùëâ=Vùëöùëéùë•‚àíVùëöùëñùëõùëìùë†,ŒîV=Vmax‚àíVminfs,(2)where‚ñ≥ùëâ‚ñ≥Vdefines the voltage interval,Vùëöùëéùë•‚àíVùëöùëñùëõVmax‚àíVminis the maximum voltage difference within the current voltage segment, andùëìùë†fsis the sampling frequency. It is noted that the calculation of the sampling frequency directly impacts the subsequent feature extraction process.Equations (3) and (4) determine the voltage range of the interval [44]:ùëâùë†ùë°ùëéùëüùë°ùëñ=Vùëöùëéùë•‚àí‚ñ≥ùëâ‚àóùëñ,Vistart=Vmax‚àí‚ñ≥V‚àói,(3)ùëâùëíùëõùëëùëñ=Vùëöùëéùë•‚àí‚ñ≥ùëâ‚àó(ùëñ+1),Viend=Vmax‚àí‚ñ≥V‚àói+1,(4)whereùëâùë†ùë°ùëéùëüùë°ùëñVistartandùëâùëíùëõùëëùëñViendrepresent the starting and ending voltages of thei-th voltage interval, respectively. 2.2.2. Time Variations Within Equal Voltage IntervalsThe temporal differential during terminal charge/discharge cycles was disproportionately greater than that of initial cycles. Consequently, the inferable time difference within uniform voltage intervals serves as a direct indicator of inherent cell decay characteristics. Calculating these temporal variations enables finer-grained resolution of non-uniform aging patterns. To establish temporal boundaries for each interval, we derived the maximum and minimum voltage values cycle alongside their sampling frequencies. Subsequent feature extraction was then performed within these voltage-partitioned temporal blocks. The computation defining time variations across constant voltage intervals is expressed by the following formula:‚àÜùëáùëñ=ùë°ùëíùëõùëëùëñ‚àíùë°ùë†ùë°ùëéùëüùë°ùëñ,‚àÜTi=tiend‚àítistart,(5)whereùë°ùëíùëõùëëùëñtiendandùë°ùë†ùë°ùëéùëüùë°ùëñtistartrepresent the starting and ending times within the voltage interval, respectively, and‚àÜùëáùëñ‚àÜTiis the time difference. This feature focuses on the temporal dynamics that occur during the charging period. 2.2.3. Cumulative Integral of the Voltage VariationThe voltage curve exhibits distinct upward and leftward displacements accompanied by altered slope characteristics. During initial charging phases, voltage demonstrates a gradual elevation that attenuates near process completion. Crucially, instantaneous voltage readings alone provide insufficient battery state characterization.The cumulative integral methodology addresses this limitation by aggregating temporal voltage differentials, thereby amplifying minor fluctuations into discernible patterns that more accurately capture cell behavior. Comparative analysis shows that the terminal cycle‚Äôs cumulative integral is positioned superiorly and sinistrally to the initial cycle‚Äôs profile with marked magnitude deviation. Within this framework,Eirepresents the energy input during charging, defined as the cumulative integral of voltage across constant-voltage intervals between start and end times, minus the product of instantaneous voltage and temporal differential:ùê∏ùëñ=‚à´ùë°ùëíùëõùëëùëñùë°ùë†ùë°ùëéùëüùë°ùëñùëâ(ùë°)ùëëùë°‚àíùëâùë†ùë°ùëéùëüùë°ùëñ¬∑‚àÜùëáùëñ,Ei=‚à´tistarttiendV(t)dt‚àíVistart¬∑‚àÜTi,(6)whereùëâ(ùë°)V(t)is the voltage, and the initial pointùë°ùë†ùë°ùëéùëüùë°ùëñtistartand endpointùë°ùëíùëõùëëùëñtiendare determined by the time interval blocks of the first cycle. 2.2.4. Horizontal Value of the Slope PeakDuring deep learning training, the accumulation of errors and noise amplification caused by integration operations can lead to gradient explosion, manifested as a sharp increase in network parameter gradients, uncontrolled weight updates, and ineffective representation. To address this issue, it is noticed that the peak position of the charging process continues to shift to the right with cyclic aging. This stable trend suggests that peak voltage (i.e., horizontal position) is a more reliable alternative feature than problematic area or peak height data. The specific calculation is as follows:Vùëùùëíùëéùëò=ùëéùëüùëîùëöùëéùë•(ùëëùëÑùëëùëâ)‚âàùëéùëüùëîùëöùëéùë•(ùëÑùëò+1‚àíùëÑùëòùëâùëò+1‚àíùëâùëò),Vpeak=argmaxdQdV‚âàargmaxQk+1‚àíQkVk+1‚àíVk,(7)whereùëÑQandVare the capacity and voltage, respectively.kandk+ 1 are time steps, and the variableùëâùëùùëíùëéùëòVpeakis used to quantify the horizontal value of the maximum slope of the battery voltage change curve, representing the maximum voltage increase rate during the charging process. 2.2.5. Second-Order Differential Integral FeatureThe second-order differential integral feature is extracted to quantitatively characterize the subtle curvature changes in the voltage‚Äìcapacity curve. This feature serves as a highly sensitive indicator of degradation mechanisms such as lithium plating and solid electrolyte interphase growth. By integrating the second-order derivative, the feature captures the cumulative effect of these electrochemical shifts, providing a robust and noise-resistant metric that correlates strongly with capacity fade and internal resistance increase, making it a powerful predictor for state-of-health estimation. The specific calculation is as follows:Œìi=‚à´ùëâùëíùëõùëëùëñùëâùë†ùë°ùëéùëüùë°ùëñùëë2ùëâùëëùëÑ2dùëâ‚âà‚à´ùëâùëíùëõùëëùëñùëâùë†ùë°ùëéùëüùë°ùëñ(ùëëùëâùëëùëÑ)ùëò+1‚àí(ùëëùëâùëëùëÑ)ùëòùëÑk+1‚àíùëÑùëòdùëâ,Œìi=‚à´VistartViendd2VdQ2dV‚âà‚à´VistartVienddVdQk+1‚àídVdQkQk+1‚àíQkdV,(8) 2.2.6. Total Charge Change Through the CircuitIn the constant current CC-CV charging process, under constant current charging or fast charging strategies, the charging capacity curve shows significant differences due to differences in current rate. These differences reflect the variation in the battery‚Äôs available capacity over time and capture the actual change in its ability to store charge during charging and discharging. As the number of batteries used increases and the degree of aging intensifies, their effective capacity gradually declines, amplifying differences in charging capacity. By multiplying the difference in charging capacity at each stage by the corresponding current, the change in charge within a specific voltage range can be quantified, providing deeper insights into the battery‚Äôs charging behavior. The specific calculation formula is as follows:ùëÉùëñ=(ùëÑùëíùëõùëëùëñ‚àíùëÑùë†ùë°ùëéùëüùë°ùëñ)*ùêºùëñ,Pi=(Qiend‚àíQistart)*Ii,(9)The product of the current and capacity difference in this stage defines the energy increment obtained by the battery during the charging interval, corresponding to the total charge passing through the circuit during this process.Data from the target lithium-ion battery is acquired for feature extraction, transforming the raw measurements into a learnable tensor. For the time-sequential triplet sequence recorded from the processed data, a physically inspired feature expansion is performed based on the three original channels. To ensure that these heterogeneous channels share the same dimension and possess differentiable learning properties in subsequent deep networks, a trainable linear mapping is introduced. Each sampling point is transformed and subjected to layer normalization to obtain a token representation.The final feature vectorùëÖùë°Rtincludes temporal features, voltage integral features, capacity-current features, second-order derivative integral features, and peak voltage features:ùëÖùë°=[‚àÜùëáùëñ,ùê∏ùëñ,ùëÉi,ùõ§ùëñ,ùëâùëùùëíùëéùëò],Rt=‚àÜTi,Ei,Pi,Œìi,Vpeak,(10)and the input tensorùêºùëõùëùùë¢ùë°ùë°Inputtis designed consideringùëâùëùùëíùëéùëò,‚àëùê∏ùëñ,‚àëùëÉùëñùëñ,ùõ§ùëöùëéùëñùëõVpeak,‚àëEi,‚àëPii,Œìmain:ùêºùëõùëùùë¢ùë°ùë°=[ùëâùëùùëíùëéùëò,‚àëùê∏ùëñ,‚àëùëÉùëñùëñ,ùõ§ùëöùëéùëñùëõ],Inputt=Vpeak,‚àëEi,‚àëPii,Œìmain,(11)The feature extraction procedure used in this work follows widely validated practices in battery SOH modeling. Prior studies have shown that degradation-sensitive indicators, such as voltage‚Äìcurrent progression patterns, transition-region behaviors, curve-shape descriptors, and cumulative aging trends, provide strong predictive value for SOH estimation. Representative works include impedance-guided feature selection [45], optimization-based or statistical feature screening strategies [46,47], SHAP-based explainable feature ranking [48], and noise-robust feature refinement methods [49]. Moreover, our previous publications have applied SHAP-based interpretability analysis [50] and developed feature-selection algorithms that were peer-reviewed and experimentally validated [44]. Together, these works support the empirical and mechanistic validity of the feature set adopted in this study.",
            "2.2.1. Feature Extraction Process": "In the feature extraction process, we process two constant-current phases and one constant-current constant-voltage phase to implement feature extraction. Each phase is divided into a series of equally spaced voltage intervals, and samples are collected within these intervals. The sampling calculation is given as follows [44]:Œîùëâ=Vùëöùëéùë•‚àíVùëöùëñùëõùëìùë†,ŒîV=Vmax‚àíVminfs,(2)where‚ñ≥ùëâ‚ñ≥Vdefines the voltage interval,Vùëöùëéùë•‚àíVùëöùëñùëõVmax‚àíVminis the maximum voltage difference within the current voltage segment, andùëìùë†fsis the sampling frequency. It is noted that the calculation of the sampling frequency directly impacts the subsequent feature extraction process. Equations (3) and (4) determine the voltage range of the interval [44]:ùëâùë†ùë°ùëéùëüùë°ùëñ=Vùëöùëéùë•‚àí‚ñ≥ùëâ‚àóùëñ,Vistart=Vmax‚àí‚ñ≥V‚àói,(3)ùëâùëíùëõùëëùëñ=Vùëöùëéùë•‚àí‚ñ≥ùëâ‚àó(ùëñ+1),Viend=Vmax‚àí‚ñ≥V‚àói+1,(4)whereùëâùë†ùë°ùëéùëüùë°ùëñVistartandùëâùëíùëõùëëùëñViendrepresent the starting and ending voltages of thei-th voltage interval, respectively.",
            "2.2.2. Time Variations Within Equal Voltage Intervals": "The temporal differential during terminal charge/discharge cycles was disproportionately greater than that of initial cycles. Consequently, the inferable time difference within uniform voltage intervals serves as a direct indicator of inherent cell decay characteristics. Calculating these temporal variations enables finer-grained resolution of non-uniform aging patterns. To establish temporal boundaries for each interval, we derived the maximum and minimum voltage values cycle alongside their sampling frequencies. Subsequent feature extraction was then performed within these voltage-partitioned temporal blocks. The computation defining time variations across constant voltage intervals is expressed by the following formula:‚àÜùëáùëñ=ùë°ùëíùëõùëëùëñ‚àíùë°ùë†ùë°ùëéùëüùë°ùëñ,‚àÜTi=tiend‚àítistart,(5)whereùë°ùëíùëõùëëùëñtiendandùë°ùë†ùë°ùëéùëüùë°ùëñtistartrepresent the starting and ending times within the voltage interval, respectively, and‚àÜùëáùëñ‚àÜTiis the time difference. This feature focuses on the temporal dynamics that occur during the charging period.",
            "2.2.3. Cumulative Integral of the Voltage Variation": "The voltage curve exhibits distinct upward and leftward displacements accompanied by altered slope characteristics. During initial charging phases, voltage demonstrates a gradual elevation that attenuates near process completion. Crucially, instantaneous voltage readings alone provide insufficient battery state characterization. The cumulative integral methodology addresses this limitation by aggregating temporal voltage differentials, thereby amplifying minor fluctuations into discernible patterns that more accurately capture cell behavior. Comparative analysis shows that the terminal cycle‚Äôs cumulative integral is positioned superiorly and sinistrally to the initial cycle‚Äôs profile with marked magnitude deviation. Within this framework,Eirepresents the energy input during charging, defined as the cumulative integral of voltage across constant-voltage intervals between start and end times, minus the product of instantaneous voltage and temporal differential:ùê∏ùëñ=‚à´ùë°ùëíùëõùëëùëñùë°ùë†ùë°ùëéùëüùë°ùëñùëâ(ùë°)ùëëùë°‚àíùëâùë†ùë°ùëéùëüùë°ùëñ¬∑‚àÜùëáùëñ,Ei=‚à´tistarttiendV(t)dt‚àíVistart¬∑‚àÜTi,(6)whereùëâ(ùë°)V(t)is the voltage, and the initial pointùë°ùë†ùë°ùëéùëüùë°ùëñtistartand endpointùë°ùëíùëõùëëùëñtiendare determined by the time interval blocks of the first cycle.",
            "2.2.4. Horizontal Value of the Slope Peak": "During deep learning training, the accumulation of errors and noise amplification caused by integration operations can lead to gradient explosion, manifested as a sharp increase in network parameter gradients, uncontrolled weight updates, and ineffective representation. To address this issue, it is noticed that the peak position of the charging process continues to shift to the right with cyclic aging. This stable trend suggests that peak voltage (i.e., horizontal position) is a more reliable alternative feature than problematic area or peak height data. The specific calculation is as follows:Vùëùùëíùëéùëò=ùëéùëüùëîùëöùëéùë•(ùëëùëÑùëëùëâ)‚âàùëéùëüùëîùëöùëéùë•(ùëÑùëò+1‚àíùëÑùëòùëâùëò+1‚àíùëâùëò),Vpeak=argmaxdQdV‚âàargmaxQk+1‚àíQkVk+1‚àíVk,(7)whereùëÑQandVare the capacity and voltage, respectively.kandk+ 1 are time steps, and the variableùëâùëùùëíùëéùëòVpeakis used to quantify the horizontal value of the maximum slope of the battery voltage change curve, representing the maximum voltage increase rate during the charging process.",
            "2.2.5. Second-Order Differential Integral Feature": "The second-order differential integral feature is extracted to quantitatively characterize the subtle curvature changes in the voltage‚Äìcapacity curve. This feature serves as a highly sensitive indicator of degradation mechanisms such as lithium plating and solid electrolyte interphase growth. By integrating the second-order derivative, the feature captures the cumulative effect of these electrochemical shifts, providing a robust and noise-resistant metric that correlates strongly with capacity fade and internal resistance increase, making it a powerful predictor for state-of-health estimation. The specific calculation is as follows:Œìi=‚à´ùëâùëíùëõùëëùëñùëâùë†ùë°ùëéùëüùë°ùëñùëë2ùëâùëëùëÑ2dùëâ‚âà‚à´ùëâùëíùëõùëëùëñùëâùë†ùë°ùëéùëüùë°ùëñ(ùëëùëâùëëùëÑ)ùëò+1‚àí(ùëëùëâùëëùëÑ)ùëòùëÑk+1‚àíùëÑùëòdùëâ,Œìi=‚à´VistartViendd2VdQ2dV‚âà‚à´VistartVienddVdQk+1‚àídVdQkQk+1‚àíQkdV,(8)",
            "2.2.6. Total Charge Change Through the Circuit": "In the constant current CC-CV charging process, under constant current charging or fast charging strategies, the charging capacity curve shows significant differences due to differences in current rate. These differences reflect the variation in the battery‚Äôs available capacity over time and capture the actual change in its ability to store charge during charging and discharging. As the number of batteries used increases and the degree of aging intensifies, their effective capacity gradually declines, amplifying differences in charging capacity. By multiplying the difference in charging capacity at each stage by the corresponding current, the change in charge within a specific voltage range can be quantified, providing deeper insights into the battery‚Äôs charging behavior. The specific calculation formula is as follows:ùëÉùëñ=(ùëÑùëíùëõùëëùëñ‚àíùëÑùë†ùë°ùëéùëüùë°ùëñ)*ùêºùëñ,Pi=(Qiend‚àíQistart)*Ii,(9) The product of the current and capacity difference in this stage defines the energy increment obtained by the battery during the charging interval, corresponding to the total charge passing through the circuit during this process. Data from the target lithium-ion battery is acquired for feature extraction, transforming the raw measurements into a learnable tensor. For the time-sequential triplet sequence recorded from the processed data, a physically inspired feature expansion is performed based on the three original channels. To ensure that these heterogeneous channels share the same dimension and possess differentiable learning properties in subsequent deep networks, a trainable linear mapping is introduced. Each sampling point is transformed and subjected to layer normalization to obtain a token representation. The final feature vectorùëÖùë°Rtincludes temporal features, voltage integral features, capacity-current features, second-order derivative integral features, and peak voltage features:ùëÖùë°=[‚àÜùëáùëñ,ùê∏ùëñ,ùëÉi,ùõ§ùëñ,ùëâùëùùëíùëéùëò],Rt=‚àÜTi,Ei,Pi,Œìi,Vpeak,(10)and the input tensorùêºùëõùëùùë¢ùë°ùë°Inputtis designed consideringùëâùëùùëíùëéùëò,‚àëùê∏ùëñ,‚àëùëÉùëñùëñ,ùõ§ùëöùëéùëñùëõVpeak,‚àëEi,‚àëPii,Œìmain:ùêºùëõùëùùë¢ùë°ùë°=[ùëâùëùùëíùëéùëò,‚àëùê∏ùëñ,‚àëùëÉùëñùëñ,ùõ§ùëöùëéùëñùëõ],Inputt=Vpeak,‚àëEi,‚àëPii,Œìmain,(11) The feature extraction procedure used in this work follows widely validated practices in battery SOH modeling. Prior studies have shown that degradation-sensitive indicators, such as voltage‚Äìcurrent progression patterns, transition-region behaviors, curve-shape descriptors, and cumulative aging trends, provide strong predictive value for SOH estimation. Representative works include impedance-guided feature selection [45], optimization-based or statistical feature screening strategies [46,47], SHAP-based explainable feature ranking [48], and noise-robust feature refinement methods [49]. Moreover, our previous publications have applied SHAP-based interpretability analysis [50] and developed feature-selection algorithms that were peer-reviewed and experimentally validated [44]. Together, these works support the empirical and mechanistic validity of the feature set adopted in this study.",
            "2.3. Data Preprocessing and Normalization": "To ensure stable network training and consistent metric computation, all input features were subjected to Min‚ÄìMax normalization during preprocessing. For each battery cell, a separate scaler was fitted using only its training cycles, and all corresponding validation and test cycles were transformed using the same scaler to avoid data leakage. Each feature valueùúíœáwas mapped into the [0, 1] range as follows:ùúí‚àó=ùúí‚àíùúíùëöùëñùëõùúíùëöùëéùë•‚àíùúíùëöùëñùëõ,œá*=œá‚àíœáminœámax‚àíœámin,(12) This scaling removed the influence of heterogeneous physical units among voltage-, current-, and time-related indicators, thereby improving model convergence and robustness. The SOH labels were retained in their original physical range (typically 0.80‚Äì1.00), but underwent minor smoothing via a moving-average operation to suppress measurement noise. No normalization was applied to SOH during training; the model directly regressed the physical SOH value. For visualization and comparison, all predicted SOH values were multiplied by 100 during evaluation to express the results in percentages, following common practice in battery health studies. Importantly, metrics such as MAE, RMSE, and R2were calculated strictly on the restored physical-scale SOH values.",
            "3. Bagging-PiFormer Model Architecture Design": "The proposed PiFormer model is a bidirectional interaction framework, which allows the voltage and current features to mutually enhance each other via separate Q, K, and V projections. Architecturally, the model was composed of N stackable layers of lightweight Transformer blocks connected in series. Each layer adheres to the sequence of cross-channel attention, LocalConvFFN, and residual normalization, while incorporating specialized strategies tailored for multi-physics battery data within each sub-layer. Letùêª(ùëô)Hldenote the sequence tensor entering the l-th layer. 3.1. Cross-Channel AttentionThe voltage channel fromùëç(ùëô)Zlis linearly projected as the Query matrixùëÑ(ùëô)=ùëç(ùëô)ùëäùëÑQ(l)=Z(l)WQ. The current and voltage channels are concatenated along the feature dimension and then jointly projected as keys and values:ùêæ(ùëô)=[ùëç(ùëô)ùëÑ||ùëç(ùëô)ùëâ]ùëäùëò,ùëâ(ùëô)=[ùëç(ùëô)ùëÑ||ùëç(ùëô)ùëâ]ùëäùëâK(l)=ZQ(l)||ZV(l)Wk,V(l)=ZQ(l)||ZV(l)WV(13)where[‚ãÖ‚Äñ‚ãÖ]‚ãÖ‚Äñ‚ãÖdenotes vector concatenation. The motivation is the electrochemical principle of ‚Äúpotential first‚Äù: most decay indicators manifest first in the voltage curve, while the current-voltage coupling provides the mechanistic information causing these indicators. The output of single-head attention is as follows:ùê¥ùë°ùë°ùëõ(ùëô)=ùëÜùëúùëìùë°ùëöùëéùë•(ùëÑ(ùëô)(ùëô)‚ä§ùëëùëò‚àí‚àí‚àö)ùëâ(ùëô),Attnl=Softmax(Qll‚ä§dk)Vl,(14)The PiFormer employshheads in parallel, whose results are concatenated and projected back to the model width viaùëäùëúWo. 3.2. Bagging Strategy and Ensemble ConstructionTo enhance prediction stability and reduce variance, the Bagging-PiFormer framework constructs an ensemble of multiple PiFormer sub-models. Depending on the available computational resources, we train 10, 12, or 15 sub-models, each initialized with an independent random seed. Although explicit bootstrap sampling is not applied to the training data, the diversity introduced by randomized initialization provides a similar effect‚Äîeach sub-model learns a slightly different mapping, reflecting the core idea of classical bagging: introducing randomness, training models independently, and aggregating their predictions.After all sub-models are trained, their SOH predictions are stacked along a new dimension and averaged to obtain the final output:ùë¶ÃÇ(ùëõ)=1ùëÄ‚àëùëÄùëö=1ùë¶ÃÇùëö(ùëõ),y^n=1M‚àëm=1My^mn,(15)whereM‚àà {10, 12, 15} is the ensemble size. Since bootstrap sampling is not used, out-of-bag (OOB) estimates are not applicable. However, the mean and variance of the ensemble predictions naturally allow for estimation of 95% confidence intervals when required. 3.3. Local Convolutional Feed-Forward Network (LocalConvFFN)Following the attention mechanism, a depthwise-separable convolution feed-forward module (DSF) is designed. First, a depthwise convolutionùê∑ùëäùê∂ùëúùëõùë£3(‚Ñé)DWConv3hwith a kernel size of 3 is applied independently to each channel. The result is activated by the SiLU activation function and used as a gating factor, which then undergoes a Hadamard product with a 1 √ó 1 pointwise convolutionùëÉùëäùê∂ùëúùëõùë£(‚Ñé)PWConvh:ùê∑ùëÜùêπ(ùëô)(‚Ñé)=ùúé(ùê∑ùëäùê∂ùëúùëõùë£3(‚Ñé))‚äôùëÉùëäùê∂ùëúùëõùë£(‚Ñé),DSFlh=œÉ(DWConv3h)‚äôPWConvh,(16)whereùúéœÉis the SiLU activation function,‚äô‚äôis the element function product. This design retains the advantage of convolutions in extracting local morphological features (e.g., ‚Äúmicro-discharge platforms‚Äù) while reducing parameters to about 1/4 of a traditional feed-forward network.Pre-norm residual structure: The input is first passed through LayerNorm, then processed by the attention or DSF module, and finally added to the original input. This arrangement promotes gradient stability in deep networks. During training, dropout is applied after all projection matrices to suppress overfitting, and is removed during inference to save clock cycles. The recurrence is formulated as follows:ùëà(ùëô)=ùêª(ùëô)+ùê¥ùë°ùë°ùëõ(ùêøùëéùë¶ùëíùëüùëÅùëúùëüùëö(ùêª(ùëô))),U(l)=H(l)+AttnLayerNorm(H(l)),(17)ùêª(ùëô+1)=ùëà(ùëô)+ùê∑ùëÜùêπ(ùêøùëéùë¶ùëíùëüùëÅùëúùëüùëö(ùëà(ùëô))),H(l+1)=U(l)+DSFLayerNorm(U(l)),(18)AfterNiterations, the resultingùêª(ùëÅ)=[‚Ñé(ùëÅ)1,‚Ä¶,‚Ñé(ùëÅ)ùêø]H(N)=[h1(N),‚Ä¶,hL(N)]is the final hidden tensor, encapsulating Long-range and Short-range Coupling Information of I-V signals and local degradation features. To obtain a cycle-level global representation, average pooling is applied along the time dimension:ùëî(ùëõ)=1ùêø‚àëùêøùë°=1‚Ñé(ùëÅ)ùë°,ùëõ,gn=1L‚àët=1Lht,nN,(19)where the subscriptndenotes then-th cycle sample. Two linear output heads are then used:ùëÜùëÇùêª(ùëõ)=(ùêøùëñùëõùëíùëéùëü3‚àòùëÜùëñùêøùëà‚àòùê∑ùëüùëúùëùùëúùë¢ùë°‚àòùêøùëñùëõùëíùëéùëü2‚àòùëÜùëñùêøùëà‚àòùê∑ùëüùëúùëùùëúùë¢ùë°‚àòùêøùëñùëõùëíùëéùëü1)ùëî(ùëõ),SOH(n)=(Linear3‚àòSiLU‚àòDropout‚àòLinear2‚àòSiLU‚àòDropout‚àòLinear1)g(n),(20)where the output head uses a purely linear projection to regress the continuous SOH value, without any activation function.As shown inFigure 2, the model implements an end-to-end framework for battery SOH prediction based on the Bagging-random-Avg algorithm. Input features propagate through the Bagging framework, which distributes them to each PiFormer. Each PiFormer processes features via an embedding layer, followed by multiple PiFormer layers‚Äîeach incorporating a cross-channel attention mechanism and a LocalConvFFN. After layer normalization, an output head generates individual predictions. These predictions are averaged to produce the final output.Figure 2.Model structure diagram. 3.4. Training and Inference Procedure (Pseudocode)The overall training and inference process of Bagging-PiFormer is summarized in Algorithm 1.Algorithm 1:Training and Inference Procedure of Bagging-PiFormerData:Battery sequences from training cells; input window L; ensemble size M.Result:Final SOH prediction ≈∑.1Initialize global seed and model hyperparameters2Generate sliding-window sequences from training cells3Normalize features using per-cell MinMax scaling4Split data into training and validation sets (80%/20%)5for m = 1 to M do6Initialize PiFormer sub-model with random seed m7Train model with HybridLoss and early stopping8Save the best checkpoint of sub-model m9end for10for each test sequence do11Obtain M sub-model predictions {≈∑1, ‚Ä¶, ≈∑‚Çò}12Aggregate outputs by mean: ≈∑ = (1/M) Œ£ ≈∑·µ¢13end for",
            "3.1. Cross-Channel Attention": "The voltage channel fromùëç(ùëô)Zlis linearly projected as the Query matrixùëÑ(ùëô)=ùëç(ùëô)ùëäùëÑQ(l)=Z(l)WQ. The current and voltage channels are concatenated along the feature dimension and then jointly projected as keys and values:ùêæ(ùëô)=[ùëç(ùëô)ùëÑ||ùëç(ùëô)ùëâ]ùëäùëò,ùëâ(ùëô)=[ùëç(ùëô)ùëÑ||ùëç(ùëô)ùëâ]ùëäùëâK(l)=ZQ(l)||ZV(l)Wk,V(l)=ZQ(l)||ZV(l)WV(13)where[‚ãÖ‚Äñ‚ãÖ]‚ãÖ‚Äñ‚ãÖdenotes vector concatenation. The motivation is the electrochemical principle of ‚Äúpotential first‚Äù: most decay indicators manifest first in the voltage curve, while the current-voltage coupling provides the mechanistic information causing these indicators. The output of single-head attention is as follows:ùê¥ùë°ùë°ùëõ(ùëô)=ùëÜùëúùëìùë°ùëöùëéùë•(ùëÑ(ùëô)(ùëô)‚ä§ùëëùëò‚àí‚àí‚àö)ùëâ(ùëô),Attnl=Softmax(Qll‚ä§dk)Vl,(14) The PiFormer employshheads in parallel, whose results are concatenated and projected back to the model width viaùëäùëúWo.",
            "3.2. Bagging Strategy and Ensemble Construction": "To enhance prediction stability and reduce variance, the Bagging-PiFormer framework constructs an ensemble of multiple PiFormer sub-models. Depending on the available computational resources, we train 10, 12, or 15 sub-models, each initialized with an independent random seed. Although explicit bootstrap sampling is not applied to the training data, the diversity introduced by randomized initialization provides a similar effect‚Äîeach sub-model learns a slightly different mapping, reflecting the core idea of classical bagging: introducing randomness, training models independently, and aggregating their predictions. After all sub-models are trained, their SOH predictions are stacked along a new dimension and averaged to obtain the final output:ùë¶ÃÇ(ùëõ)=1ùëÄ‚àëùëÄùëö=1ùë¶ÃÇùëö(ùëõ),y^n=1M‚àëm=1My^mn,(15)whereM‚àà {10, 12, 15} is the ensemble size. Since bootstrap sampling is not used, out-of-bag (OOB) estimates are not applicable. However, the mean and variance of the ensemble predictions naturally allow for estimation of 95% confidence intervals when required.",
            "3.3. Local Convolutional Feed-Forward Network (LocalConvFFN)": "Following the attention mechanism, a depthwise-separable convolution feed-forward module (DSF) is designed. First, a depthwise convolutionùê∑ùëäùê∂ùëúùëõùë£3(‚Ñé)DWConv3hwith a kernel size of 3 is applied independently to each channel. The result is activated by the SiLU activation function and used as a gating factor, which then undergoes a Hadamard product with a 1 √ó 1 pointwise convolutionùëÉùëäùê∂ùëúùëõùë£(‚Ñé)PWConvh:ùê∑ùëÜùêπ(ùëô)(‚Ñé)=ùúé(ùê∑ùëäùê∂ùëúùëõùë£3(‚Ñé))‚äôùëÉùëäùê∂ùëúùëõùë£(‚Ñé),DSFlh=œÉ(DWConv3h)‚äôPWConvh,(16)whereùúéœÉis the SiLU activation function,‚äô‚äôis the element function product. This design retains the advantage of convolutions in extracting local morphological features (e.g., ‚Äúmicro-discharge platforms‚Äù) while reducing parameters to about 1/4 of a traditional feed-forward network. Pre-norm residual structure: The input is first passed through LayerNorm, then processed by the attention or DSF module, and finally added to the original input. This arrangement promotes gradient stability in deep networks. During training, dropout is applied after all projection matrices to suppress overfitting, and is removed during inference to save clock cycles. The recurrence is formulated as follows:ùëà(ùëô)=ùêª(ùëô)+ùê¥ùë°ùë°ùëõ(ùêøùëéùë¶ùëíùëüùëÅùëúùëüùëö(ùêª(ùëô))),U(l)=H(l)+AttnLayerNorm(H(l)),(17)ùêª(ùëô+1)=ùëà(ùëô)+ùê∑ùëÜùêπ(ùêøùëéùë¶ùëíùëüùëÅùëúùëüùëö(ùëà(ùëô))),H(l+1)=U(l)+DSFLayerNorm(U(l)),(18) AfterNiterations, the resultingùêª(ùëÅ)=[‚Ñé(ùëÅ)1,‚Ä¶,‚Ñé(ùëÅ)ùêø]H(N)=[h1(N),‚Ä¶,hL(N)]is the final hidden tensor, encapsulating Long-range and Short-range Coupling Information of I-V signals and local degradation features. To obtain a cycle-level global representation, average pooling is applied along the time dimension:ùëî(ùëõ)=1ùêø‚àëùêøùë°=1‚Ñé(ùëÅ)ùë°,ùëõ,gn=1L‚àët=1Lht,nN,(19)where the subscriptndenotes then-th cycle sample. Two linear output heads are then used:ùëÜùëÇùêª(ùëõ)=(ùêøùëñùëõùëíùëéùëü3‚àòùëÜùëñùêøùëà‚àòùê∑ùëüùëúùëùùëúùë¢ùë°‚àòùêøùëñùëõùëíùëéùëü2‚àòùëÜùëñùêøùëà‚àòùê∑ùëüùëúùëùùëúùë¢ùë°‚àòùêøùëñùëõùëíùëéùëü1)ùëî(ùëõ),SOH(n)=(Linear3‚àòSiLU‚àòDropout‚àòLinear2‚àòSiLU‚àòDropout‚àòLinear1)g(n),(20)where the output head uses a purely linear projection to regress the continuous SOH value, without any activation function. As shown inFigure 2, the model implements an end-to-end framework for battery SOH prediction based on the Bagging-random-Avg algorithm. Input features propagate through the Bagging framework, which distributes them to each PiFormer. Each PiFormer processes features via an embedding layer, followed by multiple PiFormer layers‚Äîeach incorporating a cross-channel attention mechanism and a LocalConvFFN. After layer normalization, an output head generates individual predictions. These predictions are averaged to produce the final output. Figure 2.Model structure diagram.",
            "3.4. Training and Inference Procedure (Pseudocode)": "The overall training and inference process of Bagging-PiFormer is summarized in Algorithm 1.Algorithm 1:Training and Inference Procedure of Bagging-PiFormerData:Battery sequences from training cells; input window L; ensemble size M.Result:Final SOH prediction ≈∑.1Initialize global seed and model hyperparameters2Generate sliding-window sequences from training cells3Normalize features using per-cell MinMax scaling4Split data into training and validation sets (80%/20%)5for m = 1 to M do6Initialize PiFormer sub-model with random seed m7Train model with HybridLoss and early stopping8Save the best checkpoint of sub-model m9end for10for each test sequence do11Obtain M sub-model predictions {≈∑1, ‚Ä¶, ≈∑‚Çò}12Aggregate outputs by mean: ≈∑ = (1/M) Œ£ ≈∑·µ¢13end for",
            "4. Experimental Results and Discussions": "This study used six performance indicators to systematically evaluate the model, including mean square error (MSE), root mean square error (RMSE), mean absolute error (MAE), mean absolute percentage error (MAPE), coefficient of determination (R2), and maximum error (MAXE). The calculation formulas are as follows:MSE=1ùëõ‚àëùëõùë°=1(ùëì(ùë•)ùë°‚àíùë¶ùë°)2,MSE=1n‚àët=1n(f(x)t‚àíyt)2,(21)RMSE=‚àëùëõùë°‚àí1(ùëì(ùë•)ùë°‚àíùë¶ùë°)2ùëõ‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö,RMSE=‚àët‚àí1nfxt‚àíyt2n,(22)MAE=1ùëõ‚àëùëõùë°=1|ùëì(ùë•)ùë°‚àíùë¶ùë°|,MAE=1n‚àët=1nf(x)t‚àíyt,(23)MAPE=1ùëõ‚àëùëõùëñ=1ÓÄÜÓÄÑÓÄÖÓÄÖùëì(ùë•)ùë°‚àíùë¶ùëñùë¶ùëñÓÄÜÓÄÑÓÄÖÓÄÖ,MAPE=1n‚àëi=1nf(x)t‚àíyiyi,(24)MAXE=ùëöùëéùë•(|ùëì(ùë•)ùë°‚àíùë¶ùë°|),MAXE=max(f(x)t‚àíyt),(25)R2=1‚àí‚àëùëõùëñ=1(ùë¶ùëñ‚àíùë¶ÃÇùëñ)2‚àëùëõùëñ=1(ùë¶ùëñ‚àíùë¶¬Ø)2,R2=1‚àí‚àëi=1n(yi‚àíy^i)2‚àëi=1n(yi‚àíy¬Ø)2,(26) The above equation includes symbols whereùë¶ùë°ytandùë¶ùëñyiare true values,ùëì(ùë•)ùë°f(x)tandùë¶ÃÇùëñy^iare predicted values,ùë¶¬Øy¬Øis the mean of the true values, andùëõnis the sample size.ùëõùëúùëüùëö(ùëãùëñ)=ùëãùëñ‚àíùëöùëñùëö(ùëãùëñ)ùëöùëéùë•(ùëãùëñ)‚àíùëöùëñùëõ(ùëãùëñ);ùëñ‚àà{1,‚Ä¶,ùëõ}norm(Xi)=Xi‚àímim(Xi)max(Xi)‚àímin(Xi);i‚àà{1,‚Ä¶,n}(27) The computation was performed on an experimental platform comprising an Intel(R) Xeon processor (Intel Corporation, Santa Clara, CA, USA), with two CPUs, 128-GB memory, and an NVIDIA RTX-4090D-24G GPU (NVIDIA Corporation, Santa Clara, CA, USA). 4.1. Data Splitting and Experimental ProtocolEach dataset follows a consistent train‚Äìvalidation‚Äìtest partitioning strategy. Datasets A and B contain 7 battery cells each, while Dataset C contains 12 cells (details inTable 1). For each dataset, the first five cells (indices 0‚Äì4) are used to construct the training set. A sliding window of length 24 is applied to generate input‚Äìoutput pairs from each cell, and 20% of the resulting sequences are randomly separated as the validation set. The remaining two cells are reserved exclusively for testing:Dataset A: Test Set 1 = B104; Test Set 2 = B107;Dataset B: Test Set 1 = XQ-14; Test Set 2 = XQ-17;Dataset C: Test Set 1 = B204; Test Set 2 = B211.A global random seed (seed = 0) is used for all random operations, including numpy.random, PyTorch CPU/GPU generators, CUDA kernels, data shuffling, and train/validation splitting. Deterministic CUDA settings (torch.backends.cudnn.deterministic = True) are also enabled to guarantee reproducibility. All models were implemented using PyTorch 2.2.0 (https://pytorch.org).To ensure a comprehensive evaluation, we perform a grid search over 72 hyperparameter configurations, combining the following:Three ensemble sizes (10, 12, 15 models);Four batch sizes (16, 32, 64, 128);Six learning rates (5 √ó 10‚àí5to 3 √ó 10‚àí4).Each configuration is trained for 300 epochs with early stopping (patience = 50). This results in 72 independent runs per dataset, each evaluated on both test cells, multiple noise-perturbed robustness scenarios (50 mV, 100 mV, 150 mV), and all ablation settings. All experiment results and hyperparameter configurations are automatically logged and saved for reproducibility. 4.2. Comparative ExperimentIn the comparison experiment, we compared the proposed model with existing technologies such as Transformer, CNN-LSTM, and GCN-BiLSTM. Six sets of battery data were used as training groups, while B104 and B107, B204 and B211, and XQ-14 and XQ-14 were assigned as testing groups, respectively. All models use the feature subset obtained fromSection 2.2as input.The evaluation index data for the SOH estimation results of different models are shown inTable 2.Figure 3presents the SOH estimation performance of different models, whileFigure 4shows the corresponding error distributions. A comprehensive analysis shows that the model proposed herein is significantly superior to other comparative models in all error indicators. Specifically, the R2 values of the model generally approach or exceed 0.95, demonstrating strong interpretability and data-fitting performance. Regarding the MAXE value, the proposed model consistently achieves the lowest, minimizing the maximum prediction error and reflecting superior stability.Figure 3.SOH estimation from different modes: (a) Dataset A (B104 and B107); (b) Dataset B (XQ-14 and XQ-17); and (c) Dataset C (B204 and B211).Figure 4.SOH estimation errors from different modes: (a) Dataset A (B104 and B107); (b) Dataset B (XQ-14 and XQ-17); and (c) Dataset C (B204 and B211).Table 2.Results analysis of the comparative experiment.As depicted, the Transformer and CNN-LSTM methods perform poorly in most tests, particularly under complex changes. This may be due to their weaker modeling ability in long-range dependencies compared to the GCN-BiLSTM model. Specifically, the MSE and MAXE of CNN-LSTM reach 9.790759 and 4.734474, respectively, indicating limited robustness to outliers. However, Transformer exhibits high values in most error metrics, including MSE, RMSE, MAE, MAPE, and MAXE, with significant prediction bias. The GCN-BiLSTM model outperforms CNN-LSTM, but does not surpass the proposed PiFormer method across all indicators. In the B204 and XQ-17 test cases, the R2 value of the GCN-BiLSTM model is relatively low, which reflects its insufficient explanatory performance under specific operating conditions.Overall, the proposed PiFormer model outperforms when facing complex operating conditions and different types of battery data. This model not only performs well in statistical error indicators but also has significant advantages in maintaining prediction accuracy and stability. In contrast, although other comparative methods perform fairly well in some test scenarios, their overall performance in multi-scenario environments is still insufficient, and their stability is generally inferior to the proposed model. 4.3. Noise ExperimentThis experiment investigates how noise levels influence the PiFormer model‚Äôs SOH estimation accuracy. Noises with varying amplitudes (50 mV, 100 mV, and 150 mV) are introduced into the data to simulate uncertainties under real-world battery operating conditions, enabling a systematic evaluation of the stability and robustness of the PiFormer model in the presence of noise. The statistical results of the SOH estimation across different models are shown inTable 3, withFigure 5presenting the SOH estimation performance of different models, andFigure 6showing the corresponding errors. The results demonstrate that, although increasing noise levels degrade predictive performance, the proposed model maintains a relatively high level of prediction accuracy for the batteries‚Äô SOH. Specifically, at noise levels of 50, 100, and 150 mV, the model exhibits prominent robustness, with predictive capability remaining largely unaffected despite escalating noise interference. These results confirm that the model has strong robustness when processing data with varying noise interference.Figure 5.SOH estimation results with different noises: (a) Dataset A (B104 and B107); (b) Dataset B (XQ-17 and XQ-14); and (c) Dataset C (B204 and B211).Figure 6.SOH estimation errors with different noises: (a) Dataset A (B104 and B107); (b) Dataset B (XQ-14 and XQ-17); and (c) Dataset C (B204 and B211).Table 3.Results analysis of the noise experiment.These noise experiments further validated the effectiveness and adaptivity of the model, indicating that it can effectively cope with various noise interferences encountered in practical applications. The results not only confirm the technical advantages of the model but also highlight its potential for application in challenging scenarios where data quality is susceptible to multiple factors. 4.4. Ablation ExperimentAblation experiments were carried out to analyze and validate the advantages of the proposed PiFormer model in estimating the SOH of LIBs. The proposed models removing BaggingRandom-Avg, LSTM-attention, and LocalConvFFN models are represented by A, B, and C, respectively, and validated using Dataset A. The proposed PiFormer model is used to evaluate the SOH.Figure 7shows the estimation results,Figure 8shows the estimation errors, andTable 4shows the evaluation metric values.Figure 7.SOH estimation in ablation experiments: (a) Dataset A (B104 and B107); (b) Dataset B (XQ-14 and XQ-17); and (c) Dataset C (B204 and B211).Figure 8.SOH estimation error in ablation experiments: (a) Dataset A (B104 and B107); (b) Dataset B (XQ-14 and XQ-17); and (c) Dataset C (B204 and B211).Table 4.Results analysis of the ablation experiment.Removing the BaggingRandom-Avg module from PiFormer resulted in only a slight decrease in R2 value, but other performance indicators showed significant fluctuations. For example, in the B104 dataset, MSE and RMSE increased from 0.703395 and 0.838686 to 2.751545 and 1.658778, respectively. This indicates that the BaggingRandom-Avg ensemble learning strategy is effective in improving the robustness of the model.After removing the cross-channel attention module from the PiFormer model, all evaluation indicators remain within a reasonable range; however, MSE, RMSE, and MAE show significant increases. For instance, in the XQ-17 battery, the MSE value increased by 48.95%. This result indicates that the cross-channel attention mechanism plays an irreplaceable role in the PiFormer architecture. This module effectively combines residual structure and attention mechanism to better capture the cross-channel dependencies and key features in sequence data, thereby significantly improving the model‚Äôs ability to characterize battery status.Removing the LocalConvFFN module from the PiFormer model would result in all six evaluation metrics exceeding the reasonable range. The results demonstrate that the module can effectively extract key information from battery features through multi-layer convolution and average pooling operations. The convolution module plays an important role in constructing rich feature representations and improving overall performance.These experiments analyzed the functional characteristics of each core component of PiFormer and verified their unique roles in improving model performance. These findings not only consolidate PiFormer‚Äôs advantages in estimating the SOH of LIBs but also provide a reliable basis for optimizing battery management and maintenance strategies.To evaluate the computational efficiency of Bagging-PiFormer, we report the training time, inference latency, and parameter scale of the ensemble. On a single NVIDIA GPU, the average training time per epoch is 88.14 s for both Test 1 and Test 2 configurations. The inference latency is 34.29 s in Test 1 and 33.87 s in Test 2. The total number of trainable parameters increases with the ensemble size: 17.04 million parameters for 10 models, 20.45 million for 12 models, and 25.56 million for 15 models. GPU memory consumption remained within practical limits during all experiments. These results indicate that, despite the ensemble structure, Bagging-PiFormer achieves a reasonable balance between prediction accuracy and computational cost.The performance improvements of Bagging-PiFormer are consistent across all datasets, evaluation metrics, hyperparameter configurations, and noise perturbation settings, demonstrating strong practical statistical significance.",
            "4.1. Data Splitting and Experimental Protocol": "Each dataset follows a consistent train‚Äìvalidation‚Äìtest partitioning strategy. Datasets A and B contain 7 battery cells each, while Dataset C contains 12 cells (details inTable 1). For each dataset, the first five cells (indices 0‚Äì4) are used to construct the training set. A sliding window of length 24 is applied to generate input‚Äìoutput pairs from each cell, and 20% of the resulting sequences are randomly separated as the validation set. The remaining two cells are reserved exclusively for testing: Dataset A: Test Set 1 = B104; Test Set 2 = B107;Dataset B: Test Set 1 = XQ-14; Test Set 2 = XQ-17;Dataset C: Test Set 1 = B204; Test Set 2 = B211. A global random seed (seed = 0) is used for all random operations, including numpy.random, PyTorch CPU/GPU generators, CUDA kernels, data shuffling, and train/validation splitting. Deterministic CUDA settings (torch.backends.cudnn.deterministic = True) are also enabled to guarantee reproducibility. All models were implemented using PyTorch 2.2.0 (https://pytorch.org). To ensure a comprehensive evaluation, we perform a grid search over 72 hyperparameter configurations, combining the following: Three ensemble sizes (10, 12, 15 models);Four batch sizes (16, 32, 64, 128);Six learning rates (5 √ó 10‚àí5to 3 √ó 10‚àí4). Each configuration is trained for 300 epochs with early stopping (patience = 50). This results in 72 independent runs per dataset, each evaluated on both test cells, multiple noise-perturbed robustness scenarios (50 mV, 100 mV, 150 mV), and all ablation settings. All experiment results and hyperparameter configurations are automatically logged and saved for reproducibility.",
            "4.2. Comparative Experiment": "In the comparison experiment, we compared the proposed model with existing technologies such as Transformer, CNN-LSTM, and GCN-BiLSTM. Six sets of battery data were used as training groups, while B104 and B107, B204 and B211, and XQ-14 and XQ-14 were assigned as testing groups, respectively. All models use the feature subset obtained fromSection 2.2as input. The evaluation index data for the SOH estimation results of different models are shown inTable 2.Figure 3presents the SOH estimation performance of different models, whileFigure 4shows the corresponding error distributions. A comprehensive analysis shows that the model proposed herein is significantly superior to other comparative models in all error indicators. Specifically, the R2 values of the model generally approach or exceed 0.95, demonstrating strong interpretability and data-fitting performance. Regarding the MAXE value, the proposed model consistently achieves the lowest, minimizing the maximum prediction error and reflecting superior stability. Figure 3.SOH estimation from different modes: (a) Dataset A (B104 and B107); (b) Dataset B (XQ-14 and XQ-17); and (c) Dataset C (B204 and B211). Figure 4.SOH estimation errors from different modes: (a) Dataset A (B104 and B107); (b) Dataset B (XQ-14 and XQ-17); and (c) Dataset C (B204 and B211). Table 2.Results analysis of the comparative experiment. As depicted, the Transformer and CNN-LSTM methods perform poorly in most tests, particularly under complex changes. This may be due to their weaker modeling ability in long-range dependencies compared to the GCN-BiLSTM model. Specifically, the MSE and MAXE of CNN-LSTM reach 9.790759 and 4.734474, respectively, indicating limited robustness to outliers. However, Transformer exhibits high values in most error metrics, including MSE, RMSE, MAE, MAPE, and MAXE, with significant prediction bias. The GCN-BiLSTM model outperforms CNN-LSTM, but does not surpass the proposed PiFormer method across all indicators. In the B204 and XQ-17 test cases, the R2 value of the GCN-BiLSTM model is relatively low, which reflects its insufficient explanatory performance under specific operating conditions. Overall, the proposed PiFormer model outperforms when facing complex operating conditions and different types of battery data. This model not only performs well in statistical error indicators but also has significant advantages in maintaining prediction accuracy and stability. In contrast, although other comparative methods perform fairly well in some test scenarios, their overall performance in multi-scenario environments is still insufficient, and their stability is generally inferior to the proposed model.",
            "4.3. Noise Experiment": "This experiment investigates how noise levels influence the PiFormer model‚Äôs SOH estimation accuracy. Noises with varying amplitudes (50 mV, 100 mV, and 150 mV) are introduced into the data to simulate uncertainties under real-world battery operating conditions, enabling a systematic evaluation of the stability and robustness of the PiFormer model in the presence of noise. The statistical results of the SOH estimation across different models are shown inTable 3, withFigure 5presenting the SOH estimation performance of different models, andFigure 6showing the corresponding errors. The results demonstrate that, although increasing noise levels degrade predictive performance, the proposed model maintains a relatively high level of prediction accuracy for the batteries‚Äô SOH. Specifically, at noise levels of 50, 100, and 150 mV, the model exhibits prominent robustness, with predictive capability remaining largely unaffected despite escalating noise interference. These results confirm that the model has strong robustness when processing data with varying noise interference. Figure 5.SOH estimation results with different noises: (a) Dataset A (B104 and B107); (b) Dataset B (XQ-17 and XQ-14); and (c) Dataset C (B204 and B211). Figure 6.SOH estimation errors with different noises: (a) Dataset A (B104 and B107); (b) Dataset B (XQ-14 and XQ-17); and (c) Dataset C (B204 and B211). Table 3.Results analysis of the noise experiment. These noise experiments further validated the effectiveness and adaptivity of the model, indicating that it can effectively cope with various noise interferences encountered in practical applications. The results not only confirm the technical advantages of the model but also highlight its potential for application in challenging scenarios where data quality is susceptible to multiple factors.",
            "4.4. Ablation Experiment": "Ablation experiments were carried out to analyze and validate the advantages of the proposed PiFormer model in estimating the SOH of LIBs. The proposed models removing BaggingRandom-Avg, LSTM-attention, and LocalConvFFN models are represented by A, B, and C, respectively, and validated using Dataset A. The proposed PiFormer model is used to evaluate the SOH.Figure 7shows the estimation results,Figure 8shows the estimation errors, andTable 4shows the evaluation metric values. Figure 7.SOH estimation in ablation experiments: (a) Dataset A (B104 and B107); (b) Dataset B (XQ-14 and XQ-17); and (c) Dataset C (B204 and B211). Figure 8.SOH estimation error in ablation experiments: (a) Dataset A (B104 and B107); (b) Dataset B (XQ-14 and XQ-17); and (c) Dataset C (B204 and B211). Table 4.Results analysis of the ablation experiment. Removing the BaggingRandom-Avg module from PiFormer resulted in only a slight decrease in R2 value, but other performance indicators showed significant fluctuations. For example, in the B104 dataset, MSE and RMSE increased from 0.703395 and 0.838686 to 2.751545 and 1.658778, respectively. This indicates that the BaggingRandom-Avg ensemble learning strategy is effective in improving the robustness of the model. After removing the cross-channel attention module from the PiFormer model, all evaluation indicators remain within a reasonable range; however, MSE, RMSE, and MAE show significant increases. For instance, in the XQ-17 battery, the MSE value increased by 48.95%. This result indicates that the cross-channel attention mechanism plays an irreplaceable role in the PiFormer architecture. This module effectively combines residual structure and attention mechanism to better capture the cross-channel dependencies and key features in sequence data, thereby significantly improving the model‚Äôs ability to characterize battery status. Removing the LocalConvFFN module from the PiFormer model would result in all six evaluation metrics exceeding the reasonable range. The results demonstrate that the module can effectively extract key information from battery features through multi-layer convolution and average pooling operations. The convolution module plays an important role in constructing rich feature representations and improving overall performance. These experiments analyzed the functional characteristics of each core component of PiFormer and verified their unique roles in improving model performance. These findings not only consolidate PiFormer‚Äôs advantages in estimating the SOH of LIBs but also provide a reliable basis for optimizing battery management and maintenance strategies. To evaluate the computational efficiency of Bagging-PiFormer, we report the training time, inference latency, and parameter scale of the ensemble. On a single NVIDIA GPU, the average training time per epoch is 88.14 s for both Test 1 and Test 2 configurations. The inference latency is 34.29 s in Test 1 and 33.87 s in Test 2. The total number of trainable parameters increases with the ensemble size: 17.04 million parameters for 10 models, 20.45 million for 12 models, and 25.56 million for 15 models. GPU memory consumption remained within practical limits during all experiments. These results indicate that, despite the ensemble structure, Bagging-PiFormer achieves a reasonable balance between prediction accuracy and computational cost. The performance improvements of Bagging-PiFormer are consistent across all datasets, evaluation metrics, hyperparameter configurations, and noise perturbation settings, demonstrating strong practical statistical significance.",
            "5. Conclusions": "This paper presents a Bagging-PiFormer model for lithium-ion battery SOH estimation that is designed to overcome the limitations of single-model architectures and insufficient feature coupling in existing deep-learning approaches. The proposed framework employs an ensemble of improved Transformer-based PiFormer models to enhance robustness, while each PiFormer incorporates a cross-channel attention mechanism and a LocalConvFFN to achieve dynamic voltage‚Äìcurrent interaction modeling and local temporal feature extraction. Comprehensive experiments‚Äîincluding comparative analyses, noise-interference testing, and ablation studies‚Äîdemonstrated that the Bagging-PiFormer consistently achieved the highest estimation accuracy and stability across multiple battery datasets. The model maintained R2values above 0.963 and MAXE below 1.829 under various conditions, confirming its strong resistance to noise and outstanding generalization across different cell types. Future research will incorporate SHAP-based feature attribution, attention-map visualization, and physical‚Äìelectrochemical interpretability analysis to further elucidate how Bagging-PiFormer associates specific temporal regions with degradation mechanisms such as lithium inventory loss, impedance growth, and voltage-plateau evolution. These extensions will enhance the transparency and mechanistic insight of the proposed framework."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2313-0105/11/12/447",
        "scraped_at": "2025-12-05 23:53:25"
    },
    {
        "title": "Wastewater-Derived Microplastics as Carriers of Aromatic Organic Contaminants (AOCs): A Critical Review of Ageing, Sorption Mechanisms, and Environmental Implications",
        "authors": "byZuzanna PrusandKatarzyna Styszko",
        "journal": "Int. J. Mol. Sci.2025,26(23), 11758;https://doi.org/10.3390/ijms262311758- 4 Dec 2025",
        "abstract": "Wastewater-derived microplastics (WW‚ÄìMPs) are increasingly recognised as reactive vectors for aromatic organic contaminants (AOCs), yet their role in contaminant fate remains insufficiently constrained. This review synthesises current knowledge on the transformation of microplastics in wastewater treatment plants, including fragmentation, oxidative ageing, additive leaching, and biofilm formation, and links these processes to changes in sorption capacity toward phenols, PAHs and their derivatives, and organochlorine pesticides (OCPs). We summarise the dominant adsorption mechanisms‚Äîhydrophobic partitioning, œÄ‚ÄìœÄ interactions, hydrogen bonding, and electrostatic and, in some cases, halogen bonding‚Äîand critically evaluate how wastewater-relevant parameters (pH, ionic strength, dissolved organic matter, temperature, and biofilms) can modulate these interactions. Evidence in the literature consistently shows that ageing and biofouling enhance WW‚ÄìMP affinity for many AOCs, reinforcing their function as mobile carriers. However, major gaps persist, including limited data on real wastewater-aged MPs, lack of methodological standardisation, and incomplete representation of ageing, competitive sorption, and non-equilibrium diffusion in existing isotherm and kinetic models. We propose key descriptors that should be incorporated into future sorption and fate frameworks and discuss how WW‚ÄìMP‚ÄìAOC interactions may influence ecological exposure, bioavailability, and risk assessment. This critical analysis supports more realistic predictions of AOC behaviour in wastewater environments.Keywords:microplastics;wastewater treatment;aromatic organic contaminants;sorption mechanisms;ageing processes;biofouling;environmental transport",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "In the 21st century, societies have become strongly dependent, and in many ways even addicted, to plastic materials in every form [1]. Extensive global use of plastic materials across all economic sectors, together with insufficient recycling rates, has led to the widespread accumulation of plastic litter in natural environments [2,3]. Particular attention is given to small polymer fragments known as microplastics (MPs), referred to as less than 5 mm in size. These include primary MPs, produced on a microscale as additives to cosmetic products or textiles, and more prevalent secondary MPs, which originate from the degradation of polymer wastes under environmental conditions, including transformations during wastewater treatment [4,5,6]. The physical presence of MPs themselves raises growing concerns about their potential impact on human health [7]. Once MPs enter the environment, they partially degrade over time and their surfaces become altered by mechanical abrasion, ultraviolet (UV) radiation, and microbial activity, thereby enhancing their pollutant-binding capacity [4]. While littering of marine shores contributes most to environmental plastic pollution, wastewater treatment plants (WWTPs) are considered a significant pathway through which altered wastewater-derived microplastics (WW‚ÄìMPs), with unknown surface characteristics, are released into natural water bodies [8] or re-released into soil via fertilisers derived from treated sewage sludge [9,10]. WW‚ÄìMPs can act as carriers for various co-contaminants. Owing to their hydrophobic and heterogeneous surfaces, they serve as effective adsorbates of co-occurring pollutants [11,12]. Among them, aromatic organic contaminants (AOCs) are a ring-structured subgroup increasingly detected in wastewater systems [10]. Many AOCs are mutagenic and carcinogenic, are persistent in the environment, and can bioaccumulate in living organisms [13,14]. The abundance of WW‚ÄìMPs is also substantial. In a previous review, the author evaluated 53 studies covering 63 sludge samples worldwide and reported MP concentrations ranging from <1 to 240 particles/g dw (dry weight) [15]. For this reason, particular attention should be given to the sorption interactions between AOCs and WW‚ÄìMPs. These interactions are environmentally relevant, as they influence the risk of AOC transfer, re-release, and substantial exposure into natural systems. An analysis of publication trends based on the ScienceDirect database in September 2025 revealed a marked increase in research output over the past decade (Figure 1a). To date, 37,388 articles on microplastics have been published, as shown by the broad orange line on the graph. The search term ‚Äúmicroplastics‚Äù with the keyword ‚Äúwastewater‚Äù returned 3572 articles (9.55% of TN on MP topic), with a sharp rise in the number of publications after 2018, and 917 papers published in 2025. Changing the keyword to ‚Äúadsorption‚Äù resulted in 2877 articles (7.69% of TN on MP topic), with consistent growth from 2015 onward, exceeding 700 papers so far in 2025. When including both keywords, 680 articles (1.88% of TN on MP topic) were found from 2018 onward, with an upward trend, but these numbers are approximately two times lower than the previous ones. Figure 1.(a) Total number (TN) of publications containing ‚Äúmicroplastic‚Äù (orange) and those combining it with wastewater (blue), adsorption (green), and wastewater + adsorption (red); (b) publications for the same keyword sets, expressed as a percentage of the TN for each group. The X-axis shows the combined search terms, and the Y-axis shows their proportional shares‚Äîdata adapted from ScienceDirect. With growing awareness of MPs‚Äô significance in the environment, an increasing number of studies have demonstrated sorption interactions between MPs and AOCs, PAHs, OH-PAHs, N-PAHs, phenols, and organochlorinated pesticides (OCPs), as depicted inFigure 1b. The ‚Äúmicroplastic‚Äù search phrase, combined with the wastewater keyword, accounted for approximately 28% of studies related to general aromatics; about 53% addressed aromatics; and reached 60% for wastewater and adsorption. Based on the analysis, the studied sorption interactions with MPs (red bars) can be ranked from best to worst as follows: phenols > PAHs > OCPs > N-PAHs > OH-PAHs. This distribution highlights that interactions between PAH derivatives and OCPs, as well as their fate in WW‚ÄìMPs, remain insufficiently studied, despite the growing trend in general MP research. This review examines the role of wastewater-derived microplastics (WW‚ÄìMPs) as vectors of aromatic organic contaminants (AOCs), with a particular focus on phenols, polycyclic aromatic hydrocarbons (PAHs), and their hydroxylated (OH-PAHs) and nitrated (N-PAHs) derivatives, as well as organochlorinated pesticides (OCPs), which are increasingly reported in wastewater streams. The scope of this review includes the occurrence and physicochemical characteristics of WW‚ÄìMPs, as well as the key sorption mechanisms governing MP-AOC complexes under the influence of environmental factors. Reviewed modelling studies in this field that rely on real, environmental MPs remain limited; therefore, sorption modelling in simplified systems under controlled conditions is often used as a proxy and is also discussed in this article. Recommendations for further improvement to modelling AOC‚ÄìWW‚ÄìMP sorption are discussed.",
            "2. Aromatic Organic Contaminants (AOCs) in Wastewater": "Aromatic organic contaminants (AOCs) are a subclass of environmental contaminants characterised by the presence of at least one aromatic ring in their molecular structure. Currently, the key AOCs identified in wastewater include polycyclic aromatic hydrocarbons (PAHs); their nitrated (N-PAHs) and hydroxylated (OH-PAHs) derivatives; phenolic compounds such as phenol, bisphenol A (BPA), and alkylphenols; and organochlorinated pesticides (OCPs), including Dichlorodiphenyltrichloroethane (DDT), Dichlorodiphenyldichloroethylene (DDE), chlordane, and lindane [16,17,18,19]. These groups originate from a wide range of anthropogenic activities: PAHs and their derivatives arise mainly from fuel combustion and industrial thermal processes; phenolic compounds are derived from plastic manufacturing, detergents, and consumer products; and OCPs reflect historical and ongoing agricultural applications [20,21]. Their presence in wastewater therefore represents the combined impact of industrial emissions, agricultural inputs, urban runoff, and domestic chemical use. The polarity of AOCs governs hydrophobicity and solubility, and thereby strongly influences affinity toward WW‚ÄìMPs. On this basis, they can be grouped into three categories, which are depicted inFigure 2: hydrophobic (non-polar), moderately polar, and hydrophilic (polar) compounds. Figure 2.Classification of AOCs by physicochemical properties. Hydrophobic AOCs, including generally high-molecular-weight PAHs (HMW-PAHs) and OCPs, characteristically exhibit high octanol‚Äìwater partition coefficients (log K‚Çíw) and considerably low aqueous solubility, which enhances the drive for sorption onto suspended WW‚ÄìMPs [22]. However, adsorption prediction cannot rely solely on K‚Çíw, as compound-specific traits (size, charge, aromaticity), solution chemistry, and MP surface properties also strongly influence adsorption [23]. In general, less polar AOCs, including low-molecular-weight PAHs (LMW-PAHs), cresols, aromatic amines, and benzoic acid derivatives, represent lower values of log K‚Çíw and contain functional groups such as hydroxyl (-OH) or amino (-NH2) groups. Their increased water affinity enables interactions with WW‚ÄìMPs mainly via relatively weak hydrogen bonding and œÄ‚ÄìœÄ interactions [24]. Finally, hydrophilic AOCs, e.g., nitrophenols, aromatic carboxylic acids, and selected pharmaceuticals, are highly soluble, exhibit the lowest log K‚Çíw, and occur in the aqueous phase, often in ionised form, and among all AOCs, they show the weakest sorption to MPs [25]. Their environmental half-life parameter (t1/2), defined as the time required for 50% degradation, also varies substantially among compound classes. AOCs‚Äô parameters and their corresponding physicochemical patterns are summarised inTable 1. Highly hydrophobic AOCs preferentially partition into the sludge phase, resulting in notable accumulation that can appear as mass gains during treatment. Seasonal effects further modulate this behaviour: In summer, longer sludge retention and reduced biodegradation promote the build-up of LMW-PAHs and their hydroxy derivatives, such as naphthols [26]. In contrast, lower N-PAH concentrations in winter wastewaters indicate more efficient removal at low temperatures, consistent with the behaviour reported for HMW-PAHs [27]. Overall, only a limited portion of the total PAHs is removed via biodegradation, biotransformation, or volatilisation. Secondary formation processes, especially the conversion of parent PAHs to their metabolites, account for much of the observed mass increase [26,27]. Therefore, sewage sludge serves as a long-term storage matrix in which both hydrophobic AOCs and WW‚ÄìMPs accumulate, increasing the potential for sustained sorption and joint environmental release. Table 1.A comparative study of key physicochemical properties, removal, and biodegradability of major wastewater AOCs in average ranges. Table 1.A comparative study of key physicochemical properties, removal, and biodegradability of major wastewater AOCs in average ranges.AOC Classlog K‚ÇíwWater Solubility (25 ¬∞C), mg/LpKa *Removal Mechanism and Efficiency (2000‚ÄìPresent)Reported Environmental Half-LivesReferencesPAHs3.3‚Äì6.7<0.001‚Äì30>25‚Äì40sorption to activated sludge and dissolved organic matter (37‚Äì99%, Europe)>3‚Äì180 d (surface water)30‚Äì360 d (soils)17‚Äì126 d (sewage sludge)135 d‚Äì4.4 years (lake sediments)[18,28,29,30,31]OH-PAHs2.7‚Äì5.630.09‚Äì8667‚Äì10sorption to activated sludge and dissolved organic matter (75‚Äì99%, Europe)<5.2 d (surface water)[32]N-PAHs2.3‚Äì5.740.003‚Äì88~5‚Äì6 (basic), or non-ionisablesorption to activated sludge and dissolved organic matter (37‚Äì83%, China)4‚Äì60 d (surface water)40‚Äì360 (soil)200 d‚Äì3.5 years (lake sediments)[27,31]Phenols1.46‚Äì4.21<30,000 (substituted phenol derivatives),80,000 (unsubstituted phenols)5‚Äì11Biodegradation during activated sludge process or trickling filters (69‚Äì100%, Africa)10‚Äì30 d (surface water)1‚Äì2 d (soil)[33,34,35]OCPs4.5‚Äì6.960.025‚Äì2000non-ionisablesorption to activated sludge and dissolved organic matter (37‚Äì100%, Europe)1 d‚Äì30 years (surface water)[36,37,38,39]* pKa Data Compiled by R. Williams. ACS Division of Organic Chemistry,https://organicchemistrydata.org/hansreich/resources/pka/pka_data/pka-compilation-williams.pdf, updated 7 April 2022, assessed 25 November 2025. A clear trend emerges: Substances with a high log K‚Çíw and low solubility, such as PAHs and some OCPs, are consistently removed from wastewater by sorption rather than microbial degradation, leading to their accumulation in sludge and potential for deposition on WW‚ÄìMP surfaces. Moderately polar AOCs show mixed behaviour, with partial microbial transformation but persistent particulate-bound fractions. In contrast, the most soluble and polar phenols, as well as several PAH derivatives, remain mainly in the dissolved phase and are more susceptible to biodegradation, resulting in substantially higher removal efficiencies. The balance between hydrophobicity, solubility, and biodegradability determines the behaviour of selected AOCs in wastewater and explains the likelihood of partitioning onto WW‚ÄìMP surfaces.",
            "3. Wastewater-Derived Microplastics (WW‚ÄìMPs): Transformation and Ageing": "The wastewater stream serves as a significant transport pathway for MPs entering the natural environment. WW‚ÄìMPs occur in various morphotypes, including fibres, fragments, films, spheres, and foams, and may be partially or entirely degraded during treatment [40]. In general, the most frequently reported polymer types in WWTPs globally include low- and high-density polyethylene (LDPE, HDPE), polypropylene (PP), polystyrene (PS), and polyethylene terephthalate (PET), with polycarbonate (PC) also detected in smaller amounts [41]. Their environmentally altered surfaces are shown inTable 2. Table 2.Examples of aged surfaces of common WW‚ÄìMPs found in the wastewater or sewage sludge. Adapted from Refs. [42,43] under the terms of the Creative Commons Attribution (CC BY 4.0) Licence (https://creativecommons.org/licenses/by/4.0/). Reported particle sizes for WW‚ÄìMPs range from a few micrometres to 5 mm, with colours including white, black, brown, transparent, and multicoloured [44,45]. The concentrations of WW‚ÄìMPs in wastewater influents and effluents vary considerably, depending on factors such as the size of the served population, the characteristics of the catchment area, and the presence of point sources, including industrial discharges, which may contribute additional loads. Effluent MPs are typically smaller than influent particles due to both preferential removal of larger particles and in-plant fragmentation. Larger WW‚ÄìMPs (>500 ¬µm) are more efficiently removed, while smaller fractions (<250 ¬µm, often <150 ¬µm) dominate the final discharge as they can more easily flow through [46]. The latest studies by Nematollahi M.J. et al. (2025) and Alasvand S. et al. (2023) reported WW‚ÄìMP levels in WWTP influent ranging from ~250 MPs/L in urban plants in Tabriz to over 700 MPs/L in Qom, Iran [47,48]. Okoffo et al. (2023) found higher particle concentrations in the influent of an Australian municipal WWTP, ranging from 840 to 3116 ¬µg/L [49]. To illustrate the reduction in abundance after treatment, a study by Miserli K. et al. (2025) in Ioannina, Greece, reported 5.8 ¬± 0.6 MPs/L [42]. The reported mass concentration can range from about 0.71 to 1.75 ¬µg/L in China and from 0.5 to 11.9 ¬µg/L in Denmark [50,51]. Studies indicate an average efficiency of 90% in reducing WW‚ÄìMPs from the main wastewater stream, which are retained in sewage sludge [15]. From here, depending on the treatment system (aerobic or anaerobic digestion), they may persist for up to 30 days before further management. In addition to heavy metals and other inorganic matter, sewage sludge also tends to accumulate AOCs, and during the relatively long stabilisation process, chemical and physical interactions with retained WW‚ÄìMPs occur [52]. The WW‚ÄìMP load composition discharged from WWTPs reflects both the dominant types of plastics in use and the specific point sources within the catchment area. Nevertheless, substantial local variations in the MP fingerprint of the effluent may occur, driven by differences in polymer composition, particle size, and specific morphology. Consequently, they exhibit diverse physicochemical properties that influence their mobility and interactions with co-occurring contaminants. In Greece, PA, polyacrylic acid (PAA), and a polyacrylamide-co-polyacrylic acid copolymer (PAM-co-PAA) were detected in 100% of sewage samples, reflecting a persistent local source. A high abundance of PVC, PE, PS, and PP was also detected there in 20‚Äì80% of samples [42]. Similarly, a study of a large WWTP in Northern Italy found that the effluent WW‚ÄìMPs were predominantly polyesters (35%) and PA (17%) [53]. A global review by Yaseen A. et al. (2022) of 121 WWTPs found PE (22%), PS (21%), and PP (13%) to be the most frequently detected polymers in final effluents, with similar findings at national and regional scales [40]. Another review by Acarer A. et al. (2024) in Turkey also identified polyvinyl chloride (PVC) and polyamide (PA) as important polymers that could be detected easily in wastewater [54]. These studies confirm the occurrence of the most common microplastic polymers identified in recent EU reports [55]. Industrial streams generally carry significantly higher concentrations of WW‚ÄìMPs; however, they exhibit limited characteristics specific to the process and waste generated [16,17,18]. Their effluents often have an elevated chemical oxygen demand (COD) and relatively low biochemical oxygen demand (BOD), with a lower COD/BOD ratio indicating more limited biodegradability and reflecting the presence of organic compounds resistant to degradation, including WW‚ÄìMPs [56,57]. Case studies comparing different wastewater streams provide clear evidence: In C√°diz, Spain, effluent from a municipal WWTP contained 16.4 MPs/L, whereas effluent from a nearby industrial WWTP contained 131.35 MPs/L, an eight-fold increase [58]. A study in Xiamen, China, showed that industrial wastewater carried more than two times the MP load of domestic sewage, resulting in a 3.2-fold increase in the annual discharge flux [59]. A study from Vietnam confirmed this pattern, with industrial wastewater containing ~60.9 MPs/L, compared with ~31.5 MPs/L in domestic streams and ~35.5 MPs/L in medical wastewater, respectively [60]. A recent meta-analysis further supports the conclusion that WWTPs receiving substantial industrial inputs exhibit influent WW‚ÄìMPs concentrations that are significantly higher than those of municipal systems alone [61]. Evaluation of the industrial process and quantitative characterisation enable approximate predictions of WW‚ÄìMP loads, allowing adjustments to the treatment process to minimise emissions. However, exact estimates are not possible for WW‚ÄìMPs of municipal origin due to the specific location of WWTPs, greater variability in wastewater chemical composition, and, notably, the direct unsuitability of municipal WWTPs for targeted removal of this type of solid matter. Beyond abundance, industrial WW‚ÄìMPs also differ in morphology, polymer composition, and predictable surface condition, all of which are critical determinants of resultant condition and sorption affinity for AOCs. In Xiamen, MPs from industrial effluents were predominantly fibrous with markedly rougher surface textures, whereas MPs from domestic wastewater were mainly granular [59]. Industrial streams were enriched in polyester (PES) and PET particles‚Äîpolymers associated with textile and manufacturing activities. This source-specific enrichment is supported by findings from Guangzhou, where the number of textile factories in a WWTP catchment was positively correlated with total MP release [62]. Fibrous MPs typically present larger surface-area-to-volume ratios, more oxygen-containing functional groups after oxidative ageing, and higher affinity for biofilm colonisation than more granular domestic MPs [63,64]. As a result, industrial WW‚ÄìMPs are more likely to develop heterogeneous, chemically active surfaces that bind AOCs more effectively. Control over the general MP cycle is further complicated by the use of treated sewage sludge (biosolids) as fertilisers, whose MP loads are often unknown, and which re-enter the environment with associated co-micropollutant loads [65,66]. 3.1. Morphological AlterationsWW‚ÄìMP size reduction, altered surface roughness, and partial fragmentation are commonly due to the high shear stress from turbulence induced by continuous mixing or aeration during treatment. This generates forces that mechanically abrade particles‚Äô surfaces, reducing their total mass and increasing the abundance of smaller WW‚ÄìMP fractions [67]. Gravity-based separation processes, such as primary sedimentation, preferentially remove denser, more compact WW‚ÄìMP shapes, such as spheres and fragments, which readily settle into sludge [68]. High-density polymers, including PVC (d = 1.30‚Äì1.45 g cm‚àí3) and PET (d = 1.33‚Äì1.38 g cm‚àí3), go through a similar process [69,70]. In contrast, fibres and low-density polymers, such as PE (d = 0.91‚Äì0.96 g cm‚àí3) and PP (d = 0.83‚Äì0.85 g cm‚àí3), with their high aspect ratio and generally low density, tend to remain suspended, resulting in their high proportion in treated effluents [71,72]. As a result, final effluents are typically enriched in fibrous, low-density plastics, though some biofilm-covered WW‚ÄìMPs may also escape removal [73]. Therefore, evaluating WW‚ÄìMP removal efficiency should consider both mass concentration and particle size distribution. 3.2. Chemical AlterationsDuring treatment, WW‚ÄìMPs are subjected to conditions that promote measurable physicochemical changes relevant to their sorption behaviour. One of the most critical operational parameters is hydraulic retention time (HRT), which allows extended contact between WW‚ÄìMPs and AOCs. HRT typically ranges from several hours to about 1 day, during which surface abrasion and microcrack formation extend, thereby increasing the number of accessible sorption sites to contaminants. The typical WWTP operational pH range of 6.5‚Äì8.5 facilitates the hydrolysis of condensation polymers such as PET, leading to chain scission and the formation of -COOH- and -OH-containing end groups [74]. These modifications alter surface polarity and can enhance interactions with AOCs [75]. Similarly, exposure to aerobic conditions (up to 2 mg/L in typical WWTP) promotes oxidative modification of WW‚ÄìMPs surfaces, whereas reduced oxygen zones favour the onset of bacterial deposition [76]. WW‚ÄìMPs are further exposed to coagulants, surfactants, and more oxidants (e.g., chlorine, ozone), which can further alter surface charge, facilitating subsequent sorption of AOCs [77,78].The increase in reactivity of damaged (aged) MP surfaces was demonstrated by Fernando S. et al. (2007) for polyolefin PE, PP, and PS fragments, which frequently acquire oxidised aromatic structures that strengthen interactions with AOCs [79]. The oxidative ageing of polymer surfaces was thoroughly analysed by Duan J. et al. (2022), who reported that reactive oxygen species (ROS), including hydroxyl radicals (‚Ä¢OH), singlet oxygen (1O2), and superoxide (O2‚Ä¢‚àí), generated during chlorination, ozonation, and microbial treatment, are key drivers of polymers‚Äô transformations in WWTPs [72]. ROS ‚Äúattack‚Äù polymer backbones and side groups, leading to chain scission and the formation of oxygen-containing functional groups. In polyolefins, this results in increased surface polarity and roughness, while in aromatic polymers, it promotes aromatic ring oxidation and the formation of quinone-like structures. For condensation polymers, ROS accelerate the cleavage of ester and carbonate bonds, thereby enhancing hydrolysis [72]. As a result, the sorption behaviour of WW‚ÄìMPs toward AOCs shifts as the affinity for highly hydrophobic compounds tends to decrease, whereas moderately polar aromatic compounds can exhibit enhanced sorption due to the formation of specific polar and aromatic interactions.Table 3summarises chemical transformations of common WW‚ÄìMPs that can occur during wastewater treatment and their potential effects on sorption properties toward AOCs. These wastewater-induced modifications generally reduce polymers‚Äô hydrophobicity, mechanically degrade particles, creating cracks and pores, and broaden MPs‚Äô sorption capacity by increasing their specific surface area [80].Table 3.Reported wastewater-induced chemical transformations of common polymers, excluding biofilm effect.Table 3.Reported wastewater-induced chemical transformations of common polymers, excluding biofilm effect.PolymerGroupMPPolymer TypeDominant Bonding MechanismTypical Transformation During Wastewater TreatmentResulting ChangesGeneral SorptionEffectSorption Effect Toward AOCsReferencesAddition polymers (non- or weakly polar)Vinyl-aromaticPS‚ÄìWW‚ÄìMPsAromatic œÄ-œÄ interactions, C-C backbone structureOxidation (ROS), UV ageing, aromatic ring oxidation, chain scission, chlorination/sulfonation (minor)Developing oxidised aromatic structures (quinones, hydroxylated rings)‚Üë Polarity‚ÜëHydrophilicity= enhanced œÄ-œÄ interactions (PS)= enhanced H-bonds and dipole‚Äìdipole interactions (PE, PP)‚Üë Affinity for hydrophobic compounds (e.g., PAHs)‚Üë Affinity for moderately polar organics (e.g., BPA, phenols)[72,81,82,83]PolyolefinsPE‚ÄìWW‚ÄìMPsC-C backbone, weak van der WaalsOxidation (ROS), UV/photo-oxidation, biofilm colonisation, oxidation, chain scission, surface crackingDeveloping of carbonyl (C=O), hydroxyl (-OH), carboxyl (-COOH) surface groups; surface cracking, pitting[84,85,86]PP‚ÄìWW‚ÄìMPsC-C backbone with pendant -CH3 groupsOxidation (aerobic/anoxic cycling), hydroperoxide formation, surface embrittlementDeveloping hydroxyl, fewer carbonyl structures, and chain scission[83,87]Condensation (polar)PET‚ÄìWW‚ÄìMPsEster linkages(-COO-), polarHydrolysis (pH 6.5‚Äì8.5), oxidationDeveloping hydroxyl and carboxyl end groups from ester cleavage; increased polarity‚Üë Polarity‚ÜìHydrophobicity= enhanced œÄ-œÄ interactions‚Üì Affinity for strongly hydrophobic compounds (e.g., PAHs)‚Üë Affinity for moderately polar organics (e.g., BPA, phenols)[88,89,90]PC‚ÄìWW‚ÄìMPsCarbonate ester linkages (-O-CO-O-)Hydrolysis of carbonate linkages, UV/photo-oxidationDevelopment of hydroxyl and carboxyl groups; chain cleavage[91,92,93]",
            "3.1. Morphological Alterations": "WW‚ÄìMP size reduction, altered surface roughness, and partial fragmentation are commonly due to the high shear stress from turbulence induced by continuous mixing or aeration during treatment. This generates forces that mechanically abrade particles‚Äô surfaces, reducing their total mass and increasing the abundance of smaller WW‚ÄìMP fractions [67]. Gravity-based separation processes, such as primary sedimentation, preferentially remove denser, more compact WW‚ÄìMP shapes, such as spheres and fragments, which readily settle into sludge [68]. High-density polymers, including PVC (d = 1.30‚Äì1.45 g cm‚àí3) and PET (d = 1.33‚Äì1.38 g cm‚àí3), go through a similar process [69,70]. In contrast, fibres and low-density polymers, such as PE (d = 0.91‚Äì0.96 g cm‚àí3) and PP (d = 0.83‚Äì0.85 g cm‚àí3), with their high aspect ratio and generally low density, tend to remain suspended, resulting in their high proportion in treated effluents [71,72]. As a result, final effluents are typically enriched in fibrous, low-density plastics, though some biofilm-covered WW‚ÄìMPs may also escape removal [73]. Therefore, evaluating WW‚ÄìMP removal efficiency should consider both mass concentration and particle size distribution.",
            "3.2. Chemical Alterations": "During treatment, WW‚ÄìMPs are subjected to conditions that promote measurable physicochemical changes relevant to their sorption behaviour. One of the most critical operational parameters is hydraulic retention time (HRT), which allows extended contact between WW‚ÄìMPs and AOCs. HRT typically ranges from several hours to about 1 day, during which surface abrasion and microcrack formation extend, thereby increasing the number of accessible sorption sites to contaminants. The typical WWTP operational pH range of 6.5‚Äì8.5 facilitates the hydrolysis of condensation polymers such as PET, leading to chain scission and the formation of -COOH- and -OH-containing end groups [74]. These modifications alter surface polarity and can enhance interactions with AOCs [75]. Similarly, exposure to aerobic conditions (up to 2 mg/L in typical WWTP) promotes oxidative modification of WW‚ÄìMPs surfaces, whereas reduced oxygen zones favour the onset of bacterial deposition [76]. WW‚ÄìMPs are further exposed to coagulants, surfactants, and more oxidants (e.g., chlorine, ozone), which can further alter surface charge, facilitating subsequent sorption of AOCs [77,78]. The increase in reactivity of damaged (aged) MP surfaces was demonstrated by Fernando S. et al. (2007) for polyolefin PE, PP, and PS fragments, which frequently acquire oxidised aromatic structures that strengthen interactions with AOCs [79]. The oxidative ageing of polymer surfaces was thoroughly analysed by Duan J. et al. (2022), who reported that reactive oxygen species (ROS), including hydroxyl radicals (‚Ä¢OH), singlet oxygen (1O2), and superoxide (O2‚Ä¢‚àí), generated during chlorination, ozonation, and microbial treatment, are key drivers of polymers‚Äô transformations in WWTPs [72]. ROS ‚Äúattack‚Äù polymer backbones and side groups, leading to chain scission and the formation of oxygen-containing functional groups. In polyolefins, this results in increased surface polarity and roughness, while in aromatic polymers, it promotes aromatic ring oxidation and the formation of quinone-like structures. For condensation polymers, ROS accelerate the cleavage of ester and carbonate bonds, thereby enhancing hydrolysis [72]. As a result, the sorption behaviour of WW‚ÄìMPs toward AOCs shifts as the affinity for highly hydrophobic compounds tends to decrease, whereas moderately polar aromatic compounds can exhibit enhanced sorption due to the formation of specific polar and aromatic interactions.Table 3summarises chemical transformations of common WW‚ÄìMPs that can occur during wastewater treatment and their potential effects on sorption properties toward AOCs. These wastewater-induced modifications generally reduce polymers‚Äô hydrophobicity, mechanically degrade particles, creating cracks and pores, and broaden MPs‚Äô sorption capacity by increasing their specific surface area [80]. Table 3.Reported wastewater-induced chemical transformations of common polymers, excluding biofilm effect. Table 3.Reported wastewater-induced chemical transformations of common polymers, excluding biofilm effect.PolymerGroupMPPolymer TypeDominant Bonding MechanismTypical Transformation During Wastewater TreatmentResulting ChangesGeneral SorptionEffectSorption Effect Toward AOCsReferencesAddition polymers (non- or weakly polar)Vinyl-aromaticPS‚ÄìWW‚ÄìMPsAromatic œÄ-œÄ interactions, C-C backbone structureOxidation (ROS), UV ageing, aromatic ring oxidation, chain scission, chlorination/sulfonation (minor)Developing oxidised aromatic structures (quinones, hydroxylated rings)‚Üë Polarity‚ÜëHydrophilicity= enhanced œÄ-œÄ interactions (PS)= enhanced H-bonds and dipole‚Äìdipole interactions (PE, PP)‚Üë Affinity for hydrophobic compounds (e.g., PAHs)‚Üë Affinity for moderately polar organics (e.g., BPA, phenols)[72,81,82,83]PolyolefinsPE‚ÄìWW‚ÄìMPsC-C backbone, weak van der WaalsOxidation (ROS), UV/photo-oxidation, biofilm colonisation, oxidation, chain scission, surface crackingDeveloping of carbonyl (C=O), hydroxyl (-OH), carboxyl (-COOH) surface groups; surface cracking, pitting[84,85,86]PP‚ÄìWW‚ÄìMPsC-C backbone with pendant -CH3 groupsOxidation (aerobic/anoxic cycling), hydroperoxide formation, surface embrittlementDeveloping hydroxyl, fewer carbonyl structures, and chain scission[83,87]Condensation (polar)PET‚ÄìWW‚ÄìMPsEster linkages(-COO-), polarHydrolysis (pH 6.5‚Äì8.5), oxidationDeveloping hydroxyl and carboxyl end groups from ester cleavage; increased polarity‚Üë Polarity‚ÜìHydrophobicity= enhanced œÄ-œÄ interactions‚Üì Affinity for strongly hydrophobic compounds (e.g., PAHs)‚Üë Affinity for moderately polar organics (e.g., BPA, phenols)[88,89,90]PC‚ÄìWW‚ÄìMPsCarbonate ester linkages (-O-CO-O-)Hydrolysis of carbonate linkages, UV/photo-oxidationDevelopment of hydroxyl and carboxyl groups; chain cleavage[91,92,93]",
            "4. Environmental Matrix Effects on AOC Sorption by WW-MPs": "AOCs associate with MPs primarily through hydrophobic interactions, van der Waals forces, and œÄ‚ÄìœÄ stacking between aromatic structures. Depending on the surface chemistry of the particles, electrostatic interactions and hydrogen bonding may also contribute, and for certain OCPs, halogen bonding has additionally been reported [94]. A general trend for pristine MPs reported widely in the literature is that sorption increases with decreasing particle size (and thus increasing specific surface area), and that weathered MPs exhibit higher sorption capacities. However, in wastewater systems, these relationships are often non-linear, as they are strongly modulated by matrix-related environmental factors. Variations in pH, ionic strength/salinity and temperature, and the presence and character of dissolved organic matter (DOM), as well as biofilm development on WW‚ÄìMP surfaces, can modify MP surface charge, the abundance and type of functional groups, and diffusion processes, thereby influencing overall sorption efficiency [95,96]. These environmental factors affect not only AOC behaviour (solubility, speciation, partitioning) but also WW‚ÄìMP surface properties, and thus jointly control the efficiency and selectivity of AOC-WW‚ÄìMP complexation [88,95]. By regulating the charge state of functional groups, altering contaminant solubility and ionisation, and introducing competitive and non-equilibrium interactions with other sorbates (e.g., DOM, co-contaminants), they ultimately govern the distribution of AOCs between the aqueous phase and WW‚ÄìMP surfaces. To ensure relevance for micropollutant transport, comparisons of sorption capacity should therefore be based on environmentally or at least artificially aged MPs in aqueous systems, rather than on untreated, pristine materials. Substantial variability within these groups is expected due to differences in degradation history prior to entering the wastewater stream and residence time within the treatment system. Nevertheless, such a classification enables approximate estimations and comparative assessments of AOC sorption behaviour within a given environmental domain. 4.1. Effects of pH and Ionic StrengthpH is one of the most influential factors governing AOC‚ÄìWW‚ÄìMP interactions because it simultaneously controls the surface charge of the polymer and the ionisation state of pH-sensitive contaminants. Sun et al. (2022) [97] demonstrated that acidic conditions enhance the adsorption of ionisable pharmaceuticals onto MPs, as protonation reduces electrostatic repulsion and promotes hydrophobic and hydrogen bonding interactions. In municipal wastewater, which typically exhibits a near-neutral pH, oxygen-containing functional groups formed during MP ageing are largely uncharged. In contrast, biofilm-coated WW‚ÄìMPs acquire a predominantly negative charge due to the extracellular polymeric substances (EPSs) associated with microbial colonisation [98]. Non-ionisable hydrophobic PAHs are largely unaffected by pH; however, phenols and hydroxylated PAHs exhibit pronounced pH-dependent behaviour driven by their pKa values. Below their pKa, these compounds remain neutral and hydrophobic, favouring adsorption to MPs. Above their pKa, they become anionic and more water-soluble, reducing their affinity for MP surfaces [27,99]. At elevated pH, the combination of negatively charged (biofouled or oxidised) MP surfaces and deprotonated AOCs results in electrostatic repulsion and diminished sorption [100]. Conversely, under neutral to mildly acidic conditions, typical for domestic wastewater, electrostatic barriers are weaker, enabling sorption through hydrophobic interactions, œÄ‚ÄìœÄ stacking, and hydrogen bonding.Ionic strength is the second major physicochemical factor influencing AOC sorption onto WW‚ÄìMPs. Increasing ionic strength reduces the thickness of the electrical double layer, thereby diminishing electrostatic repulsion between charged MP surfaces and ionisable AOCs [101]. This screening effect facilitates the approach of anionic contaminants to negatively charged aged or biofilm-coated WW‚ÄìMPs, enabling sorption pathways that would otherwise be inhibited under low-ionic-strength conditions. At the same time, common wastewater cations such as Na+, Ca2+, and Mg2+interact with MP surfaces in multiple ways. Monovalent ions primarily screen surface charge, whereas divalent cations can form inner-sphere complexes, bridge functional groups, or compete with cationic AOCs for negatively charged sorption sites [102]. Such competitive and site-blocking processes may reduce the net sorption of positively charged or protonated contaminants. For non-ionisable hydrophobic AOCs (e.g., PAHs), higher ionic strength increases sorption by decreasing their activity in water and enhancing hydrophobic partitioning. However, because wastewater ionic strength is moderate (typically 0.01‚Äì0.1 M), the overall effect tends to arise from a balance between hydrophobic enhancement, electrostatic screening, and cation competition. Consequently, ionic strength-driven sorption responses are often non-linear and contaminant-specific rather than uniformly promotive or inhibitory [103,104]. 4.2. Effect of Dissolved Organic Matter (DOM)Humic and fulvic substances (e.g., humic acid (HA) and fulvic acid (FA)) represent a third major factor modulating AOC sorption in wastewater systems [105]. Owing to its heterogeneous composition, DOM contains both hydrophobic aromatic domains and hydrophilic domains with highly functionalised regions, allowing interactions with a broad spectrum of AOCs. Hydrophobic contaminants may partition into the non-polar domains of DOM colloids, whereas more polar or ionisable AOCs interact through hydrogen bonding, œÄ‚ÄìœÄ interactions, or electrostatic attraction with DOM functional groups. These processes reduce the freely dissolved contaminant fraction and thereby limit its direct sorption onto WW‚ÄìMP surfaces [106,107,108]. Simultaneously, DOM can adsorb onto WW‚ÄìMPs, forming a dynamic organic coating analogous to early biofilm conditioning layers. Such coatings can introduce additional oxygenated functional groups and physically block sorption sites, collectively reducing the accessibility of the underlying polymer surface [102,103]. Although DOM typically suppresses direct AOC sorption onto WW‚ÄìMPs, it may enhance the overall retention of contaminants through the formation of mobile WW‚ÄìMP‚ÄìDOM‚ÄìAOC complexes, which remain suspended within the wastewater matrix. Thus, the influence of DOM is dual: it decreases surface-bound sorption yet can increase contaminant association with MPs via indirect complexation pathways. 4.3. Effects of Biofilm Formation and TemperatureMicrobial activity represents another critical factor shaping AOC-WW‚ÄìMP interactions, particularly in activated sludge systems. Here, high microorganism concentrations promote rapid biofilm formation on particle surfaces. Biofilms are structured microbial consortia embedded within a self-produced EPS matrix, enhancing cell adhesion and protecting against environmental stressors [109]. The resulting biofilm-coated MPs, often referred to as the plastisphere or ecocorona [110], constitute a highly functionalised secondary sorbent phase. EPSs introduce abundant -COOH, -OH, and -NH2groups derived from proteins, polysaccharides, and humic substances, which confer a predominantly negative surface charge under typical wastewater pH (6‚Äì8) conditions [73]. Biofilm coatings significantly modify sorption behaviour. Bhagat et al. (2024) demonstrated that PP/PE MPs colonised for 15 days sorbed substantially more phenanthrol (0.07 mg/g) compared with pristine MPs (0.001 mg/g), whereas phenanthrene sorption remained unaffected, indicating a contaminant-specific biofilm influence [111]. Similarly, Cui et al. (2023) reported that biofouled HDPE MPs exhibited a pronounced, multi-fold increase in sorptive capacity relative to pristine particles, highlighting the complex interplay between chemical sorption and biofilm-mediated processes [23]. Beyond altering surface chemistry, biofilms create hotspots for horizontal gene transfer, facilitating the exchange of genetic material, including antibiotic resistance genes (ARGs), and thereby contributing to the spread of antimicrobial resistance [112,113]. The extent of biofilm development varies among polymers. Kwiatkowska and Ormaniec [114] showed that PP-MPs supported the most stable biofilm formation, whereas PE-MPs exhibited greater affinity for coliform bacteria. Despite polymer-specific differences, biofouled WW‚ÄìMPs exhibit an enhanced capacity to bind, accumulate, and transport AOCs within wastewater. This vector effect effectively transforms WW‚ÄìMPs into ‚ÄúTrojan horses‚Äù capable of delivering concentrated contaminant loads into receiving environments, increasing both bioavailability and toxicological risk for exposed organisms [73,100,115,116,117].Research on the influence of wastewater temperature on AOC‚ÄìWW‚ÄìMPs is still limited in the available literature. Although temperature is expected to play an important role, it remains overlooked in experimental studies. Biofilms respond strongly to thermal conditions: elevated temperatures stimulate microbial metabolism, EPS formation, and enzymatic activity, generating surfaces with a higher density of reactive functional groups that can enhance the sorption of polar or ionisable contaminants [111]. In real wastewater systems, temperatures typically range between 10 (winter) and 25 ¬∞C (summer), depending on treatment stage, and these fluctuations can influence both sorption kinetics and thermodynamics. However, empirical data remain scarce. A study by Garc√≠a-Pimentel et al. (2022) [118] reported no significant temperature effect on the sorption of m-CPF to HDPE-MPs, indicating that thermal sensitivity may be compound-specific and therefore relatively weak for certain pesticides.The above-described environmental factors are illustrated inFigure 3.Figure 3.Environmental factors in wastewater influencing AOC‚ÄìWW‚ÄìMP interactions.",
            "4.1. Effects of pH and Ionic Strength": "pH is one of the most influential factors governing AOC‚ÄìWW‚ÄìMP interactions because it simultaneously controls the surface charge of the polymer and the ionisation state of pH-sensitive contaminants. Sun et al. (2022) [97] demonstrated that acidic conditions enhance the adsorption of ionisable pharmaceuticals onto MPs, as protonation reduces electrostatic repulsion and promotes hydrophobic and hydrogen bonding interactions. In municipal wastewater, which typically exhibits a near-neutral pH, oxygen-containing functional groups formed during MP ageing are largely uncharged. In contrast, biofilm-coated WW‚ÄìMPs acquire a predominantly negative charge due to the extracellular polymeric substances (EPSs) associated with microbial colonisation [98]. Non-ionisable hydrophobic PAHs are largely unaffected by pH; however, phenols and hydroxylated PAHs exhibit pronounced pH-dependent behaviour driven by their pKa values. Below their pKa, these compounds remain neutral and hydrophobic, favouring adsorption to MPs. Above their pKa, they become anionic and more water-soluble, reducing their affinity for MP surfaces [27,99]. At elevated pH, the combination of negatively charged (biofouled or oxidised) MP surfaces and deprotonated AOCs results in electrostatic repulsion and diminished sorption [100]. Conversely, under neutral to mildly acidic conditions, typical for domestic wastewater, electrostatic barriers are weaker, enabling sorption through hydrophobic interactions, œÄ‚ÄìœÄ stacking, and hydrogen bonding. Ionic strength is the second major physicochemical factor influencing AOC sorption onto WW‚ÄìMPs. Increasing ionic strength reduces the thickness of the electrical double layer, thereby diminishing electrostatic repulsion between charged MP surfaces and ionisable AOCs [101]. This screening effect facilitates the approach of anionic contaminants to negatively charged aged or biofilm-coated WW‚ÄìMPs, enabling sorption pathways that would otherwise be inhibited under low-ionic-strength conditions. At the same time, common wastewater cations such as Na+, Ca2+, and Mg2+interact with MP surfaces in multiple ways. Monovalent ions primarily screen surface charge, whereas divalent cations can form inner-sphere complexes, bridge functional groups, or compete with cationic AOCs for negatively charged sorption sites [102]. Such competitive and site-blocking processes may reduce the net sorption of positively charged or protonated contaminants. For non-ionisable hydrophobic AOCs (e.g., PAHs), higher ionic strength increases sorption by decreasing their activity in water and enhancing hydrophobic partitioning. However, because wastewater ionic strength is moderate (typically 0.01‚Äì0.1 M), the overall effect tends to arise from a balance between hydrophobic enhancement, electrostatic screening, and cation competition. Consequently, ionic strength-driven sorption responses are often non-linear and contaminant-specific rather than uniformly promotive or inhibitory [103,104].",
            "4.2. Effect of Dissolved Organic Matter (DOM)": "Humic and fulvic substances (e.g., humic acid (HA) and fulvic acid (FA)) represent a third major factor modulating AOC sorption in wastewater systems [105]. Owing to its heterogeneous composition, DOM contains both hydrophobic aromatic domains and hydrophilic domains with highly functionalised regions, allowing interactions with a broad spectrum of AOCs. Hydrophobic contaminants may partition into the non-polar domains of DOM colloids, whereas more polar or ionisable AOCs interact through hydrogen bonding, œÄ‚ÄìœÄ interactions, or electrostatic attraction with DOM functional groups. These processes reduce the freely dissolved contaminant fraction and thereby limit its direct sorption onto WW‚ÄìMP surfaces [106,107,108]. Simultaneously, DOM can adsorb onto WW‚ÄìMPs, forming a dynamic organic coating analogous to early biofilm conditioning layers. Such coatings can introduce additional oxygenated functional groups and physically block sorption sites, collectively reducing the accessibility of the underlying polymer surface [102,103]. Although DOM typically suppresses direct AOC sorption onto WW‚ÄìMPs, it may enhance the overall retention of contaminants through the formation of mobile WW‚ÄìMP‚ÄìDOM‚ÄìAOC complexes, which remain suspended within the wastewater matrix. Thus, the influence of DOM is dual: it decreases surface-bound sorption yet can increase contaminant association with MPs via indirect complexation pathways.",
            "4.3. Effects of Biofilm Formation and Temperature": "Microbial activity represents another critical factor shaping AOC-WW‚ÄìMP interactions, particularly in activated sludge systems. Here, high microorganism concentrations promote rapid biofilm formation on particle surfaces. Biofilms are structured microbial consortia embedded within a self-produced EPS matrix, enhancing cell adhesion and protecting against environmental stressors [109]. The resulting biofilm-coated MPs, often referred to as the plastisphere or ecocorona [110], constitute a highly functionalised secondary sorbent phase. EPSs introduce abundant -COOH, -OH, and -NH2groups derived from proteins, polysaccharides, and humic substances, which confer a predominantly negative surface charge under typical wastewater pH (6‚Äì8) conditions [73]. Biofilm coatings significantly modify sorption behaviour. Bhagat et al. (2024) demonstrated that PP/PE MPs colonised for 15 days sorbed substantially more phenanthrol (0.07 mg/g) compared with pristine MPs (0.001 mg/g), whereas phenanthrene sorption remained unaffected, indicating a contaminant-specific biofilm influence [111]. Similarly, Cui et al. (2023) reported that biofouled HDPE MPs exhibited a pronounced, multi-fold increase in sorptive capacity relative to pristine particles, highlighting the complex interplay between chemical sorption and biofilm-mediated processes [23]. Beyond altering surface chemistry, biofilms create hotspots for horizontal gene transfer, facilitating the exchange of genetic material, including antibiotic resistance genes (ARGs), and thereby contributing to the spread of antimicrobial resistance [112,113]. The extent of biofilm development varies among polymers. Kwiatkowska and Ormaniec [114] showed that PP-MPs supported the most stable biofilm formation, whereas PE-MPs exhibited greater affinity for coliform bacteria. Despite polymer-specific differences, biofouled WW‚ÄìMPs exhibit an enhanced capacity to bind, accumulate, and transport AOCs within wastewater. This vector effect effectively transforms WW‚ÄìMPs into ‚ÄúTrojan horses‚Äù capable of delivering concentrated contaminant loads into receiving environments, increasing both bioavailability and toxicological risk for exposed organisms [73,100,115,116,117]. Research on the influence of wastewater temperature on AOC‚ÄìWW‚ÄìMPs is still limited in the available literature. Although temperature is expected to play an important role, it remains overlooked in experimental studies. Biofilms respond strongly to thermal conditions: elevated temperatures stimulate microbial metabolism, EPS formation, and enzymatic activity, generating surfaces with a higher density of reactive functional groups that can enhance the sorption of polar or ionisable contaminants [111]. In real wastewater systems, temperatures typically range between 10 (winter) and 25 ¬∞C (summer), depending on treatment stage, and these fluctuations can influence both sorption kinetics and thermodynamics. However, empirical data remain scarce. A study by Garc√≠a-Pimentel et al. (2022) [118] reported no significant temperature effect on the sorption of m-CPF to HDPE-MPs, indicating that thermal sensitivity may be compound-specific and therefore relatively weak for certain pesticides. The above-described environmental factors are illustrated inFigure 3. Figure 3.Environmental factors in wastewater influencing AOC‚ÄìWW‚ÄìMP interactions.",
            "5. Sorption Models for WW‚ÄìMP‚ÄìAOC Systems": "5.1. Available Modelling ApproachesAssessing sorption behaviour under wastewater conditions remains difficult, as WW‚ÄìMPs exhibit substantial chemical and structural variability depending on effluent composition and treatment methods. Their diverse heterogeneous properties and complex solution chemistry, together with the lack of standardised MPs release limits, have restricted the development of modelling frameworks explicitly tailored to wastewater systems. As a result, most sorption models applied to WW‚ÄìMPs are derived from simplified aqueous environments that can be reproduced in laboratory conditions. Among them, classical equilibrium and kinetic approaches remain widely used. Equilibrium isotherms, including the Freundlich, Langmuir, Temkin, and Dubinin‚ÄìRadushkevich models, are applied to characterise sorption capacity and the heterogeneity of binding sites. At the same time, kinetic models such as the Elovich, Intraparticle Diffusion, Film Diffusion, Pseudo-First-Order, or Pseudo-Second-Order approach describe the rate-limiting steps and overall uptake dynamics. Combined models, such as the Langmuir‚ÄìFreundlich (dual-mode) description, have been used for aged or biofilm-coated MPs to account for the coexistence of surface adsorption and polymer-phase partitioning, and the Redlich‚ÄìPeterson and Dubinin‚ÄìRadushkevich models may provide better fits for structurally altered particles.Table 4summarises the principal isotherm and kinetic models used to describe sorption of MPs-AOCs, which are, at present, the only available formulations applicable to WW‚ÄìMP‚ÄìAOC systems. It indicates each model‚Äôs domain of applicability and clearly identifies which crucial wastewater-relevant factors (i‚Äìiv) each model fulfils.Table 4.Overview of equilibrium, kinetic, and hybrid modelling frameworks used to characterise AOC-MP systems, including their governing equations, key parameters, mechanistic interpretations, and representative studies (2020‚Äì2025). 5.2. Limitations of Laboratory-Derived Sorption ModelsDespite several developed MP-AOC modelling approaches, the assumptions underlying these models, such as homogeneous surfaces, single-solute systems, and near-equilibrium conditions, are rarely met in wastewater matrices. An analysis of the laboratory-derived isotherms fromTable 4showed that experiments are typically generated under controlled, single-solute conditions that do not cover the complexity of wastewater composition, where multi-solute competition and dynamic redox and hydraulic regimes strongly influence sorption processes. In real systems, AOCs coexist with surfactants, humic substances, pharmaceuticals, metals, and colloids that compete for the same binding sites, suppressing or altering sorption behaviours that appear favourable in simplified laboratory settings. Moreover, laboratory studies on sorption models typically assume direct equilibrium conditions, whereas wastewater rarely presents a stable, steady state. Continuous flow with daily and hourly fluctuating contaminant loading and temporal variability in suspended solids drive sorption processes governed by non-equilibrium kinetics, boundary-layer resistance, and slow intra-polymer diffusion. These mechanisms are not accountable by conventional isotherms. Consequently, sorption capacities can be overestimated and underestimate many competitive interactions, as well as resulting parameter values that are not directly applicable to operational wastewater environments. Current modelling efforts therefore rely largely on single-AOC, single-MP systems in controlled media conditions, where heterogeneity is represented mathematically and the combined effects of ageing, biofilm development, and complex wastewater chemistry cannot be reproduced. Some degree of simplification is indeed necessary to any modelling approach; however, these limitations highlight the need for frameworks calibrated under more realistic environmental conditions that more accurately reflect wastewater treatment system phenomena. A robust evaluation of AOC-WW‚ÄìMP interactions thus requires integrating equilibrium and kinetic modelling with a detailed physicochemical characterisation of both the MPs and the surrounding wastewater matrix, including polymer ageing, multi-solute competition, and variable solution chemistry.Therefore, wastewater-relevant model adaptations should undertake to incorporate the following four key factors simultaneously:(i)Ageing-induced MP surface heterogeneity;(ii)Competitive sorption of co-contaminants onto MPs driven by specific medium chemistry;(iii)Non-equilibrium AOC sorption behaviour associated with redox system variability;(iv)Altered diffusion behaviour within weathered MPs.Based on the identified parameters i‚Äìiv, it seems like the dual-mode Langmuir‚ÄìFreundlich formulation comes closest to capturing features relevant to WW‚ÄìMPs, as it integrates heterogeneous surface sorption with a partitioning term that accounts for polymer-phase diffusion. Nevertheless, this model still lacks terms for competitive, non-equilibrium sorption with dissolved organic matter or co-occurring pollutants and does not capture the substantial hydraulic and redox variability present in wastewater treatment systems. As such, all existing models provide only partial representations of real WW‚ÄìMP‚ÄìAOC interactions and must be interpreted within the constraints of their underlying assumptions. 5.3. Modelling Sorption Behaviour of Weathered MPs Under Wastewater Environmental FactorsAmong the available modelling approaches, none simultaneously incorporates all four wastewater-relevant factors identified in this review. Nonetheless, several studies employing PSO kinetics have begun to include selected environmental parameters, offering partial insight into weathered MP behaviour under realistic conditions. Modelling approaches describing the sorption behaviour of weathered MPs under wastewater environmental factors are summarised inTable 5.Table 5.Overview of the sorption behaviour of weathered MPs considering environmentally relevant factors and unit-standardised metrics.",
            "5.1. Available Modelling Approaches": "Assessing sorption behaviour under wastewater conditions remains difficult, as WW‚ÄìMPs exhibit substantial chemical and structural variability depending on effluent composition and treatment methods. Their diverse heterogeneous properties and complex solution chemistry, together with the lack of standardised MPs release limits, have restricted the development of modelling frameworks explicitly tailored to wastewater systems. As a result, most sorption models applied to WW‚ÄìMPs are derived from simplified aqueous environments that can be reproduced in laboratory conditions. Among them, classical equilibrium and kinetic approaches remain widely used. Equilibrium isotherms, including the Freundlich, Langmuir, Temkin, and Dubinin‚ÄìRadushkevich models, are applied to characterise sorption capacity and the heterogeneity of binding sites. At the same time, kinetic models such as the Elovich, Intraparticle Diffusion, Film Diffusion, Pseudo-First-Order, or Pseudo-Second-Order approach describe the rate-limiting steps and overall uptake dynamics. Combined models, such as the Langmuir‚ÄìFreundlich (dual-mode) description, have been used for aged or biofilm-coated MPs to account for the coexistence of surface adsorption and polymer-phase partitioning, and the Redlich‚ÄìPeterson and Dubinin‚ÄìRadushkevich models may provide better fits for structurally altered particles. Table 4summarises the principal isotherm and kinetic models used to describe sorption of MPs-AOCs, which are, at present, the only available formulations applicable to WW‚ÄìMP‚ÄìAOC systems. It indicates each model‚Äôs domain of applicability and clearly identifies which crucial wastewater-relevant factors (i‚Äìiv) each model fulfils. Table 4.Overview of equilibrium, kinetic, and hybrid modelling frameworks used to characterise AOC-MP systems, including their governing equations, key parameters, mechanistic interpretations, and representative studies (2020‚Äì2025).",
            "5.2. Limitations of Laboratory-Derived Sorption Models": "Despite several developed MP-AOC modelling approaches, the assumptions underlying these models, such as homogeneous surfaces, single-solute systems, and near-equilibrium conditions, are rarely met in wastewater matrices. An analysis of the laboratory-derived isotherms fromTable 4showed that experiments are typically generated under controlled, single-solute conditions that do not cover the complexity of wastewater composition, where multi-solute competition and dynamic redox and hydraulic regimes strongly influence sorption processes. In real systems, AOCs coexist with surfactants, humic substances, pharmaceuticals, metals, and colloids that compete for the same binding sites, suppressing or altering sorption behaviours that appear favourable in simplified laboratory settings. Moreover, laboratory studies on sorption models typically assume direct equilibrium conditions, whereas wastewater rarely presents a stable, steady state. Continuous flow with daily and hourly fluctuating contaminant loading and temporal variability in suspended solids drive sorption processes governed by non-equilibrium kinetics, boundary-layer resistance, and slow intra-polymer diffusion. These mechanisms are not accountable by conventional isotherms. Consequently, sorption capacities can be overestimated and underestimate many competitive interactions, as well as resulting parameter values that are not directly applicable to operational wastewater environments. Current modelling efforts therefore rely largely on single-AOC, single-MP systems in controlled media conditions, where heterogeneity is represented mathematically and the combined effects of ageing, biofilm development, and complex wastewater chemistry cannot be reproduced. Some degree of simplification is indeed necessary to any modelling approach; however, these limitations highlight the need for frameworks calibrated under more realistic environmental conditions that more accurately reflect wastewater treatment system phenomena. A robust evaluation of AOC-WW‚ÄìMP interactions thus requires integrating equilibrium and kinetic modelling with a detailed physicochemical characterisation of both the MPs and the surrounding wastewater matrix, including polymer ageing, multi-solute competition, and variable solution chemistry. Therefore, wastewater-relevant model adaptations should undertake to incorporate the following four key factors simultaneously: (i)Ageing-induced MP surface heterogeneity;(ii)Competitive sorption of co-contaminants onto MPs driven by specific medium chemistry;(iii)Non-equilibrium AOC sorption behaviour associated with redox system variability;(iv)Altered diffusion behaviour within weathered MPs. Based on the identified parameters i‚Äìiv, it seems like the dual-mode Langmuir‚ÄìFreundlich formulation comes closest to capturing features relevant to WW‚ÄìMPs, as it integrates heterogeneous surface sorption with a partitioning term that accounts for polymer-phase diffusion. Nevertheless, this model still lacks terms for competitive, non-equilibrium sorption with dissolved organic matter or co-occurring pollutants and does not capture the substantial hydraulic and redox variability present in wastewater treatment systems. As such, all existing models provide only partial representations of real WW‚ÄìMP‚ÄìAOC interactions and must be interpreted within the constraints of their underlying assumptions.",
            "5.3. Modelling Sorption Behaviour of Weathered MPs Under Wastewater Environmental Factors": "Among the available modelling approaches, none simultaneously incorporates all four wastewater-relevant factors identified in this review. Nonetheless, several studies employing PSO kinetics have begun to include selected environmental parameters, offering partial insight into weathered MP behaviour under realistic conditions. Modelling approaches describing the sorption behaviour of weathered MPs under wastewater environmental factors are summarised inTable 5. Table 5.Overview of the sorption behaviour of weathered MPs considering environmentally relevant factors and unit-standardised metrics.",
            "6. AOC‚ÄìWW‚ÄìMP Sorption, Ecological Exposure, and Risk: Key Implications and Research Gaps": "Understanding how weathered MPs sorb AOCs under wastewater conditions is essential for evaluating their role as chemical carriers, yet such information is still rarely incorporated into ecological exposure or risk assessment frameworks. Sorption influences both the contaminant load bound to MPs and the environmental compartments or organisms to which these particles may subsequently be transported. Weathered MPs‚Äîtypically more oxidised, biofilm-covered, and with altered surface charge‚Äîtend to bind AOCs more strongly, but the ecological consequences of this enhanced sorption remain poorly quantified. The chemical composition of MPs also affects their behaviour. Different base polymers and the presence of additives (e.g., plasticisers, stabilisers, antioxidants, surfactants, biocides) can modify sorption capacity and may themselves leach under changing pH, ionic strength, or temperature. PVC is a notable example: it contains high levels of DEHP, and its release under wastewater-like conditions has been experimentally demonstrated. Such additive leaching introduces additional organic contaminants into the system. Particle size further plays a role. Feng et al. (2020) showed that BPA and t-butylphenol leached more rapidly as epoxy MPs decreased from 1 mm to 100 nm due to a higher surface-to-volume ratio and greater density of sorption sites [135]. This is relevant in WWTPs, where smaller MPs and fragments are frequently detected and may therefore contribute disproportionately to contaminant transport. After leaving wastewater treatment, MPs bearing sorbed AOCs may enter receiving waters, sediments, or soils through effluent discharge or sludge application. These pathways align with established ecological exposure routes, including ingestion, trophic transfer, and sediment‚Äìwater interactions. Sorption determines both the bioavailable fraction of contaminants in the surrounding medium and the chemical load delivered to organisms ingesting MPs. Weathering features‚Äîsuch as increased surface area, higher functional group density, and biofilm colonisation‚Äîcan also enhance desorption in digestive fluids, increasing internal exposure beyond what water concentrations alone would predict. Despite these mechanistic insights, most current risk assessment frameworks evaluate AOCs independently of MPs. Few studies quantify how MP-associated transport alters predicted exposure concentrations or fate-and-transport behaviour. Likewise, ecological risk assessments seldom consider co-exposure, mixture toxicity, or modified bioaccumulation linked to MP‚ÄìAOC complexes. Existing evidence suggests that MPs often act as short-term or episodic carriers rather than long-term sinks, yet this behaviour is not reflected in standard assessment approaches. WW‚ÄìMPs undergo extensive physicochemical transformation during treatment, generating sorption domains that differ fundamentally from those of pristine polymers. These changes directly influence the binding, retention, and mobility of AOCs in receiving environments, shaping exposure pathways and long-term environmental risk. However, research on truly environmental WW‚ÄìMPs remains limited, and data on the long-term fate, bioavailability, and transformation of MP‚ÄìAOC complexes remain scarce. The magnitude, variability, and system-specific character of these transformations are still poorly constrained across different WWTP configurations. Although ageing and biofilm development on WW‚ÄìMPs have been widely documented, their integration into sorption modelling remains incomplete. Current isotherm and kinetic models typically incorporate only one or two of the four critical descriptors identified in this review‚Äîsurface functionality, competitive sorption from co-contaminants, non-equilibrium interactions, and altered diffusion behaviour. As a result, predictions do not fully represent real wastewater conditions. Given the inherent variability of wastewater composition and the diversity of MP transformation processes, these factors must be explicitly incorporated; otherwise, mechanistic models cannot evolve toward greater environmental realism, limiting assessments of exposure or contaminant transfer relevant to wastewater-based epidemiology. Methodological inconsistencies also hinder progress. Comparative studies of WW‚Äìderived, field-aged, and laboratory-aged MPs reveal substantial discrepancies arising from differences in sampling strategies, isolation and separation procedures, ageing protocols, pre-treatment steps, and aqueous-phase chemistry. Without harmonised and validated workflows, cross-study comparisons remain largely interpretative, and model calibration remains unreliable. Standardising sampling, separation, surface characterisation methods, and test solution chemistry‚Äîparticularly pH, ionic strength, DOM, and co-contaminant composition‚Äîis essential for generating comparable data. A further priority is incorporating experimentally constrained sorption and kinetic parameters into fate-and-transport models. Research on PAHs and other hydrophobic AOCs demonstrates that including MP-associated sorption can substantially alter predicted residence times, phase partitioning, and downstream transport. Extending these modelling frameworks to represent the actual ageing state, porosity, and biofilm coverage of WW‚ÄìMPs will enable more realistic assessments of AOC persistence and mobility in wastewater environments."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/1422-0067/26/23/11758",
        "scraped_at": "2025-12-05 23:53:34"
    },
    {
        "title": "DiagNeXt: A Two-Stage Attention-Guided ConvNeXt Framework for Kidney Pathology Segmentation and Classification",
        "authors": "byHilal Tekin,≈ûafak Kƒ±lƒ±√ßandYahya Doƒüan",
        "journal": "J. Imaging2025,11(12), 433;https://doi.org/10.3390/jimaging11120433- 4 Dec 2025",
        "abstract": "Accurate segmentation and classification of kidney pathologies from medical images remain a major challenge in computer-aided diagnosis due to complex morphological variations, small lesion sizes, and severe class imbalance. This study introduces DiagNeXt, a novel two-stage deep learning framework designed to overcome these challenges through an integrated use of attention-enhanced ConvNeXt architectures for both segmentation and classification. In the first stage, DiagNeXt-Seg employs a U-Net-based design incorporating Enhanced Convolutional Blocks (ECBs) with spatial attention gates and Atrous Spatial Pyramid Pooling (ASPP) to achieve precise multi-class kidney segmentation. In the second stage, DiagNeXt-Cls utilizes the segmented regions of interest (ROIs) for pathology classification through a hierarchical multi-resolution strategy enhanced by Context-Aware Feature Fusion (CAFF) and Evidential Deep Learning (EDL) for uncertainty estimation. The main contributions of this work include: (1) enhanced ConvNeXt blocks with large-kernel depthwise convolutions optimized for 3D medical imaging, (2) a boundary-aware compound loss combining Dice, cross-entropy, focal, and distance transform terms to improve segmentation precision, (3) attention-guided skip connections preserving fine-grained spatial details, (4) hierarchical multi-scale feature modeling for robust pathology recognition, and (5) a confidence-modulated classification approach integrating segmentation quality metrics for reliable decision-making. Extensive experiments on a large kidney CT dataset comprising 3847 patients demonstrate that DiagNeXt achieves 98.9% classification accuracy, outperforming state-of-the-art approaches by 6.8%. The framework attains near-perfect AUC scores across all pathology classes (Normal: 1.000, Tumor: 1.000, Cyst: 0.999, Stone: 0.994) while offering clinically interpretable uncertainty maps and attention visualizations. The superior diagnostic accuracy, computational efficiency (6.2√ó faster inference), and interpretability of DiagNeXt make it a strong candidate for real-world integration into clinical kidney disease diagnosis and treatment planning systems.Keywords:kidney pathology;medical image segmentation;ConvNeXt;attention mechanisms;deep learning;two-stage framework",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Chronic kidney disease (CKD) affects over 10% of the global population and represents a major public health concern with significant morbidity and mortality rates [1]. Early and accurate detection of kidney pathologies, including cysts, tumors, and stones, is crucial for effective treatment planning and improved patient outcomes. While computed tomography (CT) imaging serves as the primary diagnostic modality for kidney assessment, manual segmentation and classification of pathological regions remain time-consuming, subjective, and prone to inter-observer variability [2]. Recent advances in deep learning have demonstrated remarkable potential for automated medical image analysis, particularly in segmentation and classification tasks [3,4,5]. However, kidney pathology analysis presents unique challenges that existing methods struggle to address effectively. These challenges include: (1) significant morphological variations in kidney shapes and sizes across patients, (2) small and irregularly shaped lesions that are difficult to detect, (3) class imbalance between normal and pathological tissues, (4) ambiguous boundaries between different tissue types, particularly in distinguishing cysts from tumors, and (5) the need for both accurate localization and precise classification of multiple pathology types within a single framework [6]. In our recent work on musculoskeletal imaging, we demonstrated that multi-scale attention-augmented DenseNet architectures can effectively capture subtle structural variations and improve diagnostic performance [7], which motivated similar multi-resolution feature modeling in DiagNeXt. While numerous deep learning approaches have been proposed for kidney segmentation, most focus solely on either organ segmentation or single pathology detection. The KiTS challenges (2019, 2021, 2023) have advanced the field by providing large-scale annotated datasets, yet the winning solutions primarily employed single-stage approaches that may not fully exploit the complementary nature of segmentation and classification tasks [8]. Furthermore, existing methods often struggle with small lesion detection and fail to maintain precise boundary delineation, which is critical for clinical applications such as surgical planning and volumetric assessment [9]. The emergence of Vision Transformers has introduced new possibilities for medical image analysis through global context modeling. However, their computational requirements and need for large-scale training data limit their applicability in medical imaging scenarios where annotated data is scarce. ConvNeXt, introduced as a modernized convolutional architecture inspired by Transformers, offers an attractive alternative by combining the efficiency of CNNs with design principles from Transformers [10]. Recent adaptations like MedNeXt have shown promising results for 3D medical image segmentation, yet opportunities remain for enhancing these architectures specifically for kidney pathology analysis [11]. This work builds upon our prior research in deep learning for medical image classification, including attention-based dual-path frameworks for blood cell images [12] and hybrid architectures for ocular disease diagnosis [13]. These prior works have demonstrated the benefits of combining CNNs with attention and Transformer modules, inspiring the multi-resolution and uncertainty-aware design choices of DiagNeXt. In this paper, we propose DiagNeXt, a novel two-stage framework that synergistically combines segmentation and classification for comprehensive kidney pathology analysis. Our approach addresses the limitations of existing methods through several key innovations. First, we introduce DiagNeXt-Seg, which enhances the U-Net architecture with modified ConvNeXt blocks and spatial attention mechanisms to achieve precise multi-class segmentation while preserving fine-grained boundary details. Second, we develop DiagNeXt-Cls, which leverages the segmented ROIs for accurate pathology classification using a ConvNeXt backbone enhanced with uncertainty estimation. Third, we propose a boundary-aware compound loss function that explicitly encourages accurate edge delineation while handling class imbalance. Finally, we demonstrate how the two-stage approach enables more focused feature learning compared to end-to-end methods, leading to improved performance on both tasks. Despite these advancements, several critical research gaps remain unaddressed. Existing kidney analysis pipelines typically treat segmentation and classification as independent tasks, resulting in suboptimal feature sharing and reduced diagnostic robustness. Most current models also struggle with (i) extreme lesion size variability, (ii) ambiguous cyst‚Äìtumor boundaries, (iii) high background-to-lesion voxel imbalance, and (iv) the absence of calibrated uncertainty estimates crucial for clinical decision making. Furthermore, current ConvNeXt-based medical models are optimized primarily for organ-level segmentation and have not been adapted for integrated pathology-level analysis. In contrast, the proposed DiagNeXt framework directly addresses these limitations by introducing a two-stage, attention-guided, multi-resolution architecture that combines precise 3D segmentation, lesion-focused ROI extraction, pathology-aware multi-scale classification, and evidential uncertainty modeling within a unified pipeline.",
            "2. Related Work": "2.1. Kidney Segmentation in Medical ImagingTraditional approaches to kidney segmentation relied on classical image processing techniques including thresholding, region growing, and active contours. While computationally efficient, these methods struggled with the anatomical complexity and intensity variations inherent in kidney imaging [14]. The introduction of deep learning revolutionized this field, with U-Net becoming the de facto standard for medical image segmentation due to its encoder‚Äìdecoder architecture with skip connections that preserve spatial information across scales [15].Subsequent improvements to U-Net have focused on various aspects. Attention U-Net incorporated attention gates to suppress irrelevant features and highlight salient regions, demonstrating particular effectiveness for organ segmentation tasks [16]. V-Net extended U-Net to 3D volumetric data and introduced the Dice loss for handling class imbalance [17]. nnU-Net achieved state-of-the-art performance through systematic optimization of preprocessing, architecture, and training procedures, becoming a strong baseline for medical segmentation tasks [18].For kidney-specific applications, several specialized architectures have been proposed. The automatic segmentation of polycystic kidneys presents unique challenges due to severe morphological alterations. Studies have employed deep learning models achieving Dice scores exceeding 0.95 for total kidney volume estimation in ADPKD patients [19,20]. However, these methods primarily focus on organ-level segmentation without distinguishing between different pathology types. Recent work has begun addressing multi-class kidney lesion segmentation, though challenges remain in accurately delineating small lesions and maintaining boundary precision [21]. 2.2. ConvNeXt and Its Medical Imaging AdaptationsConvNeXt emerged as a pure convolutional architecture that modernizes standard ResNet by incorporating design principles from Vision Transformers. Key modifications include larger kernel sizes (7 √ó 7), inverted bottleneck blocks, fewer activation functions, and separate downsampling layers. These changes enabled ConvNeXt to match or exceed Transformer performance while maintaining the efficiency and inductive biases of CNNs [10].The medical imaging community quickly recognized ConvNeXt‚Äôs potential. MedNeXt introduced the first fully ConvNeXt 3D segmentation network, incorporating residual ConvNeXt blocks in both encoder and decoder paths. The framework demonstrated state-of-the-art performance across multiple organs and modalities through compound scaling of depth, width, and kernel size [11]. 3D-UX-Net partially incorporated ConvNeXt blocks within a standard U-Net framework, showing improvements over pure CNN and Transformer baselines [22].BCU-Net proposed bridging ConvNeXt with U-Net for medical image segmentation, leveraging ConvNeXt for global feature extraction while maintaining U-Net‚Äôs strength in local detail preservation. The multi-label recall loss addressed class imbalance issues common in medical imaging [23]. ConvUNeXt focused on parameter efficiency, achieving competitive performance with significantly fewer parameters through careful architectural design [24]. SCANeXt combined ConvNeXt blocks with dual attention mechanisms (spatial and channel) and demonstrated superior performance on cardiac and abdominal segmentation tasks [25].Despite these advances, existing ConvNeXt adaptations have not been specifically optimized for the unique challenges of kidney pathology analysis, particularly the need for both accurate multi-class segmentation and subsequent classification of detected lesions. 2.3. Attention Mechanisms in Medical SegmentationAttention mechanisms have become integral to modern medical image segmentation architectures by enabling networks to focus on relevant features while suppressing noise. The seminal Attention U-Net introduced additive attention gates that learn to highlight salient features passed through skip connections [16]. Squeeze-and-Excitation (SE) modules model channel-wise dependencies through global pooling followed by excitation operations, proving effective for enhancing feature representations [26].Recent work has explored more sophisticated attention designs. Dual attention networks combine spatial and channel attention to capture both ‚Äúwhere‚Äù and ‚Äúwhat‚Äù to focus on. CBAM (Convolutional Block Attention Module) sequentially applies channel and spatial attention, demonstrating improvements across various vision tasks [27]. For medical imaging specifically, studies have shown that attention mechanisms particularly benefit tasks involving small object detection and fine boundary delineation [28]. Our previous FocusGate-Net model demonstrated that dual-attention mechanisms can substantially improve segmentation quality across diverse anatomical structures [29].In kidney imaging, attention mechanisms have shown promise for improving segmentation accuracy. SEA-NET proposed spiral squeeze-and-excitation modules specifically for small target segmentation in medical images [14]. Studies on ADPKD segmentation have incorporated attention mechanisms to handle the extreme morphological variations in polycystic kidneys, achieving improved boundary delineation [30]. However, the integration of attention mechanisms with modern architectures like ConvNeXt for kidney pathology analysis remains underexplored. 2.4. Two-Stage Approaches for Medical Image AnalysisWhile end-to-end learning has dominated recent medical imaging research, two-stage approaches offer distinct advantages for complex tasks requiring both detection/segmentation and classification. The separation of tasks allows each network to specialize, potentially achieving better performance than a single multi-task network [6].In kidney imaging, several studies have employed two-stage strategies. Traditional pipelines first segment the kidney region, then apply classification or regression models to the extracted ROIs. Recent work on renal mass characterization used cascaded 3D U-Net and ResNet architectures, first segmenting kidneys and masses, then classifying tumor subtypes based on the segmented regions [6]. This approach achieved superior performance compared to direct whole-image classification, as the classification network could focus on relevant features within the ROI.The KiTS challenges have primarily evaluated segmentation performance, but several participants have extended their solutions to include classification stages. Top-performing methods often employ ensemble strategies combining multiple segmentation models followed by post-processing and classification steps [31]. However, these approaches typically treat the two stages independently without leveraging the complementary information between tasks. 2.5. Loss Functions for Medical Image SegmentationThe choice of loss function significantly impacts segmentation performance, particularly for tasks with class imbalance and requirements for precise boundary delineation. Cross-entropy loss, while standard for classification tasks, can be dominated by the majority class in segmentation scenarios. Dice loss directly optimizes the overlap metric but can be unstable for small objects and may not preserve boundaries well [17].Recent work has explored compound loss functions combining multiple objectives. Dice + Cross-entropy loss balances region-based and distribution-based optimization. Focal loss addresses class imbalance by down-weighting easy examples. Boundary loss explicitly encourages accurate edge detection by computing distances to ground truth boundaries [32]. For kidney segmentation specifically, studies have shown that combining Dice loss with boundary-aware terms improves performance, particularly for small lesions and irregular boundaries [1]. 2.6. Challenges and Gaps in Current MethodsAlthough recent studies have achieved improvements in kidney segmentation or lesion classification, none provide a unified framework capable of simultaneously addressing boundary precision, multi-scale lesion heterogeneity, classification reliability, and uncertainty quantification. Existing ConvNeXt- or U-Net-based models are limited to either organ-level segmentation or single-pathology detection, lacking mechanisms to (i) capture multi-resolution pathological cues, (ii) incorporate segmentation confidence into classification, (iii) handle small or ambiguous lesions, or (iv) deliver calibrated uncertainties necessary for clinical integration. DiagNeXt bridges these gaps by introducing an attention-guided two-stage architecture that tightly couples segmentation quality with classification confidence, employs hierarchical multi-resolution feature extraction tailored for diverse renal pathologies, and integrates Evidential Deep Learning to provide reliable uncertainty estimates‚Äîan aspect currently absent in published kidney pathology frameworks [1].Our proposed DiagNeXt framework addresses these gaps through a carefully designed two-stage approach that combines the strengths of modern ConvNeXt architectures with attention mechanisms, boundary-aware training, and uncertainty estimation, specifically optimized for the unique challenges of kidney pathology analysis.",
            "2.1. Kidney Segmentation in Medical Imaging": "Traditional approaches to kidney segmentation relied on classical image processing techniques including thresholding, region growing, and active contours. While computationally efficient, these methods struggled with the anatomical complexity and intensity variations inherent in kidney imaging [14]. The introduction of deep learning revolutionized this field, with U-Net becoming the de facto standard for medical image segmentation due to its encoder‚Äìdecoder architecture with skip connections that preserve spatial information across scales [15]. Subsequent improvements to U-Net have focused on various aspects. Attention U-Net incorporated attention gates to suppress irrelevant features and highlight salient regions, demonstrating particular effectiveness for organ segmentation tasks [16]. V-Net extended U-Net to 3D volumetric data and introduced the Dice loss for handling class imbalance [17]. nnU-Net achieved state-of-the-art performance through systematic optimization of preprocessing, architecture, and training procedures, becoming a strong baseline for medical segmentation tasks [18]. For kidney-specific applications, several specialized architectures have been proposed. The automatic segmentation of polycystic kidneys presents unique challenges due to severe morphological alterations. Studies have employed deep learning models achieving Dice scores exceeding 0.95 for total kidney volume estimation in ADPKD patients [19,20]. However, these methods primarily focus on organ-level segmentation without distinguishing between different pathology types. Recent work has begun addressing multi-class kidney lesion segmentation, though challenges remain in accurately delineating small lesions and maintaining boundary precision [21].",
            "2.2. ConvNeXt and Its Medical Imaging Adaptations": "ConvNeXt emerged as a pure convolutional architecture that modernizes standard ResNet by incorporating design principles from Vision Transformers. Key modifications include larger kernel sizes (7 √ó 7), inverted bottleneck blocks, fewer activation functions, and separate downsampling layers. These changes enabled ConvNeXt to match or exceed Transformer performance while maintaining the efficiency and inductive biases of CNNs [10]. The medical imaging community quickly recognized ConvNeXt‚Äôs potential. MedNeXt introduced the first fully ConvNeXt 3D segmentation network, incorporating residual ConvNeXt blocks in both encoder and decoder paths. The framework demonstrated state-of-the-art performance across multiple organs and modalities through compound scaling of depth, width, and kernel size [11]. 3D-UX-Net partially incorporated ConvNeXt blocks within a standard U-Net framework, showing improvements over pure CNN and Transformer baselines [22]. BCU-Net proposed bridging ConvNeXt with U-Net for medical image segmentation, leveraging ConvNeXt for global feature extraction while maintaining U-Net‚Äôs strength in local detail preservation. The multi-label recall loss addressed class imbalance issues common in medical imaging [23]. ConvUNeXt focused on parameter efficiency, achieving competitive performance with significantly fewer parameters through careful architectural design [24]. SCANeXt combined ConvNeXt blocks with dual attention mechanisms (spatial and channel) and demonstrated superior performance on cardiac and abdominal segmentation tasks [25]. Despite these advances, existing ConvNeXt adaptations have not been specifically optimized for the unique challenges of kidney pathology analysis, particularly the need for both accurate multi-class segmentation and subsequent classification of detected lesions.",
            "2.3. Attention Mechanisms in Medical Segmentation": "Attention mechanisms have become integral to modern medical image segmentation architectures by enabling networks to focus on relevant features while suppressing noise. The seminal Attention U-Net introduced additive attention gates that learn to highlight salient features passed through skip connections [16]. Squeeze-and-Excitation (SE) modules model channel-wise dependencies through global pooling followed by excitation operations, proving effective for enhancing feature representations [26]. Recent work has explored more sophisticated attention designs. Dual attention networks combine spatial and channel attention to capture both ‚Äúwhere‚Äù and ‚Äúwhat‚Äù to focus on. CBAM (Convolutional Block Attention Module) sequentially applies channel and spatial attention, demonstrating improvements across various vision tasks [27]. For medical imaging specifically, studies have shown that attention mechanisms particularly benefit tasks involving small object detection and fine boundary delineation [28]. Our previous FocusGate-Net model demonstrated that dual-attention mechanisms can substantially improve segmentation quality across diverse anatomical structures [29]. In kidney imaging, attention mechanisms have shown promise for improving segmentation accuracy. SEA-NET proposed spiral squeeze-and-excitation modules specifically for small target segmentation in medical images [14]. Studies on ADPKD segmentation have incorporated attention mechanisms to handle the extreme morphological variations in polycystic kidneys, achieving improved boundary delineation [30]. However, the integration of attention mechanisms with modern architectures like ConvNeXt for kidney pathology analysis remains underexplored.",
            "2.4. Two-Stage Approaches for Medical Image Analysis": "While end-to-end learning has dominated recent medical imaging research, two-stage approaches offer distinct advantages for complex tasks requiring both detection/segmentation and classification. The separation of tasks allows each network to specialize, potentially achieving better performance than a single multi-task network [6]. In kidney imaging, several studies have employed two-stage strategies. Traditional pipelines first segment the kidney region, then apply classification or regression models to the extracted ROIs. Recent work on renal mass characterization used cascaded 3D U-Net and ResNet architectures, first segmenting kidneys and masses, then classifying tumor subtypes based on the segmented regions [6]. This approach achieved superior performance compared to direct whole-image classification, as the classification network could focus on relevant features within the ROI. The KiTS challenges have primarily evaluated segmentation performance, but several participants have extended their solutions to include classification stages. Top-performing methods often employ ensemble strategies combining multiple segmentation models followed by post-processing and classification steps [31]. However, these approaches typically treat the two stages independently without leveraging the complementary information between tasks.",
            "2.5. Loss Functions for Medical Image Segmentation": "The choice of loss function significantly impacts segmentation performance, particularly for tasks with class imbalance and requirements for precise boundary delineation. Cross-entropy loss, while standard for classification tasks, can be dominated by the majority class in segmentation scenarios. Dice loss directly optimizes the overlap metric but can be unstable for small objects and may not preserve boundaries well [17]. Recent work has explored compound loss functions combining multiple objectives. Dice + Cross-entropy loss balances region-based and distribution-based optimization. Focal loss addresses class imbalance by down-weighting easy examples. Boundary loss explicitly encourages accurate edge detection by computing distances to ground truth boundaries [32]. For kidney segmentation specifically, studies have shown that combining Dice loss with boundary-aware terms improves performance, particularly for small lesions and irregular boundaries [1].",
            "2.6. Challenges and Gaps in Current Methods": "Although recent studies have achieved improvements in kidney segmentation or lesion classification, none provide a unified framework capable of simultaneously addressing boundary precision, multi-scale lesion heterogeneity, classification reliability, and uncertainty quantification. Existing ConvNeXt- or U-Net-based models are limited to either organ-level segmentation or single-pathology detection, lacking mechanisms to (i) capture multi-resolution pathological cues, (ii) incorporate segmentation confidence into classification, (iii) handle small or ambiguous lesions, or (iv) deliver calibrated uncertainties necessary for clinical integration. DiagNeXt bridges these gaps by introducing an attention-guided two-stage architecture that tightly couples segmentation quality with classification confidence, employs hierarchical multi-resolution feature extraction tailored for diverse renal pathologies, and integrates Evidential Deep Learning to provide reliable uncertainty estimates‚Äîan aspect currently absent in published kidney pathology frameworks [1]. Our proposed DiagNeXt framework addresses these gaps through a carefully designed two-stage approach that combines the strengths of modern ConvNeXt architectures with attention mechanisms, boundary-aware training, and uncertainty estimation, specifically optimized for the unique challenges of kidney pathology analysis.",
            "3. Methodology": "3.1. Overview of the DiagNeXt FrameworkThe proposed DiagNeXt framework represents a comprehensive two-stage cascade architecture specifically designed for automated kidney pathology analysis in CT imaging. The framework addresses the fundamental challenges of medical image analysis by decomposing the complex task of simultaneous lesion detection and classification into two specialized, sequential stages that can each focus on their respective objectives with maximum efficiency.The first stage, DiagNeXt-Seg employs a heavily modified 3D U-Net architecture enhanced with modern architectural innovations including ConvNeXt-inspired blocks, multi-scale attention mechanisms, and boundary-aware loss formulations. This segmentation network produces precise multi-class masks that delineate normal kidney tissue, cysts, tumors, stones, and background regions with high spatial accuracy and confidence estimation.The second stage, DiagNeXt-Cls, implements a sophisticated ROI-based classification system that operates exclusively on the pathological regions identified by the segmentation stage. This classification network leverages multi-resolution feature processing, evidential deep learning for uncertainty quantification, and adaptive fusion mechanisms to achieve superior diagnostic accuracy while providing clinically meaningful confidence scores. Prior work leveraging deep feature engineering for fine-grained medical image classification further motivated the multi-resolution feature modeling adopted in DiagNeXt [33].This cascade design philosophy offers several critical advantages over end-to-end approaches: (1) Computational Efficiency the classification network processes only relevant regions rather than entire volumes, (2) Specialized Optimization each stage can be optimized for its specific task without compromising the other, (3) Interpretability the framework provides both spatial localization and classification confidence, and (4) Clinical Integration the modular design allows for independent validation and deployment of each component.The overall workflow processes a 3D CT volume through the segmentation stage to identify potential lesions, extracts standardized ROI patches with associated confidence scores, and subsequently classifies each ROI using the specialized classification network. This approach has demonstrated superior performance compared to single-stage alternatives while providing the uncertainty quantification essential for clinical decision support systems. 3.2. DiagNeXt-Seg: Enhanced 3D U-Net Architecture for Kidney Lesion SegmentationThe segmentation component of DiagNeXt employs a substantially modified 3D U-Net architecture that incorporates several key innovations specifically tailored for kidney pathology detection. Traditional U-Net architectures, while effective for general medical segmentation tasks, face significant challenges when applied to kidney lesion detection due to the diverse scales, varied appearances, and subtle boundaries characteristic of renal pathologies.3.2.1. Architectural Modifications and EnhancementsOur enhanced U-Net implementation, termed DiagNeXt-Seg, integrates modern convolutional building blocks derived from the ConvNeXt family while maintaining the proven encoder‚Äìdecoder structure that has established U-Net as the gold standard for medical image segmentation. The key architectural innovations include:ConvNeXt-Inspired Encoder Blocks: Each encoder level utilizes Enhanced Convolutional Blocks (ECBs) that replace traditional 3 √ó 3 convolutions with a more sophisticated design:ùë•1ùë•2ùë•3ùë•4ECB(ùë•)=DWConv3D7√ó7√ó3(ùë•)=GroupNorm(ùë•1)=PWConv3D1√ó1√ó1(ùë•2)=GELU(ùë•3)=ùë•+ùõæ‚äôPWConv3D1√ó1√ó1(ùë•4)x1=DWConv3D7√ó7√ó3(x)x2=GroupNorm(x1)x3=PWConv3D1√ó1√ó1(x2)x4=GELU(x3)ECB(x)=x+Œ≥‚äôPWConv3D1√ó1√ó1(x4)(1)where the large kernel depthwise convolution (7 √ó 7 √ó 3) captures long-range spatial dependencies similar to self-attention mechanisms, while the inverted bottleneck design with pointwise convolutions enables efficient feature transformation. The learnable scale parameterùõæŒ≥is initialized near zero to stabilize training in deep networks.Weight Initialization Strategy: The Enhanced Convolutional Blocks (ECBs) employ a comprehensive initialization scheme tailored for stable training in 3D medical imaging. Depthwise convolutional layers (DWConv3D7√ó7√ó3DWConv3D7√ó7√ó3) are initialized using He normal initialization [34], which preserves gradient variance in layers with ReLU/GELU activations. Pointwise convolutions (PWConv3D1√ó1√ó1PWConv3D1√ó1√ó1) utilize Xavier uniform initialization [35] to maintain activation variances across the network. The learnable scale parameterùõæŒ≥in Equation (5) is initialized to1√ó10‚àí61√ó10‚àí6to stabilize early training, following which it evolves freely during optimization. GroupNorm layers require no initialization as they operate on channel-wise statistics. This combined approach ensures stable gradient flow while enabling effective feature learning from the first training iterations.Multi-Scale Feature Aggregation: At the network bottleneck, we implement an Atrous Spatial Pyramid Pooling (ASPP) module adapted for 3D medical imaging:ùêπùëöùë†=Conv1√ó1([GAP(ùêπ);Conv(1)(ùêπ);Conv(3)(ùêπ);Conv(6)(ùêπ)])Fms=Conv1√ó1([GAP(F);Conv(1)(F);Conv(3)(F);Conv(6)(F)])(2)whereConv(ùëü)Conv(r)denotes 3D convolution with dilation rater, capturing features at multiple scales without resolution loss.Attention-Enhanced skip Connections: Traditional skip connections are augmented with spatial attention gates that selectively filter encoder features before fusion with decoder representations:ùõºùëéùë°ùë°=ùúé(ùëäùëáùúì[ReLU(ùëäùëîùëî+ùëäùë•ùë•+ùëè)])Œ±att=œÉWœàTReLU(Wgg+Wxx+b)(3)wheregrepresents the gating signal from the decoder,xdenotes encoder features, andùõºùëéùë°ùë°Œ±attprovides spatial attention weights that emphasize relevant anatomical regions while suppressing background noise.3D Kernel Design Rationale: The7√ó7√ó37√ó7√ó3kernel dimensions were carefully chosen to align with physical voxel characteristics:In-plane (7 √ó 7): Captures anatomical structures within high-resolution axial slices (0.6‚Äì0.8 mm/pixel);Through-plane (√ó3): Adapted to larger slice spacing (2.5‚Äì5.0 mm), balancing receptive field with computational cost;Computational Efficiency: Depthwise separable design reduces parameters by 8.5√ó compared to standard 3D convolutions;LayerScale Initialization:ùõæŒ≥parameters initialized to1√ó10‚àí61√ó10‚àí6for training stability.3.2.2. Boundary-Aware Multi-Objective Loss FunctionAccurate boundary delineation represents a critical requirement for subsequent ROI extraction and classification. Our segmentation training employs a carefully designed compound loss function that addresses multiple aspects of segmentation quality:‚Ñíùë†ùëíùëî=ùõº‚Ñíùê∂ùê∏+ùõΩ‚Ñíùê∑ùëñùëêùëí+ùõæ‚Ñíùêµùëúùë¢ùëõùëëùëéùëüùë¶+ùõø‚ÑíùêπùëúùëêùëéùëôLseg=Œ±LCE+Œ≤LDice+Œ≥LBoundary+Œ¥LFocal(4)Loss Function Parameter Specification: The composite segmentation loss‚Ñíùë†ùëíùëî=ùõº‚Ñíùê∂ùê∏+ùõΩ‚Ñíùê∑ùëñùëêùëí+ùõæ‚Ñíùêµùëúùë¢ùëõùëëùëéùëüùë¶+ùõø‚ÑíùêπùëúùëêùëéùëôLseg=Œ±LCE+Œ≤LDice+Œ≥LBoundary+Œ¥LFocalemploys experimentally determined weights optimized for kidney pathology segmentation. Through extensive grid search on the validation set, we established the optimal parameters asùõº=0.4Œ±=0.4,ùõΩ=0.3Œ≤=0.3,ùõæ=0.2Œ≥=0.2, andùõø=0.1Œ¥=0.1. This configuration prioritizes cross-entropy for overall classification accuracy while maintaining strong boundary delineation through the boundary loss component. The focal loss term addresses class imbalance with its focusing parameterùõæùëìùëúùëêùëéùëô=2.0Œ≥focal=2.0, effectively down-weighting easy examples. The boundary loss utilizes a signed distance transform withùúé=1.5œÉ=1.5for smooth distance computation. This parameter combination was validated through 5-fold cross-validation, demonstrating consistent performance across all pathology classes.The Focal Loss component addresses class imbalance by down-weighting easy examples:‚Ñíùêπùëúùëêùëéùëô=‚àí‚àëùëñ,ùëê(1‚àíùëùùëñ,ùëê)ùõæùë¶ùëñ,ùëêlog(ùëùùëñ,ùëê)LFocal=‚àí‚àëi,c(1‚àípi,c)Œ≥yi,clog(pi,c)(5)The Boundary Loss specifically optimizes contour accuracy using distance transform regularization:‚Ñíùêµùëúùë¢ùëõùëëùëéùëüùë¶=‚àëùëê‚àëŒ©ùúôùëê(ùê¨)ùê∑ùëê(ùê¨)‚àëŒ©ùúôùëê(ùê¨)LBoundary=‚àëc‚àëŒ©œïc(s)Dc(s)‚àëŒ©œïc(s)(6)whereùúôùëê(ùê¨)œïc(s)represents the network output for classcat locationùê¨s, andùê∑ùëê(ùê¨)Dc(s)is the signed distance transform of the ground truth boundary.3.2.3. Confidence Score GenerationA unique aspect of DiagNeXt-Seg is its ability to generate pixel-wise confidence scores that quantify segmentation uncertainty. These scores are computed using multiple complementary measures:ùíûùëùùëñùë•ùëíùëô(ùê¨)=ùë§1¬∑maxùëêùëÉ(ùëê‚à£ùê¨)+ùë§2¬∑ùêª(ùëÉ(¬∑‚à£ùê¨))+ùë§3¬∑Consistency(ùê¨)Cpixel(s)=w1¬∑maxcP(c‚à£s)+w2¬∑HP(¬∑‚à£s)+w3¬∑Consistency(s)(7)whereHdenotes prediction entropy, andConsistency(ùê¨)Consistency(s)measures agreement across multiple inference passes with dropout-based uncertainty estimation. 3.3. DiagNeXt-Cls: Multi-Scale Feature Fusion and Adaptive Classification NetworkFollowing the segmentation stage, DiagNeXt-Cls implements a sophisticated region-of-interest (ROI)-based classification approach that operates exclusively on the pathological regions identified and delineated by the modified U-Net architecture of DiagNeXt-Seg. The classification network receives precisely extracted ROI patches along with their corresponding segmentation confidence scores, enabling focused analysis of potential lesions without interference from surrounding healthy tissue. This two-stage cascade design ensures that the classification model can dedicate its full computational capacity to distinguishing between different pathology types rather than simultaneously handling lesion detection and classification tasks.The architectural overview of the classification module is illustrated inFigure 1.Figure 1.DiagNeXt-Cls multi-resolution processing architecture with Context-Aware Feature Fusion (CAFF) and Evidential Deep Learning (EDL) classifier.3.3.1. ROI Extraction and Preprocessing PipelineThe transition from segmentation to classification involves a carefully designed ROI extraction pipeline that transforms the multi-class segmentation masks produced by DiagNeXt-Seg into standardized input patches for the classification network. Given the segmentation outputùëÄ‚àà‚Ñùùêª√óùëä√óùê∑√óùê∂M‚ààRH√óW√óD√óCwhereCrepresents the number of classes (background, normal kidney, cyst, tumor, stone), the system identifies connected components for each pathological class using 3D morphological operations.For each detected lesion region‚ÑõùëñRi, the extraction process involves:Connected Component Analysis: Apply 3D connected component labeling with minimum volume thresholdùëâùëöùëñùëõ=27Vmin=27voxels to eliminate noise artifacts while preserving small but clinically significant lesions.Bounding Box Computation: Calculate the minimal 3D bounding box‚Ñ¨ùëñ={ùë•ùëöùëñùëõ,ùë¶ùëöùëñùëõ,ùëßùëöùëñùëõ,ùë•ùëöùëéùë•,ùë¶ùëöùëéùë•,ùëßùëöùëéùë•}Bi={xmin,ymin,zmin,xmax,ymax,zmax}that encompasses the entire lesion with a contextual margin of 15% to include surrounding parenchymal information.Adaptive Size Normalization: Resize all extracted ROIs to a standardized dimension of96√ó96√ó3296√ó96√ó32voxels using trilinear interpolation, with aspect ratio preservation for lesions with extreme dimensions to maintain morphological integrity.Multi-Level Intensity Standardization: Apply hierarchical normalization combining global CT windowing followed by local z-score normalization:ùêºùëõùëúùëüùëö=Clip(ùêº‚àíùúáùëÖùëÇùêºùúéùëÖùëÇùêº+ùúñ,‚àí3,3)Inorm=Clip(I‚àíŒºROIœÉROI+œµ,‚àí3,3)whereùúñ=10‚àí6œµ=10‚àí6prevents division by zero.Confidence Score Aggregation: Compute comprehensive segmentation confidenceùíÆùëñSiincorporating spatial consistency, boundary sharpness, and prediction certainty from the segmentation network.This preprocessing pipeline ensures that the classification network receives consistent, high-quality inputs that maintain critical diagnostic information while standardizing the input format for optimal network performance.3.3.2. Architecture Foundation and Design PhilosophyThe DiagNeXt-Cls architecture is built upon a substantially modified ConvNeXt backbone, specifically redesigned for 3D medical ROI classification. Our network introduces a Progressive Feature Refinement (PFR) strategy to systematically process ROI patches across multiple resolution scales and feature abstraction levels.Kidney lesion classification faces a fundamental challenge: pathological features appear at vastly different spatial scales. For instance, early-stage tumors show subtle textural changes visible only at high resolution, while advanced cysts present clear structural deformations even at lower resolutions. To address this, our architecture employs specialized parallel processing pathways that analyze the same ROI concurrently at multiple scales.The network architecture consists of four primary processing stages with progressively increasing feature complexity: {128, 256, 512, 1024} channels, respectively. Each stage contains multiple Adaptive Convolutional Blocks (ACBs) that combine the computational efficiency of depthwise separable convolutions with the representational power of inverted bottleneck designs. The ACB formulation extends beyond traditional ConvNeXt blocks by incorporating medical imaging-specific enhancements:‚Ñé1‚Ñé2‚Ñé3‚Ñé4ACB(ùë•)=DWConv3D5√ó5√ó3(GroupNorm(ùë•))=PWConv3D1√ó1√ó1(SiLU(‚Ñé1))=PWConv3D1√ó1√ó1(Dropout0.15(‚Ñé2))=SE3D(‚Ñé3)¬∑‚Ñé3=ùë•+ùõº¬∑LayerScale(‚Ñé4)h1=DWConv3D5√ó5√ó3(GroupNorm(x))h2=PWConv3D1√ó1√ó1(SiLU(h1))h3=PWConv3D1√ó1√ó1(Dropout0.15(h2))h4=SE3D(h3)¬∑h3ACB(x)=x+Œ±¬∑LayerScale(h4)(8)where SE3D represents a 3D Squeeze-and-Excitation mechanism adapted for volumetric data,ùõºŒ±is a learnable scaling parameter initialized to 0.1, andLayerScale(¬∑)LayerScale(¬∑)provides per-channel scaling for improved training stability.3.3.3. Hierarchical Multi-Resolution Processing StrategyA cornerstone innovation of DiagNeXt-Cls is the implementation of Hierarchical Multi-Resolution Processing (HMRP), which processes each extracted ROI simultaneously at three different resolution levels. This approach recognizes that kidney pathologies exhibit characteristic features across multiple spatial scales, and optimal classification requires integration of information from all relevant scales.Given an input ROIùëÖ‚àà‚Ñù96√ó96√ó32R‚ààR96√ó96√ó32, the HMRP module generates three parallel processing streams:Fine-scale pathway (ùëÖùëìRf): Processes the ROI at original resolution (96 √ó 96 √ó 32) using small kernel convolutions (3 √ó 3 √ó 3) with high channel dimensions (256‚Äì1024) to capture subtle textural patterns, microcalcifications, and early neoplastic changes.Medium-scale pathway (ùëÖùëöRm): Operates on a 0.75√ó downsampled version (72 √ó 72 √ó 24) using moderate kernel sizes (5 √ó 5 √ó 3) with intermediate channel dimensions (128‚Äì512) to identify structural patterns, lesion boundaries, and intermediate morphological features.Coarse-scale pathway (ùëÖùëêRc): Analyzes 0.5√ó downsampled representation (48 √ó 48 √ó 16) using large kernel convolutions (7 √ó 7 √ó 5) with lower channel dimensions (64‚Äì256) to capture global morphological characteristics, overall lesion shape, and spatial relationships.Each pathway employs pathway-specific architectural optimizations designed to maximize the extraction of scale-appropriate features while maintaining computational efficiency.3.3.4. Context-Aware Adaptive Feature FusionThe integration of multi-resolution features represents a critical challenge in the DiagNeXt-Cls design. Traditional concatenation or simple averaging approaches fail to account for the varying importance of different scales across different pathology types. To address this, we introduce the Context-Aware Feature Fusion (CAFF) module that dynamically determines the optimal weighting of each resolution stream based on the specific characteristics of the input ROI.ùêπfused=‚àëùë†‚àà{ùëì,ùëö,ùëê}ùë§ùë†(ùíû,ùíÆ)ùúôùë†(ùêπùë†)ùúìùë†(ùêπùë†)Ffused=‚àës‚àà{f,m,c}ws(C,S)œïs(Fs)œàs(Fs)(9)whereùë§ùë†wsrepresents dynamically computed attention weights,ùúôùë†œïsapplies global average pooling, andùúìùë†œàsimplements spatial attention mechanisms tailored for each resolution scale.3.3.5. Evidential Deep Learning Framework for Uncertainty QuantificationMedical diagnosis inherently involves uncertainty, particularly when dealing with ambiguous presentations, early-stage pathologies, or rare lesion types. DiagNeXt-Cls addresses this fundamental challenge through an innovative Evidential Deep Learning (EDL) framework that provides principled uncertainty quantification alongside classification predictions.The classification head produces concentration parametersùõº=[ùõº1,ùõº2,‚Ä¶,ùõºùêæ]Œ±=[Œ±1,Œ±2,‚Ä¶,Œ±K]for a Dirichlet distribution:ùëù(ùê≤‚à£ùê±)=Dir(ùû™)=Œì(‚àëùêæùëò=1ùõºùëò)‚àèùêæùëò=1Œì(ùõºùëò)‚àèùëò=1ùêæùë¶ùõºùëò‚àí1ùëòp(y‚à£x)=Dir(Œ±)=Œì‚àëk=1KŒ±k‚àèk=1KŒì(Œ±k)‚àèk=1KykŒ±k‚àí1(10)This formulation enables explicit quantification of both aleatoric and epistemic uncertainties, providing clinicians with confidence measures that reflect both data ambiguity and model limitations.3.3.6. Segmentation-Classification Confidence IntegrationA distinctive innovation of DiagNeXt-Cls lies in its seamless integration of segmentation quality assessment with classification decision-making. The segmentation confidence scoreùíÆùëñSiincorporates multiple quality indicators and is integrated through a Confidence-Modulated Feature Scaling (CMFS) mechanism that ensures poorly segmented regions contribute proportionally less to classification decisions.3.3.7. Multi-Objective Training Strategy with Adaptive Loss WeightingThe training of DiagNeXt-Cls employs a sophisticated multi-objective approach that balances classification accuracy, uncertainty calibration, and segmentation-awareness through a composite loss function:‚Ñíùëêùëôùë†=ùúÜ(1)ùë°‚Ñíùê∂ùê∏+ùúÜ(2)ùë°‚Ñíùê∏ùê∑ùêø+ùúÜ(3)ùë°‚Ñíùêæùêø+ùúÜ(4)ùë°‚ÑíùëêùëúùëõùëìLcls=Œªt(1)LCE+Œªt(2)LEDL+Œªt(3)LKL+Œªt(4)Lconf(11)where the adaptive weightsùúÜ(ùëñ)ùë°Œªt(i)are dynamically adjusted throughout training to maintain optimal balance between competing objectives, ensuring robust performance across diverse pathological presentations while maintaining well-calibrated uncertainty estimates.This comprehensive two-stage framework ensures that DiagNeXt delivers superior diagnostic accuracy while providing the uncertainty quantification and interpretability essential for clinical deployment in kidney pathology analysis applications.",
            "3.1. Overview of the DiagNeXt Framework": "The proposed DiagNeXt framework represents a comprehensive two-stage cascade architecture specifically designed for automated kidney pathology analysis in CT imaging. The framework addresses the fundamental challenges of medical image analysis by decomposing the complex task of simultaneous lesion detection and classification into two specialized, sequential stages that can each focus on their respective objectives with maximum efficiency. The first stage, DiagNeXt-Seg employs a heavily modified 3D U-Net architecture enhanced with modern architectural innovations including ConvNeXt-inspired blocks, multi-scale attention mechanisms, and boundary-aware loss formulations. This segmentation network produces precise multi-class masks that delineate normal kidney tissue, cysts, tumors, stones, and background regions with high spatial accuracy and confidence estimation. The second stage, DiagNeXt-Cls, implements a sophisticated ROI-based classification system that operates exclusively on the pathological regions identified by the segmentation stage. This classification network leverages multi-resolution feature processing, evidential deep learning for uncertainty quantification, and adaptive fusion mechanisms to achieve superior diagnostic accuracy while providing clinically meaningful confidence scores. Prior work leveraging deep feature engineering for fine-grained medical image classification further motivated the multi-resolution feature modeling adopted in DiagNeXt [33]. This cascade design philosophy offers several critical advantages over end-to-end approaches: (1) Computational Efficiency the classification network processes only relevant regions rather than entire volumes, (2) Specialized Optimization each stage can be optimized for its specific task without compromising the other, (3) Interpretability the framework provides both spatial localization and classification confidence, and (4) Clinical Integration the modular design allows for independent validation and deployment of each component. The overall workflow processes a 3D CT volume through the segmentation stage to identify potential lesions, extracts standardized ROI patches with associated confidence scores, and subsequently classifies each ROI using the specialized classification network. This approach has demonstrated superior performance compared to single-stage alternatives while providing the uncertainty quantification essential for clinical decision support systems.",
            "3.2. DiagNeXt-Seg: Enhanced 3D U-Net Architecture for Kidney Lesion Segmentation": "The segmentation component of DiagNeXt employs a substantially modified 3D U-Net architecture that incorporates several key innovations specifically tailored for kidney pathology detection. Traditional U-Net architectures, while effective for general medical segmentation tasks, face significant challenges when applied to kidney lesion detection due to the diverse scales, varied appearances, and subtle boundaries characteristic of renal pathologies. 3.2.1. Architectural Modifications and EnhancementsOur enhanced U-Net implementation, termed DiagNeXt-Seg, integrates modern convolutional building blocks derived from the ConvNeXt family while maintaining the proven encoder‚Äìdecoder structure that has established U-Net as the gold standard for medical image segmentation. The key architectural innovations include:ConvNeXt-Inspired Encoder Blocks: Each encoder level utilizes Enhanced Convolutional Blocks (ECBs) that replace traditional 3 √ó 3 convolutions with a more sophisticated design:ùë•1ùë•2ùë•3ùë•4ECB(ùë•)=DWConv3D7√ó7√ó3(ùë•)=GroupNorm(ùë•1)=PWConv3D1√ó1√ó1(ùë•2)=GELU(ùë•3)=ùë•+ùõæ‚äôPWConv3D1√ó1√ó1(ùë•4)x1=DWConv3D7√ó7√ó3(x)x2=GroupNorm(x1)x3=PWConv3D1√ó1√ó1(x2)x4=GELU(x3)ECB(x)=x+Œ≥‚äôPWConv3D1√ó1√ó1(x4)(1)where the large kernel depthwise convolution (7 √ó 7 √ó 3) captures long-range spatial dependencies similar to self-attention mechanisms, while the inverted bottleneck design with pointwise convolutions enables efficient feature transformation. The learnable scale parameterùõæŒ≥is initialized near zero to stabilize training in deep networks.Weight Initialization Strategy: The Enhanced Convolutional Blocks (ECBs) employ a comprehensive initialization scheme tailored for stable training in 3D medical imaging. Depthwise convolutional layers (DWConv3D7√ó7√ó3DWConv3D7√ó7√ó3) are initialized using He normal initialization [34], which preserves gradient variance in layers with ReLU/GELU activations. Pointwise convolutions (PWConv3D1√ó1√ó1PWConv3D1√ó1√ó1) utilize Xavier uniform initialization [35] to maintain activation variances across the network. The learnable scale parameterùõæŒ≥in Equation (5) is initialized to1√ó10‚àí61√ó10‚àí6to stabilize early training, following which it evolves freely during optimization. GroupNorm layers require no initialization as they operate on channel-wise statistics. This combined approach ensures stable gradient flow while enabling effective feature learning from the first training iterations.Multi-Scale Feature Aggregation: At the network bottleneck, we implement an Atrous Spatial Pyramid Pooling (ASPP) module adapted for 3D medical imaging:ùêπùëöùë†=Conv1√ó1([GAP(ùêπ);Conv(1)(ùêπ);Conv(3)(ùêπ);Conv(6)(ùêπ)])Fms=Conv1√ó1([GAP(F);Conv(1)(F);Conv(3)(F);Conv(6)(F)])(2)whereConv(ùëü)Conv(r)denotes 3D convolution with dilation rater, capturing features at multiple scales without resolution loss.Attention-Enhanced skip Connections: Traditional skip connections are augmented with spatial attention gates that selectively filter encoder features before fusion with decoder representations:ùõºùëéùë°ùë°=ùúé(ùëäùëáùúì[ReLU(ùëäùëîùëî+ùëäùë•ùë•+ùëè)])Œ±att=œÉWœàTReLU(Wgg+Wxx+b)(3)wheregrepresents the gating signal from the decoder,xdenotes encoder features, andùõºùëéùë°ùë°Œ±attprovides spatial attention weights that emphasize relevant anatomical regions while suppressing background noise.3D Kernel Design Rationale: The7√ó7√ó37√ó7√ó3kernel dimensions were carefully chosen to align with physical voxel characteristics:In-plane (7 √ó 7): Captures anatomical structures within high-resolution axial slices (0.6‚Äì0.8 mm/pixel);Through-plane (√ó3): Adapted to larger slice spacing (2.5‚Äì5.0 mm), balancing receptive field with computational cost;Computational Efficiency: Depthwise separable design reduces parameters by 8.5√ó compared to standard 3D convolutions;LayerScale Initialization:ùõæŒ≥parameters initialized to1√ó10‚àí61√ó10‚àí6for training stability. 3.2.2. Boundary-Aware Multi-Objective Loss FunctionAccurate boundary delineation represents a critical requirement for subsequent ROI extraction and classification. Our segmentation training employs a carefully designed compound loss function that addresses multiple aspects of segmentation quality:‚Ñíùë†ùëíùëî=ùõº‚Ñíùê∂ùê∏+ùõΩ‚Ñíùê∑ùëñùëêùëí+ùõæ‚Ñíùêµùëúùë¢ùëõùëëùëéùëüùë¶+ùõø‚ÑíùêπùëúùëêùëéùëôLseg=Œ±LCE+Œ≤LDice+Œ≥LBoundary+Œ¥LFocal(4)Loss Function Parameter Specification: The composite segmentation loss‚Ñíùë†ùëíùëî=ùõº‚Ñíùê∂ùê∏+ùõΩ‚Ñíùê∑ùëñùëêùëí+ùõæ‚Ñíùêµùëúùë¢ùëõùëëùëéùëüùë¶+ùõø‚ÑíùêπùëúùëêùëéùëôLseg=Œ±LCE+Œ≤LDice+Œ≥LBoundary+Œ¥LFocalemploys experimentally determined weights optimized for kidney pathology segmentation. Through extensive grid search on the validation set, we established the optimal parameters asùõº=0.4Œ±=0.4,ùõΩ=0.3Œ≤=0.3,ùõæ=0.2Œ≥=0.2, andùõø=0.1Œ¥=0.1. This configuration prioritizes cross-entropy for overall classification accuracy while maintaining strong boundary delineation through the boundary loss component. The focal loss term addresses class imbalance with its focusing parameterùõæùëìùëúùëêùëéùëô=2.0Œ≥focal=2.0, effectively down-weighting easy examples. The boundary loss utilizes a signed distance transform withùúé=1.5œÉ=1.5for smooth distance computation. This parameter combination was validated through 5-fold cross-validation, demonstrating consistent performance across all pathology classes.The Focal Loss component addresses class imbalance by down-weighting easy examples:‚Ñíùêπùëúùëêùëéùëô=‚àí‚àëùëñ,ùëê(1‚àíùëùùëñ,ùëê)ùõæùë¶ùëñ,ùëêlog(ùëùùëñ,ùëê)LFocal=‚àí‚àëi,c(1‚àípi,c)Œ≥yi,clog(pi,c)(5)The Boundary Loss specifically optimizes contour accuracy using distance transform regularization:‚Ñíùêµùëúùë¢ùëõùëëùëéùëüùë¶=‚àëùëê‚àëŒ©ùúôùëê(ùê¨)ùê∑ùëê(ùê¨)‚àëŒ©ùúôùëê(ùê¨)LBoundary=‚àëc‚àëŒ©œïc(s)Dc(s)‚àëŒ©œïc(s)(6)whereùúôùëê(ùê¨)œïc(s)represents the network output for classcat locationùê¨s, andùê∑ùëê(ùê¨)Dc(s)is the signed distance transform of the ground truth boundary. 3.2.3. Confidence Score GenerationA unique aspect of DiagNeXt-Seg is its ability to generate pixel-wise confidence scores that quantify segmentation uncertainty. These scores are computed using multiple complementary measures:ùíûùëùùëñùë•ùëíùëô(ùê¨)=ùë§1¬∑maxùëêùëÉ(ùëê‚à£ùê¨)+ùë§2¬∑ùêª(ùëÉ(¬∑‚à£ùê¨))+ùë§3¬∑Consistency(ùê¨)Cpixel(s)=w1¬∑maxcP(c‚à£s)+w2¬∑HP(¬∑‚à£s)+w3¬∑Consistency(s)(7)whereHdenotes prediction entropy, andConsistency(ùê¨)Consistency(s)measures agreement across multiple inference passes with dropout-based uncertainty estimation.",
            "3.2.1. Architectural Modifications and Enhancements": "Our enhanced U-Net implementation, termed DiagNeXt-Seg, integrates modern convolutional building blocks derived from the ConvNeXt family while maintaining the proven encoder‚Äìdecoder structure that has established U-Net as the gold standard for medical image segmentation. The key architectural innovations include: ConvNeXt-Inspired Encoder Blocks: Each encoder level utilizes Enhanced Convolutional Blocks (ECBs) that replace traditional 3 √ó 3 convolutions with a more sophisticated design:ùë•1ùë•2ùë•3ùë•4ECB(ùë•)=DWConv3D7√ó7√ó3(ùë•)=GroupNorm(ùë•1)=PWConv3D1√ó1√ó1(ùë•2)=GELU(ùë•3)=ùë•+ùõæ‚äôPWConv3D1√ó1√ó1(ùë•4)x1=DWConv3D7√ó7√ó3(x)x2=GroupNorm(x1)x3=PWConv3D1√ó1√ó1(x2)x4=GELU(x3)ECB(x)=x+Œ≥‚äôPWConv3D1√ó1√ó1(x4)(1)where the large kernel depthwise convolution (7 √ó 7 √ó 3) captures long-range spatial dependencies similar to self-attention mechanisms, while the inverted bottleneck design with pointwise convolutions enables efficient feature transformation. The learnable scale parameterùõæŒ≥is initialized near zero to stabilize training in deep networks. Weight Initialization Strategy: The Enhanced Convolutional Blocks (ECBs) employ a comprehensive initialization scheme tailored for stable training in 3D medical imaging. Depthwise convolutional layers (DWConv3D7√ó7√ó3DWConv3D7√ó7√ó3) are initialized using He normal initialization [34], which preserves gradient variance in layers with ReLU/GELU activations. Pointwise convolutions (PWConv3D1√ó1√ó1PWConv3D1√ó1√ó1) utilize Xavier uniform initialization [35] to maintain activation variances across the network. The learnable scale parameterùõæŒ≥in Equation (5) is initialized to1√ó10‚àí61√ó10‚àí6to stabilize early training, following which it evolves freely during optimization. GroupNorm layers require no initialization as they operate on channel-wise statistics. This combined approach ensures stable gradient flow while enabling effective feature learning from the first training iterations. Multi-Scale Feature Aggregation: At the network bottleneck, we implement an Atrous Spatial Pyramid Pooling (ASPP) module adapted for 3D medical imaging:ùêπùëöùë†=Conv1√ó1([GAP(ùêπ);Conv(1)(ùêπ);Conv(3)(ùêπ);Conv(6)(ùêπ)])Fms=Conv1√ó1([GAP(F);Conv(1)(F);Conv(3)(F);Conv(6)(F)])(2)whereConv(ùëü)Conv(r)denotes 3D convolution with dilation rater, capturing features at multiple scales without resolution loss. Attention-Enhanced skip Connections: Traditional skip connections are augmented with spatial attention gates that selectively filter encoder features before fusion with decoder representations:ùõºùëéùë°ùë°=ùúé(ùëäùëáùúì[ReLU(ùëäùëîùëî+ùëäùë•ùë•+ùëè)])Œ±att=œÉWœàTReLU(Wgg+Wxx+b)(3)wheregrepresents the gating signal from the decoder,xdenotes encoder features, andùõºùëéùë°ùë°Œ±attprovides spatial attention weights that emphasize relevant anatomical regions while suppressing background noise. 3D Kernel Design Rationale: The7√ó7√ó37√ó7√ó3kernel dimensions were carefully chosen to align with physical voxel characteristics: In-plane (7 √ó 7): Captures anatomical structures within high-resolution axial slices (0.6‚Äì0.8 mm/pixel);Through-plane (√ó3): Adapted to larger slice spacing (2.5‚Äì5.0 mm), balancing receptive field with computational cost;Computational Efficiency: Depthwise separable design reduces parameters by 8.5√ó compared to standard 3D convolutions;LayerScale Initialization:ùõæŒ≥parameters initialized to1√ó10‚àí61√ó10‚àí6for training stability.",
            "3.2.2. Boundary-Aware Multi-Objective Loss Function": "Accurate boundary delineation represents a critical requirement for subsequent ROI extraction and classification. Our segmentation training employs a carefully designed compound loss function that addresses multiple aspects of segmentation quality:‚Ñíùë†ùëíùëî=ùõº‚Ñíùê∂ùê∏+ùõΩ‚Ñíùê∑ùëñùëêùëí+ùõæ‚Ñíùêµùëúùë¢ùëõùëëùëéùëüùë¶+ùõø‚ÑíùêπùëúùëêùëéùëôLseg=Œ±LCE+Œ≤LDice+Œ≥LBoundary+Œ¥LFocal(4) Loss Function Parameter Specification: The composite segmentation loss‚Ñíùë†ùëíùëî=ùõº‚Ñíùê∂ùê∏+ùõΩ‚Ñíùê∑ùëñùëêùëí+ùõæ‚Ñíùêµùëúùë¢ùëõùëëùëéùëüùë¶+ùõø‚ÑíùêπùëúùëêùëéùëôLseg=Œ±LCE+Œ≤LDice+Œ≥LBoundary+Œ¥LFocalemploys experimentally determined weights optimized for kidney pathology segmentation. Through extensive grid search on the validation set, we established the optimal parameters asùõº=0.4Œ±=0.4,ùõΩ=0.3Œ≤=0.3,ùõæ=0.2Œ≥=0.2, andùõø=0.1Œ¥=0.1. This configuration prioritizes cross-entropy for overall classification accuracy while maintaining strong boundary delineation through the boundary loss component. The focal loss term addresses class imbalance with its focusing parameterùõæùëìùëúùëêùëéùëô=2.0Œ≥focal=2.0, effectively down-weighting easy examples. The boundary loss utilizes a signed distance transform withùúé=1.5œÉ=1.5for smooth distance computation. This parameter combination was validated through 5-fold cross-validation, demonstrating consistent performance across all pathology classes. The Focal Loss component addresses class imbalance by down-weighting easy examples:‚Ñíùêπùëúùëêùëéùëô=‚àí‚àëùëñ,ùëê(1‚àíùëùùëñ,ùëê)ùõæùë¶ùëñ,ùëêlog(ùëùùëñ,ùëê)LFocal=‚àí‚àëi,c(1‚àípi,c)Œ≥yi,clog(pi,c)(5) The Boundary Loss specifically optimizes contour accuracy using distance transform regularization:‚Ñíùêµùëúùë¢ùëõùëëùëéùëüùë¶=‚àëùëê‚àëŒ©ùúôùëê(ùê¨)ùê∑ùëê(ùê¨)‚àëŒ©ùúôùëê(ùê¨)LBoundary=‚àëc‚àëŒ©œïc(s)Dc(s)‚àëŒ©œïc(s)(6)whereùúôùëê(ùê¨)œïc(s)represents the network output for classcat locationùê¨s, andùê∑ùëê(ùê¨)Dc(s)is the signed distance transform of the ground truth boundary.",
            "3.2.3. Confidence Score Generation": "A unique aspect of DiagNeXt-Seg is its ability to generate pixel-wise confidence scores that quantify segmentation uncertainty. These scores are computed using multiple complementary measures:ùíûùëùùëñùë•ùëíùëô(ùê¨)=ùë§1¬∑maxùëêùëÉ(ùëê‚à£ùê¨)+ùë§2¬∑ùêª(ùëÉ(¬∑‚à£ùê¨))+ùë§3¬∑Consistency(ùê¨)Cpixel(s)=w1¬∑maxcP(c‚à£s)+w2¬∑HP(¬∑‚à£s)+w3¬∑Consistency(s)(7)whereHdenotes prediction entropy, andConsistency(ùê¨)Consistency(s)measures agreement across multiple inference passes with dropout-based uncertainty estimation.",
            "3.3. DiagNeXt-Cls: Multi-Scale Feature Fusion and Adaptive Classification Network": "Following the segmentation stage, DiagNeXt-Cls implements a sophisticated region-of-interest (ROI)-based classification approach that operates exclusively on the pathological regions identified and delineated by the modified U-Net architecture of DiagNeXt-Seg. The classification network receives precisely extracted ROI patches along with their corresponding segmentation confidence scores, enabling focused analysis of potential lesions without interference from surrounding healthy tissue. This two-stage cascade design ensures that the classification model can dedicate its full computational capacity to distinguishing between different pathology types rather than simultaneously handling lesion detection and classification tasks. The architectural overview of the classification module is illustrated inFigure 1. Figure 1.DiagNeXt-Cls multi-resolution processing architecture with Context-Aware Feature Fusion (CAFF) and Evidential Deep Learning (EDL) classifier. 3.3.1. ROI Extraction and Preprocessing PipelineThe transition from segmentation to classification involves a carefully designed ROI extraction pipeline that transforms the multi-class segmentation masks produced by DiagNeXt-Seg into standardized input patches for the classification network. Given the segmentation outputùëÄ‚àà‚Ñùùêª√óùëä√óùê∑√óùê∂M‚ààRH√óW√óD√óCwhereCrepresents the number of classes (background, normal kidney, cyst, tumor, stone), the system identifies connected components for each pathological class using 3D morphological operations.For each detected lesion region‚ÑõùëñRi, the extraction process involves:Connected Component Analysis: Apply 3D connected component labeling with minimum volume thresholdùëâùëöùëñùëõ=27Vmin=27voxels to eliminate noise artifacts while preserving small but clinically significant lesions.Bounding Box Computation: Calculate the minimal 3D bounding box‚Ñ¨ùëñ={ùë•ùëöùëñùëõ,ùë¶ùëöùëñùëõ,ùëßùëöùëñùëõ,ùë•ùëöùëéùë•,ùë¶ùëöùëéùë•,ùëßùëöùëéùë•}Bi={xmin,ymin,zmin,xmax,ymax,zmax}that encompasses the entire lesion with a contextual margin of 15% to include surrounding parenchymal information.Adaptive Size Normalization: Resize all extracted ROIs to a standardized dimension of96√ó96√ó3296√ó96√ó32voxels using trilinear interpolation, with aspect ratio preservation for lesions with extreme dimensions to maintain morphological integrity.Multi-Level Intensity Standardization: Apply hierarchical normalization combining global CT windowing followed by local z-score normalization:ùêºùëõùëúùëüùëö=Clip(ùêº‚àíùúáùëÖùëÇùêºùúéùëÖùëÇùêº+ùúñ,‚àí3,3)Inorm=Clip(I‚àíŒºROIœÉROI+œµ,‚àí3,3)whereùúñ=10‚àí6œµ=10‚àí6prevents division by zero.Confidence Score Aggregation: Compute comprehensive segmentation confidenceùíÆùëñSiincorporating spatial consistency, boundary sharpness, and prediction certainty from the segmentation network.This preprocessing pipeline ensures that the classification network receives consistent, high-quality inputs that maintain critical diagnostic information while standardizing the input format for optimal network performance. 3.3.2. Architecture Foundation and Design PhilosophyThe DiagNeXt-Cls architecture is built upon a substantially modified ConvNeXt backbone, specifically redesigned for 3D medical ROI classification. Our network introduces a Progressive Feature Refinement (PFR) strategy to systematically process ROI patches across multiple resolution scales and feature abstraction levels.Kidney lesion classification faces a fundamental challenge: pathological features appear at vastly different spatial scales. For instance, early-stage tumors show subtle textural changes visible only at high resolution, while advanced cysts present clear structural deformations even at lower resolutions. To address this, our architecture employs specialized parallel processing pathways that analyze the same ROI concurrently at multiple scales.The network architecture consists of four primary processing stages with progressively increasing feature complexity: {128, 256, 512, 1024} channels, respectively. Each stage contains multiple Adaptive Convolutional Blocks (ACBs) that combine the computational efficiency of depthwise separable convolutions with the representational power of inverted bottleneck designs. The ACB formulation extends beyond traditional ConvNeXt blocks by incorporating medical imaging-specific enhancements:‚Ñé1‚Ñé2‚Ñé3‚Ñé4ACB(ùë•)=DWConv3D5√ó5√ó3(GroupNorm(ùë•))=PWConv3D1√ó1√ó1(SiLU(‚Ñé1))=PWConv3D1√ó1√ó1(Dropout0.15(‚Ñé2))=SE3D(‚Ñé3)¬∑‚Ñé3=ùë•+ùõº¬∑LayerScale(‚Ñé4)h1=DWConv3D5√ó5√ó3(GroupNorm(x))h2=PWConv3D1√ó1√ó1(SiLU(h1))h3=PWConv3D1√ó1√ó1(Dropout0.15(h2))h4=SE3D(h3)¬∑h3ACB(x)=x+Œ±¬∑LayerScale(h4)(8)where SE3D represents a 3D Squeeze-and-Excitation mechanism adapted for volumetric data,ùõºŒ±is a learnable scaling parameter initialized to 0.1, andLayerScale(¬∑)LayerScale(¬∑)provides per-channel scaling for improved training stability. 3.3.3. Hierarchical Multi-Resolution Processing StrategyA cornerstone innovation of DiagNeXt-Cls is the implementation of Hierarchical Multi-Resolution Processing (HMRP), which processes each extracted ROI simultaneously at three different resolution levels. This approach recognizes that kidney pathologies exhibit characteristic features across multiple spatial scales, and optimal classification requires integration of information from all relevant scales.Given an input ROIùëÖ‚àà‚Ñù96√ó96√ó32R‚ààR96√ó96√ó32, the HMRP module generates three parallel processing streams:Fine-scale pathway (ùëÖùëìRf): Processes the ROI at original resolution (96 √ó 96 √ó 32) using small kernel convolutions (3 √ó 3 √ó 3) with high channel dimensions (256‚Äì1024) to capture subtle textural patterns, microcalcifications, and early neoplastic changes.Medium-scale pathway (ùëÖùëöRm): Operates on a 0.75√ó downsampled version (72 √ó 72 √ó 24) using moderate kernel sizes (5 √ó 5 √ó 3) with intermediate channel dimensions (128‚Äì512) to identify structural patterns, lesion boundaries, and intermediate morphological features.Coarse-scale pathway (ùëÖùëêRc): Analyzes 0.5√ó downsampled representation (48 √ó 48 √ó 16) using large kernel convolutions (7 √ó 7 √ó 5) with lower channel dimensions (64‚Äì256) to capture global morphological characteristics, overall lesion shape, and spatial relationships.Each pathway employs pathway-specific architectural optimizations designed to maximize the extraction of scale-appropriate features while maintaining computational efficiency. 3.3.4. Context-Aware Adaptive Feature FusionThe integration of multi-resolution features represents a critical challenge in the DiagNeXt-Cls design. Traditional concatenation or simple averaging approaches fail to account for the varying importance of different scales across different pathology types. To address this, we introduce the Context-Aware Feature Fusion (CAFF) module that dynamically determines the optimal weighting of each resolution stream based on the specific characteristics of the input ROI.ùêπfused=‚àëùë†‚àà{ùëì,ùëö,ùëê}ùë§ùë†(ùíû,ùíÆ)ùúôùë†(ùêπùë†)ùúìùë†(ùêπùë†)Ffused=‚àës‚àà{f,m,c}ws(C,S)œïs(Fs)œàs(Fs)(9)whereùë§ùë†wsrepresents dynamically computed attention weights,ùúôùë†œïsapplies global average pooling, andùúìùë†œàsimplements spatial attention mechanisms tailored for each resolution scale. 3.3.5. Evidential Deep Learning Framework for Uncertainty QuantificationMedical diagnosis inherently involves uncertainty, particularly when dealing with ambiguous presentations, early-stage pathologies, or rare lesion types. DiagNeXt-Cls addresses this fundamental challenge through an innovative Evidential Deep Learning (EDL) framework that provides principled uncertainty quantification alongside classification predictions.The classification head produces concentration parametersùõº=[ùõº1,ùõº2,‚Ä¶,ùõºùêæ]Œ±=[Œ±1,Œ±2,‚Ä¶,Œ±K]for a Dirichlet distribution:ùëù(ùê≤‚à£ùê±)=Dir(ùû™)=Œì(‚àëùêæùëò=1ùõºùëò)‚àèùêæùëò=1Œì(ùõºùëò)‚àèùëò=1ùêæùë¶ùõºùëò‚àí1ùëòp(y‚à£x)=Dir(Œ±)=Œì‚àëk=1KŒ±k‚àèk=1KŒì(Œ±k)‚àèk=1KykŒ±k‚àí1(10)This formulation enables explicit quantification of both aleatoric and epistemic uncertainties, providing clinicians with confidence measures that reflect both data ambiguity and model limitations. 3.3.6. Segmentation-Classification Confidence IntegrationA distinctive innovation of DiagNeXt-Cls lies in its seamless integration of segmentation quality assessment with classification decision-making. The segmentation confidence scoreùíÆùëñSiincorporates multiple quality indicators and is integrated through a Confidence-Modulated Feature Scaling (CMFS) mechanism that ensures poorly segmented regions contribute proportionally less to classification decisions. 3.3.7. Multi-Objective Training Strategy with Adaptive Loss WeightingThe training of DiagNeXt-Cls employs a sophisticated multi-objective approach that balances classification accuracy, uncertainty calibration, and segmentation-awareness through a composite loss function:‚Ñíùëêùëôùë†=ùúÜ(1)ùë°‚Ñíùê∂ùê∏+ùúÜ(2)ùë°‚Ñíùê∏ùê∑ùêø+ùúÜ(3)ùë°‚Ñíùêæùêø+ùúÜ(4)ùë°‚ÑíùëêùëúùëõùëìLcls=Œªt(1)LCE+Œªt(2)LEDL+Œªt(3)LKL+Œªt(4)Lconf(11)where the adaptive weightsùúÜ(ùëñ)ùë°Œªt(i)are dynamically adjusted throughout training to maintain optimal balance between competing objectives, ensuring robust performance across diverse pathological presentations while maintaining well-calibrated uncertainty estimates.This comprehensive two-stage framework ensures that DiagNeXt delivers superior diagnostic accuracy while providing the uncertainty quantification and interpretability essential for clinical deployment in kidney pathology analysis applications.",
            "3.3.1. ROI Extraction and Preprocessing Pipeline": "The transition from segmentation to classification involves a carefully designed ROI extraction pipeline that transforms the multi-class segmentation masks produced by DiagNeXt-Seg into standardized input patches for the classification network. Given the segmentation outputùëÄ‚àà‚Ñùùêª√óùëä√óùê∑√óùê∂M‚ààRH√óW√óD√óCwhereCrepresents the number of classes (background, normal kidney, cyst, tumor, stone), the system identifies connected components for each pathological class using 3D morphological operations. For each detected lesion region‚ÑõùëñRi, the extraction process involves: Connected Component Analysis: Apply 3D connected component labeling with minimum volume thresholdùëâùëöùëñùëõ=27Vmin=27voxels to eliminate noise artifacts while preserving small but clinically significant lesions.Bounding Box Computation: Calculate the minimal 3D bounding box‚Ñ¨ùëñ={ùë•ùëöùëñùëõ,ùë¶ùëöùëñùëõ,ùëßùëöùëñùëõ,ùë•ùëöùëéùë•,ùë¶ùëöùëéùë•,ùëßùëöùëéùë•}Bi={xmin,ymin,zmin,xmax,ymax,zmax}that encompasses the entire lesion with a contextual margin of 15% to include surrounding parenchymal information.Adaptive Size Normalization: Resize all extracted ROIs to a standardized dimension of96√ó96√ó3296√ó96√ó32voxels using trilinear interpolation, with aspect ratio preservation for lesions with extreme dimensions to maintain morphological integrity.Multi-Level Intensity Standardization: Apply hierarchical normalization combining global CT windowing followed by local z-score normalization:ùêºùëõùëúùëüùëö=Clip(ùêº‚àíùúáùëÖùëÇùêºùúéùëÖùëÇùêº+ùúñ,‚àí3,3)Inorm=Clip(I‚àíŒºROIœÉROI+œµ,‚àí3,3)whereùúñ=10‚àí6œµ=10‚àí6prevents division by zero.Confidence Score Aggregation: Compute comprehensive segmentation confidenceùíÆùëñSiincorporating spatial consistency, boundary sharpness, and prediction certainty from the segmentation network. This preprocessing pipeline ensures that the classification network receives consistent, high-quality inputs that maintain critical diagnostic information while standardizing the input format for optimal network performance.",
            "3.3.2. Architecture Foundation and Design Philosophy": "The DiagNeXt-Cls architecture is built upon a substantially modified ConvNeXt backbone, specifically redesigned for 3D medical ROI classification. Our network introduces a Progressive Feature Refinement (PFR) strategy to systematically process ROI patches across multiple resolution scales and feature abstraction levels. Kidney lesion classification faces a fundamental challenge: pathological features appear at vastly different spatial scales. For instance, early-stage tumors show subtle textural changes visible only at high resolution, while advanced cysts present clear structural deformations even at lower resolutions. To address this, our architecture employs specialized parallel processing pathways that analyze the same ROI concurrently at multiple scales. The network architecture consists of four primary processing stages with progressively increasing feature complexity: {128, 256, 512, 1024} channels, respectively. Each stage contains multiple Adaptive Convolutional Blocks (ACBs) that combine the computational efficiency of depthwise separable convolutions with the representational power of inverted bottleneck designs. The ACB formulation extends beyond traditional ConvNeXt blocks by incorporating medical imaging-specific enhancements:‚Ñé1‚Ñé2‚Ñé3‚Ñé4ACB(ùë•)=DWConv3D5√ó5√ó3(GroupNorm(ùë•))=PWConv3D1√ó1√ó1(SiLU(‚Ñé1))=PWConv3D1√ó1√ó1(Dropout0.15(‚Ñé2))=SE3D(‚Ñé3)¬∑‚Ñé3=ùë•+ùõº¬∑LayerScale(‚Ñé4)h1=DWConv3D5√ó5√ó3(GroupNorm(x))h2=PWConv3D1√ó1√ó1(SiLU(h1))h3=PWConv3D1√ó1√ó1(Dropout0.15(h2))h4=SE3D(h3)¬∑h3ACB(x)=x+Œ±¬∑LayerScale(h4)(8)where SE3D represents a 3D Squeeze-and-Excitation mechanism adapted for volumetric data,ùõºŒ±is a learnable scaling parameter initialized to 0.1, andLayerScale(¬∑)LayerScale(¬∑)provides per-channel scaling for improved training stability.",
            "3.3.3. Hierarchical Multi-Resolution Processing Strategy": "A cornerstone innovation of DiagNeXt-Cls is the implementation of Hierarchical Multi-Resolution Processing (HMRP), which processes each extracted ROI simultaneously at three different resolution levels. This approach recognizes that kidney pathologies exhibit characteristic features across multiple spatial scales, and optimal classification requires integration of information from all relevant scales. Given an input ROIùëÖ‚àà‚Ñù96√ó96√ó32R‚ààR96√ó96√ó32, the HMRP module generates three parallel processing streams: Fine-scale pathway (ùëÖùëìRf): Processes the ROI at original resolution (96 √ó 96 √ó 32) using small kernel convolutions (3 √ó 3 √ó 3) with high channel dimensions (256‚Äì1024) to capture subtle textural patterns, microcalcifications, and early neoplastic changes.Medium-scale pathway (ùëÖùëöRm): Operates on a 0.75√ó downsampled version (72 √ó 72 √ó 24) using moderate kernel sizes (5 √ó 5 √ó 3) with intermediate channel dimensions (128‚Äì512) to identify structural patterns, lesion boundaries, and intermediate morphological features.Coarse-scale pathway (ùëÖùëêRc): Analyzes 0.5√ó downsampled representation (48 √ó 48 √ó 16) using large kernel convolutions (7 √ó 7 √ó 5) with lower channel dimensions (64‚Äì256) to capture global morphological characteristics, overall lesion shape, and spatial relationships. Each pathway employs pathway-specific architectural optimizations designed to maximize the extraction of scale-appropriate features while maintaining computational efficiency.",
            "3.3.4. Context-Aware Adaptive Feature Fusion": "The integration of multi-resolution features represents a critical challenge in the DiagNeXt-Cls design. Traditional concatenation or simple averaging approaches fail to account for the varying importance of different scales across different pathology types. To address this, we introduce the Context-Aware Feature Fusion (CAFF) module that dynamically determines the optimal weighting of each resolution stream based on the specific characteristics of the input ROI.ùêπfused=‚àëùë†‚àà{ùëì,ùëö,ùëê}ùë§ùë†(ùíû,ùíÆ)ùúôùë†(ùêπùë†)ùúìùë†(ùêπùë†)Ffused=‚àës‚àà{f,m,c}ws(C,S)œïs(Fs)œàs(Fs)(9)whereùë§ùë†wsrepresents dynamically computed attention weights,ùúôùë†œïsapplies global average pooling, andùúìùë†œàsimplements spatial attention mechanisms tailored for each resolution scale.",
            "3.3.5. Evidential Deep Learning Framework for Uncertainty Quantification": "Medical diagnosis inherently involves uncertainty, particularly when dealing with ambiguous presentations, early-stage pathologies, or rare lesion types. DiagNeXt-Cls addresses this fundamental challenge through an innovative Evidential Deep Learning (EDL) framework that provides principled uncertainty quantification alongside classification predictions. The classification head produces concentration parametersùõº=[ùõº1,ùõº2,‚Ä¶,ùõºùêæ]Œ±=[Œ±1,Œ±2,‚Ä¶,Œ±K]for a Dirichlet distribution:ùëù(ùê≤‚à£ùê±)=Dir(ùû™)=Œì(‚àëùêæùëò=1ùõºùëò)‚àèùêæùëò=1Œì(ùõºùëò)‚àèùëò=1ùêæùë¶ùõºùëò‚àí1ùëòp(y‚à£x)=Dir(Œ±)=Œì‚àëk=1KŒ±k‚àèk=1KŒì(Œ±k)‚àèk=1KykŒ±k‚àí1(10) This formulation enables explicit quantification of both aleatoric and epistemic uncertainties, providing clinicians with confidence measures that reflect both data ambiguity and model limitations.",
            "3.3.6. Segmentation-Classification Confidence Integration": "A distinctive innovation of DiagNeXt-Cls lies in its seamless integration of segmentation quality assessment with classification decision-making. The segmentation confidence scoreùíÆùëñSiincorporates multiple quality indicators and is integrated through a Confidence-Modulated Feature Scaling (CMFS) mechanism that ensures poorly segmented regions contribute proportionally less to classification decisions.",
            "3.3.7. Multi-Objective Training Strategy with Adaptive Loss Weighting": "The training of DiagNeXt-Cls employs a sophisticated multi-objective approach that balances classification accuracy, uncertainty calibration, and segmentation-awareness through a composite loss function:‚Ñíùëêùëôùë†=ùúÜ(1)ùë°‚Ñíùê∂ùê∏+ùúÜ(2)ùë°‚Ñíùê∏ùê∑ùêø+ùúÜ(3)ùë°‚Ñíùêæùêø+ùúÜ(4)ùë°‚ÑíùëêùëúùëõùëìLcls=Œªt(1)LCE+Œªt(2)LEDL+Œªt(3)LKL+Œªt(4)Lconf(11)where the adaptive weightsùúÜ(ùëñ)ùë°Œªt(i)are dynamically adjusted throughout training to maintain optimal balance between competing objectives, ensuring robust performance across diverse pathological presentations while maintaining well-calibrated uncertainty estimates. This comprehensive two-stage framework ensures that DiagNeXt delivers superior diagnostic accuracy while providing the uncertainty quantification and interpretability essential for clinical deployment in kidney pathology analysis applications.",
            "4. Experimental Results": "4.1. Dataset and Experimental Setup3D Volume Reconstruction from 2D CT Slices: The Kaggle dataset provides 2D CT slices, which we reconstructed into 3D volumes using DICOM metadata to ensure anatomical consistency. The reconstruction process involved:Slice Sorting: Slices from each patient were sorted using DICOM Image Position (Patient) tags and Instance Number to maintain correct anatomical sequence.Volume Assembly: For patients with sufficient slices, we constructed complete kidney volumes spanning the entire organ. The average volume contained 12‚Äì18 slices with a slice spacing of 2.5‚Äì5.0 mm (as per original CT acquisition protocols).Partial Volume Handling: For patients with limited slices (<8 slices), we employed slice interpolation using B-spline interpolation to achieve the minimum required depth, while explicitly noting these as reconstructed volumes in our analysis.ROI Standardization: All extracted ROIs were resampled to the standardized96√ó96√ó3296√ó96√ó32voxel size using trilinear interpolation, with zero-padding for smaller volumes and center-cropping for larger ones.To provide a clearer view of the dataset composition and the characteristics of complete versus interpolated volumes used in our experiments, the distribution of patient studies, average slice count, and slice spacing statistics is summarized inTable 1.Table 1.Classification performance on validation and test sets. Support values indicate number of ROI instances, not patients. Each patient contributes multiple ROIs (average 2.16 ROIs/patient).Clinical Realism and Impact Assessment: We acknowledge that our approach involves reconstructing 3D volumes from 2D slices, which may affect clinical realism in the following ways:Spatial Context Preservation: The reconstruction maintains inter-slice spatial relationships, enabling genuine 3D feature learning across adjacent slices;ASPP Effectiveness: The 3D ASPP module operates on reconstructed volumes, capturing multi-scale contextual information across all three dimensions;Limitation Transparency: We explicitly note that slice interpolation (16.4% of cases) introduces synthetic data, though our ablation studies show minimal performance impact;Comparative Advantage: Despite reconstruction, our 3D approach still outperforms 2D methods by leveraging cross-slice contextual information unavailable in single-slice analysis.Comprehensive Preprocessing Pipeline: CT volumes underwent a standardized preprocessing protocol beginning with DICOM to Hounsfield Unit (HU) conversion using scanner-specific rescale slope and intercept values. For the reconstructed 3D volumes, kidney-specific windowing was applied with width/level settings of 400/40 HU. Volumes were resampled to a isotropic voxel spacing of1.0√ó1.0√ó2.51.0√ó1.0√ó2.5mm to balance through-plane and in-plane resolution, given the typically larger slice spacing in clinical CT protocols. Intensity normalization employed a three-stage approach: (1) global z-scoring across the entire volume, (2) organ-specific normalization within kidney masks, and (3) local patch-based standardization for texture analysis. Bias field correction was applied using the N4ITK algorithm to address scanner-induced intensity inhomogeneities. Finally, volumes were padded to dimensions divisible by 16 to accommodate the network‚Äôs downsampling operations while preserving spatial information.Dataset Composition and Patient-Level Splitting: The Kaggle CT Kidney Dataset [36] originally contains 12,446 CT slices from 3847 unique patients, with each patient contributing multiple slices across different anatomical levels. These slices were reconstructed into 3D volumes as described above. To ensure robust evaluation and prevent data leakage, we implemented strict patient-level splitting:Patient Identification: Patient identifiers were extracted from DICOM metadata, and all slices from the same patient were assigned to the same split;Split Strategy: 70% of patients (2693 patients) for training, 15% (577 patients) for validation, and 15% (577 patients) for testing;Data Leakage Prevention: No slices from the same patient appear in different splits, ensuring complete patient independence;Class Distribution: The splitting maintained proportional representation of all pathology classes across all splits.To ensure complete transparency regarding dataset composition, we provide a detailed summary of the patient-level split and slice distribution inTable 2. This verifies that the 70/15/15 split was strictly applied at the patient level and that no data leakage occurred across splits. Furthermore, to confirm that each pathology type was proportionally represented in all partitions, the class distribution across training, validation, and test sets is reported inTable 3.Table 2.Patient and Image Distribution Across Dataset Splits.Table 3.Pathology Class Distribution Across Splits.Ethical Compliance and Data Provenance: The Kaggle dataset [36] is fully de-identified and publicly available for research use. Original data collection was conducted with appropriate ethical approvals as reported in ref. [36]. Our study uses the data as provided without additional IRB requirements per institutional guidelines for publicly available, anonymized datasets.Data Quality Assessment:Missing Data: No missing slices detected in the dataset;Contrast Protocol: All scans are contrast-enhanced CT studies;Slice Thickness: Original slice spacing: 2.5‚Äì5.0 mm, reconstructed to 2.5 mm isotropic;Voxel Size: In-plane: 0.6‚Äì0.8 mm, reconstructed to 1.0 √ó 1.0 √ó 2.5 mm.Our comprehensive evaluation was conducted on a curated kidney CT dataset comprising 3847 patients with ground truth annotations for four pathology classes: Normal kidney tissue, Cysts, Tumors, and Stones. The dataset was obtained from Kaggle [36] and carefully processed to ensure patient-level integrity across all experimental splits. It was originally introduced by Islam et al. [36] as part of their work on Vision Transformer-based kidney segmentation. Our patient-level partitioning strategy guarantees no data leakage between splits, with each patient‚Äôs complete data exclusively assigned to one split.Generalization Assurance: The strict patient-level splitting ensures that our performance metrics reflect true generalization capability rather than memorization of patient-specific characteristics. This rigorous approach validates our comparisons with prior work and supports the clinical relevance of our findings.Reproducibility Protocol: All experiments used fixed random seeds (42 for data splitting, 123 for model initialization). The PyTorch (version 2.9.1), CUDA Toolkit (version 12.4), and Python (version 3.11) software environments were used to ensure reproducibility across all experiments.An overview of the dataset structure and class distribution is illustrated inFigure 2.Figure 2.Dataset overview showing representative CT slices with corresponding annotations for Normal tissue, Cyst, Tumor, and Stone. The red circles highlight the region of interest (ROI) manually verified by the radiologist, indicating the precise anatomical location of the pathology in each example.The DiagNeXt framework was trained using a two-stage protocol. DiagNeXt-Seg was trained for 100 epochs using AdamW optimizer with an initial learning rate of1√ó10‚àí41√ó10‚àí4and cosine annealing scheduling. DiagNeXt-Cls was subsequently trained for 50 epochs with a lower learning rate of5√ó10‚àí55√ó10‚àí5to prevent overfitting on the extracted ROI features. Data augmentation strategies included random rotation (¬±15¬±15), elastic deformation, intensity scaling, and gamma correction to enhance model generalization.Detailed Training Hyperparameters: The AdamW optimizer was configured withùõΩ1=0.9Œ≤1=0.9,ùõΩ2=0.999Œ≤2=0.999, andùúñ=1√ó10‚àí8œµ=1√ó10‚àí8for stable momentum estimation. A weight decay of0.010.01was applied for effective regularization, with gradient clipping at a maximum norm of1.01.0to prevent explosion. The learning rate followed a cosine annealing schedule with warm-up: starting from1√ó10‚àí61√ó10‚àí6for the first 5 epochs, then increasing to the maximum learning rate (1√ó10‚àí41√ó10‚àí4for DiagNeXt-Seg,5√ó10‚àí55√ó10‚àí5for DiagNeXt-Cls), followed by cosine decay to1√ó10‚àí71√ó10‚àí7over the remaining epochs. Batch sizes were set to 4 for DiagNeXt-Seg and 16 for DiagNeXt-Cls, optimized for GPU memory constraints while maintaining training stability. Mixed precision training (FP16) was employed to accelerate computation without sacrificing precision. 4.2. Performance Metrics and EvaluationWe evaluated DiagNeXt using standard medical imaging metrics including accuracy, precision, recall, F1-score, and Area Under the Curve (AUC) for each pathology class. For segmentation evaluation, we computed the Dice similarity coefficient, Hausdorff distance, and boundary IoU to assess both region overlap and boundary precision.Statistical Significance Analysis: All results include 95% confidence intervals calculated via bootstrapping (1000 samples). Ablation study variances represent standard deviations across 5 independent runs with different random seeds (seeTable 4).Table 4.Performance Metrics with 95% Confidence Intervals.4.2.1. Classification PerformanceDiagNeXt achieved exceptional classification performance across all pathology types, as demonstrated inTable 5. These results represent ROI-based pathology classification performance, where DiagNeXt-Cls operates on precisely segmented regions of interest extracted by DiagNeXt-Seg. The model attained an overall accuracy of 99.1% on the validation set and 98.9% on the test set, significantly outperforming existing state-of-the-art methods.Table 5.Classification performance on validation and test sets.The confusion matrices for both validation and test sets, shown inFigure 3andFigure 4, reveal excellent discrimination capability across all classes. Notably, the model achieved perfect classification for Normal kidney tissue with zero false positives, demonstrating robust specificity essential for clinical applications.Figure 3.Validation set confusion matrix showing near-perfect classification performance across all pathology types.Figure 4.Test set confusion matrix demonstrating consistent performance and generalization capability.Both the validation (Figure 3) and test (Figure 4) confusion matrices were inspected, and the minor label overlap observed in the figures does not affect scientific interpretation or classification clarity.4.2.2. ROC Analysis and AUC PerformanceValidation of Perfect AUC Scores: We acknowledge that near-perfect AUC scores (1.000, 0.999) are unusual in medical imaging. To validate these results and exclude data leakage, we performed the following:Patient-Level Cross-Validation: Conducted 5-fold patient-level cross-validation (AUC: Normal = 0.998 ¬± 0.001, Tumor = 0.997 ¬± 0.002, Cyst = 0.995 ¬± 0.003, Stone = 0.991 ¬± 0.004);Data Leakage Audit: Verified no patient overlap between splits using DICOM metadata hashing;Label Quality Assessment: Performed expert re-review of 200 random samples from test set;Statistical Testing: Bootstrapped 95% confidence intervals confirm AUC > 0.99 for all classes.The Receiver Operating Characteristic (ROC) analysis presented inFigure 5demonstrates exceptional discriminative performance across all pathology classes. DiagNeXt achieved near-perfect AUC scores: Normal (1.000), Tumor (1.000), Cyst (0.999), and Stone (0.994). These results significantly exceed the performance of existing kidney pathology classification methods reported in the literature.Figure 5.Receiver Operating Characteristic (ROC) curves for multi-class classification, illustrating the true positive rate versus false positive rate for each pathology type. The model achieves near-perfect discrimination, with AUC scores of 1.000 for Normal and Tumor, 0.999 for Cyst, and 0.994 for Stone, demonstrating strong generalisation across all classes.The consistently high AUC values across all classes indicate that DiagNeXt maintains excellent sensitivity-specificity balance, crucial for clinical decision-making where both false positives and false negatives carry significant consequences.Loss Function Parameter Specification: The composite segmentation loss employs experimentally determined weights:ùõº=0.4Œ±=0.4,ùõΩ=0.3Œ≤=0.3,ùõæ=0.2Œ≥=0.2,ùõø=0.1Œ¥=0.1. These were optimized through grid search on the validation set. The adaptive weightsùúÜ(ùëñ)ùë°Œªt(i)in‚ÑíclsLclsfollow a temperature-based scheduling:ùúÜ(ùëñ)ùë°=exp(ùëßùëñ/ùúè)‚àëùëóexp(ùëßùëó/ùúè)Œªt(i)=exp(zi/œÑ)‚àëjexp(zj/œÑ)whereùëßùëñzirepresents task-specific learning progress andùúè=2.0œÑ=2.0controls the softmax temperature.4.2.3. Feature Learning and Representation QualityTo evaluate the quality of learned feature representations, we performed t-SNE visualization of the extracted features from DiagNeXt-Cls, as shown inFigure 6. The visualization reveals well-separated clusters for each pathology class, indicating that the multi-resolution processing and attention mechanisms successfully capture discriminative features.Figure 6.t-SNE visualization of the learned feature embeddings, showing clear and well-separated clusters for each pathology class. The minor point-level overlap observed in the projection is inherent to dimensionality reduction and does not affect the scientific interpretation of class separability.The clear separation between Normal tissue (blue) and pathological classes, along with distinct boundaries between Cyst (orange), Tumor (green), and Stone (red) clusters, validates the effectiveness of our hierarchical multi-resolution processing approach. 4.3. Attention Visualization and GradCAM AnalysisFigure 7presents GradCAM visualizations demonstrating DiagNeXt‚Äôs ability to focus on clinically relevant regions. The attention maps reveal that the model successfully identifies and highlights pathological areas while ignoring irrelevant background regions.Figure 7.GradCAM visualizations from 50 randomly selected test set cases (12‚Äì13 per pathology class). Sample selection ensured balanced class representation. Warmer colors (yellow‚Äìred) indicate regions with higher model attention and stronger contribution to the predicted class, while cooler colors (blue‚Äìgreen) correspond to low-importance areas.The visualizations demonstrate several key insights:For Cysts: The model focuses on the characteristic hypodense, well-circumscribed lesions with smooth borders;For Tumors: Attention concentrates on heterogeneous enhancement patterns and irregular margins typical of renal cell carcinoma;For Stones: The model highlights high-density calcifications and associated inflammatory changes;For Normal tissue: Attention is distributed across normal parenchymal architecture without focal abnormalities. 4.4. Training Dynamics and Convergence AnalysisFigure 8presents the training dynamics for the DiagNeXt-Seg segmentation network, which performs voxel-wise multi-class segmentation‚Äîa fundamentally different and more challenging task than the ROI-based pathology classification reported inSection 3.2.Figure 8.DiagNeXt-Seg segmentation network training and validation accuracy/loss curves showing stable convergence and excellent generalization. Note: These accuracy values represent segmentation performance on voxel-level classification, which is inherently more challenging than the ROI-based pathology classification reported inSection 3.2.Key observations from the training dynamics:Rapid initial convergence within the first 10 epochs due to effective feature learning;Stable performance plateau after epoch 20, indicating optimal model capacity;Minimal overfitting with validation loss closely following training loss.Final convergence to 94.8% training accuracy and 86.7% validation accuracy for the DiagNeXt-Seg segmentation network, which performs voxel-level classification‚Äîa fundamentally different and more challenging task than the ROI-based pathology classification reported inSection 3.2.Clarification: The accuracy values reported here (94.8% training, 86.7% validation) correspond to voxel-level segmentation accuracy for DiagNeXt-Seg, where the model must classify each individual voxel into one of five classes (background, normal kidney, cyst, tumor, stone). This is distinct from the ROI-level pathology classification accuracy (98.9‚Äì99.1%) achieved by DiagNeXt-Cls, which operates on pre-segmented regions of interest and focuses solely on distinguishing between different pathology types. The segmentation task is inherently more challenging due to severe class imbalance (predominantly background voxels) and the fine-grained spatial precision required. 4.5. Comparison with State-of-the-Art MethodsTable 6presents a comprehensive comparison of DiagNeXt with existing kidney pathology analysis methods from recent literature.Table 6.Comparison with state-of-the-art kidney pathology analysis methods.DiagNeXt demonstrates substantial improvements over existing methods, achieving 6.8% higher accuracy than the next best performing approach. The superior performance can be attributed to:Two-stage architecture: Enables specialized optimization for segmentation and classification tasks;Multi-resolution processing: Captures features at multiple scales essential for diverse pathology sizes;Attention mechanisms: Focus learning on diagnostically relevant regions;Evidential deep learning: Provides calibrated uncertainty estimation;Boundary-aware training: Improves ROI extraction quality through precise segmentation. 4.6. Ablation StudiesTo validate the contribution of each component, we conducted comprehensive ablation studies as presented inTable 7.Table 7.Ablation study results showing the contribution of key components.Each component contributes meaningfully to the overall performance, with multi-resolution processing providing the largest individual improvement (+1.8%), followed by ConvNeXt backbone (+2.6%) and attention mechanisms (+1.3%).Ablation Study Methodology: All ablation experiments used identical test sets and random seeds. The +7.7% improvement represents the cumulative gain from all components over the baseline, accounting for synergistic effects between modules. 4.7. Computational Efficiency AnalysisDiagNeXt demonstrates excellent computational efficiency compared to end-to-end alternatives. The two-stage approach processes only 12‚Äì15% of the total image volume (ROI regions), resulting in 6.2√ó speedup during inference while maintaining superior accuracy. Training time is reduced by 40% compared to equivalent end-to-end architectures due to specialized optimization of each stage.Computational Efficiency Benchmarking: Performance comparisons conducted against nnU-Net [18] and MedNeXt [11] on identical hardware (NVIDIA RTX 4090). Metrics include:Inference Speed: 6.2√ó faster than end-to-end 3D approaches (18.2 vs. 112.8 ms/volume);Memory Efficiency: Peak VRAM usage: 8.3 GB vs. 14.2 GB (nnU-Net);Throughput: 54.9 volumes/s vs. 8.9 volumes/s;Training Time: 40% reduction (38 h vs. 63 h for full training). 4.8. Clinical Validation and Error AnalysisAnalysis of misclassified cases reveals that errors primarily occur in:Small lesions (<5 mm) at the resolution limit of CT imaging;Complex cysts with septations that mimic tumor characteristics;Inflammatory processes that obscure normal tissue boundaries;Motion artifacts affecting image quality.These failure modes align with known clinical challenges, suggesting that DiagNeXt has learned clinically relevant decision boundaries rather than exploiting dataset artifacts. 4.9. Summary of Key AchievementsDiagNeXt establishes new state-of-the-art performance for kidney pathology analysis with the following key achievements:Exceptional accuracy: 98.9% test accuracy, surpassing previous best by 6.8%;Balanced performance: Near-perfect metrics across all pathology classes;Clinical interpretability: GradCAM visualizations and uncertainty quantification;Computational efficiency: 6.2√ó faster inference through focused ROI processing;Robust generalization: Consistent performance across validation and test sets.These results demonstrate DiagNeXt‚Äôs potential for clinical deployment as a reliable computer-aided diagnosis system for kidney pathology screening and assessment.",
            "4.1. Dataset and Experimental Setup": "3D Volume Reconstruction from 2D CT Slices: The Kaggle dataset provides 2D CT slices, which we reconstructed into 3D volumes using DICOM metadata to ensure anatomical consistency. The reconstruction process involved: Slice Sorting: Slices from each patient were sorted using DICOM Image Position (Patient) tags and Instance Number to maintain correct anatomical sequence.Volume Assembly: For patients with sufficient slices, we constructed complete kidney volumes spanning the entire organ. The average volume contained 12‚Äì18 slices with a slice spacing of 2.5‚Äì5.0 mm (as per original CT acquisition protocols).Partial Volume Handling: For patients with limited slices (<8 slices), we employed slice interpolation using B-spline interpolation to achieve the minimum required depth, while explicitly noting these as reconstructed volumes in our analysis.ROI Standardization: All extracted ROIs were resampled to the standardized96√ó96√ó3296√ó96√ó32voxel size using trilinear interpolation, with zero-padding for smaller volumes and center-cropping for larger ones. To provide a clearer view of the dataset composition and the characteristics of complete versus interpolated volumes used in our experiments, the distribution of patient studies, average slice count, and slice spacing statistics is summarized inTable 1. Table 1.Classification performance on validation and test sets. Support values indicate number of ROI instances, not patients. Each patient contributes multiple ROIs (average 2.16 ROIs/patient). Clinical Realism and Impact Assessment: We acknowledge that our approach involves reconstructing 3D volumes from 2D slices, which may affect clinical realism in the following ways: Spatial Context Preservation: The reconstruction maintains inter-slice spatial relationships, enabling genuine 3D feature learning across adjacent slices;ASPP Effectiveness: The 3D ASPP module operates on reconstructed volumes, capturing multi-scale contextual information across all three dimensions;Limitation Transparency: We explicitly note that slice interpolation (16.4% of cases) introduces synthetic data, though our ablation studies show minimal performance impact;Comparative Advantage: Despite reconstruction, our 3D approach still outperforms 2D methods by leveraging cross-slice contextual information unavailable in single-slice analysis. Comprehensive Preprocessing Pipeline: CT volumes underwent a standardized preprocessing protocol beginning with DICOM to Hounsfield Unit (HU) conversion using scanner-specific rescale slope and intercept values. For the reconstructed 3D volumes, kidney-specific windowing was applied with width/level settings of 400/40 HU. Volumes were resampled to a isotropic voxel spacing of1.0√ó1.0√ó2.51.0√ó1.0√ó2.5mm to balance through-plane and in-plane resolution, given the typically larger slice spacing in clinical CT protocols. Intensity normalization employed a three-stage approach: (1) global z-scoring across the entire volume, (2) organ-specific normalization within kidney masks, and (3) local patch-based standardization for texture analysis. Bias field correction was applied using the N4ITK algorithm to address scanner-induced intensity inhomogeneities. Finally, volumes were padded to dimensions divisible by 16 to accommodate the network‚Äôs downsampling operations while preserving spatial information. Dataset Composition and Patient-Level Splitting: The Kaggle CT Kidney Dataset [36] originally contains 12,446 CT slices from 3847 unique patients, with each patient contributing multiple slices across different anatomical levels. These slices were reconstructed into 3D volumes as described above. To ensure robust evaluation and prevent data leakage, we implemented strict patient-level splitting: Patient Identification: Patient identifiers were extracted from DICOM metadata, and all slices from the same patient were assigned to the same split;Split Strategy: 70% of patients (2693 patients) for training, 15% (577 patients) for validation, and 15% (577 patients) for testing;Data Leakage Prevention: No slices from the same patient appear in different splits, ensuring complete patient independence;Class Distribution: The splitting maintained proportional representation of all pathology classes across all splits. To ensure complete transparency regarding dataset composition, we provide a detailed summary of the patient-level split and slice distribution inTable 2. This verifies that the 70/15/15 split was strictly applied at the patient level and that no data leakage occurred across splits. Furthermore, to confirm that each pathology type was proportionally represented in all partitions, the class distribution across training, validation, and test sets is reported inTable 3. Table 2.Patient and Image Distribution Across Dataset Splits. Table 3.Pathology Class Distribution Across Splits. Ethical Compliance and Data Provenance: The Kaggle dataset [36] is fully de-identified and publicly available for research use. Original data collection was conducted with appropriate ethical approvals as reported in ref. [36]. Our study uses the data as provided without additional IRB requirements per institutional guidelines for publicly available, anonymized datasets. Data Quality Assessment: Missing Data: No missing slices detected in the dataset;Contrast Protocol: All scans are contrast-enhanced CT studies;Slice Thickness: Original slice spacing: 2.5‚Äì5.0 mm, reconstructed to 2.5 mm isotropic;Voxel Size: In-plane: 0.6‚Äì0.8 mm, reconstructed to 1.0 √ó 1.0 √ó 2.5 mm. Our comprehensive evaluation was conducted on a curated kidney CT dataset comprising 3847 patients with ground truth annotations for four pathology classes: Normal kidney tissue, Cysts, Tumors, and Stones. The dataset was obtained from Kaggle [36] and carefully processed to ensure patient-level integrity across all experimental splits. It was originally introduced by Islam et al. [36] as part of their work on Vision Transformer-based kidney segmentation. Our patient-level partitioning strategy guarantees no data leakage between splits, with each patient‚Äôs complete data exclusively assigned to one split. Generalization Assurance: The strict patient-level splitting ensures that our performance metrics reflect true generalization capability rather than memorization of patient-specific characteristics. This rigorous approach validates our comparisons with prior work and supports the clinical relevance of our findings. Reproducibility Protocol: All experiments used fixed random seeds (42 for data splitting, 123 for model initialization). The PyTorch (version 2.9.1), CUDA Toolkit (version 12.4), and Python (version 3.11) software environments were used to ensure reproducibility across all experiments. An overview of the dataset structure and class distribution is illustrated inFigure 2. Figure 2.Dataset overview showing representative CT slices with corresponding annotations for Normal tissue, Cyst, Tumor, and Stone. The red circles highlight the region of interest (ROI) manually verified by the radiologist, indicating the precise anatomical location of the pathology in each example. The DiagNeXt framework was trained using a two-stage protocol. DiagNeXt-Seg was trained for 100 epochs using AdamW optimizer with an initial learning rate of1√ó10‚àí41√ó10‚àí4and cosine annealing scheduling. DiagNeXt-Cls was subsequently trained for 50 epochs with a lower learning rate of5√ó10‚àí55√ó10‚àí5to prevent overfitting on the extracted ROI features. Data augmentation strategies included random rotation (¬±15¬±15), elastic deformation, intensity scaling, and gamma correction to enhance model generalization. Detailed Training Hyperparameters: The AdamW optimizer was configured withùõΩ1=0.9Œ≤1=0.9,ùõΩ2=0.999Œ≤2=0.999, andùúñ=1√ó10‚àí8œµ=1√ó10‚àí8for stable momentum estimation. A weight decay of0.010.01was applied for effective regularization, with gradient clipping at a maximum norm of1.01.0to prevent explosion. The learning rate followed a cosine annealing schedule with warm-up: starting from1√ó10‚àí61√ó10‚àí6for the first 5 epochs, then increasing to the maximum learning rate (1√ó10‚àí41√ó10‚àí4for DiagNeXt-Seg,5√ó10‚àí55√ó10‚àí5for DiagNeXt-Cls), followed by cosine decay to1√ó10‚àí71√ó10‚àí7over the remaining epochs. Batch sizes were set to 4 for DiagNeXt-Seg and 16 for DiagNeXt-Cls, optimized for GPU memory constraints while maintaining training stability. Mixed precision training (FP16) was employed to accelerate computation without sacrificing precision.",
            "4.2. Performance Metrics and Evaluation": "We evaluated DiagNeXt using standard medical imaging metrics including accuracy, precision, recall, F1-score, and Area Under the Curve (AUC) for each pathology class. For segmentation evaluation, we computed the Dice similarity coefficient, Hausdorff distance, and boundary IoU to assess both region overlap and boundary precision. Statistical Significance Analysis: All results include 95% confidence intervals calculated via bootstrapping (1000 samples). Ablation study variances represent standard deviations across 5 independent runs with different random seeds (seeTable 4). Table 4.Performance Metrics with 95% Confidence Intervals. 4.2.1. Classification PerformanceDiagNeXt achieved exceptional classification performance across all pathology types, as demonstrated inTable 5. These results represent ROI-based pathology classification performance, where DiagNeXt-Cls operates on precisely segmented regions of interest extracted by DiagNeXt-Seg. The model attained an overall accuracy of 99.1% on the validation set and 98.9% on the test set, significantly outperforming existing state-of-the-art methods.Table 5.Classification performance on validation and test sets.The confusion matrices for both validation and test sets, shown inFigure 3andFigure 4, reveal excellent discrimination capability across all classes. Notably, the model achieved perfect classification for Normal kidney tissue with zero false positives, demonstrating robust specificity essential for clinical applications.Figure 3.Validation set confusion matrix showing near-perfect classification performance across all pathology types.Figure 4.Test set confusion matrix demonstrating consistent performance and generalization capability.Both the validation (Figure 3) and test (Figure 4) confusion matrices were inspected, and the minor label overlap observed in the figures does not affect scientific interpretation or classification clarity. 4.2.2. ROC Analysis and AUC PerformanceValidation of Perfect AUC Scores: We acknowledge that near-perfect AUC scores (1.000, 0.999) are unusual in medical imaging. To validate these results and exclude data leakage, we performed the following:Patient-Level Cross-Validation: Conducted 5-fold patient-level cross-validation (AUC: Normal = 0.998 ¬± 0.001, Tumor = 0.997 ¬± 0.002, Cyst = 0.995 ¬± 0.003, Stone = 0.991 ¬± 0.004);Data Leakage Audit: Verified no patient overlap between splits using DICOM metadata hashing;Label Quality Assessment: Performed expert re-review of 200 random samples from test set;Statistical Testing: Bootstrapped 95% confidence intervals confirm AUC > 0.99 for all classes.The Receiver Operating Characteristic (ROC) analysis presented inFigure 5demonstrates exceptional discriminative performance across all pathology classes. DiagNeXt achieved near-perfect AUC scores: Normal (1.000), Tumor (1.000), Cyst (0.999), and Stone (0.994). These results significantly exceed the performance of existing kidney pathology classification methods reported in the literature.Figure 5.Receiver Operating Characteristic (ROC) curves for multi-class classification, illustrating the true positive rate versus false positive rate for each pathology type. The model achieves near-perfect discrimination, with AUC scores of 1.000 for Normal and Tumor, 0.999 for Cyst, and 0.994 for Stone, demonstrating strong generalisation across all classes.The consistently high AUC values across all classes indicate that DiagNeXt maintains excellent sensitivity-specificity balance, crucial for clinical decision-making where both false positives and false negatives carry significant consequences.Loss Function Parameter Specification: The composite segmentation loss employs experimentally determined weights:ùõº=0.4Œ±=0.4,ùõΩ=0.3Œ≤=0.3,ùõæ=0.2Œ≥=0.2,ùõø=0.1Œ¥=0.1. These were optimized through grid search on the validation set. The adaptive weightsùúÜ(ùëñ)ùë°Œªt(i)in‚ÑíclsLclsfollow a temperature-based scheduling:ùúÜ(ùëñ)ùë°=exp(ùëßùëñ/ùúè)‚àëùëóexp(ùëßùëó/ùúè)Œªt(i)=exp(zi/œÑ)‚àëjexp(zj/œÑ)whereùëßùëñzirepresents task-specific learning progress andùúè=2.0œÑ=2.0controls the softmax temperature. 4.2.3. Feature Learning and Representation QualityTo evaluate the quality of learned feature representations, we performed t-SNE visualization of the extracted features from DiagNeXt-Cls, as shown inFigure 6. The visualization reveals well-separated clusters for each pathology class, indicating that the multi-resolution processing and attention mechanisms successfully capture discriminative features.Figure 6.t-SNE visualization of the learned feature embeddings, showing clear and well-separated clusters for each pathology class. The minor point-level overlap observed in the projection is inherent to dimensionality reduction and does not affect the scientific interpretation of class separability.The clear separation between Normal tissue (blue) and pathological classes, along with distinct boundaries between Cyst (orange), Tumor (green), and Stone (red) clusters, validates the effectiveness of our hierarchical multi-resolution processing approach.",
            "4.2.1. Classification Performance": "DiagNeXt achieved exceptional classification performance across all pathology types, as demonstrated inTable 5. These results represent ROI-based pathology classification performance, where DiagNeXt-Cls operates on precisely segmented regions of interest extracted by DiagNeXt-Seg. The model attained an overall accuracy of 99.1% on the validation set and 98.9% on the test set, significantly outperforming existing state-of-the-art methods. Table 5.Classification performance on validation and test sets. The confusion matrices for both validation and test sets, shown inFigure 3andFigure 4, reveal excellent discrimination capability across all classes. Notably, the model achieved perfect classification for Normal kidney tissue with zero false positives, demonstrating robust specificity essential for clinical applications. Figure 3.Validation set confusion matrix showing near-perfect classification performance across all pathology types. Figure 4.Test set confusion matrix demonstrating consistent performance and generalization capability. Both the validation (Figure 3) and test (Figure 4) confusion matrices were inspected, and the minor label overlap observed in the figures does not affect scientific interpretation or classification clarity.",
            "4.2.2. ROC Analysis and AUC Performance": "Validation of Perfect AUC Scores: We acknowledge that near-perfect AUC scores (1.000, 0.999) are unusual in medical imaging. To validate these results and exclude data leakage, we performed the following: Patient-Level Cross-Validation: Conducted 5-fold patient-level cross-validation (AUC: Normal = 0.998 ¬± 0.001, Tumor = 0.997 ¬± 0.002, Cyst = 0.995 ¬± 0.003, Stone = 0.991 ¬± 0.004);Data Leakage Audit: Verified no patient overlap between splits using DICOM metadata hashing;Label Quality Assessment: Performed expert re-review of 200 random samples from test set;Statistical Testing: Bootstrapped 95% confidence intervals confirm AUC > 0.99 for all classes. The Receiver Operating Characteristic (ROC) analysis presented inFigure 5demonstrates exceptional discriminative performance across all pathology classes. DiagNeXt achieved near-perfect AUC scores: Normal (1.000), Tumor (1.000), Cyst (0.999), and Stone (0.994). These results significantly exceed the performance of existing kidney pathology classification methods reported in the literature. Figure 5.Receiver Operating Characteristic (ROC) curves for multi-class classification, illustrating the true positive rate versus false positive rate for each pathology type. The model achieves near-perfect discrimination, with AUC scores of 1.000 for Normal and Tumor, 0.999 for Cyst, and 0.994 for Stone, demonstrating strong generalisation across all classes. The consistently high AUC values across all classes indicate that DiagNeXt maintains excellent sensitivity-specificity balance, crucial for clinical decision-making where both false positives and false negatives carry significant consequences. Loss Function Parameter Specification: The composite segmentation loss employs experimentally determined weights:ùõº=0.4Œ±=0.4,ùõΩ=0.3Œ≤=0.3,ùõæ=0.2Œ≥=0.2,ùõø=0.1Œ¥=0.1. These were optimized through grid search on the validation set. The adaptive weightsùúÜ(ùëñ)ùë°Œªt(i)in‚ÑíclsLclsfollow a temperature-based scheduling:ùúÜ(ùëñ)ùë°=exp(ùëßùëñ/ùúè)‚àëùëóexp(ùëßùëó/ùúè)Œªt(i)=exp(zi/œÑ)‚àëjexp(zj/œÑ)whereùëßùëñzirepresents task-specific learning progress andùúè=2.0œÑ=2.0controls the softmax temperature.",
            "4.2.3. Feature Learning and Representation Quality": "To evaluate the quality of learned feature representations, we performed t-SNE visualization of the extracted features from DiagNeXt-Cls, as shown inFigure 6. The visualization reveals well-separated clusters for each pathology class, indicating that the multi-resolution processing and attention mechanisms successfully capture discriminative features. Figure 6.t-SNE visualization of the learned feature embeddings, showing clear and well-separated clusters for each pathology class. The minor point-level overlap observed in the projection is inherent to dimensionality reduction and does not affect the scientific interpretation of class separability. The clear separation between Normal tissue (blue) and pathological classes, along with distinct boundaries between Cyst (orange), Tumor (green), and Stone (red) clusters, validates the effectiveness of our hierarchical multi-resolution processing approach.",
            "4.3. Attention Visualization and GradCAM Analysis": "Figure 7presents GradCAM visualizations demonstrating DiagNeXt‚Äôs ability to focus on clinically relevant regions. The attention maps reveal that the model successfully identifies and highlights pathological areas while ignoring irrelevant background regions. Figure 7.GradCAM visualizations from 50 randomly selected test set cases (12‚Äì13 per pathology class). Sample selection ensured balanced class representation. Warmer colors (yellow‚Äìred) indicate regions with higher model attention and stronger contribution to the predicted class, while cooler colors (blue‚Äìgreen) correspond to low-importance areas. The visualizations demonstrate several key insights: For Cysts: The model focuses on the characteristic hypodense, well-circumscribed lesions with smooth borders;For Tumors: Attention concentrates on heterogeneous enhancement patterns and irregular margins typical of renal cell carcinoma;For Stones: The model highlights high-density calcifications and associated inflammatory changes;For Normal tissue: Attention is distributed across normal parenchymal architecture without focal abnormalities.",
            "4.4. Training Dynamics and Convergence Analysis": "Figure 8presents the training dynamics for the DiagNeXt-Seg segmentation network, which performs voxel-wise multi-class segmentation‚Äîa fundamentally different and more challenging task than the ROI-based pathology classification reported inSection 3.2. Figure 8.DiagNeXt-Seg segmentation network training and validation accuracy/loss curves showing stable convergence and excellent generalization. Note: These accuracy values represent segmentation performance on voxel-level classification, which is inherently more challenging than the ROI-based pathology classification reported inSection 3.2. Key observations from the training dynamics: Rapid initial convergence within the first 10 epochs due to effective feature learning;Stable performance plateau after epoch 20, indicating optimal model capacity;Minimal overfitting with validation loss closely following training loss.Final convergence to 94.8% training accuracy and 86.7% validation accuracy for the DiagNeXt-Seg segmentation network, which performs voxel-level classification‚Äîa fundamentally different and more challenging task than the ROI-based pathology classification reported inSection 3.2. Clarification: The accuracy values reported here (94.8% training, 86.7% validation) correspond to voxel-level segmentation accuracy for DiagNeXt-Seg, where the model must classify each individual voxel into one of five classes (background, normal kidney, cyst, tumor, stone). This is distinct from the ROI-level pathology classification accuracy (98.9‚Äì99.1%) achieved by DiagNeXt-Cls, which operates on pre-segmented regions of interest and focuses solely on distinguishing between different pathology types. The segmentation task is inherently more challenging due to severe class imbalance (predominantly background voxels) and the fine-grained spatial precision required.",
            "4.5. Comparison with State-of-the-Art Methods": "Table 6presents a comprehensive comparison of DiagNeXt with existing kidney pathology analysis methods from recent literature. Table 6.Comparison with state-of-the-art kidney pathology analysis methods. DiagNeXt demonstrates substantial improvements over existing methods, achieving 6.8% higher accuracy than the next best performing approach. The superior performance can be attributed to: Two-stage architecture: Enables specialized optimization for segmentation and classification tasks;Multi-resolution processing: Captures features at multiple scales essential for diverse pathology sizes;Attention mechanisms: Focus learning on diagnostically relevant regions;Evidential deep learning: Provides calibrated uncertainty estimation;Boundary-aware training: Improves ROI extraction quality through precise segmentation.",
            "4.6. Ablation Studies": "To validate the contribution of each component, we conducted comprehensive ablation studies as presented inTable 7. Table 7.Ablation study results showing the contribution of key components. Each component contributes meaningfully to the overall performance, with multi-resolution processing providing the largest individual improvement (+1.8%), followed by ConvNeXt backbone (+2.6%) and attention mechanisms (+1.3%). Ablation Study Methodology: All ablation experiments used identical test sets and random seeds. The +7.7% improvement represents the cumulative gain from all components over the baseline, accounting for synergistic effects between modules.",
            "4.7. Computational Efficiency Analysis": "DiagNeXt demonstrates excellent computational efficiency compared to end-to-end alternatives. The two-stage approach processes only 12‚Äì15% of the total image volume (ROI regions), resulting in 6.2√ó speedup during inference while maintaining superior accuracy. Training time is reduced by 40% compared to equivalent end-to-end architectures due to specialized optimization of each stage. Computational Efficiency Benchmarking: Performance comparisons conducted against nnU-Net [18] and MedNeXt [11] on identical hardware (NVIDIA RTX 4090). Metrics include: Inference Speed: 6.2√ó faster than end-to-end 3D approaches (18.2 vs. 112.8 ms/volume);Memory Efficiency: Peak VRAM usage: 8.3 GB vs. 14.2 GB (nnU-Net);Throughput: 54.9 volumes/s vs. 8.9 volumes/s;Training Time: 40% reduction (38 h vs. 63 h for full training).",
            "4.8. Clinical Validation and Error Analysis": "Analysis of misclassified cases reveals that errors primarily occur in: Small lesions (<5 mm) at the resolution limit of CT imaging;Complex cysts with septations that mimic tumor characteristics;Inflammatory processes that obscure normal tissue boundaries;Motion artifacts affecting image quality. These failure modes align with known clinical challenges, suggesting that DiagNeXt has learned clinically relevant decision boundaries rather than exploiting dataset artifacts.",
            "4.9. Summary of Key Achievements": "DiagNeXt establishes new state-of-the-art performance for kidney pathology analysis with the following key achievements: Exceptional accuracy: 98.9% test accuracy, surpassing previous best by 6.8%;Balanced performance: Near-perfect metrics across all pathology classes;Clinical interpretability: GradCAM visualizations and uncertainty quantification;Computational efficiency: 6.2√ó faster inference through focused ROI processing;Robust generalization: Consistent performance across validation and test sets. These results demonstrate DiagNeXt‚Äôs potential for clinical deployment as a reliable computer-aided diagnosis system for kidney pathology screening and assessment.",
            "5. Conclusions": "This paper introduced DiagNeXt, a novel two-stage deep learning framework specifically designed for comprehensive kidney pathology analysis in CT imaging. Through systematic integration of modern architectural innovations with domain-specific optimizations, DiagNeXt addresses the fundamental challenges that have limited the clinical adoption of automated kidney pathology analysis systems. 5.1. Key Contributions and InnovationsThe primary contributions of this work encompass both architectural innovations and methodological advances that collectively establish new performance benchmarks for kidney pathology analysis:Architectural Innovations: DiagNeXt introduces Enhanced Convolutional Blocks (ECBs) that adapt ConvNeXt design principles for 3D medical imaging, incorporating large kernel depthwise convolutions (7 √ó 7 √ó 3) with inverted bottleneck designs optimized for volumetric pathology detection. The hierarchical multi-resolution processing strategy enables simultaneous analysis of pathological features across multiple spatial scales, addressing the inherent challenge of diverse lesion sizes in kidney imaging. The Context-Aware Feature Fusion (CAFF) module dynamically weighs multi-scale contributions based on lesion characteristics and segmentation confidence, ensuring optimal feature integration for each pathology type.Training and Loss Function Design: The boundary-aware compound loss function represents a significant methodological advance, combining cross-entropy, Dice, focal, and distance transform losses to simultaneously optimize classification accuracy, region overlap, class balance, and boundary precision. This multi-objective approach ensures robust performance across diverse pathological presentations while maintaining precise lesion delineation essential for clinical applications.Uncertainty Quantification and Clinical Integration: The integration of Evidential Deep Learning provides principled uncertainty quantification that enables clinicians to assess prediction reliability. The confidence-modulated feature scaling mechanism incorporates segmentation quality metrics into classification decisions, creating a unified framework that leverages complementary information from both processing stages. 5.2. Performance Achievements and Clinical SignificanceDiagNeXt demonstrates exceptional performance across all evaluation metrics, achieving 98.9% classification accuracy on a comprehensive dataset of 3847 patients. This represents a substantial 6.8% improvement over the previous state-of-the-art, with near-perfect AUC scores across all pathology classes. The balanced performance across Normal tissue (Precision: 1.00, Recall: 0.99), Cysts (0.99, 0.99), Tumors (0.98, 0.98), and Stones (0.98, 0.98) demonstrates robust generalization capability essential for clinical deployment.The framework‚Äôs computational efficiency, achieving 6.2√ó faster inference through focused ROI processing, addresses practical deployment constraints while maintaining superior accuracy. The interpretable attention visualizations and calibrated uncertainty estimates provide clinicians with actionable insights that support informed decision-making, addressing the critical need for explainable AI in medical applications. 5.3. Clinical Impact and Deployment ConsiderationsThe superior performance of DiagNeXt across diverse pathology types, from subtle early-stage tumors to obvious large cysts, demonstrates its potential for broad clinical impact. The framework‚Äôs ability to handle challenging cases, including small lesions (<5 mm) and complex cystic formations, aligns with real-world clinical requirements where such cases often require specialist expertise.The modular two-stage architecture facilitates flexible deployment scenarios, allowing institutions to implement segmentation and classification components independently based on their specific workflows and computational resources. The uncertainty quantification capabilities enable graduated automation, where high-confidence cases can be processed automatically while uncertain cases are flagged for expert review, optimizing both efficiency and safety. 5.4. Limitations and Future DirectionsWhile DiagNeXt demonstrates exceptional performance, several limitations warrant consideration for future development. The framework‚Äôs performance on lesions smaller than 5mm remains limited by CT resolution constraints, suggesting potential benefits from higher-resolution imaging protocols or multi-modal integration. The current evaluation focuses on CT imaging; extension to MRI and ultrasound modalities would broaden clinical applicability.Future research directions include: (1) integration with multi-modal imaging for enhanced diagnostic accuracy, (2) extension to longitudinal analysis for monitoring disease progression, (3) adaptation to pediatric populations with different anatomical characteristics, (4) incorporation of clinical metadata (laboratory values, patient history) for comprehensive diagnostic support, and (5) development of federated learning approaches for training on distributed clinical datasets while preserving patient privacy. 5.5. Broader Impact on Medical AIBeyond kidney pathology analysis, the architectural innovations introduced in DiagNeXt have broader implications for medical image analysis. The Enhanced Convolutional Blocks and hierarchical multi-resolution processing strategies are directly applicable to other organ systems and pathology types. The uncertainty quantification framework addresses the critical need for reliable confidence estimation in medical AI applications, potentially accelerating clinical adoption across diverse medical imaging tasks.The successful integration of segmentation confidence into classification decisions demonstrates the value of cascade approaches that leverage complementary information between related tasks. This principle could be extended to other multi-stage medical analysis workflows, including detection-classification pipelines for various anatomical structures and pathological conditions. 5.6. Final RemarksDiagNeXt represents a significant advancement in automated kidney pathology analysis, establishing new performance benchmarks while addressing practical deployment considerations essential for clinical adoption. The framework‚Äôs combination of superior accuracy, computational efficiency, and clinical interpretability positions it as a valuable tool for supporting radiologists and clinicians in kidney disease diagnosis and treatment planning.The comprehensive evaluation on a large-scale clinical dataset, combined with detailed ablation studies demonstrating the contribution of each architectural component, provides strong evidence for the framework‚Äôs effectiveness and reliability. The open-source availability of the implementation will facilitate reproducibility and enable the broader research community to build upon these innovations.As medical imaging continues to generate increasingly complex and voluminous data, frameworks like DiagNeXt that combine state-of-the-art deep learning techniques with domain-specific optimizations and clinical interpretability will play an increasingly important role in advancing precision medicine and improving patient outcomes. The success of DiagNeXt in kidney pathology analysis provides a roadmap for developing similarly effective solutions for other challenging medical imaging tasks, ultimately contributing to the broader goal of AI-assisted healthcare that enhances rather than replaces clinical expertise.",
            "5.1. Key Contributions and Innovations": "The primary contributions of this work encompass both architectural innovations and methodological advances that collectively establish new performance benchmarks for kidney pathology analysis: Architectural Innovations: DiagNeXt introduces Enhanced Convolutional Blocks (ECBs) that adapt ConvNeXt design principles for 3D medical imaging, incorporating large kernel depthwise convolutions (7 √ó 7 √ó 3) with inverted bottleneck designs optimized for volumetric pathology detection. The hierarchical multi-resolution processing strategy enables simultaneous analysis of pathological features across multiple spatial scales, addressing the inherent challenge of diverse lesion sizes in kidney imaging. The Context-Aware Feature Fusion (CAFF) module dynamically weighs multi-scale contributions based on lesion characteristics and segmentation confidence, ensuring optimal feature integration for each pathology type. Training and Loss Function Design: The boundary-aware compound loss function represents a significant methodological advance, combining cross-entropy, Dice, focal, and distance transform losses to simultaneously optimize classification accuracy, region overlap, class balance, and boundary precision. This multi-objective approach ensures robust performance across diverse pathological presentations while maintaining precise lesion delineation essential for clinical applications. Uncertainty Quantification and Clinical Integration: The integration of Evidential Deep Learning provides principled uncertainty quantification that enables clinicians to assess prediction reliability. The confidence-modulated feature scaling mechanism incorporates segmentation quality metrics into classification decisions, creating a unified framework that leverages complementary information from both processing stages.",
            "5.2. Performance Achievements and Clinical Significance": "DiagNeXt demonstrates exceptional performance across all evaluation metrics, achieving 98.9% classification accuracy on a comprehensive dataset of 3847 patients. This represents a substantial 6.8% improvement over the previous state-of-the-art, with near-perfect AUC scores across all pathology classes. The balanced performance across Normal tissue (Precision: 1.00, Recall: 0.99), Cysts (0.99, 0.99), Tumors (0.98, 0.98), and Stones (0.98, 0.98) demonstrates robust generalization capability essential for clinical deployment. The framework‚Äôs computational efficiency, achieving 6.2√ó faster inference through focused ROI processing, addresses practical deployment constraints while maintaining superior accuracy. The interpretable attention visualizations and calibrated uncertainty estimates provide clinicians with actionable insights that support informed decision-making, addressing the critical need for explainable AI in medical applications.",
            "5.3. Clinical Impact and Deployment Considerations": "The superior performance of DiagNeXt across diverse pathology types, from subtle early-stage tumors to obvious large cysts, demonstrates its potential for broad clinical impact. The framework‚Äôs ability to handle challenging cases, including small lesions (<5 mm) and complex cystic formations, aligns with real-world clinical requirements where such cases often require specialist expertise. The modular two-stage architecture facilitates flexible deployment scenarios, allowing institutions to implement segmentation and classification components independently based on their specific workflows and computational resources. The uncertainty quantification capabilities enable graduated automation, where high-confidence cases can be processed automatically while uncertain cases are flagged for expert review, optimizing both efficiency and safety.",
            "5.4. Limitations and Future Directions": "While DiagNeXt demonstrates exceptional performance, several limitations warrant consideration for future development. The framework‚Äôs performance on lesions smaller than 5mm remains limited by CT resolution constraints, suggesting potential benefits from higher-resolution imaging protocols or multi-modal integration. The current evaluation focuses on CT imaging; extension to MRI and ultrasound modalities would broaden clinical applicability. Future research directions include: (1) integration with multi-modal imaging for enhanced diagnostic accuracy, (2) extension to longitudinal analysis for monitoring disease progression, (3) adaptation to pediatric populations with different anatomical characteristics, (4) incorporation of clinical metadata (laboratory values, patient history) for comprehensive diagnostic support, and (5) development of federated learning approaches for training on distributed clinical datasets while preserving patient privacy.",
            "5.5. Broader Impact on Medical AI": "Beyond kidney pathology analysis, the architectural innovations introduced in DiagNeXt have broader implications for medical image analysis. The Enhanced Convolutional Blocks and hierarchical multi-resolution processing strategies are directly applicable to other organ systems and pathology types. The uncertainty quantification framework addresses the critical need for reliable confidence estimation in medical AI applications, potentially accelerating clinical adoption across diverse medical imaging tasks. The successful integration of segmentation confidence into classification decisions demonstrates the value of cascade approaches that leverage complementary information between related tasks. This principle could be extended to other multi-stage medical analysis workflows, including detection-classification pipelines for various anatomical structures and pathological conditions.",
            "5.6. Final Remarks": "DiagNeXt represents a significant advancement in automated kidney pathology analysis, establishing new performance benchmarks while addressing practical deployment considerations essential for clinical adoption. The framework‚Äôs combination of superior accuracy, computational efficiency, and clinical interpretability positions it as a valuable tool for supporting radiologists and clinicians in kidney disease diagnosis and treatment planning. The comprehensive evaluation on a large-scale clinical dataset, combined with detailed ablation studies demonstrating the contribution of each architectural component, provides strong evidence for the framework‚Äôs effectiveness and reliability. The open-source availability of the implementation will facilitate reproducibility and enable the broader research community to build upon these innovations. As medical imaging continues to generate increasingly complex and voluminous data, frameworks like DiagNeXt that combine state-of-the-art deep learning techniques with domain-specific optimizations and clinical interpretability will play an increasingly important role in advancing precision medicine and improving patient outcomes. The success of DiagNeXt in kidney pathology analysis provides a roadmap for developing similarly effective solutions for other challenging medical imaging tasks, ultimately contributing to the broader goal of AI-assisted healthcare that enhances rather than replaces clinical expertise."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2313-433X/11/12/433",
        "scraped_at": "2025-12-05 23:53:43"
    },
    {
        "title": "Coverage-Conflict-Aware RFID Reader Placement with Range Adjustment for Complete Tag Coverage in IIoT",
        "authors": "byChien-Fu ChengandBo-Yan Liao",
        "journal": "Sensors2025,25(23), 7400; https://doi.org/10.3390/s25237400 (registering¬†DOI) - 4 Dec 2025",
        "abstract": "Radio Frequency Identification (RFID) is a core enabler of the Industrial Internet of Things (IIoT), yet dense deployments suffer from tag collisions and reader interference that degrade reliability and inflate infrastructure cost. This study proposes a deterministic Reader Deployment Algorithm with Adjustable Reader range (RDA2R) to achieve full tag coverage with minimal interference and reader usage. The method divides the monitored field into grid units, evaluates tag coverage weights, activates high-weight readers with interference checks, and adaptively adjusts interrogation ranges. Simulation results under random and congregation tag distributions show that RDA2R requires about 46‚Äì47% fewer readers than ARLDL and 32‚Äì33% fewer than MR2D, while improving average tag coverage per reader by over 30%. These results demonstrate that RDA2R provides a scalable, interference-aware, and cost-efficient deployment strategy for RFID-enabled IIoT environments.",
        "keywords": ":radio frequency identification (RFID); industrial internet of things (IIoT); reader-to-tag interference; reader-to-reader interference; collision avoidance; deterministic reader deployment; adjustable reading range; tag coverage optimizationradio frequency identification (RFID);industrial internet of things (IIoT);reader-to-tag interference;reader-to-reader interference;collision avoidance;deterministic reader deployment;adjustable reading range;tag coverage optimization",
        "full_content": {
            "Abstract": "Radio Frequency Identification (RFID) is a core enabler of the Industrial Internet of Things (IIoT), yet dense deployments suffer from tag collisions and reader interference that degrade reliability and inflate infrastructure cost. This study proposes a deterministic Reader Deployment Algorithm with Adjustable Reader range (RDA2R) to achieve full tag coverage with minimal interference and reader usage. The method divides the monitored field into grid units, evaluates tag coverage weights, activates high-weight readers with interference checks, and adaptively adjusts interrogation ranges. Simulation results under random and congregation tag distributions show that RDA2R requires about 46‚Äì47% fewer readers than ARLDL and 32‚Äì33% fewer than MR2D, while improving average tag coverage per reader by over 30%. These results demonstrate that RDA2R provides a scalable, interference-aware, and cost-efficient deployment strategy for RFID-enabled IIoT environments. Keywords:radio frequency identification (RFID); industrial internet of things (IIoT); reader-to-tag interference; reader-to-reader interference; collision avoidance; deterministic reader deployment; adjustable reading range; tag coverage optimizationradio frequency identification (RFID);industrial internet of things (IIoT);reader-to-tag interference;reader-to-reader interference;collision avoidance;deterministic reader deployment;adjustable reading range;tag coverage optimization",
            "Share and Cite": "MDPI and ACS StyleCheng, C.-F.;                     Liao, B.-Y.    \n        Coverage-Conflict-Aware RFID Reader Placement with Range Adjustment for Complete Tag Coverage in IIoT.Sensors2025,25, 7400.\n    https://doi.org/10.3390/s25237400AMA StyleCheng C-F,                                 Liao B-Y.        \n                Coverage-Conflict-Aware RFID Reader Placement with Range Adjustment for Complete Tag Coverage in IIoT.Sensors. 2025; 25(23):7400.\n        https://doi.org/10.3390/s25237400Chicago/Turabian StyleCheng, Chien-Fu,                                 and Bo-Yan Liao.        \n                2025. \"Coverage-Conflict-Aware RFID Reader Placement with Range Adjustment for Complete Tag Coverage in IIoT\"Sensors25, no. 23: 7400.\n        https://doi.org/10.3390/s25237400APA StyleCheng, C.-F.,                                 & Liao, B.-Y.        \n        \n        (2025). Coverage-Conflict-Aware RFID Reader Placement with Range Adjustment for Complete Tag Coverage in IIoT.Sensors,25(23), 7400.\n        https://doi.org/10.3390/s25237400 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further detailshere.",
            "Article Metrics": "YesCitationsNo citations were found for this article, but you may check onGoogle ScholarNoArticle Access StatisticsCreated with Highcharts 4.0.4Chart context menuArticle access statisticsArticle Views6. Dec0For more information on the journal statistics, clickhere.Multiple requests from the same IP address are counted as one view.",
            "Citations": "No citations were found for this article, but you may check onGoogle Scholar",
            "Article Access Statistics": "Created with Highcharts 4.0.4Chart context menuArticle access statisticsArticle Views6. Dec0 For more information on the journal statistics, click here . Multiple requests from the same IP address are counted as one view."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/1424-8220/25/23/7400",
        "scraped_at": "2025-12-05 23:53:52"
    },
    {
        "title": "Self-Supervised Learning for Soybean Disease Detection Using UAV Hyperspectral Imagery",
        "authors": "byMustafizur Rahaman,Vasit Sagan,Felipe A. Lopes,Haireti Alifu,Cagri Gul,Hadi AliakbarpourandKannappan Palaniappan",
        "journal": "Remote Sens.2025,17(23), 3928;https://doi.org/10.3390/rs17233928- 4 Dec 2025",
        "abstract": "HighlightsWhat are the main findings?A self-supervised learning framework achieves 92% accuracy in early soybean disease detection using unlabelled UAV hyperspectral data, matching supervised baselines.A distance-based spectral pairing technique enables effective feature learning directly from canopy reflectance without manual annotations.What are the implications of the main findings?The framework addresses the annotation bottleneck in remote sensing, enabling scalable early disease detection for large-scale agricultural monitoring.The approach reduces reliance on expert-labeled field data while maintaining high accuracy, making precision agriculture more accessible and cost-effective.AbstractThe accuracy of machine learning models in plant disease detection significantly relies on large volumes of knowledge-based labeled data; the acquisition of annotation remains a significant bottleneck in domain-specific research such as plant disease detection. While unsupervised learning alleviates the need for labeled data, its effectiveness is constrained by the intrinsic separability of feature clusters. These limitations underscore the need for approaches that enable supervised early disease detection without extensive annotation. To this end, we propose a self-supervised learning (SSL) framework for the early detection of soybean‚Äôs sudden death syndrome (SDS) using hyperspectral data acquired from an unmanned aerial vehicle (UAV). The methodology employs a novel distance-based spectral pairing technique that derives intermediate labels directly from the data. In addition, we introduce an adapted contrastive loss function designed to improve cluster separability and reinforce discriminative feature learning. The proposed approach yields an 11% accuracy gain over agglomerative hierarchical clustering and attains both classification accuracy and F1 score of 0.92, matching supervised baselines. Reflectance frequency analysis further demonstrates robustness to label noise, highlighting its suitability in label-scarce settings.Keywords:self-supervised learning;hyperspectral;plant disease;clustering;remote sensing",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Extreme climate events have intensified plant disease outbreaks over the past two decades, driven by rising temperatures, altered precipitation regimes, and¬†humidity fluctuations [1,2,3]. Soybeans, a¬†globally important crop for protein and oil, are particularly vulnerable to climate-driven stresses, with¬†heightened disease susceptibility threatening yield stability and long-term food security [4,5]. Conventional visual inspection remains the primary diagnostic method, but¬†is labor-intensive, subjective, and¬†inadequate for early detection when timely intervention is most critical [6]. Advances in remote sensing, integrated with machine learning, now offer scalable, non-invasive monitoring solutions that enable earlier diagnosis, targeted pesticide application, and¬†potential yield improvements, while reducing environmental impacts [7]. However, these approaches predominantly rely on supervised learning frameworks that require large volumes of manually annotated training¬†data. Recent work on plant disease detection has largely centered on leaf-image analysis with computer vision. Convolutional neural networks (CNNs) achieve high accuracy in controlled settings, predominantly on leaf image datasets [8]. However, leaf-image methods are poorly suited for identifying diseases before visible symptoms emerge, as¬†many stressors do not manifest visibly on the leaf surface [9], and¬†image acquisition at scale is operationally burdensome for commercial agriculture. This limitation extends to self-supervised learning approaches, which have primarily focused on controlled leaf-image datasets [10,11] rather than field-scale canopy¬†monitoring. To overcome these spatial and operational limitations of leaf-level approaches, unmanned aerial vehicles (UAVs) with hyperspectral sensors offer a field-scale alternative and have shown promise for early disease detection in experimental and commercial settings [12,13]. Narrow bands in the red-edge and near-infrared regions are sensitive to water status and subtle physiological changes that precede visible symptoms [14]. Despite these potential, the¬†practical deployment of UAV-based hyperspectral imaging for early detection faces a critical bottleneck, as¬†most supervised pipelines require large volumes of expertly labeled, in¬†situ data [15,16]. This dependency is especially problematic when symptoms are pre-visual and subtle, making annotation time-consuming, costly, and subjective [17,18]. The¬†challenge is amplified for root-origin diseases such as soybean sudden death syndrome (SDS), where early canopy signals are minimal and demand specialized expertise to identify reliably [19]. As¬†a result, assembling sufficiently large labeled datasets for robust supervised learning remains prohibitively expensive for large-scale monitoring, underscoring the need for methods that reduce annotation burden while preserving¬†accuracy. Self-supervised learning (SSL) offers a methodology with particular promise for addressing scenarios characterized by limited labeled data. Unlike unsupervised learning, which is hindered by the curse of dimensionality and limited feature separability [20,21,22], SSL creates pseudo-labels through pretext tasks. This approach enables representation learning without extensive manual annotation while maintaining discriminative power. Recent SSL frameworks, including contrastive learning [23], Siamese networks [24,25], BYOL [26], and¬†SimSiam [27], have demonstrated strong capabilities in extracting meaningful features from unlabeled data [28,29]. Despite SSL‚Äôs proven effectiveness in agricultural applications such as crop type classification and leaf-based disease detection [30], its potential for large-scale UAV-based hyperspectral monitoring of early-stage field diseases remains untapped. This gap is particularly critical because existing SSL frameworks face three key limitations when applied to field-scale monitoring. First, most existing studies focus on controlled leaf-image datasets [10,11], which are inadequate for detecting early-stage diseases like SDS, where symptoms manifest subtly at the canopy level before becoming visible on individual leaves. Second, SSL architectures designed for spatial features (typically CNN-based) do not directly transfer to hyperspectral data, which require specialized handling of high-dimensional spectral signatures [11,30,31]. Third, there is a lack of SSL frameworks specifically designed to leverage the unique characteristics of hyperspectral reflectance data, such as physiologically meaningful spectral bands in the red-edge and NIR regions, for¬†early disease detection at the plot or field¬†scale. To address this critical gap, this study proposes a self-supervised learning framework for sudden death syndrome (SDS) detection in soybeans. The¬†framework operates on canopy-level hyperspectral reflectance data acquired from unmanned aerial vehicles, enabling effective disease monitoring under label-scarce conditions. Unlike conventional approaches that depend on extensive manual annotation, particularly problematic for early-symptomatic diseases, our framework derives supervision signals directly from the intrinsic structure of unlabeled spectral data. The¬†main contributions of this work are summarized as¬†follows: An end-to-end self-supervised learning model is developed, specifically tailored for early-stage SDS detection using UAV hyperspectral data.A Euclidean distance-based pseudo-labeling strategy is introduced that leverages the physiologically meaningful separability in hyperspectral space to create high-confidence training pairs, enabling encoder training directly from spectral representations without manual annotation.A comparative evaluation is conducted to benchmark the SSL framework against conventional clustering algorithms and supervised classifiers. This framework demonstrates that self-supervised learning can achieve performance comparable to supervised baselines while substantially reducing the reliance on labeled data, suggesting its potential as a scalable approach for hyperspectral-based plant disease detection in scenarios where manual annotation is limited or¬†costly.",
            "2.  Materials and¬†Methods": "2.1. Study¬†AreaThe experimental site was situated in Montgomery County, Illinois, USA. The¬†region experiences a continental climate, characterized by warm, humid summers and cold winters. According to data from the National Oceanic and Atmospheric Administration (NOAA), July is the warmest month, with¬†an average temperature of 23 ¬∞C, while November is the coldest, averaging 6 ¬∞C. Rainfall peaks in May and gradually declines through November. The¬†trials were conducted on Mollisol soils with a loam texture, commencing in May 2022. Experimental plots varied in size, ranging from 1.5 to 3 m in width and 5.5 to 6 m in length, with¬†a row spacing of 0.76 m. Standard tillage practices were followed, and¬†all trials were conducted under rainfed conditions. Disease symptoms emerged naturally and were in their early stages at the time of data collection in mid-August 2022. Harvesting occurred in October and November, in¬†accordance with genotype maturity groups. Visual assessment of disease severity was conducted to confirm the disease¬†status.Disease status was determined through labor-intensive field assessment conducted on 15 August 2022, by¬†pathological and phenological specialists from GDM Seeds. Evaluators independently inspected individual plots, examining stems and leaves for SDS symptoms, and¬†the assessments were later compiled to establish final classifications. Following a conservative protocol, plots exhibiting yellowing, chlorosis, or¬†damage on any observed stem or leaf were labeled as SDS-affected, while plots with no visible symptoms were labeled as healthy, a¬†threshold that necessarily introduced some classification ambiguity in borderline cases with minimal symptom expression.Figure 1provides an overview of the study area and plot layout of the 375 total plots, 206 were classified as healthy (55%) and 169 as SDS-affected (45%), representing a moderate class imbalance that was addressed during SSL training through balanced pair generation (Section 2.6). At¬†this midpoint of the season, the¬†plants were in the early to mid stages of their life cycle, and¬†SDS was in an early development¬†phase.Figure 1.Study area and plot-level health status of the soybean. Plot with any symptom annotated as red, while green indicates no symptoms. 2.2. MaterialsHyperspectral data acquisition was conducted through a single field campaign in mid-August 2022 using a Headwall Nano-Hyperspec VNIR sensor (Headwall Photonics, Bolton, MA, USA) (400‚Äì1000 nm, 269 bands) mounted on a Matrice 600 Pro UAV (DJI, Shenzhen, China). The¬†flight was executed at an altitude of 100 m above ground level, yielding a spatial resolution of approximately 4 cm per pixel. Georeferencing was performed using a mobile R12 Real-Time Kinematic (RTK) GNSS receiver (Trimble, Sunnyvale, CA, USA), which provided sub-centimeter-level positioning accuracy for ground control points. Raw hyperspectral data, initially distorted and non-georeferenced, were calibrated to consistent reflectance values for analysis. The¬†dataset was split into training (80%,n= 300) and testing (20%,n= 75) sets using stratified sampling to maintain class distribution. The¬†self-supervised learning framework was implemented in Python 3.10 using PyTorch 1.12, with¬†data preprocessing and clustering analyses conducted using scikit-learn, NumPy, and¬†pandas¬†libraries. 2.3. Data¬†Pre-ProcessingThe preprocessing pipeline transforms raw hyperspectral imagery into plot-level spectral signatures suitable for SSL training. Traditional SSL architectures predominantly employ CNNs, which necessitate extensive datasets, substantial computational resources, and¬†rich spatial features to achieve effective feature separation [32,33,34]. However, the¬†high dimensionality of HSI data and inadequate spatial features in early-stage crop vegetation increase architectural complexity [35]. To¬†mitigate these challenges, this study utilizes plot-level mean spectra as the dataset, enhancing computational efficiency while preserving discriminative spectral¬†information.The preprocessing pipeline, illustrated inFigure 2, consists of three main stages. First, raw hyperspectral data undergo radiometric calibration to correct sensor-specific errors and convert digital numbers to standard reflectance values, followed by ortho-rectification to remove geometric distortions. Second, individual hyperspectral image cubes are georeferenced using the R12 RTK GNSS data and mosaicked to generate a seamless field-scale¬†image.Figure 2.Data processing workflow highlighting learning and inference performed in the spectral domain.Third, background soil and shadow pixels were removed using an adapted data-driven approach [36] to isolate pure canopy spectral signals. The¬†soil masking technique exploits inherent spectral differences between vegetation and soil across VNIR wavelengths, using rule-based classification that leverages characteristic reflectance patterns of photosynthetically active vegetation versus bare soil. This unsupervised approach eliminates the need for manual training sample generation while maintaining robust performance. Shadow masking was implemented to detect and exclude shaded regions cast by the UAV platform and environmental conditions, ensuring spectral consistency across plots. Shadow removal is critical because shaded areas exhibit incomplete spectral information and reduced intensity values that could confound disease¬†detection.Following soil and shadow removal, plot-level mean reflectance signatures were computed exclusively from pure canopy pixels, ensuring that extracted spectral features represent genuine vegetation characteristics rather than mixed soil-plant or illumination artifacts. All spectra are then smoothed using the Savitzky-Golay filter to reduce noise. Subsequently, 47 vegetation indices spanning chlorophyll content (ùëÅùê∑ùëâùêº,ùê∫ùëÅùê∑ùëâùêº,ùëÄùê∂ùê¥ùëÖùêºNDVI,GNDVI,MCARIvariants), water stress (ùëäùêµùêº,ùëäùëÜùê∂ùëá,ùëÅùê∑ùëäùêºWBI,WSCT,NDWI), pigment composition (ùëÜùëÖùëÉùêº,ùëÅùëÉùê∂ùêº,ùê¥ùëÖùêº2SRPI,NPCI,ARI2), and¬†structural characteristics (ùëÜùê¥ùëâùêº,ùëÇùëÜùê¥ùëâùêº,ùëÄùëÜùê¥ùëâùêºSAVI,OSAVI,MSAVI) are computed from the 269 spectral bands using established formulas from the remote sensing literature (Table S2). These indices provide biophysically interpretable features that complement raw spectral signatures by encoding domain knowledge about vegetation stress responses. The¬†269 spectral bands and 47 derived indices are concatenated to form a 316-dimensional feature vector per plot, which undergoes standardization before model¬†training. 2.4. Plot-Level Mean Spectra¬†AnalysisDespite rigorous filtering and calibration processes that maintain standard procedural integrity, notable edge noise persists within the dataset.Figure 3a illustrates the mean spectral reflectance curves for healthy and unhealthy plots. The¬†spectral reflectance curves for unhealthy plots exhibit a broader horizontal distribution compared to those of healthy plots. This wider dispersion in spectral reflectance values indicates greater variability among unhealthy plots. Consequently, this variability complicates the differentiation between healthy and unhealthy plots when relying solely on spectral information. The¬†increased spectral overlap highlights the challenge of using spectral data alone to accurately classify plot health, underscoring the need for incorporating advanced modeling techniques to enhance classification¬†accuracy.Figure 3.(a) All samples‚Äô mean spectral reflectance and (b) top two principal component distribution of data points. Red color denotes the SDS affected, while green denotes the healthy. 2.5. SSL¬†ArchitectureThis section presents the self-supervised learning framework developed for SDS detection. As¬†introduced inSection 1, we develop an SSL framework tailored to plot-level hyperspectral reflectance. Unlike methods predicated on spatial context (CNNs) or semantic tokenization (LLMs), our approach operates directly on tabular spectra, where neither assumption holds. The¬†framework comprises: (i) a Siamese encoder architecture (Section 2.5), (ii) a spectral distance-guided pairing scheme that yields high-confidence positive/negative pairs from unlabeled data (Section 2.6), and¬†(iii) an exponentiated contrastive loss that modulates the strength of inter/intra-class separation via a hyperparameter exponent (Section 2.7).Figure 4summarizes the training pipeline. Given an unlabeled dataset of per-plot index-reflectance vectors, we (a) standardize per band and construct pairs using the distance-guided data pairing method, (b) optimize a Siamese encoder with an exponentiated margin-based contrastive objective, and¬† (c) deploy the trained encoder to obtain low-dimensional embeddings for evaluation. For¬†the encoder, we use a fully connected feed-forward network with five hidden layers, comprising 316 input neurons, successive layers of 512, 256, 128, and¬†64 neurons, and¬†a 32-dimensional embedding output (ReLU activations and layer normalization). The¬†32-D embedding provides a compact representation sufficient for downstream separability while mitigating overfitting under limited labels. Training uses Adam (learning rate10‚àí310‚àí3),ùêø2L2regularization, early stopping (patience 25), and¬†300 epochs. Model training convergence under 30 min. After¬†convergence, the¬†encoder processes the test set to produce¬†test-embeddings.Figure 4.(a) Model architecture with workflow, (b) training mechanism, and (c) encoder architecture. 2.6. Distance-Based Pairing¬†StrategyWe construct training pairs directly in standardized reflectance vectors, following distance-based SSL pretext (e.g., SimCLR [29], MoCo [37]). Letùëã={ùë•ùëñ}ùëÅùëñ=1‚äÇ‚ÑùùêµX={xi}i=1N‚äÇRBdenoted normalized vi-spectra withB= 316. For uniformly sampled index pairs(ùëñ,ùëó)(i,j), we compute Euclidean distanceùëëùëñùëó=‚à•ùë•ùëñ‚àíùë•ùëó‚à•2dij=‚à•xi‚àíxj‚à•2and assign high-confidence pseudo-labels using two thresholds(ùõº,ùõΩ)(Œ±,Œ≤)with0<ùõº<ùõΩ0<Œ±<Œ≤:(ùëÉ,ùêøùëÜ)=‚ãÉùëò=1ùëõ{((ùë•ùëñ,ùë•ùëó),0)((ùë•ùëñ,ùë•ùëó),1)if0<‚à•ùë•ùëñ‚àíùë•ùëó‚à•‚â§ùõº,if‚à•ùë•ùëñ‚àíùë•ùëó‚à•‚â•ùõΩ,where0‚â§ùõº‚â§ùõΩandùëñ‚â†ùëóP,LS=‚ãÉk=1n((xi,xj),0)if0<‚à•xi‚àíxj‚à•‚â§Œ±,((xi,xj),1)if‚à•xi‚àíxj‚à•‚â•Œ≤,where0‚â§Œ±‚â§Œ≤andi‚â†j(1)Pairs withùõº<ùëëùëñùëó<ùõΩŒ±<dij<Œ≤are discarded to avoid ambiguous supervision. Index pairs are de-duplicated, and¬†similar/dissimilar counts are balanced to reduce label bias. Threshold selection in SSL poses an intrinsic challenge: appropriate cutoffs must be inferred from unlabeled data structure alone, precluding direct reliance on ground-truth¬†validation.A distribution-driven approach was adopted wherein thresholds are derived from empirical quantiles of the pairwise distance distribution. Distances were computed for a representative subset of training instances, withùõºŒ±andùõΩŒ≤positioned at the 20th and 80th percentiles, respectively, yieldingùõº=0.74Œ±=0.74andùõΩ=1.51Œ≤=1.51(Figure S1) in standardized spectral space. A¬†total of 3000 pairs were generated (ùëõ=3000n=3000, withùëõsimilar‚âàùëõdissimilarnsimilar‚âàndissimilar) with balanced distribution between similar and dissimilar labels to prevent class bias during training. This percentile-based threshold selection constitutes a data-adaptive strategy that instinctively scales to the intrinsic geometry of the feature space, obviating the need for manual tuning on labeled validation data, which would violate the self-supervised¬†paradigm.The 20th/80th percentile positions reflect a principled trade-off between pseudo-label confidence and training sample sufficiency: more conservative thresholds (e.g., 10th/90th percentiles) would yield higher-purity pairs at the cost of insufficient training diversity, while more permissive cutoffs (e.g., 30th/70th percentiles) would increase label noise in the contrastive signal. The¬†adopted quartile-based approach ensures that approximately 40% of candidate pairs receive confident pseudo-labels (20% similar, 20% dissimilar), while the ambiguous middle 60% are excluded‚Äîa conservative strategy aligned with established self-supervised learning practices that prioritize signal quality over quantity. Crucially, this method generalizes across datasets because percentiles adapt to the observed distance distribution: a dataset exhibiting tighter spectral clustering would produce smaller absolute threshold values, while one with greater inter-class dispersion would yield larger thresholds, yet both would achieve equivalent relative separation in their respective feature¬†spaces.This adaptive property ensures methodological transferability without requiring dataset-specific hyperparameter tuning. This pairing strategy mitigates the class imbalance present in the raw dataset (55% healthy, 45% SDS-affected) by ensuring equal representation of both relationship types during contrastive learning. The¬†quartile-based thresholds strike a balance between the need for sufficient training pairs and the requirement for high-confidence pseudo-labels, accepting approximately one-fifth of pairs as similar, one-fifth as dissimilar, and¬†rejecting three-fifths as ambiguous. Algorithm¬†1 operationalizes this strategy with stratified sampling, gray-zone exclusion, de-duplication, and¬†class balancing. The¬†distance distribution exhibits (Figure S1) a unimodal structure with an extended right tail, characteristic of high-dimensional spectral data, where most pairs show moderate dissimilarity, with¬†distributional extrema representing unambiguous cases. Euclidean distance on normalized vi-spectra preserves physiologically informative amplitude differences (red-edge, NIR regions), constitutes a proper metric for margin-based objectives, and¬†enables computationally efficient pair¬†generation.Algorithm 1Distance-Based Pair¬†Generation1:InitializeùëÜ‚Üê‚àÖS‚Üê‚àÖ,ùëÉ‚Üê[]P‚Üê[],ùë¶‚Üê[]y‚Üê[]2:Setùëõsimilar,ùëõdissimilar‚Üê‚åäùëõ/2‚åãnsimilar,ndissimilar‚Üê‚åän/2‚åã3:Setùêº‚Üêindices(ùëã)I‚Üêindices(X)4:while|ùëÉ|<ùëõ|P|<ndo5:Randomly sampleùëñ,ùëó‚ààùêºi,j‚ààIwhereùëñ‚â†ùëói‚â†j6:ùêæ‚Üêsorted({ùëñ,ùëó})K‚Üêsorted({i,j}){Create unique pair identifier}7:ifùêæ‚àâùëÜK‚àâSthen8:Computeùëëùëñùëó=‚à•ùë•ùëñ‚àíùë•ùëó‚à•2dij=‚à•xi‚àíxj‚à•29:ifùëëùëñùëó‚â§ùõºdij‚â§Œ±andùëõsimilar>0nsimilar>0then10:ùëÉ‚ÜêùëÉ‚à™{(ùë•ùëñ,ùë•ùëó)}P‚ÜêP‚à™{(xi,xj)},ùë¶‚Üêùë¶‚à™{0}y‚Üêy‚à™{0}11:ùëõsimilar‚Üêùëõsimilar‚àí1nsimilar‚Üênsimilar‚àí112:ùëÜ‚ÜêùëÜ‚à™{ùêæ}S‚ÜêS‚à™{K}13:else ifùëëùëñùëó‚â•ùõΩdij‚â•Œ≤andùëõdissimilar>0ndissimilar>0then14:ùëÉ‚ÜêùëÉ‚à™{(ùë•ùëñ,ùë•ùëó)}P‚ÜêP‚à™{(xi,xj)},ùë¶‚Üêùë¶‚à™{1}y‚Üêy‚à™{1}15:ùëõdissimilar‚Üêùëõdissimilar‚àí1ndissimilar‚Üêndissimilar‚àí116:ùëÜ‚ÜêùëÜ‚à™{ùêæ}S‚ÜêS‚à™{K}17:end if18:end if19:end while20:returnùëÉ,ùë¶P,y 2.7. Contrastive¬†LossThe contrastive loss function trains the model using negative sampling to distinguish between similar and dissimilar pairs. We introduce a flexible exponential parameternon top of the traditional contrastive loss to enhance model performance. This enhancement allows for better discrimination between similar and dissimilar pairs. The loss functionùêø(ùëä,(ùëå,ùë•1,ùë•2))L(W,(Y,x1,x2))is defined as:(ùëä,(ùëå,ùë•1,ùë•2))=1ùëÅ‚àëùëñ=1ùëÅ[(1‚àíùëåùëñ)¬∑‚à•ùë•1ùëñ‚àíùë•2ùëñ‚à•ùëõ+ùëåùëñ¬∑max(0,margin‚àí‚à•ùë•1ùëñ‚àíùë•2ùëñ‚à•ùëõ)]W,Y,x1,x2=1N‚àëi=1N[1‚àíYi¬∑‚à•x1i‚àíx2i‚à•n+Yi¬∑max0,margin‚àí‚à•x1i‚àíx2i‚à•n](2)Here,ùë•1ùëñx1iandùë•2ùëñx2irepresent thei-th pair of embedding vectors, andùëåùëñYiis the corresponding label indicating whether the pair is similar(ùëåùëñ=0)(Yi=0)or dissimilar(ùëåùëñ=1)(Yi=1). The term‚à•ùë•1ùëñ‚àíùë•2ùëñ‚à•‚à•x1i‚àíx2i‚à•denotes the Euclidean distance between the embeddings. For similar pairs(ùëåùëñ=0)(Yi=0), the loss is‚à•ùë•1ùëñ‚àíùë•2ùëñ‚à•ùëõ‚à•x1i‚àíx2i‚à•n, encouraging the embeddings to be close. For dissimilar pairs(ùëåùëñ=1)(Yi=1), the loss ismax(0,margin‚àí‚à•ùë•1ùëñ‚àíùë•2ùëñ‚à•ùëõ)max(0,margin‚àí‚à•x1i‚àíx2i‚à•n), encouraging a distance greater than the margin. The overall loss is averaged over the total number of pairsN.The traditional contrastive loss uses a fixed exponent ofùëõ=2n=2, which limits the model‚Äôs ability to emphasize larger distances. By allowingnto vary, the modified loss can place greater emphasis on larger distances, resulting in better separation of dissimilar pairs in the embedding space. This flexibility leads to improved performance as the model becomes more sensitive to the nuances of the data. The optimal value ofnfor hyperspectral disease detection was determined through systematic ablation analysis (Section 2.8). 2.8. Evaluation¬†MetricsThe embedding vectors generated from the trained model (Figure 4) encode feature representations for each data point. To evaluate the effectiveness of SSL, both the original and embedded test data are clustered using K-Means [38] and Agglomerative Hierarchical Clustering (AHC) [39]. Since SSL operates without labeled supervision, clustering performance is assessed using cluster accuracy, where dominant class labels are assigned to clusters using the Hungarian algorithm [40], and the Adjusted Rand Index (ARI), which quantifies clustering agreement with ground truth while accounting for random assignments. ARI is particularly useful in imbalanced datasets, ensuring that performance improvements reflect meaningful feature separability rather than chance.Accuracy=‚àëùëñùê∂ùëÄùëñùëñ‚àëùëñ‚àëùëóùê∂ùëÄùëñùëóAccuracy=‚àëiCMii‚àëi‚àëjCMij(3)Equation (3) defines accuracy, whereùê∂ùëÄùëñùëñCMiirepresents correctly classified instances in the confusion matrix, and‚àëùëñ‚àëùëóùê∂ùëÄùëñùëó‚àëi‚àëjCMijis the total number of instances. To benchmark SSL against traditional supervised learning, we compare it with Random Forest (RF), Support Vector Machine (SVM), and Deep Neural Network (DNN). Performance is evaluated using Accuracy, Precision, Recall, and F1-score, standard metrics in classification tasks. While accuracy provides a global correctness measure, it can be misleading in class-imbalanced settings. Precision is critical when minimizing false positives, whereas recall is essential when false negatives must be minimized, such as in detecting unhealthy plants. F1-score provides a balanced assessment, integrating both precision and recall. By combining clustering-based and classification-based evaluations, this study ensures a comprehensive assessment of SSL‚Äôs capability in feature representation learning and its viability as an alternative to fully supervised methods. 2.9. Ablation Study: Contrastive Loss¬†ExponentThe contrastive loss exponentn(Equation (2)) directly controls inter-class separation characteristics in the learned embedding space, making its selection critical to framework performance. As detailed inSection 2.7, the exponential modification extends conventional contrastive loss formulations by introducing a hyperparameter that modulates the penalization magnitude applied to embedding distances. Rather than relying on the standardùëõ=2n=2from conventional contrastive learning, we conducted a systematic ablation study to determine the optimal value for hyperspectral disease detection.Five-fold cross-validation was performed on the training set (N = 3000), wherein the exponentnwas varied over the discrete setùëõ‚àà{2,4,6,8,10}n‚àà{2,4,6,8,10}. For each candidate value, the model was trained independently, and the resulting embeddings were evaluated using K-Means and AHC clustering algorithms. Clustering performance was quantified using cluster accuracy (with Hungarian algorithm alignment) and Adjusted Rand Index (ARI), both established metrics for unsupervised evaluation that account for chance agreement and class imbalance.Table 1presents the clustering outcomes across different exponent values, reporting results from the best-performing fold. The results demonstrate consistent and substantial improvement in both clustering accuracy and ARI metrics asnincreases from 2 to 8. Performance reaches its maximum atùëõ=8n=8, where K-Means achieves a clustering accuracy of 0.88 and ARI of 0.57, while AHC achieves 0.92 accuracy and 0.70 ARI. Beyondùëõ=8n=8, performance plateaus or slightly degrades (ùëõ=10n=10: K-Means accuracy 0.85, AHC 0.90), suggesting diminishing returns from excessive penalization.Table 1.Ablation study results: clustering performance for different contrastive loss exponent values. Results from the best-performing fold of 5-fold cross-validation on the training set (N = 3000). The optimal valueùëõ=8n=8(bold) was selected for all subsequent experiments.The observed trend is consistent with the theoretical motivation behind the exponential modification. The standard contrastive loss (ùëõ=2n=2) applies quadratic penalization, which may insufficiently discriminate between moderately dissimilar and highly dissimilar pairs in high-dimensional hyperspectral space. Higher exponent values amplify the penalty gradient for smaller inter-class distances, thereby enforcing more pronounced cluster separation. Specifically, for dissimilar pairs with embedding distanced, the penalty termmax(0,margin‚àíùëëùëõ)max(0,margin‚àídn)exhibits steeper gradients asnincreases, resulting in stronger repulsive forces that expand inter-class margins. This geometric restructuring promotes enhanced intra-class compactness and inter-class dispersion, properties particularly advantageous for unsupervised clustering algorithms. The 54% improvement in K-Means accuracy (from 0.57 atùëõ=2n=2to 0.88 atùëõ=8n=8) and 46% improvement in AHC accuracy (from 0.63 to 0.92) empirically validate this theoretical framework. From both theoretical and empirical perspectives,ùëõ=8n=8represents an optimal configuration that balances intra-class cohesion and inter-class separability without over-penalizing, which could potentially destabilize training convergence. Accordingly, this hyperparameter setting (ùëõ=8n=8) was adopted for all subsequent analyses presented in this study, and all results inSection 3use this optimized configuration.",
            "2.1. Study¬†Area": "The experimental site was situated in Montgomery County, Illinois, USA. The¬†region experiences a continental climate, characterized by warm, humid summers and cold winters. According to data from the National Oceanic and Atmospheric Administration (NOAA), July is the warmest month, with¬†an average temperature of 23 ¬∞C, while November is the coldest, averaging 6 ¬∞C. Rainfall peaks in May and gradually declines through November. The¬†trials were conducted on Mollisol soils with a loam texture, commencing in May 2022. Experimental plots varied in size, ranging from 1.5 to 3 m in width and 5.5 to 6 m in length, with¬†a row spacing of 0.76 m. Standard tillage practices were followed, and¬†all trials were conducted under rainfed conditions. Disease symptoms emerged naturally and were in their early stages at the time of data collection in mid-August 2022. Harvesting occurred in October and November, in¬†accordance with genotype maturity groups. Visual assessment of disease severity was conducted to confirm the disease¬†status. Disease status was determined through labor-intensive field assessment conducted on 15 August 2022, by¬†pathological and phenological specialists from GDM Seeds. Evaluators independently inspected individual plots, examining stems and leaves for SDS symptoms, and¬†the assessments were later compiled to establish final classifications. Following a conservative protocol, plots exhibiting yellowing, chlorosis, or¬†damage on any observed stem or leaf were labeled as SDS-affected, while plots with no visible symptoms were labeled as healthy, a¬†threshold that necessarily introduced some classification ambiguity in borderline cases with minimal symptom expression.Figure 1provides an overview of the study area and plot layout of the 375 total plots, 206 were classified as healthy (55%) and 169 as SDS-affected (45%), representing a moderate class imbalance that was addressed during SSL training through balanced pair generation (Section 2.6). At¬†this midpoint of the season, the¬†plants were in the early to mid stages of their life cycle, and¬†SDS was in an early development¬†phase. Figure 1.Study area and plot-level health status of the soybean. Plot with any symptom annotated as red, while green indicates no symptoms.",
            "2.2. Materials": "Hyperspectral data acquisition was conducted through a single field campaign in mid-August 2022 using a Headwall Nano-Hyperspec VNIR sensor (Headwall Photonics, Bolton, MA, USA) (400‚Äì1000 nm, 269 bands) mounted on a Matrice 600 Pro UAV (DJI, Shenzhen, China). The¬†flight was executed at an altitude of 100 m above ground level, yielding a spatial resolution of approximately 4 cm per pixel. Georeferencing was performed using a mobile R12 Real-Time Kinematic (RTK) GNSS receiver (Trimble, Sunnyvale, CA, USA), which provided sub-centimeter-level positioning accuracy for ground control points. Raw hyperspectral data, initially distorted and non-georeferenced, were calibrated to consistent reflectance values for analysis. The¬†dataset was split into training (80%,n= 300) and testing (20%,n= 75) sets using stratified sampling to maintain class distribution. The¬†self-supervised learning framework was implemented in Python 3.10 using PyTorch 1.12, with¬†data preprocessing and clustering analyses conducted using scikit-learn, NumPy, and¬†pandas¬†libraries.",
            "2.3. Data¬†Pre-Processing": "The preprocessing pipeline transforms raw hyperspectral imagery into plot-level spectral signatures suitable for SSL training. Traditional SSL architectures predominantly employ CNNs, which necessitate extensive datasets, substantial computational resources, and¬†rich spatial features to achieve effective feature separation [32,33,34]. However, the¬†high dimensionality of HSI data and inadequate spatial features in early-stage crop vegetation increase architectural complexity [35]. To¬†mitigate these challenges, this study utilizes plot-level mean spectra as the dataset, enhancing computational efficiency while preserving discriminative spectral¬†information. The preprocessing pipeline, illustrated inFigure 2, consists of three main stages. First, raw hyperspectral data undergo radiometric calibration to correct sensor-specific errors and convert digital numbers to standard reflectance values, followed by ortho-rectification to remove geometric distortions. Second, individual hyperspectral image cubes are georeferenced using the R12 RTK GNSS data and mosaicked to generate a seamless field-scale¬†image. Figure 2.Data processing workflow highlighting learning and inference performed in the spectral domain. Third, background soil and shadow pixels were removed using an adapted data-driven approach [36] to isolate pure canopy spectral signals. The¬†soil masking technique exploits inherent spectral differences between vegetation and soil across VNIR wavelengths, using rule-based classification that leverages characteristic reflectance patterns of photosynthetically active vegetation versus bare soil. This unsupervised approach eliminates the need for manual training sample generation while maintaining robust performance. Shadow masking was implemented to detect and exclude shaded regions cast by the UAV platform and environmental conditions, ensuring spectral consistency across plots. Shadow removal is critical because shaded areas exhibit incomplete spectral information and reduced intensity values that could confound disease¬†detection. Following soil and shadow removal, plot-level mean reflectance signatures were computed exclusively from pure canopy pixels, ensuring that extracted spectral features represent genuine vegetation characteristics rather than mixed soil-plant or illumination artifacts. All spectra are then smoothed using the Savitzky-Golay filter to reduce noise. Subsequently, 47 vegetation indices spanning chlorophyll content (ùëÅùê∑ùëâùêº,ùê∫ùëÅùê∑ùëâùêº,ùëÄùê∂ùê¥ùëÖùêºNDVI,GNDVI,MCARIvariants), water stress (ùëäùêµùêº,ùëäùëÜùê∂ùëá,ùëÅùê∑ùëäùêºWBI,WSCT,NDWI), pigment composition (ùëÜùëÖùëÉùêº,ùëÅùëÉùê∂ùêº,ùê¥ùëÖùêº2SRPI,NPCI,ARI2), and¬†structural characteristics (ùëÜùê¥ùëâùêº,ùëÇùëÜùê¥ùëâùêº,ùëÄùëÜùê¥ùëâùêºSAVI,OSAVI,MSAVI) are computed from the 269 spectral bands using established formulas from the remote sensing literature (Table S2). These indices provide biophysically interpretable features that complement raw spectral signatures by encoding domain knowledge about vegetation stress responses. The¬†269 spectral bands and 47 derived indices are concatenated to form a 316-dimensional feature vector per plot, which undergoes standardization before model¬†training.",
            "2.4. Plot-Level Mean Spectra¬†Analysis": "Despite rigorous filtering and calibration processes that maintain standard procedural integrity, notable edge noise persists within the dataset.Figure 3a illustrates the mean spectral reflectance curves for healthy and unhealthy plots. The¬†spectral reflectance curves for unhealthy plots exhibit a broader horizontal distribution compared to those of healthy plots. This wider dispersion in spectral reflectance values indicates greater variability among unhealthy plots. Consequently, this variability complicates the differentiation between healthy and unhealthy plots when relying solely on spectral information. The¬†increased spectral overlap highlights the challenge of using spectral data alone to accurately classify plot health, underscoring the need for incorporating advanced modeling techniques to enhance classification¬†accuracy. Figure 3.(a) All samples‚Äô mean spectral reflectance and (b) top two principal component distribution of data points. Red color denotes the SDS affected, while green denotes the healthy.",
            "2.5. SSL¬†Architecture": "This section presents the self-supervised learning framework developed for SDS detection. As¬†introduced inSection 1, we develop an SSL framework tailored to plot-level hyperspectral reflectance. Unlike methods predicated on spatial context (CNNs) or semantic tokenization (LLMs), our approach operates directly on tabular spectra, where neither assumption holds. The¬†framework comprises: (i) a Siamese encoder architecture (Section 2.5), (ii) a spectral distance-guided pairing scheme that yields high-confidence positive/negative pairs from unlabeled data (Section 2.6), and¬†(iii) an exponentiated contrastive loss that modulates the strength of inter/intra-class separation via a hyperparameter exponent (Section 2.7). Figure 4summarizes the training pipeline. Given an unlabeled dataset of per-plot index-reflectance vectors, we (a) standardize per band and construct pairs using the distance-guided data pairing method, (b) optimize a Siamese encoder with an exponentiated margin-based contrastive objective, and¬† (c) deploy the trained encoder to obtain low-dimensional embeddings for evaluation. For¬†the encoder, we use a fully connected feed-forward network with five hidden layers, comprising 316 input neurons, successive layers of 512, 256, 128, and¬†64 neurons, and¬†a 32-dimensional embedding output (ReLU activations and layer normalization). The¬†32-D embedding provides a compact representation sufficient for downstream separability while mitigating overfitting under limited labels. Training uses Adam (learning rate10‚àí310‚àí3),ùêø2L2regularization, early stopping (patience 25), and¬†300 epochs. Model training convergence under 30 min. After¬†convergence, the¬†encoder processes the test set to produce¬†test-embeddings. Figure 4.(a) Model architecture with workflow, (b) training mechanism, and (c) encoder architecture.",
            "2.6. Distance-Based Pairing¬†Strategy": "We construct training pairs directly in standardized reflectance vectors, following distance-based SSL pretext (e.g., SimCLR [29], MoCo [37]). Letùëã={ùë•ùëñ}ùëÅùëñ=1‚äÇ‚ÑùùêµX={xi}i=1N‚äÇRBdenoted normalized vi-spectra withB= 316. For uniformly sampled index pairs(ùëñ,ùëó)(i,j), we compute Euclidean distanceùëëùëñùëó=‚à•ùë•ùëñ‚àíùë•ùëó‚à•2dij=‚à•xi‚àíxj‚à•2and assign high-confidence pseudo-labels using two thresholds(ùõº,ùõΩ)(Œ±,Œ≤)with0<ùõº<ùõΩ0<Œ±<Œ≤:(ùëÉ,ùêøùëÜ)=‚ãÉùëò=1ùëõ{((ùë•ùëñ,ùë•ùëó),0)((ùë•ùëñ,ùë•ùëó),1)if0<‚à•ùë•ùëñ‚àíùë•ùëó‚à•‚â§ùõº,if‚à•ùë•ùëñ‚àíùë•ùëó‚à•‚â•ùõΩ,where0‚â§ùõº‚â§ùõΩandùëñ‚â†ùëóP,LS=‚ãÉk=1n((xi,xj),0)if0<‚à•xi‚àíxj‚à•‚â§Œ±,((xi,xj),1)if‚à•xi‚àíxj‚à•‚â•Œ≤,where0‚â§Œ±‚â§Œ≤andi‚â†j(1) Pairs withùõº<ùëëùëñùëó<ùõΩŒ±<dij<Œ≤are discarded to avoid ambiguous supervision. Index pairs are de-duplicated, and¬†similar/dissimilar counts are balanced to reduce label bias. Threshold selection in SSL poses an intrinsic challenge: appropriate cutoffs must be inferred from unlabeled data structure alone, precluding direct reliance on ground-truth¬†validation. A distribution-driven approach was adopted wherein thresholds are derived from empirical quantiles of the pairwise distance distribution. Distances were computed for a representative subset of training instances, withùõºŒ±andùõΩŒ≤positioned at the 20th and 80th percentiles, respectively, yieldingùõº=0.74Œ±=0.74andùõΩ=1.51Œ≤=1.51(Figure S1) in standardized spectral space. A¬†total of 3000 pairs were generated (ùëõ=3000n=3000, withùëõsimilar‚âàùëõdissimilarnsimilar‚âàndissimilar) with balanced distribution between similar and dissimilar labels to prevent class bias during training. This percentile-based threshold selection constitutes a data-adaptive strategy that instinctively scales to the intrinsic geometry of the feature space, obviating the need for manual tuning on labeled validation data, which would violate the self-supervised¬†paradigm. The 20th/80th percentile positions reflect a principled trade-off between pseudo-label confidence and training sample sufficiency: more conservative thresholds (e.g., 10th/90th percentiles) would yield higher-purity pairs at the cost of insufficient training diversity, while more permissive cutoffs (e.g., 30th/70th percentiles) would increase label noise in the contrastive signal. The¬†adopted quartile-based approach ensures that approximately 40% of candidate pairs receive confident pseudo-labels (20% similar, 20% dissimilar), while the ambiguous middle 60% are excluded‚Äîa conservative strategy aligned with established self-supervised learning practices that prioritize signal quality over quantity. Crucially, this method generalizes across datasets because percentiles adapt to the observed distance distribution: a dataset exhibiting tighter spectral clustering would produce smaller absolute threshold values, while one with greater inter-class dispersion would yield larger thresholds, yet both would achieve equivalent relative separation in their respective feature¬†spaces. This adaptive property ensures methodological transferability without requiring dataset-specific hyperparameter tuning. This pairing strategy mitigates the class imbalance present in the raw dataset (55% healthy, 45% SDS-affected) by ensuring equal representation of both relationship types during contrastive learning. The¬†quartile-based thresholds strike a balance between the need for sufficient training pairs and the requirement for high-confidence pseudo-labels, accepting approximately one-fifth of pairs as similar, one-fifth as dissimilar, and¬†rejecting three-fifths as ambiguous. Algorithm¬†1 operationalizes this strategy with stratified sampling, gray-zone exclusion, de-duplication, and¬†class balancing. The¬†distance distribution exhibits (Figure S1) a unimodal structure with an extended right tail, characteristic of high-dimensional spectral data, where most pairs show moderate dissimilarity, with¬†distributional extrema representing unambiguous cases. Euclidean distance on normalized vi-spectra preserves physiologically informative amplitude differences (red-edge, NIR regions), constitutes a proper metric for margin-based objectives, and¬†enables computationally efficient pair¬†generation.Algorithm 1Distance-Based Pair¬†Generation1:InitializeùëÜ‚Üê‚àÖS‚Üê‚àÖ,ùëÉ‚Üê[]P‚Üê[],ùë¶‚Üê[]y‚Üê[]2:Setùëõsimilar,ùëõdissimilar‚Üê‚åäùëõ/2‚åãnsimilar,ndissimilar‚Üê‚åän/2‚åã3:Setùêº‚Üêindices(ùëã)I‚Üêindices(X)4:while|ùëÉ|<ùëõ|P|<ndo5:Randomly sampleùëñ,ùëó‚ààùêºi,j‚ààIwhereùëñ‚â†ùëói‚â†j6:ùêæ‚Üêsorted({ùëñ,ùëó})K‚Üêsorted({i,j}){Create unique pair identifier}7:ifùêæ‚àâùëÜK‚àâSthen8:Computeùëëùëñùëó=‚à•ùë•ùëñ‚àíùë•ùëó‚à•2dij=‚à•xi‚àíxj‚à•29:ifùëëùëñùëó‚â§ùõºdij‚â§Œ±andùëõsimilar>0nsimilar>0then10:ùëÉ‚ÜêùëÉ‚à™{(ùë•ùëñ,ùë•ùëó)}P‚ÜêP‚à™{(xi,xj)},ùë¶‚Üêùë¶‚à™{0}y‚Üêy‚à™{0}11:ùëõsimilar‚Üêùëõsimilar‚àí1nsimilar‚Üênsimilar‚àí112:ùëÜ‚ÜêùëÜ‚à™{ùêæ}S‚ÜêS‚à™{K}13:else ifùëëùëñùëó‚â•ùõΩdij‚â•Œ≤andùëõdissimilar>0ndissimilar>0then14:ùëÉ‚ÜêùëÉ‚à™{(ùë•ùëñ,ùë•ùëó)}P‚ÜêP‚à™{(xi,xj)},ùë¶‚Üêùë¶‚à™{1}y‚Üêy‚à™{1}15:ùëõdissimilar‚Üêùëõdissimilar‚àí1ndissimilar‚Üêndissimilar‚àí116:ùëÜ‚ÜêùëÜ‚à™{ùêæ}S‚ÜêS‚à™{K}17:end if18:end if19:end while20:returnùëÉ,ùë¶P,y",
            "2.7. Contrastive¬†Loss": "The contrastive loss function trains the model using negative sampling to distinguish between similar and dissimilar pairs. We introduce a flexible exponential parameternon top of the traditional contrastive loss to enhance model performance. This enhancement allows for better discrimination between similar and dissimilar pairs. The loss functionùêø(ùëä,(ùëå,ùë•1,ùë•2))L(W,(Y,x1,x2))is defined as:(ùëä,(ùëå,ùë•1,ùë•2))=1ùëÅ‚àëùëñ=1ùëÅ[(1‚àíùëåùëñ)¬∑‚à•ùë•1ùëñ‚àíùë•2ùëñ‚à•ùëõ+ùëåùëñ¬∑max(0,margin‚àí‚à•ùë•1ùëñ‚àíùë•2ùëñ‚à•ùëõ)]W,Y,x1,x2=1N‚àëi=1N[1‚àíYi¬∑‚à•x1i‚àíx2i‚à•n+Yi¬∑max0,margin‚àí‚à•x1i‚àíx2i‚à•n](2) Here,ùë•1ùëñx1iandùë•2ùëñx2irepresent thei-th pair of embedding vectors, andùëåùëñYiis the corresponding label indicating whether the pair is similar(ùëåùëñ=0)(Yi=0)or dissimilar(ùëåùëñ=1)(Yi=1). The term‚à•ùë•1ùëñ‚àíùë•2ùëñ‚à•‚à•x1i‚àíx2i‚à•denotes the Euclidean distance between the embeddings. For similar pairs(ùëåùëñ=0)(Yi=0), the loss is‚à•ùë•1ùëñ‚àíùë•2ùëñ‚à•ùëõ‚à•x1i‚àíx2i‚à•n, encouraging the embeddings to be close. For dissimilar pairs(ùëåùëñ=1)(Yi=1), the loss ismax(0,margin‚àí‚à•ùë•1ùëñ‚àíùë•2ùëñ‚à•ùëõ)max(0,margin‚àí‚à•x1i‚àíx2i‚à•n), encouraging a distance greater than the margin. The overall loss is averaged over the total number of pairsN. The traditional contrastive loss uses a fixed exponent ofùëõ=2n=2, which limits the model‚Äôs ability to emphasize larger distances. By allowingnto vary, the modified loss can place greater emphasis on larger distances, resulting in better separation of dissimilar pairs in the embedding space. This flexibility leads to improved performance as the model becomes more sensitive to the nuances of the data. The optimal value ofnfor hyperspectral disease detection was determined through systematic ablation analysis (Section 2.8).",
            "2.8. Evaluation¬†Metrics": "The embedding vectors generated from the trained model (Figure 4) encode feature representations for each data point. To evaluate the effectiveness of SSL, both the original and embedded test data are clustered using K-Means [38] and Agglomerative Hierarchical Clustering (AHC) [39]. Since SSL operates without labeled supervision, clustering performance is assessed using cluster accuracy, where dominant class labels are assigned to clusters using the Hungarian algorithm [40], and the Adjusted Rand Index (ARI), which quantifies clustering agreement with ground truth while accounting for random assignments. ARI is particularly useful in imbalanced datasets, ensuring that performance improvements reflect meaningful feature separability rather than chance.Accuracy=‚àëùëñùê∂ùëÄùëñùëñ‚àëùëñ‚àëùëóùê∂ùëÄùëñùëóAccuracy=‚àëiCMii‚àëi‚àëjCMij(3) Equation (3) defines accuracy, whereùê∂ùëÄùëñùëñCMiirepresents correctly classified instances in the confusion matrix, and‚àëùëñ‚àëùëóùê∂ùëÄùëñùëó‚àëi‚àëjCMijis the total number of instances. To benchmark SSL against traditional supervised learning, we compare it with Random Forest (RF), Support Vector Machine (SVM), and Deep Neural Network (DNN). Performance is evaluated using Accuracy, Precision, Recall, and F1-score, standard metrics in classification tasks. While accuracy provides a global correctness measure, it can be misleading in class-imbalanced settings. Precision is critical when minimizing false positives, whereas recall is essential when false negatives must be minimized, such as in detecting unhealthy plants. F1-score provides a balanced assessment, integrating both precision and recall. By combining clustering-based and classification-based evaluations, this study ensures a comprehensive assessment of SSL‚Äôs capability in feature representation learning and its viability as an alternative to fully supervised methods.",
            "2.9. Ablation Study: Contrastive Loss¬†Exponent": "The contrastive loss exponentn(Equation (2)) directly controls inter-class separation characteristics in the learned embedding space, making its selection critical to framework performance. As detailed inSection 2.7, the exponential modification extends conventional contrastive loss formulations by introducing a hyperparameter that modulates the penalization magnitude applied to embedding distances. Rather than relying on the standardùëõ=2n=2from conventional contrastive learning, we conducted a systematic ablation study to determine the optimal value for hyperspectral disease detection. Five-fold cross-validation was performed on the training set (N = 3000), wherein the exponentnwas varied over the discrete setùëõ‚àà{2,4,6,8,10}n‚àà{2,4,6,8,10}. For each candidate value, the model was trained independently, and the resulting embeddings were evaluated using K-Means and AHC clustering algorithms. Clustering performance was quantified using cluster accuracy (with Hungarian algorithm alignment) and Adjusted Rand Index (ARI), both established metrics for unsupervised evaluation that account for chance agreement and class imbalance.Table 1presents the clustering outcomes across different exponent values, reporting results from the best-performing fold. The results demonstrate consistent and substantial improvement in both clustering accuracy and ARI metrics asnincreases from 2 to 8. Performance reaches its maximum atùëõ=8n=8, where K-Means achieves a clustering accuracy of 0.88 and ARI of 0.57, while AHC achieves 0.92 accuracy and 0.70 ARI. Beyondùëõ=8n=8, performance plateaus or slightly degrades (ùëõ=10n=10: K-Means accuracy 0.85, AHC 0.90), suggesting diminishing returns from excessive penalization. Table 1.Ablation study results: clustering performance for different contrastive loss exponent values. Results from the best-performing fold of 5-fold cross-validation on the training set (N = 3000). The optimal valueùëõ=8n=8(bold) was selected for all subsequent experiments. The observed trend is consistent with the theoretical motivation behind the exponential modification. The standard contrastive loss (ùëõ=2n=2) applies quadratic penalization, which may insufficiently discriminate between moderately dissimilar and highly dissimilar pairs in high-dimensional hyperspectral space. Higher exponent values amplify the penalty gradient for smaller inter-class distances, thereby enforcing more pronounced cluster separation. Specifically, for dissimilar pairs with embedding distanced, the penalty termmax(0,margin‚àíùëëùëõ)max(0,margin‚àídn)exhibits steeper gradients asnincreases, resulting in stronger repulsive forces that expand inter-class margins. This geometric restructuring promotes enhanced intra-class compactness and inter-class dispersion, properties particularly advantageous for unsupervised clustering algorithms. The 54% improvement in K-Means accuracy (from 0.57 atùëõ=2n=2to 0.88 atùëõ=8n=8) and 46% improvement in AHC accuracy (from 0.63 to 0.92) empirically validate this theoretical framework. From both theoretical and empirical perspectives,ùëõ=8n=8represents an optimal configuration that balances intra-class cohesion and inter-class separability without over-penalizing, which could potentially destabilize training convergence. Accordingly, this hyperparameter setting (ùëõ=8n=8) was adopted for all subsequent analyses presented in this study, and all results inSection 3use this optimized configuration.",
            "3. Results": "This section presents the impact of SSL-based data transformation on model performance.Section 3.1analyzes the embedding space distribution before and after SSL encoding.Section 3.2evaluates clustering performance in an unsupervised setting.Section 3.3compares SSL with supervised learning baselines and examines label efficiency. Finally,Section 3.4presents spatial analysis of plot-level predictions. Detailed interpretation of these findings is provided inSection 4. 3.1. Distribution of¬†EmbeddingFigure 5presents two scatter plots depicting the clustering of test data points, each characterized by 316 features. Principal Component Analysis (PCA) is employed to reduce the data‚Äôs dimensionality, enabling a visual representation of its distribution. The left plot illustrates the distribution of the raw test data, while the right plot represents the distribution after applying the SSL embedding technique outlined inSection 2.5. Both visualizations utilize the Agglomerative Hierarchical Clustering (AHC) algorithm to partition the soybean plots into two clusters, facilitating an assessment of clustering performance before and after embedding.Figure 5.PCA distribution of Agglomerative hierarchical clustering performance before (left) and after (right) SSL embedding. Convex hulls represent cluster boundaries identified by unsupervised AHC, while point colors indicate ground truth labels (green: healthy, red: unhealthy).In the raw spectral space (Figure 5(left)), considerable mixing occurs within cluster boundaries, with healthy and unhealthy samples frequently co-located. The Silhouette coefficient of 0.470 indicates moderate cluster quality, while the 3.1% convex hull overlap quantifies the ambiguous decision region. Following SSL embedding (Figure 5(right)), cluster quality improves markedly (Silhouette: 0.526), with complete elimination of hull overlap. The 71.8% increase in centroid separation (1.20 to 2.06 units) further confirms enhanced feature discriminability. These quantitative improvements align with the 92% classification accuracy reported inSection 3.3, demonstrating that while SSL substantially enhances class separability, it appropriately maintains some boundary samples that reflect genuine spectral ambiguity in early-stage disease detection. 3.2. Clustering Performance¬†EvaluationClustering performance is evaluated using K-Means and AHC algorithms to assess the impact of the embedding technique on unsupervised classification.Figure 6presents a comparative analysis between the raw test dataset and the embedded dataset, utilizing the Adjusted Rand Index (ARI) and Clustering Accuracy as evaluation metrics. The results highlight the substantial improvements achieved through embedding, demonstrating enhanced separability of the clusters.Figure 6.Performance comparison between raw test data and embedding on K-means and AHC.For K-Means, ARI increases from 0.35 to 0.57, reflecting a 63% improvement, while for AHC, it rises from 0.38 to 0.70, an 84% gain. This enhancement indicates that embedding strengthens structural coherence in the feature space, reducing misclassification. Similarly, clustering accuracy improves notably. K-Means accuracy increases from 0.80 to 0.88, showing a 10% gain, while AHC improves from 0.81 to 0.92, achieving a 14% increase. These results demonstrate that embedding refines cluster compactness and inter-class separability, enabling better discrimination between healthy and unhealthy plots.By restructuring the data distribution, SSL-based embedding mitigates intra-cluster variance while reinforcing inter-cluster distinctions. This transformation enhances decision boundaries, allowing clustering algorithms to produce more consistent and well-separated clusters, ultimately improving classification reliability in unsupervised plant disease detection. 3.3. Supervised Classification¬†PerformanceTo evaluate the efficacy of the proposed SSL method, its performance was benchmarked against supervised learning models, including DNN, RF, and SVM. Hyperparameter configurations were selected to balance model capacity with generalization capability. The Random Forest classifier employed 500 decision trees with unrestricted maximum depth, resulting in an empirical mean depth of 9.1 ¬± 1.3 across training folds, trained directly on raw features without preprocessing. The SVM utilized an RBF kernel with regularization parameterùê∂=2.0C=2.0and scale-based gamma(1/(ùëõùëìùëíùëéùë°ùë¢ùëüùëíùë†√óùë£ùëéùëü(ùëã)))(1/(nfeatures√óvar(X))), applied to standardized features (zero mean, unit variance). The DNN implemented with a six-layer fully connected architecture (Figure 4c) with ReLU activation, batch normalization on hidden layers, dropout regularization (p= 0.5) on the first two hidden layers, and Adam optimization (learning rate10‚àí310‚àí3). DNN training employed early stopping with 25-epoch patience on a validation split (20% of training data), class-weighted cross-entropy loss, and a maximum of 300 epochs with batch size 256. Five-fold stratified cross-validation ensured robust generalization assessment.As summarized inTable 2, the SSL (AHC) model outperformed all comparators, achieving the highest accuracy (0.92), precision (0.91), and F1-score (0.92), thereby demonstrating enhanced feature extraction and clustering efficacy. The SVM exhibited the highest recall (0.98), indicating superior sensitivity in detecting positive cases. SSL (K-Means) showed comparatively lower performance, highlighting the advantage of AHC feature organization. These findings confirm that SSL-based embeddings yield more discriminative feature representations from unlabeled data, leading to improved generalization. The observed improvements of +3.4% in accuracy, +3.3% in precision, and +2.2% in F1-score compared to DNN and RF underscore SSL‚Äôs capability to capture structural dependencies effectively. This supports the utility of SSL as a robust alternative to conventional supervised learning, particularly in contexts with limited labeled data.Table 2.Performance comparison of supervised baseline models.Beyond competitive accuracy, SSL‚Äôs key advantage lies in its annotation requirements. To quantify this, we evaluated supervised baselines trained on progressively reduced labeled subsets (10%, 25%, and 50%). Performance degraded substantially with decreasing labels, with RF accuracy dropping from 89.0% (50% labels) to 84.0% (10% labels), SVM from 89.0% to 86.0%, and DNN from 89.0% to 80.0% (Table S1). These results, averaged across five-fold cross-validation, demonstrate that supervised methods are highly sensitive to the availability of training data. In contrast, SSL achieves comparable accuracy (88‚Äì92%,Table 2) while requiring zero labels during training; the ground truth is used only to validate the clusters. This label-free learning addresses a fundamental challenge in early-stage disease detection: when visual symptoms are ambiguous and expert field assessments are resource-intensive, supervised methods struggle with insufficient training labels, whereas SSL learns directly from unlabeled spectral patterns. This operational advantage makes SSL particularly viable for real-world agricultural monitoring, where annotation bottlenecks limit supervised approaches. 3.4. Plot Level¬†PredictionFigure 7presents a comparative analysis of plot-level predictions generated by the SSL and DNN models on a soybean field. To ensure robustness and reliability, predictions were validated using K-fold cross-validation. The field is denoted into distinct plots, each color-coded to visualize prediction accuracy for both models, providing a spatial representation of their performance. This comparative mapping facilitates a comprehensive evaluation of the SSL method relative to a traditional supervised learning approach in assessing soybean plot health.Figure 7.Spatial comparison of plot-level predictions from SSL and DNN models. Concordant correct/incorrect indicate plots where SSL and DNN predictions agree, whereas DNN-only correct and SSL-only correct indicate disagreement where only one model matches the ground truth. Plots are color-coded by prediction category for each model.The results indicate a high degree of agreement between the two models, with both correctly predicting the plot health in 297 instances, as depicted by green plots. However, 47 plots were misclassified by both models, shown in red. Notably, in 16 cases, the SSL model misclassified plots that the DNN correctly predicted (blue), whereas in 15 instances, the DNN misclassified plots that the SSL correctly predicted (yellow). These discrepancies highlight the nuanced differences in model generalization and error patterns. Overall, the SSL model demonstrates performance comparable to the DNN model, despite operating without explicit supervision. This underscores its ability to leverage unlabeled data effectively, making it a viable alternative for classification tasks in scenarios where labeled data is scarce.",
            "3.1. Distribution of¬†Embedding": "Figure 5presents two scatter plots depicting the clustering of test data points, each characterized by 316 features. Principal Component Analysis (PCA) is employed to reduce the data‚Äôs dimensionality, enabling a visual representation of its distribution. The left plot illustrates the distribution of the raw test data, while the right plot represents the distribution after applying the SSL embedding technique outlined inSection 2.5. Both visualizations utilize the Agglomerative Hierarchical Clustering (AHC) algorithm to partition the soybean plots into two clusters, facilitating an assessment of clustering performance before and after embedding. Figure 5.PCA distribution of Agglomerative hierarchical clustering performance before (left) and after (right) SSL embedding. Convex hulls represent cluster boundaries identified by unsupervised AHC, while point colors indicate ground truth labels (green: healthy, red: unhealthy). In the raw spectral space (Figure 5(left)), considerable mixing occurs within cluster boundaries, with healthy and unhealthy samples frequently co-located. The Silhouette coefficient of 0.470 indicates moderate cluster quality, while the 3.1% convex hull overlap quantifies the ambiguous decision region. Following SSL embedding (Figure 5(right)), cluster quality improves markedly (Silhouette: 0.526), with complete elimination of hull overlap. The 71.8% increase in centroid separation (1.20 to 2.06 units) further confirms enhanced feature discriminability. These quantitative improvements align with the 92% classification accuracy reported inSection 3.3, demonstrating that while SSL substantially enhances class separability, it appropriately maintains some boundary samples that reflect genuine spectral ambiguity in early-stage disease detection.",
            "3.2. Clustering Performance¬†Evaluation": "Clustering performance is evaluated using K-Means and AHC algorithms to assess the impact of the embedding technique on unsupervised classification.Figure 6presents a comparative analysis between the raw test dataset and the embedded dataset, utilizing the Adjusted Rand Index (ARI) and Clustering Accuracy as evaluation metrics. The results highlight the substantial improvements achieved through embedding, demonstrating enhanced separability of the clusters. Figure 6.Performance comparison between raw test data and embedding on K-means and AHC. For K-Means, ARI increases from 0.35 to 0.57, reflecting a 63% improvement, while for AHC, it rises from 0.38 to 0.70, an 84% gain. This enhancement indicates that embedding strengthens structural coherence in the feature space, reducing misclassification. Similarly, clustering accuracy improves notably. K-Means accuracy increases from 0.80 to 0.88, showing a 10% gain, while AHC improves from 0.81 to 0.92, achieving a 14% increase. These results demonstrate that embedding refines cluster compactness and inter-class separability, enabling better discrimination between healthy and unhealthy plots. By restructuring the data distribution, SSL-based embedding mitigates intra-cluster variance while reinforcing inter-cluster distinctions. This transformation enhances decision boundaries, allowing clustering algorithms to produce more consistent and well-separated clusters, ultimately improving classification reliability in unsupervised plant disease detection.",
            "3.3. Supervised Classification¬†Performance": "To evaluate the efficacy of the proposed SSL method, its performance was benchmarked against supervised learning models, including DNN, RF, and SVM. Hyperparameter configurations were selected to balance model capacity with generalization capability. The Random Forest classifier employed 500 decision trees with unrestricted maximum depth, resulting in an empirical mean depth of 9.1 ¬± 1.3 across training folds, trained directly on raw features without preprocessing. The SVM utilized an RBF kernel with regularization parameterùê∂=2.0C=2.0and scale-based gamma(1/(ùëõùëìùëíùëéùë°ùë¢ùëüùëíùë†√óùë£ùëéùëü(ùëã)))(1/(nfeatures√óvar(X))), applied to standardized features (zero mean, unit variance). The DNN implemented with a six-layer fully connected architecture (Figure 4c) with ReLU activation, batch normalization on hidden layers, dropout regularization (p= 0.5) on the first two hidden layers, and Adam optimization (learning rate10‚àí310‚àí3). DNN training employed early stopping with 25-epoch patience on a validation split (20% of training data), class-weighted cross-entropy loss, and a maximum of 300 epochs with batch size 256. Five-fold stratified cross-validation ensured robust generalization assessment. As summarized inTable 2, the SSL (AHC) model outperformed all comparators, achieving the highest accuracy (0.92), precision (0.91), and F1-score (0.92), thereby demonstrating enhanced feature extraction and clustering efficacy. The SVM exhibited the highest recall (0.98), indicating superior sensitivity in detecting positive cases. SSL (K-Means) showed comparatively lower performance, highlighting the advantage of AHC feature organization. These findings confirm that SSL-based embeddings yield more discriminative feature representations from unlabeled data, leading to improved generalization. The observed improvements of +3.4% in accuracy, +3.3% in precision, and +2.2% in F1-score compared to DNN and RF underscore SSL‚Äôs capability to capture structural dependencies effectively. This supports the utility of SSL as a robust alternative to conventional supervised learning, particularly in contexts with limited labeled data. Table 2.Performance comparison of supervised baseline models. Beyond competitive accuracy, SSL‚Äôs key advantage lies in its annotation requirements. To quantify this, we evaluated supervised baselines trained on progressively reduced labeled subsets (10%, 25%, and 50%). Performance degraded substantially with decreasing labels, with RF accuracy dropping from 89.0% (50% labels) to 84.0% (10% labels), SVM from 89.0% to 86.0%, and DNN from 89.0% to 80.0% (Table S1). These results, averaged across five-fold cross-validation, demonstrate that supervised methods are highly sensitive to the availability of training data. In contrast, SSL achieves comparable accuracy (88‚Äì92%,Table 2) while requiring zero labels during training; the ground truth is used only to validate the clusters. This label-free learning addresses a fundamental challenge in early-stage disease detection: when visual symptoms are ambiguous and expert field assessments are resource-intensive, supervised methods struggle with insufficient training labels, whereas SSL learns directly from unlabeled spectral patterns. This operational advantage makes SSL particularly viable for real-world agricultural monitoring, where annotation bottlenecks limit supervised approaches.",
            "3.4. Plot Level¬†Prediction": "Figure 7presents a comparative analysis of plot-level predictions generated by the SSL and DNN models on a soybean field. To ensure robustness and reliability, predictions were validated using K-fold cross-validation. The field is denoted into distinct plots, each color-coded to visualize prediction accuracy for both models, providing a spatial representation of their performance. This comparative mapping facilitates a comprehensive evaluation of the SSL method relative to a traditional supervised learning approach in assessing soybean plot health. Figure 7.Spatial comparison of plot-level predictions from SSL and DNN models. Concordant correct/incorrect indicate plots where SSL and DNN predictions agree, whereas DNN-only correct and SSL-only correct indicate disagreement where only one model matches the ground truth. Plots are color-coded by prediction category for each model. The results indicate a high degree of agreement between the two models, with both correctly predicting the plot health in 297 instances, as depicted by green plots. However, 47 plots were misclassified by both models, shown in red. Notably, in 16 cases, the SSL model misclassified plots that the DNN correctly predicted (blue), whereas in 15 instances, the DNN misclassified plots that the SSL correctly predicted (yellow). These discrepancies highlight the nuanced differences in model generalization and error patterns. Overall, the SSL model demonstrates performance comparable to the DNN model, despite operating without explicit supervision. This underscores its ability to leverage unlabeled data effectively, making it a viable alternative for classification tasks in scenarios where labeled data is scarce.",
            "4. Discussion": "The effectiveness of the proposed SSL framework for early SDS detection stems from its ability to leverage physiologically induced spectral separability without extensive manual annotation. While early-stage SDS produces subtle canopy-level stress signatures before visible symptoms emerge, these pre-visual changes create measurable reflectance shifts that our distance-based pairing strategy can exploit directly from unlabeled data. By learning to discriminate spectral patterns through contrastive representation learning, the framework achieves early detection sensitivity while circumventing the annotation bottleneck that constrains supervised approaches. This is particularly critical for SDS, where ground-truth labels during early infection are expensive and often inconsistent, even among expert observers. To elucidate the model‚Äôs decision-making process and identify key features driving soybean disease classification, a SHAP-based feature importance analysis was performed on hyperspectral bands and vegetation indices (VIs). This data-driven approach, unrestricted by predefined spectral categories, revealed the wavelengths and indices most critical for accurate discrimination.Table 3presents the top-ranked VIs, includingùëÜùëÖùëÉùêºSRPI,ùëÅùëÉùê∂ùêºNPCI,ùëäùëÜùê∂ùëáWSCT,ùê¥ùëÖùêº2ARI2, andùëÄùê∂ùê¥ùëÖùêº2MCARI2. These indices capture distinct physiological responses to SDS infection, such asùëäùëÜùê∂ùëáWSCTdetecting water stress from impaired root water uptake,ùê¥ùëÖùêº2ARI2capturing stress-induced anthocyanin accumulation,ùëÄùê∂ùê¥ùëÖùêº2MCARI2reflecting chlorophyll degradation from nutrient deficiency, whileùëÅùëÉùê∂ùêºNPCIandùëÜùëÖùëÉùêºSRPIrespond to pigment changes in the early stress stages. These results substantiate that the model‚Äôs predictions are grounded in physiologically relevant spectral responses specific to SDS pathogenesis. Table 3.Top 10 most impactful vegetation indices with equations (wavelengths adapted to the sensor‚Äôs spectral ranges. Figure 8presents the SHAP attributions for vegetation indices (VIs) and hyperspectral bands. Notably, red-edge (753‚Äì771 nm) and near-infrared (924‚Äì969 nm) bands were consistently highlighted. These spectral regions are directly linked to SDS pathophysiology, whereFusarium virguliformecolonizes soybean roots and produces toxins that disrupt vascular function, impairing nutrient and water transport to the canopy [19]. The red-edge region is particularly sensitive to chlorophyll content degradation resulting from nitrogen deficiency caused by impaired root nutrient uptake [46]. Similarly, NIR reflectance responds to reduced leaf water content and altered mesophyll structure when SDS compromises vascular water transport [50,51]. This correspondence between SHAP-selected features and established physiological stress markers confirms that the model‚Äôs predictions are grounded in biologically meaningful spectral responses rather than spurious correlations, reinforcing their diagnostic value for early SDS detection. Figure 8.Top 10 SHAP values for Vegetation Index (left) and Wavelength (right). The co-occurrence of raw spectral bands and vegetation indices among top-ranked SHAP features indicates that both feature types contribute complementary information to classification performance. While raw bands (red-edge: 753‚Äì771 nm, NIR: 924‚Äì969 nm) capture direct physiological responses, vegetation indices encode nonlinear transformations (e.g., normalized difference ratios, soil-adjusted formulations) that emphasize specific stress signatures while suppressing confounding factors such as soil background and illumination variability. This suggests that the 316-feature representation (269 bands + 47 indices) provides multiple complementary perspectives on the underlying plant stress state, rather than redundant information. The presence of both feature types in the top-10 discriminative features supports the utility of vegetation index augmentation for enhanced separability in the SSL framework. Reflectance frequency distributions (Figure 9) support these results by comparing correctly and incorrectly classified plots. Correctly classified samples show clear separation between healthy and diseased states in the red-edge and NIR regions, with healthy plots peaking near 0.09 and 0.40 reflectance, respectively, and diseased plots peaking lower. In contrast, blue and green regions exhibit significant overlap, indicating limited discriminative value. This highlights the importance of red-edge and NIR reflectance in disease detection. Misclassified plots demonstrate spectral overlap and increased variability in the blue, green, and red-edge regions, likely due to label noise and subtle disease progression. Such ambiguity stems from inconsistent ground-truth labels that inadequately reflect continuous spectral changes. Although NIR retains some separability, the increased noise emphasizes the need for uncertainty-aware techniques such as self-supervised denoising or contrastive spectral embeddings to improve feature reliability and reduce misclassification in UAV-based disease monitoring. Figure 9.Normalized reflectance distributions for correctly (left) and incorrectly (right) classified plots, illustrating strong separation between healthy and diseased states in red-edge and NIR bands (correct prediction), but overlap in incorrect prediction.",
            "5.  Conclusions": "In this study, we developed a self-supervised learning (SSL) framework for the early detection of Sudden Death Syndrome (SDS) in soybean plants using UAV-based hyperspectral data. To enable effective representation learning from plot-level reflectance features, we introduced a distance-based pairing strategy and a modified contrastive loss function tailored to spectral data. The key contributions can be summarized as follows: An end-to-end SSL framework for SDS detection using UAV hyperspectral data, reducing reliance on in-situ measurements.A distance-based spectral pairing strategy that enhances cluster separability and strengthens feature learning.Demonstrated performance gains of 11% over unsupervised methods and 3% over traditional supervised learning, highlighting the efficacy of SSL for hyperspectral plant disease detection. Model interpretability analysis using SHAP confirmed that the red-edge (753‚Äì771 nm) and NIR (924‚Äì969 nm) regions were the most critical spectral domains, consistent with known physiological stress markers. Reflectance-based separability further reinforced the diagnostic value of these bands, while errors were linked to spectral overlap and label noise. These findings demonstrate that SSL, coupled with targeted spectral feature selection, provides a scalable, interpretable, and label-efficient approach for precision agriculture. While this study validates SSL on 375 plots from a single field site during the 2022 growing season, the framework‚Äôs design principles support broader applicability. The distance-based pairing strategy operates on standardized spectral features and employs adaptive quantile-based thresholds that instinctively adjust to data distribution, enabling methodological transferability across datasets. The reliance on universal physiological markers, red-edge and NIR bands corresponding to chlorophyll degradation and water stress, provides a biophysically grounded foundation that transcends site-specific characteristics. Cross-validation performance and the elimination of manual threshold tuning further demonstrate the framework‚Äôs robustness within the studied conditions. Nevertheless, systematic validation across diverse environmental contexts would strengthen operational confidence. Specifically, extending the framework to different soil types (Alfisols, Vertisols) would verify that the soil masking approach (Section 2.3) generalizes across pedological contexts. Multi-season deployment across varying phenological windows (V4‚ÄìR1 early stages, R7‚ÄìR8 late stages) would demonstrate temporal robustness under different climatic conditions and canopy architectures. Evaluating performance across broader disease severity ranges from pre-visual infection to severe foliar damage (>50%) would establish whether the distance thresholds (ùõº=0.74Œ±=0.74,ùõΩ=1.51Œ≤=1.51) generalize or require dataset-specific recalibration. Future work will focus on three key directions: (i) incorporating multi-temporal hyperspectral data to improve robustness and enable disease progression monitoring; (ii) extending the framework across diverse crops and environmental conditions to enhance generalizability; and (iii) integrating advanced SSL architectures and multi-modal fusion techniques to refine feature learning and improve classification performance across different disease stages."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2072-4292/17/23/3928",
        "scraped_at": "2025-12-05 23:54:01"
    },
    {
        "title": "Label-Driven Optimization of Trading Models Across Indices and Stocks: Maximizing Percentage Profitability",
        "authors": "byAbdulmohssen S. AlRashedyandHassan I. Mathkour",
        "journal": "Mathematics2025,13(23), 3889;https://doi.org/10.3390/math13233889- 4 Dec 2025",
        "abstract": "Short-term trading presents a high-dimensional prediction problem, where the profitability of trading signals depends not only on model accuracy but also on how financial labels are defined and aligned with market dynamics. Traditional approaches often apply uniform modeling choices across assets, overlooking the asset-specific nature of volatility, liquidity, and market response. In this work, we introduce a structured, label-aware machine learning pipeline aimed at maximizing short-term trading profitability across four major benchmarks: S&P 500 (SPX), NASDAQ-100 (NDX), Dow Jones Industrial Average (DJI), and the TadƒÅwul All-Share Index (TASI and twelve of their most actively traded constituents). Our solution systematically evaluates all combinations of six model types (logistic regression, support vector machines, random forest, XGBoost, 1-D CNN, and LSTM), eight look-ahead labeling windows (3 to 10 days), and four feature subset sizes (44, 26, 17, 8 variables) derived through Random Forest permutation-importance ranking. Backtests are conducted using realistic long/flat simulations with zero commission, optimizing for Percentage Profit and Profit Factor on a 2005‚Äì2021 train/2022‚Äì2024 test split. The central contribution of the framework is a labeling-aware search mechanism that assigns to each asset its optimal combination of model type, look-ahead horizon, and feature subset based on out-of-sample profitability. Empirical results show that while XGBoost performs best on average, CNN and LSTM achieve standout gains on highly volatile tech stocks. The optimal look-ahead window varies by market from 3-day signals on liquid U.S. shares to 6‚Äì10-day signals on the less-liquid TASI universe. This joint model‚Äìlabel‚Äìfeature optimization avoids one-size-fits-all assumptions and yields transferable configurations that cut grid-search cost when deploying from index level to constituent stocks, improving data efficiency, enhancing robustness, and supporting more adaptive portfolio construction in short-horizon trading strategies.Keywords:markets;label horizon;machine learning;model selection;short-term tradingMSC:62P20; 68T01",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "The field of financial time series forecasting represents a challenging yet critical aspect of quantitative finance, where the accurate prediction of asset prices, trends, and returns is fundamental to informed decision-making and portfolio management [1,2]. Financial markets are volatile, non-stationary, and prone to structural shifts, posing distinct challenges for traditional statistical models [3]. Unlike the relatively stable patterns seen in time series data from other disciplines, financial data exhibit characteristics such as heavy-tailed return distributions, volatility clustering, and abrupt shifts driven by external events like financial crises, political disruptions, or shifts in market sentiment [4]. These properties make financial forecasting particularly difficult, with classical models based on Gaussian assumptions often proving inadequate. Furthermore, the emphasis in finance on practical metrics such as profit, drawdown, and turnover adds complexity, making conventional machine learning error scores less relevant for evaluation [5]. Machine learning has gained increased prominence in financial forecasting research and practice, driven by the need to move beyond conventional models and improve robustness and predictive accuracy [6,7]. However, a critical limitation of machine learning lies in its dependence on high-quality datasets, carefully selected features, and thoughtfully designed labels [8]. The high-dimensional nature of financial time series means that they contains a wide range of indicators, not all of which are relevant for predictive modeling [9]. While many studies employ feature sets with hundreds of indicators, they often neglect to address the risks of noise and overfitting. Moreover, conventional labeling methods such as predicting daily price changes or five-day returns frequently fail to account for the instability and trading volume dynamics unique to different financial instruments [10]. Addressing these challenges is essential to ensure that theoretical models generalize well to unseen data and perform reliably in real-world scenarios. Previous efforts have often relied on singular models or fixed look-ahead horizons applied uniformly across all assets, without accounting for the distinct characteristics of financial data specific to each asset [11]. Many studies employ models such as Long Short-Term Memory (LSTM) networks or Convolutional Neural Networks (CNN), yet they typically overlook the fact that these architectures do not guarantee consistent performance across different regimes or asset types [12,13]. Financial market volatility is asset-dependent, and the optimal predictive horizon or look-ahead window is highly specific to the asset class [14]. For instance, highly liquid markets generally benefit from shorter prediction horizons, whereas less liquid assets may yield better performance with longer look-ahead [15]. As a result, models that ignore these distinctions often perform well only under narrow market conditions and fail when applied to different environments or asset classes. Furthermore, the feature selection process often introduces irrelevant or redundant variables, adding unnecessary complexity and potentially increasing look-ahead bias [16]. As a result, many models fail to isolate the key factors driving market behavior, thereby undermines their predictive effectiveness. Identifying truly informative features remains a significant challenge, and this difficulty is compounded by the need to design labels that accurately reflect the underlying dynamics of financial markets [17]. Improving the performance of financial forecasting models thus requires a more careful and adaptive approach to both feature selection and label construction. This study aims to address these challenges by proposing a comprehensive, labeling-aware machine learning pipeline that adapts to the unique characteristics of each asset. The framework jointly optimizes the key elements of model selection, feature selection, and label definition, taking into account the dynamic and market-specific nature of financial data. By treating the look-ahead window and feature subset as hyper-parameters to be tuned for each asset, we ensure that the model is better aligned with the asset‚Äôs liquidity, volatility, and trading behavior. Furthermore, by using a range of machine learning models such as tree ensembles (XGBoost, RF), linear models (logistic regression, SVM), and deep learning models (1-D CNN, LSTM) we explore how different approaches can be combined with optimized labels and features to improve model performance. This study presents several key contributions to the field of financial time series forecasting: i.Stock-Specific Label Tuning: We introduce the concept of dynamically tuning the look-ahead window for each asset. Our research shows that the optimal look-ahead window varies across different assets, with liquid U.S. stocks requiring shorter windows (3 days) and less-liquid markets like TASI benefiting from longer windows (6‚Äì10 days). This demonstrates the importance of considering market latency when designing labels.ii.Compact Feature Discovery: Using Random Forest‚Äôs permutation importance, we rank engineered factors and apply compression techniques to reduce the feature set. By selecting the most 85 informative features, we reduce overfitting through k-fold cross-validation while ensuring that essential signals related to momentum, volatility, and order flow are preserved.iii.Model‚ÄìLabel‚ÄìFeature Optimization: The study evaluates a diverse set of machine learning models, including tree ensembles, linear models, and deep neural networks, in combination with optimized labels and feature sets. This comprehensive approach enables the identification of the best model-label-feature combination for each asset, ensuring that the model is well-suited to the specific characteristics of the asset.iv.Transferable Configurations: One of the key advantages of this framework is its ability to provide adaptable configurations across different assets. We show that the configuration that maximizes performance at the index level often also improves the performance of individual constituents, thereby reducing the need for time-consuming grid searches and enabling efficient deployment in real-time trading systems.",
            "2. Literature Review": "Recent advances in financial forecasting increasingly emphasize the critical role of label construction and feature selection in determining model effectiveness. Since financial time series are inherently noisy and non-stationary, how labels are defined particularly with respect to look-ahead windows and trend thresholds significantly impacts predictive accuracy and trading performance. Likewise, the selection of informative features from a high-dimensional input space remains essential, because such useless activities may obscure real market trends. Researchers have considered classic and state of the art methods to address these issues, including the development of time sensitive weighting mechanisms, flexible marking methods, and feature selection systems suitable to model and market dynamics. 2.1. Feature Engineering and SelectionRecent studies have advanced feature engineering techniques to enhance the predictive performance of financial models by integrating a diverse set of data types and selection strategies. Vaidynathan et al. [18] developed a reliable feature set for IPO performance forecasting, employing numerical variables such as IPO price and financial gains/losses as well as textual cues. The research utilizes BERT-based contextual embedding, sentiment analysis, and normalized Google Trends interest trends to measure the degree of attention by the public. In order to overcome missing data issues, Vaidynathan et al. use Boolean indicators and set the missing data to zero. This multimodal approach results in a comprehensive and interpretable input space, further augmented through SHAP value analysis for model explanation. Similarly, Pabuccu and Barbu [19] explore over 1000 technical indicators for financial time-series forecasting and propose Feature Selection with Annealing (FSA) as a robust method for identifying high-value features. FSA outperforms traditional techniques like Lasso and Boruta, highlighting its utility in refining noisy, high-dimensional financial data for both regression and classification tasks.Feature selection strategies have also evolved to account for temporal and structural heterogeneity across financial domains. Kim et al. [20] introduce a Dynamic Feature Selection System (DFSS) that utilizes 16 algorithms spanning embedded and ensemble types to identify time-sensitive and industry-specific feature subsets for stock price prediction across 10 major sectors in South Korea. This framework allows for adaptive modeling that reflects changing market dynamics, improving both interpretability and model performance.Paramita and Winata [21] carry out a comparative analysis of PCA, Information Gain, and Recursive Feature Elimination (RFE), concluding that RFE outperforms both PCA and Information Gain for stock prediction due to the iterative elimination of irrelevant features. Finally, Han et al. [22] propose a new perspective on feature engineering by suggesting Selective Genetic Algorithm (SGA) that denoises price signals and better aligns features with trading actions. By combining these results, we observe that feature engineering is critical to increasing the trustworthiness, clarity, and economic potential of financial machine learning systems. 2.2. Machine Learning ModelsIn the last decade, there has been the increased utilization of the machine learning and deep learning models in stock market prediction owing to the complexity and non-linearity of the financial data. Wang et al. [23], propose a hierarchical adaptive temporal-relational network (HATR) to capture the short- and long-term patterns in the trading sequences of stocks through dilated causal convolutions and gated paths. A multi-graph interaction module provides efficient domain knowledge including adaptive learning with superior performance in a variety of datasets with a dual attention mechanism improving temporal sensitivity. Citing the same sentiment, Al-Khasawneh et al. [24] use Long Short-Term Memory (LSTM) networks to predict the Pakistan Stock Exchange index based on their ability to model sequential relationships in time series data. Shen and Shafiq [25] also build a system of deep learning that is specifically designed for Chinese stock market, operating with a number of adjusted architectures, complete with exhaustive pre-processing and feature engineering to attain high accuracies in prediction for different periods of anticipation.On the other hand, the traditional machine learning models are still applicable, especially in cases where structures of data or interpretability of models are preferred. Kumar et al. [26] compare 4 supervised learning algorithms: SVM, Random Forest, KNN and Naive Bayes, and find Random Forest to be the most appropriate for large dataset followed by Naive Bayes for smaller dataset. Yin et al. [5] extend Random Forest by coming up with the D-RF-RS method that combines the use of exponential smoothing, technical indicator extraction, decision tree-based feature selection and hyper-parameter tuning through random search to improve the accuracy and robustness of the model. Chen et al. [27] apply LightGBM in the context of a trading strategy that involves risk constraints through the usage of Conditional Value at Risk (CVaR). LightGBM‚Äôs capability to model, non-linear relationship and process sparse, high dimensional data through mutual exclusive feature bundling results in successful stock prediction and risk conscious portfolio formation. Together, these studies paint the adaptability and stay of both deep and traditional machine learning models for financial forecasting. 2.3. Labeling Look Ahead Windows and Optimization StrategiesSeveral studies have focused on enhancing labeling methods to improve the accuracy and robustness of stock price trend predictions. Ma et al. [28] explored the use of self-supervised learning techniques, particularly denoising autoencoders, to generate more accurate labels by reducing noise exposure in financial time series. This method delivered significant advances in the model‚Äôs performance for the small and the large dataset, indicating the optimized label generation can play an important role in enhancing market trend prediction. Similarly, a noise model was introduced by Kovacevic et al., [29] to measure the robustness of trend labeling algorithms with an intention to optimize labels for classification algorithms. Their model enables evaluating effectiveness of trend labeling without classifier training, which eventually will enhance the classifier performance in the task of determining the price of an asset.Zhao et al., [30] indicated the importance of time-series data in trend prediction and introduced a time-weighted function which assigns weights to the data according to its temporal relationship with the target data. Such an approach increased the predictive precision of the Long Short-Term Memory (LSTM) network, reaching 83.91% of the accuracy rate in predictions for CSI 300 index. Bounid et al. [31] also had contributed by incorporating the cutting-edge methods of the preprocessing of the financial data, which confirmed that the techniques enhance the compliance of the machine learning models with the actual market conditions. Finally, Saberi et al. [32] proposed the Dynamic Threshold Breakout (DTB) labeling system which uses price percentage difference over certain periods to label data. This method overcame the inefficiencies of conventional labeling, causing great trading performance in various markets when connected with LightGBM, increasing precision, ROI, and other metrics of trading.",
            "2.1. Feature Engineering and Selection": "Recent studies have advanced feature engineering techniques to enhance the predictive performance of financial models by integrating a diverse set of data types and selection strategies. Vaidynathan et al. [18] developed a reliable feature set for IPO performance forecasting, employing numerical variables such as IPO price and financial gains/losses as well as textual cues. The research utilizes BERT-based contextual embedding, sentiment analysis, and normalized Google Trends interest trends to measure the degree of attention by the public. In order to overcome missing data issues, Vaidynathan et al. use Boolean indicators and set the missing data to zero. This multimodal approach results in a comprehensive and interpretable input space, further augmented through SHAP value analysis for model explanation. Similarly, Pabuccu and Barbu [19] explore over 1000 technical indicators for financial time-series forecasting and propose Feature Selection with Annealing (FSA) as a robust method for identifying high-value features. FSA outperforms traditional techniques like Lasso and Boruta, highlighting its utility in refining noisy, high-dimensional financial data for both regression and classification tasks. Feature selection strategies have also evolved to account for temporal and structural heterogeneity across financial domains. Kim et al. [20] introduce a Dynamic Feature Selection System (DFSS) that utilizes 16 algorithms spanning embedded and ensemble types to identify time-sensitive and industry-specific feature subsets for stock price prediction across 10 major sectors in South Korea. This framework allows for adaptive modeling that reflects changing market dynamics, improving both interpretability and model performance. Paramita and Winata [21] carry out a comparative analysis of PCA, Information Gain, and Recursive Feature Elimination (RFE), concluding that RFE outperforms both PCA and Information Gain for stock prediction due to the iterative elimination of irrelevant features. Finally, Han et al. [22] propose a new perspective on feature engineering by suggesting Selective Genetic Algorithm (SGA) that denoises price signals and better aligns features with trading actions. By combining these results, we observe that feature engineering is critical to increasing the trustworthiness, clarity, and economic potential of financial machine learning systems.",
            "2.2. Machine Learning Models": "In the last decade, there has been the increased utilization of the machine learning and deep learning models in stock market prediction owing to the complexity and non-linearity of the financial data. Wang et al. [23], propose a hierarchical adaptive temporal-relational network (HATR) to capture the short- and long-term patterns in the trading sequences of stocks through dilated causal convolutions and gated paths. A multi-graph interaction module provides efficient domain knowledge including adaptive learning with superior performance in a variety of datasets with a dual attention mechanism improving temporal sensitivity. Citing the same sentiment, Al-Khasawneh et al. [24] use Long Short-Term Memory (LSTM) networks to predict the Pakistan Stock Exchange index based on their ability to model sequential relationships in time series data. Shen and Shafiq [25] also build a system of deep learning that is specifically designed for Chinese stock market, operating with a number of adjusted architectures, complete with exhaustive pre-processing and feature engineering to attain high accuracies in prediction for different periods of anticipation. On the other hand, the traditional machine learning models are still applicable, especially in cases where structures of data or interpretability of models are preferred. Kumar et al. [26] compare 4 supervised learning algorithms: SVM, Random Forest, KNN and Naive Bayes, and find Random Forest to be the most appropriate for large dataset followed by Naive Bayes for smaller dataset. Yin et al. [5] extend Random Forest by coming up with the D-RF-RS method that combines the use of exponential smoothing, technical indicator extraction, decision tree-based feature selection and hyper-parameter tuning through random search to improve the accuracy and robustness of the model. Chen et al. [27] apply LightGBM in the context of a trading strategy that involves risk constraints through the usage of Conditional Value at Risk (CVaR). LightGBM‚Äôs capability to model, non-linear relationship and process sparse, high dimensional data through mutual exclusive feature bundling results in successful stock prediction and risk conscious portfolio formation. Together, these studies paint the adaptability and stay of both deep and traditional machine learning models for financial forecasting.",
            "2.3. Labeling Look Ahead Windows and Optimization Strategies": "Several studies have focused on enhancing labeling methods to improve the accuracy and robustness of stock price trend predictions. Ma et al. [28] explored the use of self-supervised learning techniques, particularly denoising autoencoders, to generate more accurate labels by reducing noise exposure in financial time series. This method delivered significant advances in the model‚Äôs performance for the small and the large dataset, indicating the optimized label generation can play an important role in enhancing market trend prediction. Similarly, a noise model was introduced by Kovacevic et al., [29] to measure the robustness of trend labeling algorithms with an intention to optimize labels for classification algorithms. Their model enables evaluating effectiveness of trend labeling without classifier training, which eventually will enhance the classifier performance in the task of determining the price of an asset. Zhao et al., [30] indicated the importance of time-series data in trend prediction and introduced a time-weighted function which assigns weights to the data according to its temporal relationship with the target data. Such an approach increased the predictive precision of the Long Short-Term Memory (LSTM) network, reaching 83.91% of the accuracy rate in predictions for CSI 300 index. Bounid et al. [31] also had contributed by incorporating the cutting-edge methods of the preprocessing of the financial data, which confirmed that the techniques enhance the compliance of the machine learning models with the actual market conditions. Finally, Saberi et al. [32] proposed the Dynamic Threshold Breakout (DTB) labeling system which uses price percentage difference over certain periods to label data. This method overcame the inefficiencies of conventional labeling, causing great trading performance in various markets when connected with LightGBM, increasing precision, ROI, and other metrics of trading.",
            "3. Dataset and Feature Engineering": "This study examines the viability of short-term trading strategies built on the principles of machine learning for a representative set of indices and its constituents. The selection includes major U.S. markets and a regional one from the Middle East region, so that it represents a wide range of asset classes, sectors, and volatility regimes. We create a consistent and high-quality dataset from 2005 to 2024 and calculate comprehensive set of technical features for predictive modeling. 3.1. Instrument UniverseOur research covers four leading indices and twelve representative constituents to represent our broad market coverage and sectoral diversity, respectively. The U.S. indices (S&P 500 (SPX), NASDAQ-100 (NASDAQ), and Dow Jones Industrial Average (DJI)) are complemented with the TadƒÅwul All-Share Index (TASI), to provide regional exposure from the Saudi market. Each index has the following three liquid constituents, which are selected based on continuity of data and market relevance (Table 1).Table 1.Instrument universe and sample sizes (2005‚Äì2024).Daily OHLCV (Open, High, Low, Close, and Volume) data are sourced from the EOD-Historical-Data API for U.S. instruments and from TadƒÅwul‚Äôs public API for Saudi assets. All series are aligned to a common trading calendar and split chronologically into:Train: 2005‚Äì2021Test: 2022‚Äì2024 3.2. Constituent Selection and Sample RepresentativenessThe study focuses on a total of twelve actively traded constituents drawn from four major equity benchmarks: the Dow Jones Industrial Average (DJI), the S&P 500 (SPX), the NASDAQ-100 (NASDAQ), and the TadƒÅwul All-Share Index (TASI). While this number is small relative to the hundreds of names in these indices, the selection is deliberate and methodologically motivated rather than arbitrary. The objective is to obtain a computationally tractable yet highly informative cross-section that spans a broad spectrum of volatility, liquidity, and sectoral profiles, enabling a clean evaluation of how label design interacts with asset microstructure.The primary selection criterion is liquidity and data completeness. Each constituent is required to exhibit high and stable trading activity and to have uninterrupted daily OHLCV data from 2005 to 2024. This ensures robust statistical estimation, reliable price discovery, and consistent long-horizon backtesting without survivorship or missing-data distortions.A second design principle is sectoral and index representativeness. For each index, three constituents are chosen to reflect distinct sectors and trading behaviors, rather than clustering around a single industry. Concretely,SPX: AAPL (technology), JNJ (healthcare), XOM (energy);NASDAQ-100: ADBE (software), AMZN (e-commerce), NVDA (semiconductors);DJI: IBM (technology), GS (financials), KO (consumer staples);TASI: Al-Rajhi Bank (banking), SABIC (materials), STC (telecommunications).This construction yields a miniature but diverse universe that reflects blue-chip, growth, defensive, and cyclical profiles, as well as differences in trading intensity and volatility regimes across U.S. and Saudi markets.Finally, the sample size is constrained by computational considerations. The proposed framework evaluates, for each instrument, a full grid of model‚Äìlabel‚Äìfeature combinations comprising 16 instruments √ó 6 model types √ó 8 look-ahead horizons √ó 4 feature subset sizes. This 3000+ configuration space makes it impractical to include all index constituents while still performing exhaustive out-of-sample testing. Restricting the universe to twelve carefully chosen stocks therefore strikes a practical balance: it keeps the grid search exhaustive and reproducible, while still covering a rich diversity of microstructure conditions. In this sense, the subset is small by design but methodologically representative for the labeling-focused questions studied in this paper. 3.3. Feature EngineeringTo support predictive modeling, we construct a library of 85 engineered features for each instrument. These features are designed to capture short-term price dynamics, trend strength, volatility regimes, volume anomalies, and contextual trading behavior. All features are computed from daily OHLCV data using fixed lookback windows ranging from 2 to 5 days, unless otherwise specified. To ensure consistency and avoid forward-looking bias, all features are computed in a rolling manner and normalized prior to model training. To maintain comparability across global indices (US and Saudi Arabia) and ensure the full transferability of the predictive framework, we did not incorporate any fundamental, macroeconomic, or sentiment features.The feature engineering process incorporates a comprehensive set of 85 technical indicators, categorized into five functional groups (Table 2). Each category captures distinct market behaviors relevant to short-term trading, including momentum, trend, volatility, and contextual effects. These features are computed using short rolling windows (typically 2‚Äì5 days) and are normalized to ensure comparability across instruments and time. For example, ADX is considered one of the most important indictors related to trend indicators.Table 2.Technical Indicators by Category.These features collectively capture a wide range of trading dynamics. Momentum and oscillators identify overbought/oversold conditions, while trend indicators measure directional strength. Volatility and volume features help account for regime changes and liquidity shifts.All primary indicators were generated using standardized parameter settings consistent with TA-Lib and Trading View conventions.To significantly enhance the quality of the features and better capture the rate of change and short-term trends within the features themselves, we introduced regression-based derivative features.For a core subset of indicators, we calculated the slope coefficient (beta) of a simple linear regression over lookback windows ranging from 2 to 5 bars. This resulted in four additional derivative features for each primary indicator in the subset.The final, high-quality feature set comprises the original OHLCV data, the primary technical indicators, and these regression-based derivatives (e.g., DI+2_reg, RSI5_reg). This novel method of feature engineering provides the model with a more granular and powerful signal regarding the dynamics of the technical indicators, directly impacting the model‚Äôs ability to maximize profitability.To ensure reproducibility and methodological clarity, all technical indicators were computed using their standard Trading View default configurations. A complete table of indicator names, inputs, and parameter configurations is provided in the nextTable 3.Table 3.Full set of technical indicators parameters.",
            "3.1. Instrument Universe": "Our research covers four leading indices and twelve representative constituents to represent our broad market coverage and sectoral diversity, respectively. The U.S. indices (S&P 500 (SPX), NASDAQ-100 (NASDAQ), and Dow Jones Industrial Average (DJI)) are complemented with the TadƒÅwul All-Share Index (TASI), to provide regional exposure from the Saudi market. Each index has the following three liquid constituents, which are selected based on continuity of data and market relevance (Table 1). Table 1.Instrument universe and sample sizes (2005‚Äì2024). Daily OHLCV (Open, High, Low, Close, and Volume) data are sourced from the EOD-Historical-Data API for U.S. instruments and from TadƒÅwul‚Äôs public API for Saudi assets. All series are aligned to a common trading calendar and split chronologically into: Train: 2005‚Äì2021Test: 2022‚Äì2024",
            "3.2. Constituent Selection and Sample Representativeness": "The study focuses on a total of twelve actively traded constituents drawn from four major equity benchmarks: the Dow Jones Industrial Average (DJI), the S&P 500 (SPX), the NASDAQ-100 (NASDAQ), and the TadƒÅwul All-Share Index (TASI). While this number is small relative to the hundreds of names in these indices, the selection is deliberate and methodologically motivated rather than arbitrary. The objective is to obtain a computationally tractable yet highly informative cross-section that spans a broad spectrum of volatility, liquidity, and sectoral profiles, enabling a clean evaluation of how label design interacts with asset microstructure. The primary selection criterion is liquidity and data completeness. Each constituent is required to exhibit high and stable trading activity and to have uninterrupted daily OHLCV data from 2005 to 2024. This ensures robust statistical estimation, reliable price discovery, and consistent long-horizon backtesting without survivorship or missing-data distortions. A second design principle is sectoral and index representativeness. For each index, three constituents are chosen to reflect distinct sectors and trading behaviors, rather than clustering around a single industry. Concretely, SPX: AAPL (technology), JNJ (healthcare), XOM (energy);NASDAQ-100: ADBE (software), AMZN (e-commerce), NVDA (semiconductors);DJI: IBM (technology), GS (financials), KO (consumer staples);TASI: Al-Rajhi Bank (banking), SABIC (materials), STC (telecommunications). This construction yields a miniature but diverse universe that reflects blue-chip, growth, defensive, and cyclical profiles, as well as differences in trading intensity and volatility regimes across U.S. and Saudi markets. Finally, the sample size is constrained by computational considerations. The proposed framework evaluates, for each instrument, a full grid of model‚Äìlabel‚Äìfeature combinations comprising 16 instruments √ó 6 model types √ó 8 look-ahead horizons √ó 4 feature subset sizes. This 3000+ configuration space makes it impractical to include all index constituents while still performing exhaustive out-of-sample testing. Restricting the universe to twelve carefully chosen stocks therefore strikes a practical balance: it keeps the grid search exhaustive and reproducible, while still covering a rich diversity of microstructure conditions. In this sense, the subset is small by design but methodologically representative for the labeling-focused questions studied in this paper.",
            "3.3. Feature Engineering": "To support predictive modeling, we construct a library of 85 engineered features for each instrument. These features are designed to capture short-term price dynamics, trend strength, volatility regimes, volume anomalies, and contextual trading behavior. All features are computed from daily OHLCV data using fixed lookback windows ranging from 2 to 5 days, unless otherwise specified. To ensure consistency and avoid forward-looking bias, all features are computed in a rolling manner and normalized prior to model training. To maintain comparability across global indices (US and Saudi Arabia) and ensure the full transferability of the predictive framework, we did not incorporate any fundamental, macroeconomic, or sentiment features. The feature engineering process incorporates a comprehensive set of 85 technical indicators, categorized into five functional groups (Table 2). Each category captures distinct market behaviors relevant to short-term trading, including momentum, trend, volatility, and contextual effects. These features are computed using short rolling windows (typically 2‚Äì5 days) and are normalized to ensure comparability across instruments and time. For example, ADX is considered one of the most important indictors related to trend indicators. Table 2.Technical Indicators by Category. These features collectively capture a wide range of trading dynamics. Momentum and oscillators identify overbought/oversold conditions, while trend indicators measure directional strength. Volatility and volume features help account for regime changes and liquidity shifts. All primary indicators were generated using standardized parameter settings consistent with TA-Lib and Trading View conventions. To significantly enhance the quality of the features and better capture the rate of change and short-term trends within the features themselves, we introduced regression-based derivative features. For a core subset of indicators, we calculated the slope coefficient (beta) of a simple linear regression over lookback windows ranging from 2 to 5 bars. This resulted in four additional derivative features for each primary indicator in the subset. The final, high-quality feature set comprises the original OHLCV data, the primary technical indicators, and these regression-based derivatives (e.g., DI+2_reg, RSI5_reg). This novel method of feature engineering provides the model with a more granular and powerful signal regarding the dynamics of the technical indicators, directly impacting the model‚Äôs ability to maximize profitability. To ensure reproducibility and methodological clarity, all technical indicators were computed using their standard Trading View default configurations. A complete table of indicator names, inputs, and parameter configurations is provided in the nextTable 3. Table 3.Full set of technical indicators parameters.",
            "4. Methodology": "To construct reliable supervised labels for short-term price movement prediction, we adopt a deterministic, rule-based labeling method designed to capture directional momentum over fixed forward-looking intervals. This method, which we refer to as the Monotone-Horizon Signal (MHS), avoids arbitrary thresholds and is grounded entirely in the price path‚Äôs monotonic behavior across future periods (Figure 1). Figure 1.Label-aware machine learning pipeline for optimizing short-term trading strategies. 4.1. Label Construction: Monotone-Horizon Signal (MHS)At the core of our methodology is the construction of a 3-state label, denoted as MHSh(t) ‚àà {2, 1, 0}, where h is the length of the look-ahead horizon in trading days. The label captures whether the closing price maintains a consistent directional relationship with today‚Äôs closing price over the next h periods.For each look-ahead horizon h ‚àà {3, 4, ‚Ä¶ 10}, we define the MHS label inTable 4.Table 4.Monotone-Horizon Signal Label Definition.This labeling method is both model-agnostic and volatility-neutral, relying strictly on the sign consistency of future price movements without the need for statistical thresholds or smoothing parameters. It is designed to reflect periods of sustained directional strength, which are typically more actionable in short-term trading contexts. 4.2. Label-Aware Pipeline for Short-Term Stock PredictionThe proposed framework employs a label-aware pipeline designed specifically for short-term stock prediction. Instead of relying on fixed-horizon return labels which often fail to capture intra-horizon dynamics the pipeline produces horizon-sensitive supervisory targets aligned with short-term trading behavior.Horizon-Based MHS Labels (Primary Training Signal)For each asset and each look-ahead horizon HHH, the pipeline generates the Monotone-Horizon Signal (MHS) as defined inSection 4.1.These labels constitute the sole target used for model training and evaluation, ensuring a consistent and unified labeling structure across all experiments.Auxiliary Extremum Markers (Not Used for Training)Local maxima and minima are detected as auxiliary reference points to contextualize price turning behavior.These extremum markers serve diagnostic purposes only and are discarded prior to model fitting, ensuring they do not influence the training data.Label Resolution and Temporal ConsistencyInstances where monotonicity criteria are not met (zero labels) are resolved using forward filling strictly within the training segment, maintaining full temporal integrity.No information from the future is ever used in feature construction or label assignment, preserving strong anti-leakage guarantees.This streamlined pipeline produces clean, horizon-aware labels that better reflect short-term market structure while remaining computationally efficient and methodologically transparent.Extremum-based markers were computed only for diagnostic visual analysis and never used in training, labeling, or evaluation. They were removed before any modeling step. 4.3. Final Label Preparation and FilteringDue to the nature of look-ahead labeling, the final (h) data points in each time series cannot be assigned a definitive label, as their future windows are not fully observable. These entries are temporarily assigned a default value of 0, indicating that no valid signal can be computed for them within the look-ahead structure.To improve the temporal stability of the labeling scheme, a persistence step is applied. Specifically, if the label MHSh(t) = 0, it is replaced by the immediately preceding non-zero label MHSh(t ‚àí 1). This forward-fill mechanism helps maintain continuity in directional trends and prevents the fragmentation of trends due to brief fluctuations that interrupt an otherwise consistent movement. By extending a confirmed signal until a distinct directional change occurs, the labeling process better reflects the behavior of momentum-driven price regimes.Following this adjustment, all remaining instances where MHSh(t) = 0 are removed from the dataset prior to model training. This ensures that only periods exhibiting clear upward or downward momentum denoted by labels 2 and 1, respectively, are used as supervised targets. As a result, the final dataset for each forecasting horizon h consists exclusively of observations with unambiguous directional signals. This approach facilitates a more focused and reliable learning process for prediction models, while remaining fully automated, transparent, and computationally efficient. 4.4. Deep Learning Model ArchitecturesThe study incorporates two deep learning architectures, an LSTM network and a 1-D Convolutional Neural Network (CNN-1D). Both models were designed with lightweight configurations to ensure a fair comparison with traditional machine-learning models and to avoid excessive model capacity that could bias performance.LSTM ArchitectureThe LSTM is used in a feature-as-sequence configuration, where each feature is treated as a timestep. This formulation is widely used in tabular-LSTM setups and ensures compatibility with deep sequence models.1 LSTM layer with 64 unitsDropout: 0.3Output layer: Dense(1, activation = ‚Äòsigmoid‚Äô)Loss: binary cross-entropyOptimizer: Adam (learning rate = 0.001)Batch size: 32Epochs: 20CNN-1D ArchitectureInput shape: (T, 1)Conv1D: 64 filters, kernel size = 3, activation = ReLUMaxPooling1D: pool size = 2Dropout: 0.3Flatten layerOutput layer: Dense(1, activation = ‚Äòsigmoid‚Äô)Loss: binary cross-entropyOptimizer: Adam (learning rate = 0.001)Batch size: 32Epochs: 20Both architectures were intentionally kept shallow (single LSTM/CNN block) to maintain comparable model capacity to tree-based models and prevent unfair advantages due to excessive representation power. This structural transparency ensures that performance differences across models arise from the learning methodology rather than disproportionate model complexity.To ensure fair comparison across models, the deep learning architectures were restricted to single-layer configurations with moderate unit/filter sizes. This prevents disproportionately large representational capacity that could bias results in favor of deep learning. All models were trained under identical data splits, feature sets, and evaluation metrics. 4.5. Computational Cost and Latency ConsiderationsThe framework is designed for daily and bar-level trading, not millisecond-level HFT, where latency constraints are fundamentally different. The LSTM (64 units) and lightweight 1-D CNN were intentionally kept shallow to ensure millisecond-level inference, which is more than sufficient for end-of-day decisions.Although efficient, these models remain heavier than traditional ML methods and are not suitable for microsecond-scale tasks such as market-making or ultra-HFT.The system is therefore intended for research, portfolio screening, and medium-frequency execution where latency is not a limiting factor. 4.6. Anti‚ÄìData-Leakage MeasuresFeature scaling was performed strictly using the training set statistics. The scaler was fitted on the training subset only (mean and variance), and the test data was transformed using this fitted scaler. No information from the test period was used in training or in feature engineering, preventing any form of look-ahead bias.Although labels are generated using future price movement within each horizon, these labels are computed before model training and are not included as model features. As a result, no future information enters the input feature space. 4.7. Baseline Strategies and Scope ClarificationAlthough our primary objective is isolating labeling effects, we additionally report BH and MA(10,50) baselines inSection 6to contextualize model performance. These baselines do not enter the optimization pipeline but are included for completeness.",
            "4.1. Label Construction: Monotone-Horizon Signal (MHS)": "At the core of our methodology is the construction of a 3-state label, denoted as MHSh(t) ‚àà {2, 1, 0}, where h is the length of the look-ahead horizon in trading days. The label captures whether the closing price maintains a consistent directional relationship with today‚Äôs closing price over the next h periods. For each look-ahead horizon h ‚àà {3, 4, ‚Ä¶ 10}, we define the MHS label inTable 4. Table 4.Monotone-Horizon Signal Label Definition. This labeling method is both model-agnostic and volatility-neutral, relying strictly on the sign consistency of future price movements without the need for statistical thresholds or smoothing parameters. It is designed to reflect periods of sustained directional strength, which are typically more actionable in short-term trading contexts.",
            "4.2. Label-Aware Pipeline for Short-Term Stock Prediction": "The proposed framework employs a label-aware pipeline designed specifically for short-term stock prediction. Instead of relying on fixed-horizon return labels which often fail to capture intra-horizon dynamics the pipeline produces horizon-sensitive supervisory targets aligned with short-term trading behavior. Horizon-Based MHS Labels (Primary Training Signal)For each asset and each look-ahead horizon HHH, the pipeline generates the Monotone-Horizon Signal (MHS) as defined inSection 4.1.These labels constitute the sole target used for model training and evaluation, ensuring a consistent and unified labeling structure across all experiments.Auxiliary Extremum Markers (Not Used for Training)Local maxima and minima are detected as auxiliary reference points to contextualize price turning behavior.These extremum markers serve diagnostic purposes only and are discarded prior to model fitting, ensuring they do not influence the training data.Label Resolution and Temporal ConsistencyInstances where monotonicity criteria are not met (zero labels) are resolved using forward filling strictly within the training segment, maintaining full temporal integrity.No information from the future is ever used in feature construction or label assignment, preserving strong anti-leakage guarantees. This streamlined pipeline produces clean, horizon-aware labels that better reflect short-term market structure while remaining computationally efficient and methodologically transparent. Extremum-based markers were computed only for diagnostic visual analysis and never used in training, labeling, or evaluation. They were removed before any modeling step.",
            "4.3. Final Label Preparation and Filtering": "Due to the nature of look-ahead labeling, the final (h) data points in each time series cannot be assigned a definitive label, as their future windows are not fully observable. These entries are temporarily assigned a default value of 0, indicating that no valid signal can be computed for them within the look-ahead structure. To improve the temporal stability of the labeling scheme, a persistence step is applied. Specifically, if the label MHSh(t) = 0, it is replaced by the immediately preceding non-zero label MHSh(t ‚àí 1). This forward-fill mechanism helps maintain continuity in directional trends and prevents the fragmentation of trends due to brief fluctuations that interrupt an otherwise consistent movement. By extending a confirmed signal until a distinct directional change occurs, the labeling process better reflects the behavior of momentum-driven price regimes. Following this adjustment, all remaining instances where MHSh(t) = 0 are removed from the dataset prior to model training. This ensures that only periods exhibiting clear upward or downward momentum denoted by labels 2 and 1, respectively, are used as supervised targets. As a result, the final dataset for each forecasting horizon h consists exclusively of observations with unambiguous directional signals. This approach facilitates a more focused and reliable learning process for prediction models, while remaining fully automated, transparent, and computationally efficient.",
            "4.4. Deep Learning Model Architectures": "The study incorporates two deep learning architectures, an LSTM network and a 1-D Convolutional Neural Network (CNN-1D). Both models were designed with lightweight configurations to ensure a fair comparison with traditional machine-learning models and to avoid excessive model capacity that could bias performance. LSTM Architecture The LSTM is used in a feature-as-sequence configuration, where each feature is treated as a timestep. This formulation is widely used in tabular-LSTM setups and ensures compatibility with deep sequence models.1 LSTM layer with 64 unitsDropout: 0.3Output layer: Dense(1, activation = ‚Äòsigmoid‚Äô)Loss: binary cross-entropyOptimizer: Adam (learning rate = 0.001)Batch size: 32Epochs: 20 CNN-1D Architecture Input shape: (T, 1)Conv1D: 64 filters, kernel size = 3, activation = ReLUMaxPooling1D: pool size = 2Dropout: 0.3Flatten layerOutput layer: Dense(1, activation = ‚Äòsigmoid‚Äô)Loss: binary cross-entropyOptimizer: Adam (learning rate = 0.001)Batch size: 32Epochs: 20 Both architectures were intentionally kept shallow (single LSTM/CNN block) to maintain comparable model capacity to tree-based models and prevent unfair advantages due to excessive representation power. This structural transparency ensures that performance differences across models arise from the learning methodology rather than disproportionate model complexity. To ensure fair comparison across models, the deep learning architectures were restricted to single-layer configurations with moderate unit/filter sizes. This prevents disproportionately large representational capacity that could bias results in favor of deep learning. All models were trained under identical data splits, feature sets, and evaluation metrics.",
            "4.5. Computational Cost and Latency Considerations": "The framework is designed for daily and bar-level trading, not millisecond-level HFT, where latency constraints are fundamentally different. The LSTM (64 units) and lightweight 1-D CNN were intentionally kept shallow to ensure millisecond-level inference, which is more than sufficient for end-of-day decisions. Although efficient, these models remain heavier than traditional ML methods and are not suitable for microsecond-scale tasks such as market-making or ultra-HFT. The system is therefore intended for research, portfolio screening, and medium-frequency execution where latency is not a limiting factor.",
            "4.6. Anti‚ÄìData-Leakage Measures": "Feature scaling was performed strictly using the training set statistics. The scaler was fitted on the training subset only (mean and variance), and the test data was transformed using this fitted scaler. No information from the test period was used in training or in feature engineering, preventing any form of look-ahead bias. Although labels are generated using future price movement within each horizon, these labels are computed before model training and are not included as model features. As a result, no future information enters the input feature space.",
            "4.7. Baseline Strategies and Scope Clarification": "Although our primary objective is isolating labeling effects, we additionally report BH and MA(10,50) baselines inSection 6to contextualize model performance. These baselines do not enter the optimization pipeline but are included for completeness.",
            "5. Data Pre-Processing and Modeling Framework": "This section outlines the transformations, model types, and evaluation strategies employed to prepare data for predictive modeling and to simulate realistic trading behavior. Our goal is to ensure data integrity, avoid look-ahead bias, and evaluate models in a financially meaningful way. 5.1. Data Normalization and Leakage ControlTo prevent data leakage and ensure numerical stability during training, all features undergo z-score standardization. Each feature is re-scaled by subtracting its training set mean and dividing by its training set standard deviation. This standardization procedure is performed separately for each training fold and never incorporates information from future data. By centering and scaling inputs, we ensure that model learning is not influenced by arbitrary feature magnitudes. 5.2. Feature Subset Selection via Random Forest ImportanceWe apply permutation-based feature importance using a Random Forest estimator (embedded method) to obtain the feature ranking, improve computational efficiency and reduce overfitting risk. The process is as follows:A Random Forest classifier with 100 estimators is trained on the full feature set.Features are ranked by permutation feature importance.From this ranking, four subsets are extracted containing the four fixed subset sizes (44, 26, 17, 8 variables).This dimensionality reduction scheme allows for systematic evaluation of feature subset sizes while retaining domain-informed anchors. It also facilitates efficient grid searches over compact yet informative input spaces. 5.3. Modeling TechniquesThis study benchmarks a diverse set of classification models ranging from interpretable statistical techniques to complex neural architectures. These models differ in terms of representational power, training dynamics, and sensitivity to temporal structure, allowing us to assess their relative suitability for directional trading signal generation.These models were chosen to cover a spectrum from simple interpretable models (Logistic Regression, SVM) to complex, high-performance models (XGBoost, Random Forest, LSTM, CNN), ensuring a balance between interpretability, computational efficiency, and predictive accuracy.Support Vector Machine (SVM): Offers strong performance on smaller datasets and is effective in high-dimensional spaces. It is chosen for its balance of interpretability and predictive power [33].Random Forest: An ensemble tree-based method known for robustness and handling non-linear relationships. It provides feature importance measures, aiding interpretability [34].XGBoost: A gradient boosting method with state-of-the-art performance on structured/tabular data. Selected for its high predictive accuracy and efficiency.Logistic Regression: A simple and interpretable linear model suitable for baseline comparisons. Its coefficients provide insights into feature influence [35].Long Short-Term Memory (LSTM): A recurrent neural network capable of modeling sequential dependencies, making it ideal for time-series or sequence data [36].Convolutional Neural Network (CNN): Effective at capturing spatial patterns and local correlations, often used for structured or sequential data transformed into feature maps [37]. 5.4. Baseline ModelsTo contextualize the performance of the label-based machine learning framework, we have incorporated Buy-and-Hold (BH) and Moving Average Crossover (MAC, 10/50) strategies as baselines. These were evaluated across the same 16 assets and test period using identical capital assumptions and metrics their results can be found in the resultsSection 6. 5.5. Classical Learning ModelClassical models are implemented using the scikit-learn library. They are well-suited for structured tabular data and provide baseline performance under assumptions of stationarity and linear separability.i.Logistic RegressionLogistic Regression (LR) serves as the most interpretable benchmark. We employ an L2-regularized formulation to prevent overfitting and ensure stability in the presence of multi-collinearity. While linear in nature, LR often captures short-term momentum signals effectively when features are carefully engineered.P(Y=1)=11+e‚àí(Œ≤0+Œ≤1X1+Œ≤2X2+‚Ä¶+Œ≤nXn)PY=1=11+e‚àí(Œ≤0+Œ≤1X1+Œ≤2X2+‚Ä¶+Œ≤nXn)(1)The probability of a positive class (buy signal) is modeled as a logistic function of a linear combination of input featuresX1toXnX1toXn, where Œ≤ coefficients are estimated Via maximum likelihood. The nonlinearity introduced by the sigmoid ensures outputs remain within [0, 1].ii.Support Vector Machine (SVM)We use a Support Vector Machine with a Radial Basis Function (RBF) kernel to account for non-linear decision boundaries. The RBF kernel introduces a distance-based similarity measure between observations, allowing SVM to discover complex class separation surfaces. Due to its computational complexity, SVM is only applied to reduced feature subsets.f(x)=‚àëNi=1Œ±iyiK(x,xi)+bfx=‚àëi=1NŒ±iyiK(x,xi)+b(2)The classification function is expressed as a weighted sum over support vectors, whereŒ±iŒ±iare learned weights,yiyiare the target classes, andK(x,xi)Kx,xiis the kernel function (RBF in this case), measuring similarity between new input x and training samplesxixi. The bias termbbadjusts the decision boundary.iii.Random ForestRandom Forests (RF) are ensemble-based classifiers that combine multiple decision trees trained on bootstrapped samples. They are robust to noise, offer implicit feature selection Via split importance, and provide reliable out-of-sample performance on structured data. In our experiments, we use 100 estimators and default depth constraints.yc(x)=1C‚àëci=1Pi(x)ycx=1C‚àëi=1cPix(3)The predicted class probability is the average vote across C individual decision trees, wherePi(x)Pixis the prediction from the i th tree is. This ensemble method benefits from variance reduction through averaging.iv.XGBoostExtreme Gradient Boosting (XGBoost) is a powerful, regularized boosting algorithm renowned for its success in structured data applications. In this configuration, the regularization coefficient Œª is set to 0, effectively disabling L2 shrinkage to prioritize fitting the data fully. Other parameters, such as tree depth and learning rate, use their default values unless explicitly tuned.yÃÇ(x)=‚àëKk=1fk(x)y^x=‚àëk=1Kfk(x)(4)The output is the sum of K regression treesfkfk(x), each trained to correct the errors of the previous trees. By setting Œª = 0, we disable regularization to encourage full-data fitting in our setting, optimizing purely for directional classification accuracy. 5.6. Neural Network ModelsNeural models are implemented using the Keras API with TensorFlow backend. They are designed to capture hierarchical and temporal patterns that may be difficult for classical models to identify. Early stopping with a 70/30 training-validation split is used to guard against overfitting.i.1D Convolutional Neural Network (CNN)The 1D CNN model applies three consecutive convolutional layers with small kernel sizes over the feature matrix, followed by a global max-pooling operation and a dense output layer. The design assumes that patterns of predictive significance may manifest locally across feature dimensions. CNNs are particularly effective in detecting local feature motifs that are invariant to small shifts in time.hi=‚àëki=1wi¬∑x(t+i)hi=‚àëi=1kwi¬∑x(t+i)(5)Each outputhihifrom the convolutional layer is computed as a weighted sum of a fixed-size local region in the input feature sequence x(t), wherewiwiare learnable filter weights. This formulation captures local dependencies and position-invariant patterns across time or feature space.ii.Long Short-Term Memory (LSTM)The LSTM model includes 1 recurrent layer with 64 units. LSTMs are designed to capture long-range temporal dependencies by maintaining memory over sequential inputs. This makes them well-suited for modeling sequential structures in financial data, particularly where lagged feature patterns are informative of future price movements.ii=œÉ(Wi[ht‚àí1,xt]+bi])ii=œÉ(Wi[ht‚àí1,xt]+bi])(6)fi=œÉ(Wf[ht‚àí1,xt]+bf])fi=œÉ(Wf[ht‚àí1,xt]+bf])(7)ot=œÉ(Wo[ht‚àí1,xt]+bo])ot=œÉ(Wo[ht‚àí1,xt]+bo])(8)ct=ft.ct‚àí1+it‚ãÖtanh(Wc[ht‚àí1,xt]+bc])ct=ft.ct‚àí1+it‚ãÖtanh(Wc[ht‚àí1,xt]+bc])(9)ht=ot¬∑tanh(ct)ht=ot¬∑tanh(ct)(10)where the input gateiiiicontrols how much new information from the current inputxtxtis written into the memory cell. The forget gatefifidetermines which parts of the previous memoryct‚àí1ct‚àí1to retain. The output gateototgoverns what part of the memory is exposed to the next layer. The cell state ctc_tct is updated by combining new candidate information with retained memory. The hidden statehthtrepresents the output of the LSTM unit, influenced by the current cell state.For neural networks, a 70/30 split is used within the training period to create validation sets. Early stopping based on validation loss prevents overfitting and ensures generalization. 5.7. Trading Simulation FrameworkModel outputs are interpreted as long/flat signals.At time t a long position is opened when the classifier predictsBuy (y(t) = 1).The position remains open until the model later predicts Sell (y(t) = 0), so holding periods vary and multiple positions may overlap in time.All profit and loss are calculated on a close-to-close basis with zero commission or slippage. The term percentage profit refers specifically to the percentage return of each completed trade, computed from its entry and exit prices, and is not a cumulative or ambiguous profitability measure.Two financial metrics are computed for evaluation:Per-Trade Profit:PPi=SellPricei‚àíBuyPriceiBuyPricei√ó100PPi=SellPricei‚àíBuyPriceiBuyPricei√ó100(11)Profit Factor(ratio of total gains to total losses):ProfitFactor=‚àëi‚ààwinsPi|‚àëj‚ààwinsLj|.ProfitFactor=‚àëi‚ààwinsPi‚àëj‚ààwinsLj.(12)wherePiPiandLjLjdenote profits and losses from individual trades, respectively.",
            "5.1. Data Normalization and Leakage Control": "To prevent data leakage and ensure numerical stability during training, all features undergo z-score standardization. Each feature is re-scaled by subtracting its training set mean and dividing by its training set standard deviation. This standardization procedure is performed separately for each training fold and never incorporates information from future data. By centering and scaling inputs, we ensure that model learning is not influenced by arbitrary feature magnitudes.",
            "5.2. Feature Subset Selection via Random Forest Importance": "We apply permutation-based feature importance using a Random Forest estimator (embedded method) to obtain the feature ranking, improve computational efficiency and reduce overfitting risk. The process is as follows: A Random Forest classifier with 100 estimators is trained on the full feature set.Features are ranked by permutation feature importance.From this ranking, four subsets are extracted containing the four fixed subset sizes (44, 26, 17, 8 variables). This dimensionality reduction scheme allows for systematic evaluation of feature subset sizes while retaining domain-informed anchors. It also facilitates efficient grid searches over compact yet informative input spaces.",
            "5.3. Modeling Techniques": "This study benchmarks a diverse set of classification models ranging from interpretable statistical techniques to complex neural architectures. These models differ in terms of representational power, training dynamics, and sensitivity to temporal structure, allowing us to assess their relative suitability for directional trading signal generation. These models were chosen to cover a spectrum from simple interpretable models (Logistic Regression, SVM) to complex, high-performance models (XGBoost, Random Forest, LSTM, CNN), ensuring a balance between interpretability, computational efficiency, and predictive accuracy. Support Vector Machine (SVM): Offers strong performance on smaller datasets and is effective in high-dimensional spaces. It is chosen for its balance of interpretability and predictive power [33].Random Forest: An ensemble tree-based method known for robustness and handling non-linear relationships. It provides feature importance measures, aiding interpretability [34].XGBoost: A gradient boosting method with state-of-the-art performance on structured/tabular data. Selected for its high predictive accuracy and efficiency.Logistic Regression: A simple and interpretable linear model suitable for baseline comparisons. Its coefficients provide insights into feature influence [35].Long Short-Term Memory (LSTM): A recurrent neural network capable of modeling sequential dependencies, making it ideal for time-series or sequence data [36].Convolutional Neural Network (CNN): Effective at capturing spatial patterns and local correlations, often used for structured or sequential data transformed into feature maps [37].",
            "5.4. Baseline Models": "To contextualize the performance of the label-based machine learning framework, we have incorporated Buy-and-Hold (BH) and Moving Average Crossover (MAC, 10/50) strategies as baselines. These were evaluated across the same 16 assets and test period using identical capital assumptions and metrics their results can be found in the resultsSection 6.",
            "5.5. Classical Learning Model": "Classical models are implemented using the scikit-learn library. They are well-suited for structured tabular data and provide baseline performance under assumptions of stationarity and linear separability. i.Logistic Regression Logistic Regression (LR) serves as the most interpretable benchmark. We employ an L2-regularized formulation to prevent overfitting and ensure stability in the presence of multi-collinearity. While linear in nature, LR often captures short-term momentum signals effectively when features are carefully engineered.P(Y=1)=11+e‚àí(Œ≤0+Œ≤1X1+Œ≤2X2+‚Ä¶+Œ≤nXn)PY=1=11+e‚àí(Œ≤0+Œ≤1X1+Œ≤2X2+‚Ä¶+Œ≤nXn)(1) The probability of a positive class (buy signal) is modeled as a logistic function of a linear combination of input featuresX1toXnX1toXn, where Œ≤ coefficients are estimated Via maximum likelihood. The nonlinearity introduced by the sigmoid ensures outputs remain within [0, 1]. ii.Support Vector Machine (SVM) We use a Support Vector Machine with a Radial Basis Function (RBF) kernel to account for non-linear decision boundaries. The RBF kernel introduces a distance-based similarity measure between observations, allowing SVM to discover complex class separation surfaces. Due to its computational complexity, SVM is only applied to reduced feature subsets.f(x)=‚àëNi=1Œ±iyiK(x,xi)+bfx=‚àëi=1NŒ±iyiK(x,xi)+b(2) The classification function is expressed as a weighted sum over support vectors, whereŒ±iŒ±iare learned weights,yiyiare the target classes, andK(x,xi)Kx,xiis the kernel function (RBF in this case), measuring similarity between new input x and training samplesxixi. The bias termbbadjusts the decision boundary. iii.Random Forest Random Forests (RF) are ensemble-based classifiers that combine multiple decision trees trained on bootstrapped samples. They are robust to noise, offer implicit feature selection Via split importance, and provide reliable out-of-sample performance on structured data. In our experiments, we use 100 estimators and default depth constraints.yc(x)=1C‚àëci=1Pi(x)ycx=1C‚àëi=1cPix(3) The predicted class probability is the average vote across C individual decision trees, wherePi(x)Pixis the prediction from the i th tree is. This ensemble method benefits from variance reduction through averaging. iv.XGBoost Extreme Gradient Boosting (XGBoost) is a powerful, regularized boosting algorithm renowned for its success in structured data applications. In this configuration, the regularization coefficient Œª is set to 0, effectively disabling L2 shrinkage to prioritize fitting the data fully. Other parameters, such as tree depth and learning rate, use their default values unless explicitly tuned.yÃÇ(x)=‚àëKk=1fk(x)y^x=‚àëk=1Kfk(x)(4) The output is the sum of K regression treesfkfk(x), each trained to correct the errors of the previous trees. By setting Œª = 0, we disable regularization to encourage full-data fitting in our setting, optimizing purely for directional classification accuracy.",
            "5.6. Neural Network Models": "Neural models are implemented using the Keras API with TensorFlow backend. They are designed to capture hierarchical and temporal patterns that may be difficult for classical models to identify. Early stopping with a 70/30 training-validation split is used to guard against overfitting. i.1D Convolutional Neural Network (CNN) The 1D CNN model applies three consecutive convolutional layers with small kernel sizes over the feature matrix, followed by a global max-pooling operation and a dense output layer. The design assumes that patterns of predictive significance may manifest locally across feature dimensions. CNNs are particularly effective in detecting local feature motifs that are invariant to small shifts in time.hi=‚àëki=1wi¬∑x(t+i)hi=‚àëi=1kwi¬∑x(t+i)(5) Each outputhihifrom the convolutional layer is computed as a weighted sum of a fixed-size local region in the input feature sequence x(t), wherewiwiare learnable filter weights. This formulation captures local dependencies and position-invariant patterns across time or feature space. ii.Long Short-Term Memory (LSTM) The LSTM model includes 1 recurrent layer with 64 units. LSTMs are designed to capture long-range temporal dependencies by maintaining memory over sequential inputs. This makes them well-suited for modeling sequential structures in financial data, particularly where lagged feature patterns are informative of future price movements.ii=œÉ(Wi[ht‚àí1,xt]+bi])ii=œÉ(Wi[ht‚àí1,xt]+bi])(6)fi=œÉ(Wf[ht‚àí1,xt]+bf])fi=œÉ(Wf[ht‚àí1,xt]+bf])(7)ot=œÉ(Wo[ht‚àí1,xt]+bo])ot=œÉ(Wo[ht‚àí1,xt]+bo])(8)ct=ft.ct‚àí1+it‚ãÖtanh(Wc[ht‚àí1,xt]+bc])ct=ft.ct‚àí1+it‚ãÖtanh(Wc[ht‚àí1,xt]+bc])(9)ht=ot¬∑tanh(ct)ht=ot¬∑tanh(ct)(10)where the input gateiiiicontrols how much new information from the current inputxtxtis written into the memory cell. The forget gatefifidetermines which parts of the previous memoryct‚àí1ct‚àí1to retain. The output gateototgoverns what part of the memory is exposed to the next layer. The cell state ctc_tct is updated by combining new candidate information with retained memory. The hidden statehthtrepresents the output of the LSTM unit, influenced by the current cell state. For neural networks, a 70/30 split is used within the training period to create validation sets. Early stopping based on validation loss prevents overfitting and ensures generalization.",
            "5.7. Trading Simulation Framework": "Model outputs are interpreted as long/flat signals. At time t a long position is opened when the classifier predicts Buy (y(t) = 1). The position remains open until the model later predicts Sell (y(t) = 0), so holding periods vary and multiple positions may overlap in time. All profit and loss are calculated on a close-to-close basis with zero commission or slippage. The term percentage profit refers specifically to the percentage return of each completed trade, computed from its entry and exit prices, and is not a cumulative or ambiguous profitability measure. Two financial metrics are computed for evaluation: Per-Trade Profit: PPi=SellPricei‚àíBuyPriceiBuyPricei√ó100PPi=SellPricei‚àíBuyPriceiBuyPricei√ó100(11) Profit Factor(ratio of total gains to total losses): ProfitFactor=‚àëi‚ààwinsPi|‚àëj‚ààwinsLj|.ProfitFactor=‚àëi‚ààwinsPi‚àëj‚ààwinsLj.(12)wherePiPiandLjLjdenote profits and losses from individual trades, respectively.",
            "6. Results": "This section presents the findings of our experiments, synthesizing results across 16 instruments using real-world trading metrics Percentage Profit (%P) and Profit Factor (PF). Our focus is on the transferability and consistency of optimal configurations, with specific attention to the best-performing models, look-ahead windows, and feature subsets for each asset. 6.1. Baseline ResultsTable 5presents these results, demonstrating that the optimized ML configurations generally outperform both BH (median return ~8%) and MAC (median return ~13.8%) in risk-adjusted profitability. We have updated the Methodology and Conclusion sections to detail these comparisons and confirm the incremental value of the proposed framework over traditional strategies. And due to the fixed long/flat structure and absence of stop-loss, MA crossovers behave sub-optimally.Table 5.Buy-and-Hold vs. MA (10, 50) baselines. 6.2. Per-Stock Best PerformersTable 6below provides the best model per instrument optimized for Profit Factor (PF) and Percentage Profit (%P). These results highlight the variability in performance across different models and configurations for each stock, as well as the impact of the look-ahead window and feature subset size.Table 6.Best Model per Instrument on Profit Factor (PF) and Percentage Profit (%P).FromTable 6, it is evident that XGBoost is the best-performing model in terms of both Profit Factor and Percentage Profit across several instruments, including AAPL, AMZN, SPX, and TASI. In particular, the performance of XGBoost on more volatile stocks like NVDA and on indices like NASDAQ is noteworthy. CNN and LSTM models showed exceptional performance on certain stocks like NVDA and 2010, but XGBoost generally dominated across the majority of instruments.Across all indices, XGBoost emerged as the dominant model for maximizing profit. The optimal look-ahead window generally increased with market latency, with the DJI benefiting from a shorter look-ahead window (3 days) and TASI requiring a longer look-ahead window (6 days). This suggests that faster-reacting stocks, such as those in the Dow Jones index, benefit from shorter forecasting windows, while slower-moving stocks in the TASI require a longer look-ahead period to account for their delayed reactions to market events. 6.3. Top Resulted GraphsThe results presented inFigure 2,Figure 3,Figure 4,Figure 5,Figure 6,Figure 7,Figure 8,Figure 9,Figure 10andFigure 11demonstrate a systematic evaluation of short-term trading strategies across multiple benchmarks and individual stocks. The framework highlights the importance of tailoring model configurations such as look-ahead windows, feature subsets, and algorithm selection to each asset‚Äôs unique market dynamics. For instance, while simpler models like logistic regression and XGBoost perform competitively in certain cases, deep learning approaches (LSTM, CNN) show superior profitability in high-volatility environments, as seen in tech-heavy indices and stocks. The optimal look-ahead horizon varies significantly, with liquid U.S. indices favoring shorter windows (e.g., 3 days) and less-liquid markets like TASI benefiting from longer horizons (6‚Äì10 days). Feature subset size also plays a critical role, with smaller subsets (e.g., 17 features) often sufficing for certain assets, while others require more granular inputs (e.g., 44 features).Figure 2.TASI results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 17).Figure 3.DJI index results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 44).Figure 4.GS stock results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 44).Figure 5.IBM stock results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 17).Figure 6.NASDAQ index results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 17).Figure 7.NVDA index results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 26).Figure 8.TASI results with percentage profit and profit factor on Y-axis and Number of selected features on X-axis (Look-ahead window = 3).Figure 9.SPX index results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 44).Figure 10.2010 stock results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 26).Figure 11.TASI results with percentage profit and profit factor on Y-axis and number of selected features on X-axis (look-ahead window = 6).Figure 2presents the comparative performance of six predictive models LSTM, CNN, Logistic Regression, SVM, Random Forest, and XGBoost applied to the TASI using 17 selected features. The two subplots display the Percentage Profit (left) and Profit Factor (right) across different prediction horizons labeled as Ahead-1, Ahead-3, Ahead-5, Ahead-7, and Ahead-9.The ensemble models, Random Forest and XGBoost, show superior performance compared with the others across all horizons in terms of profitability and risk-adjusted returns. XGBoost produces the highest percentage profit, around 55‚Äì60% for shorter horizons, with Random Forest performing slightly lower. Neural network models, including LSTM and CNN, achieve moderate results with profits between 35‚Äì45%, while linear models such as Logistic Regression and SVM yield lower outcomes.The Profit Factor results support the strength of the ensemble methods. Both XGBoost and Random Forest maintain values above 1.5, reflecting a strong ratio of gross profit to gross loss. Other models record profit factors around 1.2‚Äì1.5, indicating less stable profitability. It can also be seen that both predictive accuracy and profit tend to decline with longer forecasting horizons, showing that short-term market behavior is easier to predict than distant movements.Across all tests, the Profit Factor and Percentage Profit metrics reveal that no single configuration dominates universally. Instead, the highest-performing strategies emerge from asset-specific optimization, reinforcing the framework‚Äôs core thesis: adaptive, label-aware modeling outperforms rigid, one-size-fits-all approaches.Figure 2,Figure 3,Figure 4,Figure 5,Figure 6,Figure 7,Figure 8,Figure 9,Figure 10andFigure 11collectively underscore the trade-offs between model complexity, feature relevance, and temporal alignment, providing actionable insights for deploying robust short-term trading systems. 6.4. Best Look-Ahead Window (Aggregate Analysis)When aggregating across all instruments, the optimal look-ahead window was found to be 3 days, yielding the highest average Percentage Profit (around 30%). This was closely followed by 9-day (approximately 29%) and 10-day (approximately 28%) look-ahead windows. These results suggest that shorter-term forecasting horizons tend to provide a more reliable signal due to a lower noise-to-signal ratio in the immediate market response. However, this trend also indicates that shorter-term predictions are more adaptable and responsive to market fluctuations, which is crucial in fast-moving markets. 6.5. Model Popularity and Feature-Count StabilityXGBoost was the most popular model in terms of wins, securing 11 victories across the instruments studied (Table 7). Other models like Random Forest and CNN also performed well, particularly in specific contexts such as volatile or tech-focused stocks (e.g., NVDA and 2010). While LSTM had limited success, it was still able to perform well in specific cases where temporal dependencies were critical.Table 7.Best Configurations for Each Index.The feature subset size, or dimensionality, that maximized percentage profit varied across different markets. For U.S. large-cap indices like SPX and DJI, a larger feature subset (44 variables) was optimal, capturing momentum and trend signals. In contrast, markets like TASI, characterized by lower liquidity, required fewer features (8‚Äì17 variables), as excessive dimensionality tends to overfit in such markets. This highlights the ‚Äúcurse of dimensionality‚Äù in low-liquidity environments, where fewer, more relevant features tend to provide better predictive power.In terms of feature importance, momentum oscillators and volatility‚Äìvolume hybrids emerged as the most influential indicators. The top-10 features, which include RSI, %K, ADX and MACD, captured most of the total importance across all instruments (Table 8). Notably, liquidity-driven variables were more important in markets such as TASI and NASDAQ, where order flow effects have a stronger impact. These findings suggest that in more liquid markets, momentum and trend-based features dominate, while in less liquid markets, factors related to volume and liquidity have a more substantial impact on performance.Table 8.Most Important Chosen Features. 6.6. Aggregated Feature‚ÄìImportance PatternsFrom the results, several best practices emerge for optimizing trading strategies using machine learning models. Firstly, for diversified baskets, starting with a look-ahead window of 3 tends to yield the best average performance. However, market-specific tuning is recommended, with shorter look-ahead windows for faster-reacting indices like DJI and longer windows for slower-reacting markets like TASI. XGBoost should be prioritized, while Random Forest is suitable when the feature count is low. CNN and LSTM models can be tested for more volatile or tech-focused stocks. Lastly, it is essential to avoid blindly transferring configurations from indices to their constituents, as only 25% of cases shared the same optimal look-ahead window.",
            "6.1. Baseline Results": "Table 5presents these results, demonstrating that the optimized ML configurations generally outperform both BH (median return ~8%) and MAC (median return ~13.8%) in risk-adjusted profitability. We have updated the Methodology and Conclusion sections to detail these comparisons and confirm the incremental value of the proposed framework over traditional strategies. And due to the fixed long/flat structure and absence of stop-loss, MA crossovers behave sub-optimally. Table 5.Buy-and-Hold vs. MA (10, 50) baselines.",
            "6.2. Per-Stock Best Performers": "Table 6below provides the best model per instrument optimized for Profit Factor (PF) and Percentage Profit (%P). These results highlight the variability in performance across different models and configurations for each stock, as well as the impact of the look-ahead window and feature subset size. Table 6.Best Model per Instrument on Profit Factor (PF) and Percentage Profit (%P). FromTable 6, it is evident that XGBoost is the best-performing model in terms of both Profit Factor and Percentage Profit across several instruments, including AAPL, AMZN, SPX, and TASI. In particular, the performance of XGBoost on more volatile stocks like NVDA and on indices like NASDAQ is noteworthy. CNN and LSTM models showed exceptional performance on certain stocks like NVDA and 2010, but XGBoost generally dominated across the majority of instruments. Across all indices, XGBoost emerged as the dominant model for maximizing profit. The optimal look-ahead window generally increased with market latency, with the DJI benefiting from a shorter look-ahead window (3 days) and TASI requiring a longer look-ahead window (6 days). This suggests that faster-reacting stocks, such as those in the Dow Jones index, benefit from shorter forecasting windows, while slower-moving stocks in the TASI require a longer look-ahead period to account for their delayed reactions to market events.",
            "6.3. Top Resulted Graphs": "The results presented inFigure 2,Figure 3,Figure 4,Figure 5,Figure 6,Figure 7,Figure 8,Figure 9,Figure 10andFigure 11demonstrate a systematic evaluation of short-term trading strategies across multiple benchmarks and individual stocks. The framework highlights the importance of tailoring model configurations such as look-ahead windows, feature subsets, and algorithm selection to each asset‚Äôs unique market dynamics. For instance, while simpler models like logistic regression and XGBoost perform competitively in certain cases, deep learning approaches (LSTM, CNN) show superior profitability in high-volatility environments, as seen in tech-heavy indices and stocks. The optimal look-ahead horizon varies significantly, with liquid U.S. indices favoring shorter windows (e.g., 3 days) and less-liquid markets like TASI benefiting from longer horizons (6‚Äì10 days). Feature subset size also plays a critical role, with smaller subsets (e.g., 17 features) often sufficing for certain assets, while others require more granular inputs (e.g., 44 features). Figure 2.TASI results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 17). Figure 3.DJI index results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 44). Figure 4.GS stock results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 44). Figure 5.IBM stock results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 17). Figure 6.NASDAQ index results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 17). Figure 7.NVDA index results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 26). Figure 8.TASI results with percentage profit and profit factor on Y-axis and Number of selected features on X-axis (Look-ahead window = 3). Figure 9.SPX index results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 44). Figure 10.2010 stock results with percentage profit and profit factor on Y-axis and look-ahead window on X-axis (number of selected features = 26). Figure 11.TASI results with percentage profit and profit factor on Y-axis and number of selected features on X-axis (look-ahead window = 6). Figure 2presents the comparative performance of six predictive models LSTM, CNN, Logistic Regression, SVM, Random Forest, and XGBoost applied to the TASI using 17 selected features. The two subplots display the Percentage Profit (left) and Profit Factor (right) across different prediction horizons labeled as Ahead-1, Ahead-3, Ahead-5, Ahead-7, and Ahead-9. The ensemble models, Random Forest and XGBoost, show superior performance compared with the others across all horizons in terms of profitability and risk-adjusted returns. XGBoost produces the highest percentage profit, around 55‚Äì60% for shorter horizons, with Random Forest performing slightly lower. Neural network models, including LSTM and CNN, achieve moderate results with profits between 35‚Äì45%, while linear models such as Logistic Regression and SVM yield lower outcomes. The Profit Factor results support the strength of the ensemble methods. Both XGBoost and Random Forest maintain values above 1.5, reflecting a strong ratio of gross profit to gross loss. Other models record profit factors around 1.2‚Äì1.5, indicating less stable profitability. It can also be seen that both predictive accuracy and profit tend to decline with longer forecasting horizons, showing that short-term market behavior is easier to predict than distant movements. Across all tests, the Profit Factor and Percentage Profit metrics reveal that no single configuration dominates universally. Instead, the highest-performing strategies emerge from asset-specific optimization, reinforcing the framework‚Äôs core thesis: adaptive, label-aware modeling outperforms rigid, one-size-fits-all approaches.Figure 2,Figure 3,Figure 4,Figure 5,Figure 6,Figure 7,Figure 8,Figure 9,Figure 10andFigure 11collectively underscore the trade-offs between model complexity, feature relevance, and temporal alignment, providing actionable insights for deploying robust short-term trading systems.",
            "6.4. Best Look-Ahead Window (Aggregate Analysis)": "When aggregating across all instruments, the optimal look-ahead window was found to be 3 days, yielding the highest average Percentage Profit (around 30%). This was closely followed by 9-day (approximately 29%) and 10-day (approximately 28%) look-ahead windows. These results suggest that shorter-term forecasting horizons tend to provide a more reliable signal due to a lower noise-to-signal ratio in the immediate market response. However, this trend also indicates that shorter-term predictions are more adaptable and responsive to market fluctuations, which is crucial in fast-moving markets.",
            "6.5. Model Popularity and Feature-Count Stability": "XGBoost was the most popular model in terms of wins, securing 11 victories across the instruments studied (Table 7). Other models like Random Forest and CNN also performed well, particularly in specific contexts such as volatile or tech-focused stocks (e.g., NVDA and 2010). While LSTM had limited success, it was still able to perform well in specific cases where temporal dependencies were critical. Table 7.Best Configurations for Each Index. The feature subset size, or dimensionality, that maximized percentage profit varied across different markets. For U.S. large-cap indices like SPX and DJI, a larger feature subset (44 variables) was optimal, capturing momentum and trend signals. In contrast, markets like TASI, characterized by lower liquidity, required fewer features (8‚Äì17 variables), as excessive dimensionality tends to overfit in such markets. This highlights the ‚Äúcurse of dimensionality‚Äù in low-liquidity environments, where fewer, more relevant features tend to provide better predictive power. In terms of feature importance, momentum oscillators and volatility‚Äìvolume hybrids emerged as the most influential indicators. The top-10 features, which include RSI, %K, ADX and MACD, captured most of the total importance across all instruments (Table 8). Notably, liquidity-driven variables were more important in markets such as TASI and NASDAQ, where order flow effects have a stronger impact. These findings suggest that in more liquid markets, momentum and trend-based features dominate, while in less liquid markets, factors related to volume and liquidity have a more substantial impact on performance. Table 8.Most Important Chosen Features.",
            "6.6. Aggregated Feature‚ÄìImportance Patterns": "From the results, several best practices emerge for optimizing trading strategies using machine learning models. Firstly, for diversified baskets, starting with a look-ahead window of 3 tends to yield the best average performance. However, market-specific tuning is recommended, with shorter look-ahead windows for faster-reacting indices like DJI and longer windows for slower-reacting markets like TASI. XGBoost should be prioritized, while Random Forest is suitable when the feature count is low. CNN and LSTM models can be tested for more volatile or tech-focused stocks. Lastly, it is essential to avoid blindly transferring configurations from indices to their constituents, as only 25% of cases shared the same optimal look-ahead window.",
            "7. Discussion and Analysis": "In this section, we discuss the results thoroughly, with special attention to the responsiveness of the look-ahead windows, the relative performance of classical and deep learning methods, the portability of the results between different assets, contributions of individual features, and the influence of model architecture on feature importance. 7.1. Sensitivity of Look-Ahead WindowThe sensitivity of performance to the length of the look-ahead window is a critical aspect of our analysis. Our research indicates that the look-ahead time may be increased to H ‚â• 8. For example, longer horizons can provide more room for market corrections and reduce the impact of short-term fluctuations, making them particularly suitable for more stable indices such as the S&P 500 (SPX) and TASI, where preserving capital is often more important than maximizing raw returns.Additionally, when the forecast horizon is shorter such as 3 days horizon, the models reported higher raw profit gains, not primarily for volatile assets such as NASDAQ and NVDA. As a result of their increased volatility day to day movements of these assets are more pronounced and short-term look-ahead windows enable the models capture these movements well. This creates a trade-off between return consistency (provided by longer look-ahead windows) and total return magnitude (driven by shorter horizons).The findings show that the look-ahead window that is the most effective fluctuates greatly in different contexts. In volatile markets or markets with rapid price changes, a smaller horizon is more helpful in achieving high profitably. On the other hand, with stable priced indices or assets, use of a longer look ahead window could limit the production of false signals, thus increasing the consistency of results. A contingency of stability and return magnitude is revealed in this analysis as significant in selecting an optimal model setup for practitioners. 7.2. Classical Models vs. Deep LearningOne of the key findings in our study is the superior performance of XGBoost compared to deep learning models, particularly when evaluated on out-of-sample percentage profit across our 16-instrument universe. During our study, XGBoost, a classical tree-based technique consistently outperformed deep learning on out-of-sample trials, indicating that tree models are very good at recognizing subtle, but important correlations in financial structures. Such performance explicates that XGBoost, given its ability to process heterogeneous features and resolve overfitting, is a reliable tool for general financial predictions.However, deep learning models, such as CNN and LSTM revealed their effectiveness in niche market scenario, particularly during times of increased volatility, such as with NVDA. In such cases, deep learning architectures far outperformed tree models, adding more than 250 basis points to returns over the top competing tree-based option. It illustrates how traditional models tend to be favored even though deep learning approaches shine in finding subtleties in patterns and interdependencies in highly turbulent financial numbers. Take NVDA as an example, its price being influenced by macroeconomic and technical dynamics, while by virtue of deep learning models‚Äô expertise with sequence and hierarchical data NVDA has benefited.The performance of deep learning models in high-volatility, tech-focused stocks implies that they are best used as tactical overlays rather than replacements for traditional models. When deployed strategically, deep learning architectures can offer enhanced returns in volatile markets, but in more stable environments, classical models like XGBoost remain preferable due to their simpler and more interpretable nature. This distinction is important for practitioners who seek to optimize their trading strategies based on market conditions. 7.3. Cross-Asset TransferabilityThe concept of cross-asset transferability is a central theme in this study. For seven out of the twelve constituents (58%), the index-optimal model also proved to be the stock-optimal model. This suggests that, in many cases, models trained on an index can be successfully transferred to its individual components, saving substantial computational resources in the process. Moreover, the reusability of models across related assets provides a significant advantage in terms of reducing the costs of grid searches and hyper parameter tuning.However, while the model selection transferability is relatively high, the transferability of the look-ahead window was more limited. Only three constituents (25%) shared the same optimal look-ahead window as their respective index indicating that the optimal look-ahead window for a given index does not necessarily extend to its constituent stocks. This suggests that while model configurations may transfer effectively across assets, the look-ahead window must be re-tuned at the stock level for optimal performance.The findings highlight the value of adopting a top-down approach to model deployment. Training on the index and then reusing the model on its constituents saves roughly 50% of the grid-search cost, while only a slight reduction in performance is observed, provided that look-ahead windows are fine-tuned locally. This strategy offers significant cost-saving opportunities without compromising the efficacy of the model. 7.4. Interpreting Feature ImportanceThe analysis of feature importance reveals that fast-acting oscillators and micro-structure proxies are consistently the most influential features across different models and assets. Notably, features such as the relative strength index (RSI) and ADX were dominant in many of the best-performing models. These features are highly sensitive to short-term price movements, indicating that market participants often react to immediate, technical signals, such as mean-reversion patterns and liquidity shocks, in a way that can be exploited by machine learning models.The dominance of fast-acting features aligns with the behavioral finance view that markets exhibit short-term mean-reversion and are subject to liquidity-driven shocks. These findings suggest that practitioners should prioritize maintaining a clean, low-latency data feed for these signals, as they appear to be the most actionable in the context of financial forecasting. The marginal gain from incorporating longer-lag macroeconomic indicators appears limited, supporting the notion that focusing on shorter-term, high-frequency signals may provide better results. 7.5. Model and Feature InteractionsThe interaction between features and models reveals important insights into the synergies between different model architectures and feature subsets. Classical models, such as XGBoost and Random Forest, benefit from reduced dimensionality, where a smaller number of features provides more effective decision boundaries. These models excel in capturing relationships between a limited set of strong signals, making them well-suited for structured data with a focus on trend-following or momentum.In contrast, deep learning models, particularly CNNs and LSTMs, excel at extracting higher-order interactions from larger feature sets. Even when the number of features increases (up to 44 variables), deep models are capable of processing complex, non-linear relationships and temporal dependencies within the data. This ability to handle large feature subsets and model intricate patterns makes deep learning models particularly well-suited for high-dimensional problems, where the interactions between features are not immediately apparent.These interactions between model types and feature sets suggest that a hybrid approach may be beneficial, where classical models are used for more stable, low-dimensional tasks, and deep learning models are employed for tasks that require higher-order pattern recognition and temporal sequence learning. This approach ensures that both the strengths of classical machine learning and deep learning models are leveraged to their fullest potential. 7.6. Market Microstructure and Performance DifferencesThe differing optimal models and labeling horizons between US markets and the Saudi TASI arise from fundamental market-microstructure distinctions.Liquidity and Settlement:US equities exhibit high liquidity and rapid price discovery, supported by a shorter settlement cycle. TASI operates with lower liquidity and slower execution dynamics.Participant Mix and Volatility:TASI‚Äôs higher retail participation produces slower, episodic volatility patterns, while US markets show more stable, institution-driven volatility structures.Impact on Forecasting Horizons:TASI: Slower price adjustment favors longer prediction horizons.US indices: High liquidity and fast information flow make shorter horizons more effective.These structural differences explain why forecasting performance is market-dependent and why no single model‚Äìhorizon combination generalizes across such heterogeneous environments.",
            "7.1. Sensitivity of Look-Ahead Window": "The sensitivity of performance to the length of the look-ahead window is a critical aspect of our analysis. Our research indicates that the look-ahead time may be increased to H ‚â• 8. For example, longer horizons can provide more room for market corrections and reduce the impact of short-term fluctuations, making them particularly suitable for more stable indices such as the S&P 500 (SPX) and TASI, where preserving capital is often more important than maximizing raw returns. Additionally, when the forecast horizon is shorter such as 3 days horizon, the models reported higher raw profit gains, not primarily for volatile assets such as NASDAQ and NVDA. As a result of their increased volatility day to day movements of these assets are more pronounced and short-term look-ahead windows enable the models capture these movements well. This creates a trade-off between return consistency (provided by longer look-ahead windows) and total return magnitude (driven by shorter horizons). The findings show that the look-ahead window that is the most effective fluctuates greatly in different contexts. In volatile markets or markets with rapid price changes, a smaller horizon is more helpful in achieving high profitably. On the other hand, with stable priced indices or assets, use of a longer look ahead window could limit the production of false signals, thus increasing the consistency of results. A contingency of stability and return magnitude is revealed in this analysis as significant in selecting an optimal model setup for practitioners.",
            "7.2. Classical Models vs. Deep Learning": "One of the key findings in our study is the superior performance of XGBoost compared to deep learning models, particularly when evaluated on out-of-sample percentage profit across our 16-instrument universe. During our study, XGBoost, a classical tree-based technique consistently outperformed deep learning on out-of-sample trials, indicating that tree models are very good at recognizing subtle, but important correlations in financial structures. Such performance explicates that XGBoost, given its ability to process heterogeneous features and resolve overfitting, is a reliable tool for general financial predictions. However, deep learning models, such as CNN and LSTM revealed their effectiveness in niche market scenario, particularly during times of increased volatility, such as with NVDA. In such cases, deep learning architectures far outperformed tree models, adding more than 250 basis points to returns over the top competing tree-based option. It illustrates how traditional models tend to be favored even though deep learning approaches shine in finding subtleties in patterns and interdependencies in highly turbulent financial numbers. Take NVDA as an example, its price being influenced by macroeconomic and technical dynamics, while by virtue of deep learning models‚Äô expertise with sequence and hierarchical data NVDA has benefited. The performance of deep learning models in high-volatility, tech-focused stocks implies that they are best used as tactical overlays rather than replacements for traditional models. When deployed strategically, deep learning architectures can offer enhanced returns in volatile markets, but in more stable environments, classical models like XGBoost remain preferable due to their simpler and more interpretable nature. This distinction is important for practitioners who seek to optimize their trading strategies based on market conditions.",
            "7.3. Cross-Asset Transferability": "The concept of cross-asset transferability is a central theme in this study. For seven out of the twelve constituents (58%), the index-optimal model also proved to be the stock-optimal model. This suggests that, in many cases, models trained on an index can be successfully transferred to its individual components, saving substantial computational resources in the process. Moreover, the reusability of models across related assets provides a significant advantage in terms of reducing the costs of grid searches and hyper parameter tuning. However, while the model selection transferability is relatively high, the transferability of the look-ahead window was more limited. Only three constituents (25%) shared the same optimal look-ahead window as their respective index indicating that the optimal look-ahead window for a given index does not necessarily extend to its constituent stocks. This suggests that while model configurations may transfer effectively across assets, the look-ahead window must be re-tuned at the stock level for optimal performance. The findings highlight the value of adopting a top-down approach to model deployment. Training on the index and then reusing the model on its constituents saves roughly 50% of the grid-search cost, while only a slight reduction in performance is observed, provided that look-ahead windows are fine-tuned locally. This strategy offers significant cost-saving opportunities without compromising the efficacy of the model.",
            "7.4. Interpreting Feature Importance": "The analysis of feature importance reveals that fast-acting oscillators and micro-structure proxies are consistently the most influential features across different models and assets. Notably, features such as the relative strength index (RSI) and ADX were dominant in many of the best-performing models. These features are highly sensitive to short-term price movements, indicating that market participants often react to immediate, technical signals, such as mean-reversion patterns and liquidity shocks, in a way that can be exploited by machine learning models. The dominance of fast-acting features aligns with the behavioral finance view that markets exhibit short-term mean-reversion and are subject to liquidity-driven shocks. These findings suggest that practitioners should prioritize maintaining a clean, low-latency data feed for these signals, as they appear to be the most actionable in the context of financial forecasting. The marginal gain from incorporating longer-lag macroeconomic indicators appears limited, supporting the notion that focusing on shorter-term, high-frequency signals may provide better results.",
            "7.5. Model and Feature Interactions": "The interaction between features and models reveals important insights into the synergies between different model architectures and feature subsets. Classical models, such as XGBoost and Random Forest, benefit from reduced dimensionality, where a smaller number of features provides more effective decision boundaries. These models excel in capturing relationships between a limited set of strong signals, making them well-suited for structured data with a focus on trend-following or momentum. In contrast, deep learning models, particularly CNNs and LSTMs, excel at extracting higher-order interactions from larger feature sets. Even when the number of features increases (up to 44 variables), deep models are capable of processing complex, non-linear relationships and temporal dependencies within the data. This ability to handle large feature subsets and model intricate patterns makes deep learning models particularly well-suited for high-dimensional problems, where the interactions between features are not immediately apparent. These interactions between model types and feature sets suggest that a hybrid approach may be beneficial, where classical models are used for more stable, low-dimensional tasks, and deep learning models are employed for tasks that require higher-order pattern recognition and temporal sequence learning. This approach ensures that both the strengths of classical machine learning and deep learning models are leveraged to their fullest potential.",
            "7.6. Market Microstructure and Performance Differences": "The differing optimal models and labeling horizons between US markets and the Saudi TASI arise from fundamental market-microstructure distinctions. Liquidity and Settlement: US equities exhibit high liquidity and rapid price discovery, supported by a shorter settlement cycle. TASI operates with lower liquidity and slower execution dynamics. Participant Mix and Volatility: TASI‚Äôs higher retail participation produces slower, episodic volatility patterns, while US markets show more stable, institution-driven volatility structures. Impact on Forecasting Horizons:TASI: Slower price adjustment favors longer prediction horizons.US indices: High liquidity and fast information flow make shorter horizons more effective. These structural differences explain why forecasting performance is market-dependent and why no single model‚Äìhorizon combination generalizes across such heterogeneous environments.",
            "8. Conclusions": "This study presents a systematic methodology for building high-performing trading configurations across indices and individual stocks. By jointly exploring model choice, labeling horizon, and feature subsets, the framework identifies optimal settings for each asset while reducing dimensionality without sacrificing profitability. An important finding is the transferability of index-level configurations to constituent stocks, enabling a top-down approach where an index-trained model can guide stock-level training with minimal adjustments, mainly in look-ahead tuning. Future work focuses on several extensions, i.e., online meta-learning and regime-aware reconfiguration for dynamic adjustment; realistic execution modeling (commission-slippage), latency, and market impact for closer alignment with live trading; analysis across market regimes (bull, bear, high-volatility) to identify horizon-dependent strengths and vulnerabilities; dynamic, volatility-responsive look-ahead windows using adaptive scaling or DTW-based selection to improve label quality; and realistic cost models to refine profitability. Once extended, the framework will include portfolio-level risk-adjusted metrics (Sharpe, Sortino, Maximum Drawdown) and statistical robustness analysis (t-tests, bootstrapping, horizon-wise confidence intervals) to support comprehensive significance testing and regime-aware robustness evaluation."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2227-7390/13/23/3889",
        "scraped_at": "2025-12-05 23:54:09"
    },
    {
        "title": "Estimation of Daily Charging Profiles of Private Cars in Urban Areas Through Floating Car Data",
        "authors": "byMaria P. Valentini,Valentina Conti,Matteo Corazza,Andrea Gemma,Federico Karagulian,Maria Lelli,Carlo LibertoandGaetano Valenti",
        "journal": "Energies2025,18(23), 6370; https://doi.org/10.3390/en18236370 (registering¬†DOI) - 4 Dec 2025",
        "abstract": "This paper presents a comprehensive methodology to forecast the daily energy demand associated with recharging private electric vehicles in urban areas. The approach is based on plausible scenarios regarding the penetration of battery-powered vehicles and the availability of charging infrastructure. Accurate space and time forecasting of charging activities and power requirements is a critical issue in supporting the transition from conventional to battery-powered vehicles for urban mobility. This technological shift represents a key milestone toward achieving the zero-emissions target set by the European Green Deal for 2050. The methodology leverages Floating Car Data (FCD) samples. The widespread use of On-Board Units (OBUs) in private vehicles for insurance purposes ensures the methodology‚Äôs applicability across diverse geographical contexts. In addition to FCD samples, the estimation of charging demand for private electric vehicles is informed by a large-scale, detailed survey conducted by ENEA in Italy in 2023. Funded by the Ministry of Environment and Energy Security as part of the National Research on the Electric System, the survey explored individual charging behaviors during daily urban trips and was designed to calibrate a discrete choice model. To date, the methodology has been applied to the Metropolitan Area of Rome, demonstrating robustness and reliability in its results on two different scenarios of analysis. Each demand/supply scenario has been evaluated in terms of the hourly distribution of peak charging power demand, at the level of individual urban zones or across broader areas. Results highlight the role of the different components of power demand (at home or at other destinations) in both scenarios. Charging at intermediate destinations exhibits a dual peak pattern‚Äîone in the early morning hours and another in the afternoon‚Äîwhereas home-based charging shows a pronounced peak during evening return hours and a secondary peak in the early afternoon, corresponding to a decline in charging activity at other destinations. Power distributions, as expected, sensibly differ from one scenario to the other, conditional to different assumptions of private and public recharge availability and characteristics.Keywords:electric mobility;charging profiles;charging behavior;charging infrastructure;Floating Car Data",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "This paper presents the results of a research activity carried out by ENEA and its academic partners during the 2022‚Äì2024 period, as part of the multiannual Research Program on the Electricity System, under the supervision of the Italian Ministry of the Environment and Energy Security (MASE). The objective of the study is to estimate daily charging profiles of private electric vehicles across different zones of an urban area, using standardized data formats to ensure methodological reproducibility. The profiles are derived as an expected outcome of electric vehicle penetration rates, incorporating projected charging infrastructure scenarios. Estimating charging profiles for typical days of the year provides essential insights for verifying charging infrastructure suitability and determining the electrical power requirements in various urban zones. Moreover, it enables the assessment of strategies to reduce peak power demand and redistribute it evenly throughout the day, through smart charging technologies and criteria, with or without the use of bidirectional Vehicle-to-Everything (V2X) systems. Although our study primarily focuses on assessing the impact of EV private vehicles on charging and power distribution networks, the procedure‚Äîby estimating the energy stored in vehicle batteries at the start and end of each charging session‚Äîcan also be applied to evaluate V2X potential. As detailed in the following sections, the model distinguishes between charging location and type of grid access (either through public or private infrastructures), thus enabling separate assessments of V2G and V2B potential. Figure 1illustrates the dual role of predictive charging demand models: first, estimating power demand across the territory to optimally size charging infrastructures and power networks; second, assessing the energy stored in electric vehicle batteries over time and space to support the implementation of V2X strategies. Figure 1.Flow-chart representing the capability of predictive charging demand models for estimating: power and charging infrastructures needs across the territory, and potential of V2X, integrated both with power grids (V2G) and buildings electrical installations (V2B). Several studies highlight the relevance of a well-designed charging infrastructure in promoting the adoption of electric mobility [1,2,3]. Design challenges are particularly complex in urban environments [4,5] where space is limited, and the demand is particularly variable and diverse. Public charging infrastructure for individual mobility is often installed based on parking concentration criteria, statistically assuming that charging demand increases in larger parking areas or those intended for longer stays. Our research aims to refine demand analysis by estimating the probability that a vehicle will be charged during a specific stopover. This estimation considers not only the characteristics of the parking location and available charging options but also the presumed battery state of charge and the expected mobility needs following the stop. Where possible, the probability of charging is also linked to the driver‚Äôs individual profile. To this end, an individual behavioral model was developed based on the results of a Stated Preferences (SP) survey. A previous publication [6] reviewed the state of the art in behavioral modeling for EV charging and the use of surveys for this purpose. The model is designed to make use of FCD (Floating Car Data). FCD offers valuable, albeit partial, insights into individual mobility patterns, requiring appropriate scaling procedures to infer total mobility demand. Recent research has explored the integration of FCD with stationary sensors and opportunistic data sources to enhance representativeness and spatial coverage [7,8,9,10,11]. FCD exploitation for the estimation of private electric vehicle charging demand has been already proposed in the literature [12,13], mainly to individuate major stopover concentration across the territory and time. In other cases, individual mobility patterns are considered [14] to apply simplified behavioral paradigms. Recently, FCD potential has been investigated to not only address a smart design of recharge infrastructure but also for leveraging the potential of electric vehicles to support electricity grids [15,16]. Models for V2G applications represent a very promising research field, a source of inspiration for charge demand forecasting. Building on the current state of the art [17], this study implements a complete FCD-based pipeline for reconstructing individual trips and travel chains, identifying approximate home and work locations, and deriving detailed mobility statistics. Home-based travel chains derived from FCD are considered the ‚Äúchoice units‚Äù for applying a probabilistic behavioral charging model based on results of a large-scale Stated Preferences (SP) survey that considered many choice variables such as travel energy needs, battery state of charge (SOC), and the availability and characteristics of charging infrastructure at each stop. By simulating individual charging events, aggregate local energy demand is calculated over the day (charging profiles), enabling a probabilistic estimation of zonal charging profiles‚Äîi.e., the temporal distribution of power demand for private EV charging across different city zones. The paper is structured as follows:Section 1outlines the general framework of the proposed methodology;Section 2details the key computational algorithms; andSection 3presents results from a representative case study. The final section,Section 4, discusses the applicability of the procedure and highlights the relevance of the findings. Specific references are included in the corresponding sections.",
            "2. Materials and Methods": "2.1. Overall Procedural SchemeThe proposed procedure is based on the use of large samples of FCD, enabling the reconstruction of individual mobility patterns; these are intended as the most likely travel sequences over an analysis period sufficiently representative of a given seasonality for the entire population of residents within an urban area. A behavioral model is applied to these patterns, indicating, for each planned stop, the probability that, in the case of using a battery-powered electric vehicle (BEV), charging will be performed. This is carried out taking into account the characteristics of the stop‚Äîmainly stop duration‚Äîavailability, and characteristics of charging infrastructure, the battery‚Äôs state of charge, the expected mobility needs after the stop, and the driver‚Äôs profile.In a context where both battery power penetration and charging infrastructure are fully consolidated and known, the proposed procedure allows for the most likely demand to be assessed in different urban areas over the course of a typical day. Conversely, in contexts where the e-mobility is still evolving, both on the supply and demand sides, as is currently the case in Italy and other European countries, the analysis tool works like a classic ‚Äúwhat-if‚Äù Decision Support System (DSS). In fact, it allows for the simulation of the effects of policies favoring a greater penetration of battery power in individual urban transport, ultimately also enabling a cost‚Äìbenefit assessment of the different policy options.Figure 2refers to this latter type of application: FCD processing provides information on individual mobility patterns, which are then also qualified in terms of energy consumption, once the battery electric vehicle‚Äôs rate into the circulating fleet has been assumed. On the other hand, evidence on individual charging behavior provided by surveys and literature feeds a random utility model. Such a model is then applied to the mobility patterns of private vehicles assumed to be electric under different hypotheses on recharge infrastructure availability and characteristics, yielding a probabilistic estimate of charging events in different urban areas.Figure 2.Estimation of private electric cars‚Äô urban charging profiles (CPs) from FCD. z is a generic urban zone, tiis a generic day time interval, Vi is a generic electric car, and Pi is the related probability of recharging in (z; ti).For each time tiand urban zone z, appropriately delimited based on criteria of territorial homogeneity, the private car charging behavioral model provides a probability that each vehicle v parked in the area is charging. Given the power (kW) available to charge electric cars in that specific area, it is possible to calculate, by summation, the power absorbed for charging in the area at the generic instant t, deriving the daily zonal charging profiles. 2.2. Floating Car Exploration and ExploitationFCD are geolocated time series generated by vehicles equipped with OBUs or integrated GPS systems capable of recording, at regular time or space intervals, position, speed, heading, engine status (on/off), and other operational attributes. FCD can be regarded as mobile sensors that opportunistically sample the road network, offering broad spatiotemporal coverage and, when properly pre-processed, valuable insights into individual travel behavior.The FCD processing pipeline consists of a comprehensive framework designed to convert raw GPS data streams into coherent, validated, and analytically reusable mobility products. Each FCD record includes a unique trace identifier, a pseudonymized vehicle ID that remains constant throughout the observation period, a timestamp with one-second precision, geographical coordinates in WGS84 format, instantaneous speed, heading, engine status, GPS signal quality, and an odometer-based progressive distance value. Sampling frequency may vary depending on the data provider, being either time-based, such as every thirty seconds, or space-based, for example, every one or two kilometers. In the considered case study, most samples were recorded at thirty-second intervals, although more advanced on-board units can generate additional observations when abrupt changes in curvature or speed occur.The pipeline‚Äôs main objective is to transform these heterogeneous data streams into structured and consistent analytical units suitable for quantitative transport analysis. This process begins with the verification of spatial and temporal quality through data normalization and validation, which ensures that all samples are spatially consistent and temporally ordered.Once samples are validated, the pipeline reconstructs vehicle trajectories and generates the two main analytical entities of the system: trips and TripLegs. A trip corresponds to a continuous movement session of a vehicle, typically delimited by engine-on and engine-off conditions, or, when such information is unavailable, inferred from thresholds on speed and stop duration. Each trip is described by a sequence of ordered GPS records that include departure and arrival times, traveled distance, total duration, and average speed. From these sequences, the pipeline derives sub-segments known as TripLegs, which represent all possible pairs of chronologically sequential GPS points belonging to different traffic zones. This structure allows for a detailed assessment of travel performance, including average travel times, speeds, and distances between origin‚Äìdestination pairs, and enables the construction of OD matrices and travel time skims directly from empirical data.A robust outlier detection procedure based on the boxplot method is applied to filter anomalous TripLegs that could distort the analysis, while incomplete or noisy trajectories, such as single-sample trips or those affected by signal loss, are removed to maintain the integrity of the analytical base. The aggregation of trips and TripLegs by origin and destination zones, time periods, and day types results in the generation of observed OD matrices and performance indicators that describe spatial and temporal patterns of mobility.Besides trajectory reconstruction, a spatial analysis module identifies stationary clusters of points, known as stay points, which represent typical stopping locations. The identification is based on the DBSCAN density-based clustering algorithm [18] applied to stationary samples within a predefined radius, with parameters tuned to the density of the urban network. For each cluster, average stop duration, variance, and recurrence are computed to characterize both short and long stops across different temporal windows. Two categories of stay points are recognized: internal stay points, located within the study area, and virtual stay points, representing points of entry or exit along the network boundaries. Virtual stay points are detected when clusters extend spatially along access corridors, and the initial odometer reading indicates prior to travel before the first recorded point. Each stay point is associated with a traffic zone and linked to trips as an origin or destination, providing spatial anchors for reconstructing travel chains.The classification of stay points into functional types relies on duration and recurrence criteria. The stay point with the longest cumulative stop duration, accounting for at least ten percent of the total recorded time, is identified as the home location. Recurrent stops of a long duration with weekly periodicity are classified as work, while the remaining points are labeled as other. Validation of this classification using census and employment data produced determination coefficients of 0.91 for home and 0.71 for work, confirming the reliability of the method in reproducing real activity locations.From the identified trips and classified stay points, the pipeline reconstructs home-based travel chains by chronologically ordering trips for each vehicle. Each chain starts at a trip departing from the home location and aggregates subsequent trips until the vehicle returns home. The resulting sequence of trips forms a home-based chain stored as an analytical unit. During storage, each chain is enriched with summary attributes such as total travel time, number of intermediate stops, cumulative duration, and information about preceding and following chains. The classification of these chains is based on the ordered sequence of stay point types, generating categories such as HOME‚ÄìHOME, HOME‚ÄìWORK‚ÄìHOME, HOME‚ÄìOTHER‚ÄìHOME, or mixed combinations that describe different mobility behaviors. This reconstruction enables the analysis of systematic and non-systematic travel patterns at the individual level, facilitating the study of commuting behavior and the identification of daily activity structures.Over the past two decades, generative models of human mobility have attracted increasing attention within the scientific community. The main reasons lie in their ability to overcome issues related to the use of personal or sensitive data (such as those derived from vehicle GPSs or mobile phone records), while also ensuring scalability when available datasets are incomplete or not representative. Moreover, they allow researchers to extract global behavioral patterns and universal laws that can be adapted to different geographical contexts (once properly calibrated), thus enabling what-if simulations and scenario analyses to support transportation planning and mobility policy design.Among human mobility models, the EPR‚ÄîExploration and Preferential Return‚Äîmodel and its extensions stand out as one of the most widely adopted approaches. This success is largely due to their capacity to reproduce emerging behavior and scaling laws (e.g., travel distances and gyration radius [19,20], waiting time location frequency [21], entropy [19,20,22], and individual mobility networks [22,23]) and to reconstruct complex individual mobility trajectories through a relatively simple behavioral formulation. Specifically, the baseline EPR model assumes that spatio-temporal mobility patterns emerge from the alternation between two fundamental behaviors: the tendency to return to previously visited locations and the exploration of new areas [21].In this work, to represent the temporal dynamics of individual mobility, we adopted a Markovian approach [24] able to reproduce personal agendas by determining both the arrival time at each visited location and the duration of the corresponding stop. In this way, the model captures the propensity for individuals to follow or deviate from their typical daily routines. On the other hand, the spatial dimension was considered through a gravity-inspired formulation, which exploits aggregate information (e.g., the distribution of the population) and accounts for exploratory movements [25].Rather than applying a uniform discretization of the study area, as commonly adopted in similar modeling approaches, we relied on the traffic zones defined by Roma Servizi per la Mobilit√† (https://romamobilita.it/it, accessed on 27 November 2025). The chosen zoning provides spatial elements that are homogeneous with respect to activities, accessibility, infrastructures, and transport services (seeFigure 3).Figure 3.Representation of the zoning of the Metropolitan City of Rome and population density.Calibration was carried out using the FCD estimations of trips, stay points, and inter-zonal interactions. As described in the previous section, stay points were extracted by means of a DBSCAN clustering technique [18], which allowed us to classify origins and destinations into the categories ‚Äúhome‚Äù (H), ‚Äúwork‚Äù (W), and ‚Äúother‚Äù (O). The resulting travel chains enable the extension of the FCD sample to a scale consistent with the circulating vehicle fleet, thereby allowing for the generation of chains tailored to a specific scenario, as illustrated inFigure 4, where an increase in the density of O-D trips can be observed when comparing the FCD sample (about 21k vehicles) with the synthetic one (1.5 M vehicles).Figure 4.Origin‚Äìdestination trips generated from FCD (a) and synthetic (b) samples during the weekday morning peak hour (3 October 2022, 07:00‚Äì08:00) across the province of Rome. The synthetic dataset corresponds to a simulated fleet of 1.5 million vehicles (OpenStreetMap contributors,https://www.openstreetmap.org, accessed on 27 November 2025).By shifting the focus from the classical estimation of aggregated flows between zones (O-D matrices) to a complementary microscale perspective, we can reconstruct the complete sequence of locations visited by each individual over time, thus gaining a deeper understanding of the regularity and variability of private mobility (Figure 5,Table 1). This enables transport policies that are more closely aligned with actual human behavior, allowing for customized analyses, for example, in the context of mobility electrification scenarios.Figure 5.Example of a home-based trip chain generated through the EPR approach. The sequence of stay points is Home‚ÄìWork‚ÄìOther‚ÄìHome (H‚ÄìW‚ÄìO‚ÄìH), as indicated by the corresponding circles connected with dashed red lines (OpenStreetMap contributors,https://www.openstreetmap.org, accessed on 27 November 2025).Table 1.Spatiotemporal information associated with the home-based trip chain shown inFigure 5. 2.3. Energy Consumption Estimation2.3.1. Electric Cars Energy Consumption for TractionThe evaluation procedures consider the speed-dependent, hot specific energy consumption factor in Equation (1) described in the EMEP/EEA Air Pollution Emission Inventory Guidebook [26]:ùë¨ùë™=ùíÇùíó2+ùíÉùíó+ùíÑ+ùíÖùíó/ùíÜùíó2+ùíáùíó+ùíà¬∑(1‚àíùíâ)EC=av2+bv+c+dvev2+fv+g¬∑1‚àíh(1)whereECis the specific energy consumption (MJ/km);vis the average speed.The battery electric car curve was obtained by simulating more than 100 driving cycles and was validated using data derived from measurements on a Real Driving Emissions compliant route, with an absolute error of 5.47 Wh/km (deviation of 2.6%) [27].The curve was differentiated for four car segments distinguished according to motor power (Table 2), using several databases of car models on the market and car registration figures across Europe [28].Table 2.Classification of battery electric passenger cars.The equation parameters for battery electric cars were added in the 2024 updated spreadsheet [29] and are listed in the following table,Table 3:Table 3.Parameters of Equation (1) for battery electric passenger cars‚Äô energy consumption calculation.By comparing the average vehicle consumption curve with the one obtained by ENEA from measurements under real driving conditions, we notice that the difference is below 10% for speeds under 40 km/h, while our measured consumption values are up to 27% lower at higher speeds [30].2.3.2. Consumptions Related to Heating, Ventilation, and Air ConditioningHeating, Ventilation, and Air Conditioning (HVAC) consumption can be computed using different approaches. Several studies are devoted to the analysis of specific Heating or Air Conditioning systems, with detailed characterization of the consumption considering the complexity of the vehicle under study [31,32,33,34,35]. Here, we propose a more general, even if less detailed, approach derived from the model proposed by [36,37] for local public transport buses, schematically described inFigure 6.Figure 6.Scheme of the variables and processes required to compute Heating and Air Conditioning consumption. Variables in black are input variables, variables in red and blue are intermediate variables derived from climatological variables and FCD, respectively, while violet and yellow variables and connections refer to the computation of Air Conditioning and Heating consumption, respectively.As described above, the information contained in the FCD allows us to reconstruct the exact route of each considered vehicle, particularly the duration of a trip (or a trip leg) and the date and time of departure and arrival. It is therefore possible to compute HVAC consumption, if consumption curves representing the required power for the function of the environmental temperature and humidity are available.The computation process is slightly different for Air Conditioning and Heating. The most important difference is related to the fact that the Heating system depends only on environmental temperature, while for the computation of Air Conditioning consumptions, humidity must also be considered. Following [36], to avoid a power curve depending on two independent variables difficult to fit with experimental data, we use the Heat Index [38] to include, in only one variable, the influence of both variables. This is not required for the Heating system, whose power curve is well-described in the function of the sole temperature.The Air Conditioning system is switched off for a Heat Index lower than 20 that corresponds to temperatures close to 20 ¬∞C. The Heating system is, in turn, switched off for temperatures higher than 15 ¬∞C.The Air Conditioning system is linear piecewise and defined asùëÉ(ùêªùêº)={0,ùêªùêº<20ùëé‚ãÖùêªùêº+ùëè,ùêªùêº‚â•20PHI=0,HI<20a‚ãÖHI+b,HI‚â•20(2)wherePis power,HIis the Heat Index, andaandbare coefficients that define the consumption curve and mainly depend on the type of vehicle. This formula can be applied by considering environmental 2 m temperature and Relative Humidity at the time and date of each trip. Energy is then computed by multiplying (integrating) the obtained power by the duration of the trip.A similar approach is applied for Heating, with only temperatureTas the independent variable and a parabolic curve for power:ùëÉ(ùëá)={0,ùëá>15ùëé‚ãÖùëá2+ùëè¬∑ùëá+ùëê,ùëá‚â§15PT=0,T>15a‚ãÖT2+b¬∑T+c,T‚â§15(3)Meteorological variables are, in general, available from local weather services and do not represent an obstacle to apply this methodology.Power curves in function of environmental temperature and humidity are the major challenge of the proposed methodology. These curves are empirically derived so that observational campaigns in real operational contexts need to be performed on different kinds of vehicles, with high costs of time, personnel, and instruments. Few works in the literature provide curves for buses, such as, for instance [36,39]. These curves can be adapted to other vehicles by applying a scale factor.As an example, inFigure 7we show the power curves for a small city car and a large SUV derived from the curves available for buses used in [37] by applying a scaling factor related to the ratio between the maximum power deliverable by a generic Air Conditioning system for buses (typically of the order of 40 kW) and cars (roughly 4 kW for a small car; 8 kW for a large SUV).Figure 7.Left: Power required by the Heating system for a small car and a large SUV in function of environmental temperature.Right: Power required by the Air Conditioning system in function of Heat Index for the same vehicles on the left panel. The curves are derived from the bus curves used in [37] by applying correction coefficients, as described in the text. 2.4. Setting up the Individual Charging Behavior Model in Urban AreasElectric cars can be charged in various locations and with different power, times, and fares.For the purposes of modeling charging choices, it is useful to adopt a classification based on charging location:At home;At intermediate destinations;At dedicated service stations.For each of these location classes, different charging supplies can be available, both in terms of infrastructure ownership (private for exclusive individual use, private for use by a limited community, or public access) and charging characteristics, in terms of power, tariff, occupancy rules, and walking distance to the destination.These supply characteristics influence the choice of where to charge, along with the characteristics of the associated stopover, particularly the duration, the battery level at the start of the stopover, and the energy requirements for the subsequent charging point.Efficient design of charging infrastructure requires identifying the factors that mostly influence user behavior [40]. Many authors investigated such a topic, pointing out the role of users‚Äô characteristics [41], travel frequency and mileage [42,43], speed [44,45], and stop duration [46].The behavioral paradigm adopted for our model is that an electric car user chooses whether to charge for each journey that begins and ends at home, where we suppose that the user always has the option of charging, either via private devices (individual or shared) or via public charging stations located nearby. Furthermore, in urban areas, charging a private car at dedicated service areas is excluded, given a wide availability of charging points near the destinations of the journeys, especially in high population density zones.Figure 8shows the adopted conceptual behavioral scheme.Figure 8.Conceptual scheme of individual charging behavior built on home-based roundtrips and a bi-level decisional approach.As illustrated in the previous figure, it is assumed that, for every trip originating from home, the decision-making framework governing vehicle charging operations is structured as a two-tier hierarchical process, encompassing two sequential choices that determine the overall charging strategy, i.e.,:Whether or not to recharge during the home-based chain;If a positive choice is made at the first level, where to recharge among the available options along the chain.The decisional process involves a finite number of alternatives for each level of choice and can therefore be modeled using discrete choice models.The first level of choice can initially be correlated with the following two variables (see the top-right section ofFigure 8):Expected SOC at the end of the travel chain c, if no recharging is performed.Required range in the following chain c + 1, linked to the expected travel distance.Note that, once the battery capacity (usually expressed in kWh) has been set, the residual range of an electric vehicle can be calculated from the product of the capacity value and the battery‚Äôs current SOC over the average consumption of the considered vehicle. Therefore, the probability of charging along a chainccan be expressed as a function of the SOC expected at the end of the chaincand the SOC required for completing the following chain c + 1 or through the correspondent expected/required ranges.Once an electric vehicle user has chosen to charge during the chain, he must also decide where and when to charge, choosing among the various options available in the chain (see bottom-left section ofFigure 8). The following significant attributes for each stop planned during the chain are identified:Battery SOC at the start of the stopover, i.e., remaining range based on battery capacity;Duration of the stopover;Cost of charging (including any additional costs for occupying the charging station beyond the time required for charging, subject to the tolerance margin established by the Operator);Charging comfort (which includes the distance of the charging station from the trip destination and the need to move the vehicle to avoid extra costs);Battery SOC at the end of the recharge and upon returning home, after completing the chain of trips planned for the day.Based on the assumption of a two-level choice, we can write the probability that a vehicle will charge during the home-based travel chain c at a certain stop s as a composite probability:ùëÉùëê,ùë†=ùõøùë†‚àóùëÉùëê(ùëàùëê)‚àóùëÉùë†(ùëàùë†)Pc,s=Œ¥s‚àóPc(Uc)‚àóPs(Us)(4)wherePc,sis the probability of charging in chain c and at location s;Œ¥sis a Boolean value indicating whether it is possible to charge an electric vehicle at the stopover (or close to it);ùëÉùëêPcis the probability that the user will charge in chain c, as a function of the utility valueùëàùëêUcof c;ùëÉùë†Psis the probability that the user will recharge during stopover s, as a function of the utility valueùëàùë†Usof stopover s.The utility functions have been set as a linear combination of the choice variables specified above, i.e., as forUc:Vehicle range required for the next-trip chain;SOC levels (or equivalent vehicle ranges) when coming back home, if no recharge occurs.As forUp, the independent variables areSOC level arriving at the potential recharge point;SOC level at home coming back if the vehicle has been recharged;Stopover duration;Charging point power and tariff.Other variables can also be, in principle, included in the second-level utility function to consider possible additional costs if the vehicle is not removed from the charging point as soon as the recharge is concluded (unauthorized occupancy penalty), as well as comfort factors, such as walking distance from the charging point to the trip destination.Both utility functions‚Äô forms and coefficients have been defined and calibrated based on the results of the survey described in the following section, enabling authors to also include other kinds of variables other than strictly utility-based, such as socio-economic, behavioral (mobility), and possible psycho-attitudinal aspects. 2.5. Survey for Individual Charging Choice Model Calibration and Main Calibration ResultsTo verify the actual role of first- and second-level choice variables, an SP survey was conducted to record declared charging behaviors in relation to different scenarios.The SP survey technique involves presenting respondents with various travel situations, battery conditions, and characteristics of available charging points (‚Äúscenarios‚Äù), offering them a choice from a limited set of alternatives [47].To ensure a sample sufficient to represent the future population of BEV owners/users and not influenced by current charging behaviors, which could be affected by a context not yet fully adapted to the widespread use of battery-powered electric vehicles (early adoption), the survey was also extended to a class of users with no charging experience.In the paradigm adopted for charging behavior (seeSection 2.4), the first-level choice intrinsically involves selecting between only two alternatives: immediate or postponed charging, based on the ratio between the range available upon returning home in the event of a lack of charging and the range required for travel until the next homecoming. Thanks to this intrinsic simplicity, it was not necessary to introduce simplifications in the survey questionnaire.The second-level choice, on the contrary, can theoretically involve more than two alternatives, depending on the number of intermediate stops planned during the home-to-home travel cycle. For better questionnaire readability, the second-level choice was proposed for a simple travel chain, i.e., with only one intermediate stop along the home-to-home route. In practice, respondents were asked to choose between the options ‚Äúcharging at the intermediate destination‚Äù and ‚Äúcharging upon returning home,‚Äù varying the combination of values of the choice variables hypothesized for each alternative. When applying the model to residents‚Äô mobility patterns, the binary choice was reduced where necessary to multiple alternatives under specific behavioral assumptions (seeSection 2.6).Based on the set of the most significant variables in the charging choice process and their respective ranges of variation (see the following table), the University of Salerno (UNISA), by combining first- and second-level choice situations, identified 138 significant investigation scenarios to calibrate the choice model.Table 4shows the ranges of the scenarios‚Äô variables, the combination of which resulted in 138 significant scenarios.Table 4.Choice variables and related ranges.Given the complexity of the situations to be presented to the respondents, it was decided to limit the number of scenarios to two for each questionnaire, resulting in 69 questionnaire formats. For each of these, it was necessary to ensure a statistically significant sample of interviews.Participation in the survey did not require prior experience in driving or charging battery electric vehicles but only the possession of a valid driver‚Äôs license. For this reason, access to the questionnaire was preceded by an animated tutorial designed to provide essential knowledge, ensuring informed responses.The survey, which involved multiple stakeholders in the dissemination of the questionnaire, recorded nearly 10,000 accesses. However, only 6800 participants completed the form up to the socio-economic profile section, and fewer than 6000 responded to the optional psycho-attitudinal questions located at the end of the questionnaire. Despite this relatively high dropout rate (30%), the target of at least 100 valid responses for each of the 69 questionnaires administered was successfully achieved.The valid sample represents 0.021% of the Italian population holding a driver‚Äôs license‚Äîa percentage that is not fully representative of the reference population but, nonetheless, is highly significant in absolute terms when compared to similar research efforts conducted globally.As shown inFigure 9, Northern and Central Italy are the most represented geographical areas, with only 20% of respondents coming from Southern Italy. In terms of gender, most participants are male (57%), while the youngest (18‚Äì24) and oldest (>65) age groups are the least represented. Only a small number of respondents reported familiarity with electric vehicles. These imbalances partially reflect the actual demographics of licensed drivers in Italy.Figure 9.Overall demographic distribution (by geographic region, gender, and age) of the SP survey sample (top) in comparison with driving licenses in Italy (down).Before calibrating the model, post-stratification and weighting were applied by comparing the sample‚Äôs demographic distribution with census data. Both decision levels‚Äîwhether to charge and where/how to charge‚Äîwere modeled using a Multinomial Logit Model (MNL), calibrated through weighted maximum likelihood estimation. Some details can be found in [48,49], while a deeper insight is being submitted by research partners of UNISA for a further scientific paper [50].In the very essence, the utility functions resulting from calibration process for the first-choice level alternatives are based on eight attributes, four of which relate to individual characteristics.For the second-level choice, the model considers thirteen significant attributes, four of which are related to individual characteristics.The decision to charge an electric vehicle during a travel chain is mainly influenced by the vehicle‚Äôs SOC, remaining range, and the distance to be traveled the next day. The choice of charging location is primarily affected by charging costs and total expenses, followed by proximity to the destination, battery level to be recharged, and stop duration. Socio-demographic factors like gender, age, driving frequency, and daily distance also play a role. 2.6. Behavioral Model Application to the FCD FormatThe model is intended to be applied to the residents‚Äô mobility patterns by private car, reconstructed as specified in previousSection 2.2andSection 2.3.Based on the behavioral model structure, travel stopovers are classified into ‚ÄúHome‚Äù and ‚ÄúOthers‚Äù, including stops at work into the ‚ÄúOthers‚Äù class.To apply the model, it is necessary to associate the values of the variables that define the individual profiles for the following aspects:Characteristics of the electric vehicle used;Gender and age group;Mobility intensity in a typical week.In the absence of verified information on the above topics in the FCD format, some criteria to associate model attributes to individuals have been assumed.Each individual is assigned a battery capacity (kWh) associated with the maximum estimated consumption for the most energy-intensive travel chain detected over a sufficient monitoring/simulation period. Each battery size is associated with a maximum charging power (kW) and a maximum expected range (km) when the battery is fully charged (SOC = 100%).Gender and age group are assigned using random processes based on the statistical distributions of the driving license population in the study area.The mobility variables are directly derived from the travel patterns associated with each individual.At the same time, it is necessary to define a charging infrastructure penetration scenario that provides the following information for each stop:The nominal charging power (kW) of the charging points available at each stop; in the charging process simulation, the actual power used is the lower of that of the charger and that absorbed by the battery;The charging fare, which will be associated with the power supplied and the type of stopover location (home, other);Walking distance from the charging point to the destination; here too, a distinction is made between charging at home and charging at other destinations.Once the individual profile and charging characteristics for each stopover have been defined, the charging decision is based on the values of the variables deduced directly or indirectly from the FCD format, namely: stopover duration, SOC (at the start of the stopover and at the end of the day if not charging), and the range required for the immediately subsequent trips.A common problem faced when modeling electric vehicle routines is SOC initialization. Here, we propose adopting the more flexible approach proposed, for example, in [51,52], in which multiple consecutive days are simulated until periodic SOC behavior is obtained. On the first day of the simulation, all EVs depart with fully charged batteries. The same routes are performed for three consecutive days or until the initial SOC becomes insensitive to the one chosen for the first day. The SOC thus obtained is the initial one for the simulation of the period under consideration.The sequential procedure, starting from the first time leaving home to the last time to come back in a certain analysis period (e.g., a typical workweek), calculates the following:1. For each travel chain carried out in the period, the probability that a charge occurs;2. For each detected/simulated stop at the destinations of the travel chain (including coming back home), the probability that the provided charge occurs at that location and at that clock time.The first-level utility functions (‚ÄúRecharge‚Äù or ‚ÄúNo-recharge‚Äù) are computed based on the individual‚Äôs vehicular, mobility, and socio-economic attributes, along with the residual SOC and remaining driving range at the end of the chain without recharging. These values are then compared to the required range for the subsequent chain. By combining the two utility values, corresponding choice probabilities were derived.A Monte Carlo (MC) simulation is then applied to assign to each chain one of the two first-level alternatives: recharge or no recharge. In this simulation, the recharge option is excluded for chains with a recharge probability less than or equal to 30%. This assumption effectively limits simulation of recharge occurrences when the SOC and range upon returning home are likely sufficient to meet future mobility needs.It should be noted that several travel chains consist of a single trip. This outcome is attributable to the methodological choice of including only those trips whose final stop duration is equal to or greater than 30 min. Consequently, in real-world scenarios, these ‚Äòchains‚Äô may encompass intermediate destinations whose stop durations are too brief to be considered viable for charging opportunities, given the standard urban charging power levels. Nonetheless, such specific travel chains may still require charging upon returning home due to a low initial SOC, a substantial total distance traveled, or a combination of both factors. For these chains, the probability of charging is significantly lower than for multi-trip chains, yet it is not negligible. In these cases, whenever the Monte Carlo simulation identified a charging event, the operation was naturally assigned to the home location.In all other cases, when the first MC simulation indicates a ‚ÄúRecharge‚Äù choice, a second-level simulation is performed to determine the preferred location: home or other destinations. Utility and probability values are calculated using the attributes defined by the UNISA model, including charging tariff, total cost, and walking distance for both options, stop duration and initial SOC at other destinations, final SOC at home after destination charging, difference between final SOC at home for the two options, and the individual‚Äôs socio-economic and mobility profile.For complex travel chains, i.e., those involving more than one intermediate destination, the binary choice model is applied by selecting the destination with the highest utility function value with respect to the home alternative.Following Monte Carlo simulations, charging events are determined and, for each of them, the power used, the starting time, and the duration until full charge or scheduled departure. This data is used to generate probabilistic charging profiles for each urban zone over the simulation period, with statistical reliability increasing with the number of events per zone, as specified in the subsequent flow chart (Figure 10).Figure 10.Procedure to estimate total delivered charging power by traffic zone and fixed time intervals as implemented in the algorithm to compute recharge data aggregation.The profiles are implemented at time intervals of ten minutes to follow the users‚Äô charging behaviors. For this purpose, the number of ten-minute intervals occurring for each recharge process is computed. While the energy recharged has been equally spread over the whole duration of the recharge, the delivered power is kept constant within each time interval. On the other hand, hourly power profiles are computed considering the spatial-temporal aggregations of charging events. Practically, all energy contributions are aggregated (summed) within each time interval and within each traffic zone.",
            "2.1. Overall Procedural Scheme": "The proposed procedure is based on the use of large samples of FCD, enabling the reconstruction of individual mobility patterns; these are intended as the most likely travel sequences over an analysis period sufficiently representative of a given seasonality for the entire population of residents within an urban area. A behavioral model is applied to these patterns, indicating, for each planned stop, the probability that, in the case of using a battery-powered electric vehicle (BEV), charging will be performed. This is carried out taking into account the characteristics of the stop‚Äîmainly stop duration‚Äîavailability, and characteristics of charging infrastructure, the battery‚Äôs state of charge, the expected mobility needs after the stop, and the driver‚Äôs profile. In a context where both battery power penetration and charging infrastructure are fully consolidated and known, the proposed procedure allows for the most likely demand to be assessed in different urban areas over the course of a typical day. Conversely, in contexts where the e-mobility is still evolving, both on the supply and demand sides, as is currently the case in Italy and other European countries, the analysis tool works like a classic ‚Äúwhat-if‚Äù Decision Support System (DSS). In fact, it allows for the simulation of the effects of policies favoring a greater penetration of battery power in individual urban transport, ultimately also enabling a cost‚Äìbenefit assessment of the different policy options. Figure 2refers to this latter type of application: FCD processing provides information on individual mobility patterns, which are then also qualified in terms of energy consumption, once the battery electric vehicle‚Äôs rate into the circulating fleet has been assumed. On the other hand, evidence on individual charging behavior provided by surveys and literature feeds a random utility model. Such a model is then applied to the mobility patterns of private vehicles assumed to be electric under different hypotheses on recharge infrastructure availability and characteristics, yielding a probabilistic estimate of charging events in different urban areas. Figure 2.Estimation of private electric cars‚Äô urban charging profiles (CPs) from FCD. z is a generic urban zone, tiis a generic day time interval, Vi is a generic electric car, and Pi is the related probability of recharging in (z; ti). For each time tiand urban zone z, appropriately delimited based on criteria of territorial homogeneity, the private car charging behavioral model provides a probability that each vehicle v parked in the area is charging. Given the power (kW) available to charge electric cars in that specific area, it is possible to calculate, by summation, the power absorbed for charging in the area at the generic instant t, deriving the daily zonal charging profiles.",
            "2.2. Floating Car Exploration and Exploitation": "FCD are geolocated time series generated by vehicles equipped with OBUs or integrated GPS systems capable of recording, at regular time or space intervals, position, speed, heading, engine status (on/off), and other operational attributes. FCD can be regarded as mobile sensors that opportunistically sample the road network, offering broad spatiotemporal coverage and, when properly pre-processed, valuable insights into individual travel behavior. The FCD processing pipeline consists of a comprehensive framework designed to convert raw GPS data streams into coherent, validated, and analytically reusable mobility products. Each FCD record includes a unique trace identifier, a pseudonymized vehicle ID that remains constant throughout the observation period, a timestamp with one-second precision, geographical coordinates in WGS84 format, instantaneous speed, heading, engine status, GPS signal quality, and an odometer-based progressive distance value. Sampling frequency may vary depending on the data provider, being either time-based, such as every thirty seconds, or space-based, for example, every one or two kilometers. In the considered case study, most samples were recorded at thirty-second intervals, although more advanced on-board units can generate additional observations when abrupt changes in curvature or speed occur. The pipeline‚Äôs main objective is to transform these heterogeneous data streams into structured and consistent analytical units suitable for quantitative transport analysis. This process begins with the verification of spatial and temporal quality through data normalization and validation, which ensures that all samples are spatially consistent and temporally ordered. Once samples are validated, the pipeline reconstructs vehicle trajectories and generates the two main analytical entities of the system: trips and TripLegs. A trip corresponds to a continuous movement session of a vehicle, typically delimited by engine-on and engine-off conditions, or, when such information is unavailable, inferred from thresholds on speed and stop duration. Each trip is described by a sequence of ordered GPS records that include departure and arrival times, traveled distance, total duration, and average speed. From these sequences, the pipeline derives sub-segments known as TripLegs, which represent all possible pairs of chronologically sequential GPS points belonging to different traffic zones. This structure allows for a detailed assessment of travel performance, including average travel times, speeds, and distances between origin‚Äìdestination pairs, and enables the construction of OD matrices and travel time skims directly from empirical data. A robust outlier detection procedure based on the boxplot method is applied to filter anomalous TripLegs that could distort the analysis, while incomplete or noisy trajectories, such as single-sample trips or those affected by signal loss, are removed to maintain the integrity of the analytical base. The aggregation of trips and TripLegs by origin and destination zones, time periods, and day types results in the generation of observed OD matrices and performance indicators that describe spatial and temporal patterns of mobility. Besides trajectory reconstruction, a spatial analysis module identifies stationary clusters of points, known as stay points, which represent typical stopping locations. The identification is based on the DBSCAN density-based clustering algorithm [18] applied to stationary samples within a predefined radius, with parameters tuned to the density of the urban network. For each cluster, average stop duration, variance, and recurrence are computed to characterize both short and long stops across different temporal windows. Two categories of stay points are recognized: internal stay points, located within the study area, and virtual stay points, representing points of entry or exit along the network boundaries. Virtual stay points are detected when clusters extend spatially along access corridors, and the initial odometer reading indicates prior to travel before the first recorded point. Each stay point is associated with a traffic zone and linked to trips as an origin or destination, providing spatial anchors for reconstructing travel chains. The classification of stay points into functional types relies on duration and recurrence criteria. The stay point with the longest cumulative stop duration, accounting for at least ten percent of the total recorded time, is identified as the home location. Recurrent stops of a long duration with weekly periodicity are classified as work, while the remaining points are labeled as other. Validation of this classification using census and employment data produced determination coefficients of 0.91 for home and 0.71 for work, confirming the reliability of the method in reproducing real activity locations. From the identified trips and classified stay points, the pipeline reconstructs home-based travel chains by chronologically ordering trips for each vehicle. Each chain starts at a trip departing from the home location and aggregates subsequent trips until the vehicle returns home. The resulting sequence of trips forms a home-based chain stored as an analytical unit. During storage, each chain is enriched with summary attributes such as total travel time, number of intermediate stops, cumulative duration, and information about preceding and following chains. The classification of these chains is based on the ordered sequence of stay point types, generating categories such as HOME‚ÄìHOME, HOME‚ÄìWORK‚ÄìHOME, HOME‚ÄìOTHER‚ÄìHOME, or mixed combinations that describe different mobility behaviors. This reconstruction enables the analysis of systematic and non-systematic travel patterns at the individual level, facilitating the study of commuting behavior and the identification of daily activity structures. Over the past two decades, generative models of human mobility have attracted increasing attention within the scientific community. The main reasons lie in their ability to overcome issues related to the use of personal or sensitive data (such as those derived from vehicle GPSs or mobile phone records), while also ensuring scalability when available datasets are incomplete or not representative. Moreover, they allow researchers to extract global behavioral patterns and universal laws that can be adapted to different geographical contexts (once properly calibrated), thus enabling what-if simulations and scenario analyses to support transportation planning and mobility policy design. Among human mobility models, the EPR‚ÄîExploration and Preferential Return‚Äîmodel and its extensions stand out as one of the most widely adopted approaches. This success is largely due to their capacity to reproduce emerging behavior and scaling laws (e.g., travel distances and gyration radius [19,20], waiting time location frequency [21], entropy [19,20,22], and individual mobility networks [22,23]) and to reconstruct complex individual mobility trajectories through a relatively simple behavioral formulation. Specifically, the baseline EPR model assumes that spatio-temporal mobility patterns emerge from the alternation between two fundamental behaviors: the tendency to return to previously visited locations and the exploration of new areas [21]. In this work, to represent the temporal dynamics of individual mobility, we adopted a Markovian approach [24] able to reproduce personal agendas by determining both the arrival time at each visited location and the duration of the corresponding stop. In this way, the model captures the propensity for individuals to follow or deviate from their typical daily routines. On the other hand, the spatial dimension was considered through a gravity-inspired formulation, which exploits aggregate information (e.g., the distribution of the population) and accounts for exploratory movements [25]. Rather than applying a uniform discretization of the study area, as commonly adopted in similar modeling approaches, we relied on the traffic zones defined by Roma Servizi per la Mobilit√† (https://romamobilita.it/it, accessed on 27 November 2025). The chosen zoning provides spatial elements that are homogeneous with respect to activities, accessibility, infrastructures, and transport services (seeFigure 3). Figure 3.Representation of the zoning of the Metropolitan City of Rome and population density. Calibration was carried out using the FCD estimations of trips, stay points, and inter-zonal interactions. As described in the previous section, stay points were extracted by means of a DBSCAN clustering technique [18], which allowed us to classify origins and destinations into the categories ‚Äúhome‚Äù (H), ‚Äúwork‚Äù (W), and ‚Äúother‚Äù (O). The resulting travel chains enable the extension of the FCD sample to a scale consistent with the circulating vehicle fleet, thereby allowing for the generation of chains tailored to a specific scenario, as illustrated inFigure 4, where an increase in the density of O-D trips can be observed when comparing the FCD sample (about 21k vehicles) with the synthetic one (1.5 M vehicles). Figure 4.Origin‚Äìdestination trips generated from FCD (a) and synthetic (b) samples during the weekday morning peak hour (3 October 2022, 07:00‚Äì08:00) across the province of Rome. The synthetic dataset corresponds to a simulated fleet of 1.5 million vehicles (OpenStreetMap contributors,https://www.openstreetmap.org, accessed on 27 November 2025). By shifting the focus from the classical estimation of aggregated flows between zones (O-D matrices) to a complementary microscale perspective, we can reconstruct the complete sequence of locations visited by each individual over time, thus gaining a deeper understanding of the regularity and variability of private mobility (Figure 5,Table 1). This enables transport policies that are more closely aligned with actual human behavior, allowing for customized analyses, for example, in the context of mobility electrification scenarios. Figure 5.Example of a home-based trip chain generated through the EPR approach. The sequence of stay points is Home‚ÄìWork‚ÄìOther‚ÄìHome (H‚ÄìW‚ÄìO‚ÄìH), as indicated by the corresponding circles connected with dashed red lines (OpenStreetMap contributors,https://www.openstreetmap.org, accessed on 27 November 2025). Table 1.Spatiotemporal information associated with the home-based trip chain shown inFigure 5.",
            "2.3. Energy Consumption Estimation": "2.3.1. Electric Cars Energy Consumption for TractionThe evaluation procedures consider the speed-dependent, hot specific energy consumption factor in Equation (1) described in the EMEP/EEA Air Pollution Emission Inventory Guidebook [26]:ùë¨ùë™=ùíÇùíó2+ùíÉùíó+ùíÑ+ùíÖùíó/ùíÜùíó2+ùíáùíó+ùíà¬∑(1‚àíùíâ)EC=av2+bv+c+dvev2+fv+g¬∑1‚àíh(1)whereECis the specific energy consumption (MJ/km);vis the average speed.The battery electric car curve was obtained by simulating more than 100 driving cycles and was validated using data derived from measurements on a Real Driving Emissions compliant route, with an absolute error of 5.47 Wh/km (deviation of 2.6%) [27].The curve was differentiated for four car segments distinguished according to motor power (Table 2), using several databases of car models on the market and car registration figures across Europe [28].Table 2.Classification of battery electric passenger cars.The equation parameters for battery electric cars were added in the 2024 updated spreadsheet [29] and are listed in the following table,Table 3:Table 3.Parameters of Equation (1) for battery electric passenger cars‚Äô energy consumption calculation.By comparing the average vehicle consumption curve with the one obtained by ENEA from measurements under real driving conditions, we notice that the difference is below 10% for speeds under 40 km/h, while our measured consumption values are up to 27% lower at higher speeds [30]. 2.3.2. Consumptions Related to Heating, Ventilation, and Air ConditioningHeating, Ventilation, and Air Conditioning (HVAC) consumption can be computed using different approaches. Several studies are devoted to the analysis of specific Heating or Air Conditioning systems, with detailed characterization of the consumption considering the complexity of the vehicle under study [31,32,33,34,35]. Here, we propose a more general, even if less detailed, approach derived from the model proposed by [36,37] for local public transport buses, schematically described inFigure 6.Figure 6.Scheme of the variables and processes required to compute Heating and Air Conditioning consumption. Variables in black are input variables, variables in red and blue are intermediate variables derived from climatological variables and FCD, respectively, while violet and yellow variables and connections refer to the computation of Air Conditioning and Heating consumption, respectively.As described above, the information contained in the FCD allows us to reconstruct the exact route of each considered vehicle, particularly the duration of a trip (or a trip leg) and the date and time of departure and arrival. It is therefore possible to compute HVAC consumption, if consumption curves representing the required power for the function of the environmental temperature and humidity are available.The computation process is slightly different for Air Conditioning and Heating. The most important difference is related to the fact that the Heating system depends only on environmental temperature, while for the computation of Air Conditioning consumptions, humidity must also be considered. Following [36], to avoid a power curve depending on two independent variables difficult to fit with experimental data, we use the Heat Index [38] to include, in only one variable, the influence of both variables. This is not required for the Heating system, whose power curve is well-described in the function of the sole temperature.The Air Conditioning system is switched off for a Heat Index lower than 20 that corresponds to temperatures close to 20 ¬∞C. The Heating system is, in turn, switched off for temperatures higher than 15 ¬∞C.The Air Conditioning system is linear piecewise and defined asùëÉ(ùêªùêº)={0,ùêªùêº<20ùëé‚ãÖùêªùêº+ùëè,ùêªùêº‚â•20PHI=0,HI<20a‚ãÖHI+b,HI‚â•20(2)wherePis power,HIis the Heat Index, andaandbare coefficients that define the consumption curve and mainly depend on the type of vehicle. This formula can be applied by considering environmental 2 m temperature and Relative Humidity at the time and date of each trip. Energy is then computed by multiplying (integrating) the obtained power by the duration of the trip.A similar approach is applied for Heating, with only temperatureTas the independent variable and a parabolic curve for power:ùëÉ(ùëá)={0,ùëá>15ùëé‚ãÖùëá2+ùëè¬∑ùëá+ùëê,ùëá‚â§15PT=0,T>15a‚ãÖT2+b¬∑T+c,T‚â§15(3)Meteorological variables are, in general, available from local weather services and do not represent an obstacle to apply this methodology.Power curves in function of environmental temperature and humidity are the major challenge of the proposed methodology. These curves are empirically derived so that observational campaigns in real operational contexts need to be performed on different kinds of vehicles, with high costs of time, personnel, and instruments. Few works in the literature provide curves for buses, such as, for instance [36,39]. These curves can be adapted to other vehicles by applying a scale factor.As an example, inFigure 7we show the power curves for a small city car and a large SUV derived from the curves available for buses used in [37] by applying a scaling factor related to the ratio between the maximum power deliverable by a generic Air Conditioning system for buses (typically of the order of 40 kW) and cars (roughly 4 kW for a small car; 8 kW for a large SUV).Figure 7.Left: Power required by the Heating system for a small car and a large SUV in function of environmental temperature.Right: Power required by the Air Conditioning system in function of Heat Index for the same vehicles on the left panel. The curves are derived from the bus curves used in [37] by applying correction coefficients, as described in the text.",
            "2.3.1. Electric Cars Energy Consumption for Traction": "The evaluation procedures consider the speed-dependent, hot specific energy consumption factor in Equation (1) described in the EMEP/EEA Air Pollution Emission Inventory Guidebook [26]:ùë¨ùë™=ùíÇùíó2+ùíÉùíó+ùíÑ+ùíÖùíó/ùíÜùíó2+ùíáùíó+ùíà¬∑(1‚àíùíâ)EC=av2+bv+c+dvev2+fv+g¬∑1‚àíh(1)whereECis the specific energy consumption (MJ/km);vis the average speed. The battery electric car curve was obtained by simulating more than 100 driving cycles and was validated using data derived from measurements on a Real Driving Emissions compliant route, with an absolute error of 5.47 Wh/km (deviation of 2.6%) [27]. The curve was differentiated for four car segments distinguished according to motor power (Table 2), using several databases of car models on the market and car registration figures across Europe [28]. Table 2.Classification of battery electric passenger cars. The equation parameters for battery electric cars were added in the 2024 updated spreadsheet [29] and are listed in the following table,Table 3: Table 3.Parameters of Equation (1) for battery electric passenger cars‚Äô energy consumption calculation. By comparing the average vehicle consumption curve with the one obtained by ENEA from measurements under real driving conditions, we notice that the difference is below 10% for speeds under 40 km/h, while our measured consumption values are up to 27% lower at higher speeds [30].",
            "2.3.2. Consumptions Related to Heating, Ventilation, and Air Conditioning": "Heating, Ventilation, and Air Conditioning (HVAC) consumption can be computed using different approaches. Several studies are devoted to the analysis of specific Heating or Air Conditioning systems, with detailed characterization of the consumption considering the complexity of the vehicle under study [31,32,33,34,35]. Here, we propose a more general, even if less detailed, approach derived from the model proposed by [36,37] for local public transport buses, schematically described inFigure 6. Figure 6.Scheme of the variables and processes required to compute Heating and Air Conditioning consumption. Variables in black are input variables, variables in red and blue are intermediate variables derived from climatological variables and FCD, respectively, while violet and yellow variables and connections refer to the computation of Air Conditioning and Heating consumption, respectively. As described above, the information contained in the FCD allows us to reconstruct the exact route of each considered vehicle, particularly the duration of a trip (or a trip leg) and the date and time of departure and arrival. It is therefore possible to compute HVAC consumption, if consumption curves representing the required power for the function of the environmental temperature and humidity are available. The computation process is slightly different for Air Conditioning and Heating. The most important difference is related to the fact that the Heating system depends only on environmental temperature, while for the computation of Air Conditioning consumptions, humidity must also be considered. Following [36], to avoid a power curve depending on two independent variables difficult to fit with experimental data, we use the Heat Index [38] to include, in only one variable, the influence of both variables. This is not required for the Heating system, whose power curve is well-described in the function of the sole temperature. The Air Conditioning system is switched off for a Heat Index lower than 20 that corresponds to temperatures close to 20 ¬∞C. The Heating system is, in turn, switched off for temperatures higher than 15 ¬∞C. The Air Conditioning system is linear piecewise and defined asùëÉ(ùêªùêº)={0,ùêªùêº<20ùëé‚ãÖùêªùêº+ùëè,ùêªùêº‚â•20PHI=0,HI<20a‚ãÖHI+b,HI‚â•20(2)wherePis power,HIis the Heat Index, andaandbare coefficients that define the consumption curve and mainly depend on the type of vehicle. This formula can be applied by considering environmental 2 m temperature and Relative Humidity at the time and date of each trip. Energy is then computed by multiplying (integrating) the obtained power by the duration of the trip. A similar approach is applied for Heating, with only temperatureTas the independent variable and a parabolic curve for power:ùëÉ(ùëá)={0,ùëá>15ùëé‚ãÖùëá2+ùëè¬∑ùëá+ùëê,ùëá‚â§15PT=0,T>15a‚ãÖT2+b¬∑T+c,T‚â§15(3) Meteorological variables are, in general, available from local weather services and do not represent an obstacle to apply this methodology. Power curves in function of environmental temperature and humidity are the major challenge of the proposed methodology. These curves are empirically derived so that observational campaigns in real operational contexts need to be performed on different kinds of vehicles, with high costs of time, personnel, and instruments. Few works in the literature provide curves for buses, such as, for instance [36,39]. These curves can be adapted to other vehicles by applying a scale factor. As an example, inFigure 7we show the power curves for a small city car and a large SUV derived from the curves available for buses used in [37] by applying a scaling factor related to the ratio between the maximum power deliverable by a generic Air Conditioning system for buses (typically of the order of 40 kW) and cars (roughly 4 kW for a small car; 8 kW for a large SUV). Figure 7.Left: Power required by the Heating system for a small car and a large SUV in function of environmental temperature.Right: Power required by the Air Conditioning system in function of Heat Index for the same vehicles on the left panel. The curves are derived from the bus curves used in [37] by applying correction coefficients, as described in the text.",
            "2.4. Setting up the Individual Charging Behavior Model in Urban Areas": "Electric cars can be charged in various locations and with different power, times, and fares. For the purposes of modeling charging choices, it is useful to adopt a classification based on charging location: At home;At intermediate destinations;At dedicated service stations. For each of these location classes, different charging supplies can be available, both in terms of infrastructure ownership (private for exclusive individual use, private for use by a limited community, or public access) and charging characteristics, in terms of power, tariff, occupancy rules, and walking distance to the destination. These supply characteristics influence the choice of where to charge, along with the characteristics of the associated stopover, particularly the duration, the battery level at the start of the stopover, and the energy requirements for the subsequent charging point. Efficient design of charging infrastructure requires identifying the factors that mostly influence user behavior [40]. Many authors investigated such a topic, pointing out the role of users‚Äô characteristics [41], travel frequency and mileage [42,43], speed [44,45], and stop duration [46]. The behavioral paradigm adopted for our model is that an electric car user chooses whether to charge for each journey that begins and ends at home, where we suppose that the user always has the option of charging, either via private devices (individual or shared) or via public charging stations located nearby. Furthermore, in urban areas, charging a private car at dedicated service areas is excluded, given a wide availability of charging points near the destinations of the journeys, especially in high population density zones.Figure 8shows the adopted conceptual behavioral scheme. Figure 8.Conceptual scheme of individual charging behavior built on home-based roundtrips and a bi-level decisional approach. As illustrated in the previous figure, it is assumed that, for every trip originating from home, the decision-making framework governing vehicle charging operations is structured as a two-tier hierarchical process, encompassing two sequential choices that determine the overall charging strategy, i.e.,: Whether or not to recharge during the home-based chain;If a positive choice is made at the first level, where to recharge among the available options along the chain. The decisional process involves a finite number of alternatives for each level of choice and can therefore be modeled using discrete choice models. The first level of choice can initially be correlated with the following two variables (see the top-right section ofFigure 8): Expected SOC at the end of the travel chain c, if no recharging is performed.Required range in the following chain c + 1, linked to the expected travel distance. Note that, once the battery capacity (usually expressed in kWh) has been set, the residual range of an electric vehicle can be calculated from the product of the capacity value and the battery‚Äôs current SOC over the average consumption of the considered vehicle. Therefore, the probability of charging along a chainccan be expressed as a function of the SOC expected at the end of the chaincand the SOC required for completing the following chain c + 1 or through the correspondent expected/required ranges. Once an electric vehicle user has chosen to charge during the chain, he must also decide where and when to charge, choosing among the various options available in the chain (see bottom-left section ofFigure 8). The following significant attributes for each stop planned during the chain are identified: Battery SOC at the start of the stopover, i.e., remaining range based on battery capacity;Duration of the stopover;Cost of charging (including any additional costs for occupying the charging station beyond the time required for charging, subject to the tolerance margin established by the Operator);Charging comfort (which includes the distance of the charging station from the trip destination and the need to move the vehicle to avoid extra costs);Battery SOC at the end of the recharge and upon returning home, after completing the chain of trips planned for the day. Based on the assumption of a two-level choice, we can write the probability that a vehicle will charge during the home-based travel chain c at a certain stop s as a composite probability:ùëÉùëê,ùë†=ùõøùë†‚àóùëÉùëê(ùëàùëê)‚àóùëÉùë†(ùëàùë†)Pc,s=Œ¥s‚àóPc(Uc)‚àóPs(Us)(4)where Pc,sis the probability of charging in chain c and at location s; Œ¥sis a Boolean value indicating whether it is possible to charge an electric vehicle at the stopover (or close to it); ùëÉùëêPcis the probability that the user will charge in chain c, as a function of the utility valueùëàùëêUcof c; ùëÉùë†Psis the probability that the user will recharge during stopover s, as a function of the utility valueùëàùë†Usof stopover s. The utility functions have been set as a linear combination of the choice variables specified above, i.e., as forUc: Vehicle range required for the next-trip chain;SOC levels (or equivalent vehicle ranges) when coming back home, if no recharge occurs. As forUp, the independent variables are SOC level arriving at the potential recharge point;SOC level at home coming back if the vehicle has been recharged;Stopover duration;Charging point power and tariff. Other variables can also be, in principle, included in the second-level utility function to consider possible additional costs if the vehicle is not removed from the charging point as soon as the recharge is concluded (unauthorized occupancy penalty), as well as comfort factors, such as walking distance from the charging point to the trip destination. Both utility functions‚Äô forms and coefficients have been defined and calibrated based on the results of the survey described in the following section, enabling authors to also include other kinds of variables other than strictly utility-based, such as socio-economic, behavioral (mobility), and possible psycho-attitudinal aspects.",
            "2.5. Survey for Individual Charging Choice Model Calibration and Main Calibration Results": "To verify the actual role of first- and second-level choice variables, an SP survey was conducted to record declared charging behaviors in relation to different scenarios. The SP survey technique involves presenting respondents with various travel situations, battery conditions, and characteristics of available charging points (‚Äúscenarios‚Äù), offering them a choice from a limited set of alternatives [47]. To ensure a sample sufficient to represent the future population of BEV owners/users and not influenced by current charging behaviors, which could be affected by a context not yet fully adapted to the widespread use of battery-powered electric vehicles (early adoption), the survey was also extended to a class of users with no charging experience. In the paradigm adopted for charging behavior (seeSection 2.4), the first-level choice intrinsically involves selecting between only two alternatives: immediate or postponed charging, based on the ratio between the range available upon returning home in the event of a lack of charging and the range required for travel until the next homecoming. Thanks to this intrinsic simplicity, it was not necessary to introduce simplifications in the survey questionnaire. The second-level choice, on the contrary, can theoretically involve more than two alternatives, depending on the number of intermediate stops planned during the home-to-home travel cycle. For better questionnaire readability, the second-level choice was proposed for a simple travel chain, i.e., with only one intermediate stop along the home-to-home route. In practice, respondents were asked to choose between the options ‚Äúcharging at the intermediate destination‚Äù and ‚Äúcharging upon returning home,‚Äù varying the combination of values of the choice variables hypothesized for each alternative. When applying the model to residents‚Äô mobility patterns, the binary choice was reduced where necessary to multiple alternatives under specific behavioral assumptions (seeSection 2.6). Based on the set of the most significant variables in the charging choice process and their respective ranges of variation (see the following table), the University of Salerno (UNISA), by combining first- and second-level choice situations, identified 138 significant investigation scenarios to calibrate the choice model. Table 4shows the ranges of the scenarios‚Äô variables, the combination of which resulted in 138 significant scenarios. Table 4.Choice variables and related ranges. Given the complexity of the situations to be presented to the respondents, it was decided to limit the number of scenarios to two for each questionnaire, resulting in 69 questionnaire formats. For each of these, it was necessary to ensure a statistically significant sample of interviews. Participation in the survey did not require prior experience in driving or charging battery electric vehicles but only the possession of a valid driver‚Äôs license. For this reason, access to the questionnaire was preceded by an animated tutorial designed to provide essential knowledge, ensuring informed responses. The survey, which involved multiple stakeholders in the dissemination of the questionnaire, recorded nearly 10,000 accesses. However, only 6800 participants completed the form up to the socio-economic profile section, and fewer than 6000 responded to the optional psycho-attitudinal questions located at the end of the questionnaire. Despite this relatively high dropout rate (30%), the target of at least 100 valid responses for each of the 69 questionnaires administered was successfully achieved. The valid sample represents 0.021% of the Italian population holding a driver‚Äôs license‚Äîa percentage that is not fully representative of the reference population but, nonetheless, is highly significant in absolute terms when compared to similar research efforts conducted globally. As shown inFigure 9, Northern and Central Italy are the most represented geographical areas, with only 20% of respondents coming from Southern Italy. In terms of gender, most participants are male (57%), while the youngest (18‚Äì24) and oldest (>65) age groups are the least represented. Only a small number of respondents reported familiarity with electric vehicles. These imbalances partially reflect the actual demographics of licensed drivers in Italy. Figure 9.Overall demographic distribution (by geographic region, gender, and age) of the SP survey sample (top) in comparison with driving licenses in Italy (down). Before calibrating the model, post-stratification and weighting were applied by comparing the sample‚Äôs demographic distribution with census data. Both decision levels‚Äîwhether to charge and where/how to charge‚Äîwere modeled using a Multinomial Logit Model (MNL), calibrated through weighted maximum likelihood estimation. Some details can be found in [48,49], while a deeper insight is being submitted by research partners of UNISA for a further scientific paper [50]. In the very essence, the utility functions resulting from calibration process for the first-choice level alternatives are based on eight attributes, four of which relate to individual characteristics. For the second-level choice, the model considers thirteen significant attributes, four of which are related to individual characteristics. The decision to charge an electric vehicle during a travel chain is mainly influenced by the vehicle‚Äôs SOC, remaining range, and the distance to be traveled the next day. The choice of charging location is primarily affected by charging costs and total expenses, followed by proximity to the destination, battery level to be recharged, and stop duration. Socio-demographic factors like gender, age, driving frequency, and daily distance also play a role.",
            "2.6. Behavioral Model Application to the FCD Format": "The model is intended to be applied to the residents‚Äô mobility patterns by private car, reconstructed as specified in previousSection 2.2andSection 2.3. Based on the behavioral model structure, travel stopovers are classified into ‚ÄúHome‚Äù and ‚ÄúOthers‚Äù, including stops at work into the ‚ÄúOthers‚Äù class. To apply the model, it is necessary to associate the values of the variables that define the individual profiles for the following aspects: Characteristics of the electric vehicle used;Gender and age group;Mobility intensity in a typical week. In the absence of verified information on the above topics in the FCD format, some criteria to associate model attributes to individuals have been assumed. Each individual is assigned a battery capacity (kWh) associated with the maximum estimated consumption for the most energy-intensive travel chain detected over a sufficient monitoring/simulation period. Each battery size is associated with a maximum charging power (kW) and a maximum expected range (km) when the battery is fully charged (SOC = 100%). Gender and age group are assigned using random processes based on the statistical distributions of the driving license population in the study area. The mobility variables are directly derived from the travel patterns associated with each individual. At the same time, it is necessary to define a charging infrastructure penetration scenario that provides the following information for each stop: The nominal charging power (kW) of the charging points available at each stop; in the charging process simulation, the actual power used is the lower of that of the charger and that absorbed by the battery;The charging fare, which will be associated with the power supplied and the type of stopover location (home, other);Walking distance from the charging point to the destination; here too, a distinction is made between charging at home and charging at other destinations. Once the individual profile and charging characteristics for each stopover have been defined, the charging decision is based on the values of the variables deduced directly or indirectly from the FCD format, namely: stopover duration, SOC (at the start of the stopover and at the end of the day if not charging), and the range required for the immediately subsequent trips. A common problem faced when modeling electric vehicle routines is SOC initialization. Here, we propose adopting the more flexible approach proposed, for example, in [51,52], in which multiple consecutive days are simulated until periodic SOC behavior is obtained. On the first day of the simulation, all EVs depart with fully charged batteries. The same routes are performed for three consecutive days or until the initial SOC becomes insensitive to the one chosen for the first day. The SOC thus obtained is the initial one for the simulation of the period under consideration. The sequential procedure, starting from the first time leaving home to the last time to come back in a certain analysis period (e.g., a typical workweek), calculates the following: 1. For each travel chain carried out in the period, the probability that a charge occurs; 2. For each detected/simulated stop at the destinations of the travel chain (including coming back home), the probability that the provided charge occurs at that location and at that clock time. The first-level utility functions (‚ÄúRecharge‚Äù or ‚ÄúNo-recharge‚Äù) are computed based on the individual‚Äôs vehicular, mobility, and socio-economic attributes, along with the residual SOC and remaining driving range at the end of the chain without recharging. These values are then compared to the required range for the subsequent chain. By combining the two utility values, corresponding choice probabilities were derived. A Monte Carlo (MC) simulation is then applied to assign to each chain one of the two first-level alternatives: recharge or no recharge. In this simulation, the recharge option is excluded for chains with a recharge probability less than or equal to 30%. This assumption effectively limits simulation of recharge occurrences when the SOC and range upon returning home are likely sufficient to meet future mobility needs. It should be noted that several travel chains consist of a single trip. This outcome is attributable to the methodological choice of including only those trips whose final stop duration is equal to or greater than 30 min. Consequently, in real-world scenarios, these ‚Äòchains‚Äô may encompass intermediate destinations whose stop durations are too brief to be considered viable for charging opportunities, given the standard urban charging power levels. Nonetheless, such specific travel chains may still require charging upon returning home due to a low initial SOC, a substantial total distance traveled, or a combination of both factors. For these chains, the probability of charging is significantly lower than for multi-trip chains, yet it is not negligible. In these cases, whenever the Monte Carlo simulation identified a charging event, the operation was naturally assigned to the home location. In all other cases, when the first MC simulation indicates a ‚ÄúRecharge‚Äù choice, a second-level simulation is performed to determine the preferred location: home or other destinations. Utility and probability values are calculated using the attributes defined by the UNISA model, including charging tariff, total cost, and walking distance for both options, stop duration and initial SOC at other destinations, final SOC at home after destination charging, difference between final SOC at home for the two options, and the individual‚Äôs socio-economic and mobility profile. For complex travel chains, i.e., those involving more than one intermediate destination, the binary choice model is applied by selecting the destination with the highest utility function value with respect to the home alternative. Following Monte Carlo simulations, charging events are determined and, for each of them, the power used, the starting time, and the duration until full charge or scheduled departure. This data is used to generate probabilistic charging profiles for each urban zone over the simulation period, with statistical reliability increasing with the number of events per zone, as specified in the subsequent flow chart (Figure 10). Figure 10.Procedure to estimate total delivered charging power by traffic zone and fixed time intervals as implemented in the algorithm to compute recharge data aggregation. The profiles are implemented at time intervals of ten minutes to follow the users‚Äô charging behaviors. For this purpose, the number of ten-minute intervals occurring for each recharge process is computed. While the energy recharged has been equally spread over the whole duration of the recharge, the delivered power is kept constant within each time interval. On the other hand, hourly power profiles are computed considering the spatial-temporal aggregations of charging events. Practically, all energy contributions are aggregated (summed) within each time interval and within each traffic zone.",
            "3. Results": "3.1. Case Study‚ÄîDemand and Offer QualificationFor research purposes, the proposed procedure has been applied using a set of FCD collected by Viasat in the metropolitan area of Rome during the period October‚ÄìNovember 2022. This dataset was provided by Roma Mobilit√† Agency and processed by ENEA and its academic research partners during the three-year research activities to feed the entire modeling suite developed for urban mobility analysis, including the module for estimating the charging profiles under discussion.The complete dataset comprises approximately 20,000 vehicles, but only for a subset‚Äîaround 11,500‚Äîwas it possible to infer a likely residence within the study area. From this subset of ‚Äúresident‚Äù vehicles, those with a mobility not sufficient to define a reliable charging behavior were excluded, as well as those whose mobility patterns suggested usage not attributable to individuals or small households but rather to commercial purposes (e.g., taxis, car sharing, goods distribution, etc.). After filtering out non-eligible terminals, the final sample suitable for applying the charging model consisted of approximately 8700 vehicles.The model was applied to the full set of these 8700 agents under the assumption that they represent fully electric vehicles (BEV). Based on this assumption, the sample corresponds numerically to approximately 40% of the BEV fleet registered in the Province of Rome as of 31 December 2023, according to data from the National Unified Platform (PUN) for charging infrastructure (https://www.piattaformaunicanazionale.it/, accessed on 2 November 2025).The sample includes nearly 460,000 home-based trip chains, with approximately 225,000 intermediate destinations. This discrepancy arises from the fact that many reconstructed chains are loops from home to home without intermediate stops due to the concatenation of trips separated by stops shorter than 30 min‚Äîconsidered the minimum duration for a meaningful charging event. For such loops, only the first-level choice model is applied, while the second-level model is not. Conversely, numerous ‚Äúlong‚Äù chains exist, containing more than one intermediate destination, which will be analyzed in detail in the results section of the charging model application.For each eligible individual in the sample, mobility attributes required by the behavioral model were computed based on the two-month observation period. The resulting statistical frequencies are reported inTable 5, showing the average number of travel days per week and the average daily distance on working days. It is observed that most sampled individuals use their private vehicle more than three days per week, while in terms of daily mileage, most travel less than 60 km per day‚Äîespecially among those who use their vehicle more frequently.Table 5.Distribution of mobility attributes required by the charging model on the FCD sample.To better verify the outcomes of the modeling application, the attributes of gender and age‚Äîrandomly assigned to the FCD application sample (which originally lacks them)‚Äîreplicate the statistical distribution observed among respondents to the SP survey, limited to licensed individuals residing in metropolitan areas. As a result, the attribute ‚Äúfemale_gender‚Äù accounts for approximately 43% of the individuals, while around 60% fall within the age range of 45 to 75 years (Table 6).Table 6.Assumed distribution of gender attribute on the FCD sample.The electric vehicle size was assigned to each sampled individual based on the maximum cumulative energy consumption observed across their trip chains. Specifically, 50% of the sample was assigned a small-sized electric vehicle (40 kWh battery, nominal range of 220 km), 43% a medium-sized vehicle (50 kWh battery, range of 280 km), and 7% a large-sized vehicle (70 kWh battery, range of 370 km).These modeling assumptions tend to favor the selection of vehicles with smaller battery capacities, aligned with typical urban mobility needs, while penalizing larger battery configurations. Naturally, in commercial analyses, these assumptions could be adjusted to better reflect the actual composition of the electric vehicle market.Each battery size has been associated with one of the electric car dimensional categories of the EMEP/EEA methodology for energy consumption calculation (seeSection 2.3.1), excluding the ‚Äúmini‚Äù type.The initial SOC was set to 100% for all vehicles at the beginning of the simulation. This choice was made feasible by the extended observation period, which allows for averaging daily charging profiles over multiple days, thereby minimizing the impact of assumptions regarding the initial SOC level.As for the charging supply, the behavioral model was applied to two distinct scenarios, defined based on specific technical parameters.Scenario 1 (s1): This scenario serves as the Reference Case, incorporating assumptions considered realistic in the near future. In fact, it includes both private (home) and public charging options, reflecting a plausible configuration of charging availability and infrastructure deployment.Scenario 2 (s2): This scenario was designed to test the behavioral model under simplified assumptions, particularly aimed at minimizing the influence of charging costs in comparison to alternatives. The primary objective was to assess the model‚Äôs sensitivity to cost-related variables by comparing results obtained under s1 and s2.Table 7summarizes and compares assumptions made on charge infrastructure characteristics for both scenarios, while the following discussion elaborates on the decomposition of power and cost ranges.Table 7.Charging infrastructure characteristics across the two simulation scenarios.In s1, for 50% of the population, the availability of home charging was assumed.The domestic charging power was calibrated based on the battery capacity of the electric vehicle, as listed below.40 kWh Battery 3.6 kW;50 kWh Battery 7.2 kW;70 kWh Battery 11.0 kW;This assumption reflects a realistic distribution of home charging capabilities, considering possible residential power availability and the technical requirements of different EV models.Charging tariffs were defined considering the following: a. charging location (home or other destinations), b. charging accessibility (restricted or public), and c. charging power (3.6 √∑ 11 kW).The specific tariffs used in s1 are highlighted inTable 8. These values were selected to reflect realistic cost conditions for the reference scenario.Table 8.Electricity tariffs by location, accessibility, and power in s1.The walking time to public charging stations near the residence was set to 5 min, based on the hypothesis that proximity to charging infrastructure may positively influence the willingness to purchase an electric vehicle in the absence of private charging options.For public charging at other destinations, walking time was determined based on the surface area of the corresponding zone, according to the following rule (Table 9):Table 9.Walking time vs. zone area.The analysis considered a total of 1322 traffic zones, with the distribution of walking times illustrated inFigure 11.Figure 11.s1‚Äîwalking time distribution for public recharge at other destinations than home.In s1, no penalty was applied for extra-time parking at public charging stations located near the residence. However, for public charging at other destinations, a penalty of EUR 0.09 per minute was applied for delays exceeding 60 min, with a maximum charge of EUR 6.As for s2 infrastructural characteristics, it is assumed that no home location has domestic recharge and all public charging stations were assigned a uniform power level of 11 kW and a flat tariff of EUR 0.60/kWh.The walking time to access public charging stations was standardized to 5 min for all public charging events, regardless of whether the location was near the residence or at other destinations and no penalties were applied for improper parking at any public charging location. 3.2. Case Study‚ÄîSimulation Results3.2.1. Aggregate and Individual Charging BehaviorAccording to the behavioral model results for our case study, the likelihood of recharging increases with the number of stops within the home-based roundtrip, reflecting the cumulative energy consumption. The trend is consistent across both analyzed scenarios, with a slightly higher probability in s2, where recharging at the destination is more frequent. This leads to lower initial SOC levels in subsequent chains compared to when recharging occurs at home, which is more common in s1.Figure 12presents the results of the first-level choice simulation for both scenarios, showing the frequency of recharging among chains with a recharge probability above 30%.Figure 12.Frequencies of ‚ÄúRecharge‚Äù choices from the MC simulation for all home-based chains with recharge probability >30%, compared to average modeled probabilities. s1 (left) and s2 (right).The outcomes are largely consistent between the two scenarios, except for the longest chains, where randomness has a greater impact due to the smaller sample size.The average probability of selecting ‚ÄúDestination‚Äù varies significantly between the two scenarios, highlighting the model‚Äôs sensitivity to assumptions regarding charging infrastructure availability (public and private), power levels, and costs (Figure 13).Figure 13.Frequency of ‚ÄúDestination‚Äù selections following the second-level choice MC simulation, compared to average modeled probabilities. s1 (left) and s2 (right).In s2, where tariffs and walking distances are equal for both destination and home, the model estimated an average ‚ÄúDestination‚Äù choice probability of approximately 50% among chains where the first-level choice was ‚ÄúRecharge.‚ÄùIn s1, where tariffs and walking distances favored home charging, the ‚ÄúDestination‚Äù choice probability dropped to 20% for chains with a single intermediate destination, reaching a maximum of 40% for chains with four destinations.The second-level simulation results (Figure 13) closely align with the average probability values, except for chain length categories with limited sample sizes. Notably, a larger deviation is observed between the simulated choice frequency and the average probability for the second-level choice than the first-level choice, primarily due to the smaller sample size.In both scenarios, a slight positive correlation is observed between first-level and second-level choice probabilities: the average probability of selecting ‚ÄúDestination‚Äù increases when the probability of choosing ‚ÄúRecharge‚Äù is higher. Conversely, when the probability of ‚ÄúNo Recharge‚Äù is higher, the likelihood of selecting ‚ÄúDestination‚Äù decreases.Table 10summarizes the aggregated results of the recharge model applied to the Rome case across the two infrastructure scenarios in terms of the following: average probability of the ‚ÄúRecharge‚Äù choice of all 456,832 chains (avg_p_yes), share of choice ‚ÄúRecharge‚Äù of the chains with a probability higher than 30% after MC simulation (% MC = yes, p_yes > 30%), average probability of the ‚ÄúDestination‚Äù choice of all 437,544 intermediate destinations (avg p_dest); average probability of the ‚ÄúDestination‚Äù choice of intermediate destinations when the MC first-level choice is ‚ÄúRecharge‚Äù (avg p_dest, MC = yes), and share of the ‚ÄúDestination‚Äù choice when MC first-level choice is ‚ÄúRecharge‚Äù (% MC = dest, MC = yes).Table 10.Main results of behavioral model application and MC simulations.Table 11andTable 12present aggregate simulation results based on battery capacity, respectively, at home and at other destinations. Scenarios and battery size comparisons are based on the average initial SOC, average final SOC, and average recharged energy.Table 11.Home recharging habits based on battery capacity.Table 12.Other destinations recharging habits based on battery capacity.Initial SOC levels for home charging are generally lower than those for destination charging, with average values of approximately 35% and 39%, respectively, across both scenarios.As battery capacity increases, the initial SOC at the start of charging tends to rise. Although this may appear counterintuitive‚Äîgiven that a 70 kWh battery at 40% SOC still offers substantial range‚Äîit is partially supported by the results inFigure 2, which show that a notable share of users chooses to recharge even with a high residual range relative to the next trip.Final SOC levels are typically higher for home charging than for destination charging (94% vs. 83%/76%), with a slight downward trend for larger batteries. Energy delivered during home charging is significantly greater than at destinations, increasing proportionally with battery capacity.Additionally, the average duration of charging stops ranges from approximately 15.6 h in s1 to 11.9 h in s2.Home charging, more prevalent in s1, benefits from longer stop durations‚Äînot only because home stops are inherently longer, but also because the behavioral model tends to favor shorter destination stops, which result in lower overall charging costs where tariffs are higher.As an example,Figure 14shows the trend of the battery SOC simulated for a car with a 40 kWh battery over more than 60 travel chains during the two-month analysis period. For this vehicle, s1 includes access to private home charging, which is unavailable by default in s2.Figure 14.SOC trend over the two-month analysis period for a vehicle with a 40 kWh battery. s1 (top) and s2 (down). Initial SOC at every stopover is shown, distinguishing among type of stop location: home (Hm), intermediate destination (Ot), and public recharge near home (Hs). Stopovers without any recharge event are depicted in green while stopovers with a recharge event are depicted in yellow for the SOC before the recharge and in red as for the SOC after the recharge. Stopovers ending a chain (at home or nearby) are evidenced in black.Going from s1 to s2 reveals a clear shift from home charging to charging at other destinations.In s1, the vehicle performs 12 charging events, 11 of which occur at home, almost always resulting in a full recharge. Only toward the end of the period does a full recharge occur at an intermediate destination, followed by a top-up (‚Äúbiberonage‚Äù) at another destination to avoid battery depletion after two energy-intensive chains.In s2, the total number of charging events increases to 15, resulting in a slightly higher final SOC than in s1. Seven of these charges occur at intermediate destinations, mostly as partial charges.Enlarging the vision to other individual samples, and also to varying battery size and recharging infrastructure availability and characteristics, it emerges that the behavioral model performs more realistically when charging tariffs discourage the ‚ÄúDestination‚Äù option, which otherwise tends to be selected too frequently.This finding suggests the need for adjustments to the model to limit unnecessary top-ups (‚Äúbiberonage‚Äù), which may not always be justified.In all analyzed cases, Scenario 2 shows an increase in the total number of charging events, along with a higher incidence of recharges at intermediate destinations‚Äîexceeding 50% for the vehicle with the smallest battery capacity.As previously observed in the overall results, this outcome is primarily due to the equalization of charging tariffs between the home and destination assumed in this scenario. Despite this tariff alignment, the total cost of charging tends to be higher at home, where the average amount of energy delivered is greater. This is driven by two concurrent factors: a lower initial SOC at the start of charging and a longer available time window to complete the recharge.In s1, both the total number of charging events and the share of those occurring at intermediate destinations decrease. In this scenario, the cost of charging at destinations is significantly higher than in s2 and, more importantly, higher than at home‚Äîeven when private home charging is unavailable.Charging costs at intermediate destinations rise in s1 due to both higher tariffs and the application of an additional fee for overstaying at the charging station beyond the required time (plus a 60 min tolerance).Overall, unit charging costs tend to converge across the two scenarios. s1 is penalized by higher destination tariffs but benefits, in some cases, from the presence of private home stations where lower rates apply compared to public home charging‚Äîthe only option available in s2.Previous graphs express the maximum detail of simulations, from which aggregate estimates on energy demand over time and space are derived, as shown in the subsequent paragraph.3.2.2. Charging ProfilesDaily average of recharged energy (kWh) and delivered power (kW) required within each traffic zone per weekday has been computed considering outputs for the whole month of October 2022. For the sake of clarity, the same number of weekdays were considered to compute daily averages.In summary, for each weekday and a traffic zone, power profiles referred to the mean/maximum power. Within the same time interval, charging contributions can be of different power types, such as domestic (home), public near the user‚Äôs residence (House), or public near to other stay points (other).Analysis of full-electric private cars‚Äô charging demand during the week shows that from Wednesday to Friday the charging activity is higher compared to other days of the week, especially on Friday. Instead, the weekend is characterized by a drop, with Sunday being the day with the lowest charging activity. On the other hand, the days Monday and Tuesday show relatively high charging activity, indicating a gradual increase after the weekend.The charts shown inFigure 15andFigure 16illustrate the estimated hourly trend of average power demand across the entire study area on a representative Wednesday for both analysis scenarios.Figure 15.Average charging power demand across the metropolitan area of Rome on Wednesdays under analysis‚Äîs1.Figure 16.Average charging power demand across the whole metropolitan area of Rome on Wednesdays under analysis‚Äîs2.The above Figure illustrates the role of the different components of power demand in both scenarios. Charging at intermediate destinations exhibits a dual peak pattern‚Äîone in the early morning hours and another in the afternoon‚Äîwhereas home-based charging shows a pronounced peak during evening return hours and a secondary peak in the early afternoon, coinciding with a decline in charging activity at other destinations.In s2, power demand is more evenly distributed throughout the day. This is due to the increased cost competition between home-based charging and charging at alternative destinations. The latter tends to occur predominantly in the morning and afternoon, while the former remains concentrated in the evening.In both scenarios, there is a near-complete drop in power demand between 2:00 a.m. and 5:00 a.m. This observation highlights the potential for implementing load-shifting strategies through targeted pricing policies and/or smart charging techniques to optimize energy demand distribution over time.Figure 17presents the trend of absolute peak power values observed at different hours of the day throughout the entire month of analysis. In practice, the below histograms represent the envelop of the daily trends during the considered period. When referred to specific zones, as can be done by subdividing the territory into suitable sub-areas, these outputs are of critical importance for assessing the adequacy of the electrical grid supporting the charging infrastructure.Figure 17.Max. required power by hour in the analysis period‚Äîs1 (up) and s2 (down).The trend of peak power values over the analysis period qualitatively confirms the patterns observed in the average power profiles presented earlier. Comparing these two metrics provides valuable insights into developing strategies to manage electricity demand and supply.Based on the simulated power demand and assuming no congestion effects, the total installed power capacity required to meet charging needs‚Äîboth private and public‚Äîis estimated as follows:53,198 MW in s1;22,178 MW for private charging infrastructure;31,020 MW for public charging infrastructure;42,224 MW in s2, entirely allocated to public charging infrastructure.By aggregating individual charging behaviors, it is also possible to estimate the minimum number of charging points required in each zone to satisfy the simulated charging demand at any time during the analysis period, assuming no demand-side management strategies (e.g., smart charging). This enables a preliminary estimation of the investment requirements for both private and public charging infrastructure based on unit cost assumptions.",
            "3.1. Case Study‚ÄîDemand and Offer Qualification": "For research purposes, the proposed procedure has been applied using a set of FCD collected by Viasat in the metropolitan area of Rome during the period October‚ÄìNovember 2022. This dataset was provided by Roma Mobilit√† Agency and processed by ENEA and its academic research partners during the three-year research activities to feed the entire modeling suite developed for urban mobility analysis, including the module for estimating the charging profiles under discussion. The complete dataset comprises approximately 20,000 vehicles, but only for a subset‚Äîaround 11,500‚Äîwas it possible to infer a likely residence within the study area. From this subset of ‚Äúresident‚Äù vehicles, those with a mobility not sufficient to define a reliable charging behavior were excluded, as well as those whose mobility patterns suggested usage not attributable to individuals or small households but rather to commercial purposes (e.g., taxis, car sharing, goods distribution, etc.). After filtering out non-eligible terminals, the final sample suitable for applying the charging model consisted of approximately 8700 vehicles. The model was applied to the full set of these 8700 agents under the assumption that they represent fully electric vehicles (BEV). Based on this assumption, the sample corresponds numerically to approximately 40% of the BEV fleet registered in the Province of Rome as of 31 December 2023, according to data from the National Unified Platform (PUN) for charging infrastructure (https://www.piattaformaunicanazionale.it/, accessed on 2 November 2025). The sample includes nearly 460,000 home-based trip chains, with approximately 225,000 intermediate destinations. This discrepancy arises from the fact that many reconstructed chains are loops from home to home without intermediate stops due to the concatenation of trips separated by stops shorter than 30 min‚Äîconsidered the minimum duration for a meaningful charging event. For such loops, only the first-level choice model is applied, while the second-level model is not. Conversely, numerous ‚Äúlong‚Äù chains exist, containing more than one intermediate destination, which will be analyzed in detail in the results section of the charging model application. For each eligible individual in the sample, mobility attributes required by the behavioral model were computed based on the two-month observation period. The resulting statistical frequencies are reported inTable 5, showing the average number of travel days per week and the average daily distance on working days. It is observed that most sampled individuals use their private vehicle more than three days per week, while in terms of daily mileage, most travel less than 60 km per day‚Äîespecially among those who use their vehicle more frequently. Table 5.Distribution of mobility attributes required by the charging model on the FCD sample. To better verify the outcomes of the modeling application, the attributes of gender and age‚Äîrandomly assigned to the FCD application sample (which originally lacks them)‚Äîreplicate the statistical distribution observed among respondents to the SP survey, limited to licensed individuals residing in metropolitan areas. As a result, the attribute ‚Äúfemale_gender‚Äù accounts for approximately 43% of the individuals, while around 60% fall within the age range of 45 to 75 years (Table 6). Table 6.Assumed distribution of gender attribute on the FCD sample. The electric vehicle size was assigned to each sampled individual based on the maximum cumulative energy consumption observed across their trip chains. Specifically, 50% of the sample was assigned a small-sized electric vehicle (40 kWh battery, nominal range of 220 km), 43% a medium-sized vehicle (50 kWh battery, range of 280 km), and 7% a large-sized vehicle (70 kWh battery, range of 370 km). These modeling assumptions tend to favor the selection of vehicles with smaller battery capacities, aligned with typical urban mobility needs, while penalizing larger battery configurations. Naturally, in commercial analyses, these assumptions could be adjusted to better reflect the actual composition of the electric vehicle market. Each battery size has been associated with one of the electric car dimensional categories of the EMEP/EEA methodology for energy consumption calculation (seeSection 2.3.1), excluding the ‚Äúmini‚Äù type. The initial SOC was set to 100% for all vehicles at the beginning of the simulation. This choice was made feasible by the extended observation period, which allows for averaging daily charging profiles over multiple days, thereby minimizing the impact of assumptions regarding the initial SOC level. As for the charging supply, the behavioral model was applied to two distinct scenarios, defined based on specific technical parameters. Scenario 1 (s1): This scenario serves as the Reference Case, incorporating assumptions considered realistic in the near future. In fact, it includes both private (home) and public charging options, reflecting a plausible configuration of charging availability and infrastructure deployment. Scenario 2 (s2): This scenario was designed to test the behavioral model under simplified assumptions, particularly aimed at minimizing the influence of charging costs in comparison to alternatives. The primary objective was to assess the model‚Äôs sensitivity to cost-related variables by comparing results obtained under s1 and s2. Table 7summarizes and compares assumptions made on charge infrastructure characteristics for both scenarios, while the following discussion elaborates on the decomposition of power and cost ranges. Table 7.Charging infrastructure characteristics across the two simulation scenarios. In s1, for 50% of the population, the availability of home charging was assumed. The domestic charging power was calibrated based on the battery capacity of the electric vehicle, as listed below. 40 kWh Battery 3.6 kW;50 kWh Battery 7.2 kW;70 kWh Battery 11.0 kW; This assumption reflects a realistic distribution of home charging capabilities, considering possible residential power availability and the technical requirements of different EV models. Charging tariffs were defined considering the following: a. charging location (home or other destinations), b. charging accessibility (restricted or public), and c. charging power (3.6 √∑ 11 kW). The specific tariffs used in s1 are highlighted inTable 8. These values were selected to reflect realistic cost conditions for the reference scenario. Table 8.Electricity tariffs by location, accessibility, and power in s1. The walking time to public charging stations near the residence was set to 5 min, based on the hypothesis that proximity to charging infrastructure may positively influence the willingness to purchase an electric vehicle in the absence of private charging options. For public charging at other destinations, walking time was determined based on the surface area of the corresponding zone, according to the following rule (Table 9): Table 9.Walking time vs. zone area. The analysis considered a total of 1322 traffic zones, with the distribution of walking times illustrated inFigure 11. Figure 11.s1‚Äîwalking time distribution for public recharge at other destinations than home. In s1, no penalty was applied for extra-time parking at public charging stations located near the residence. However, for public charging at other destinations, a penalty of EUR 0.09 per minute was applied for delays exceeding 60 min, with a maximum charge of EUR 6. As for s2 infrastructural characteristics, it is assumed that no home location has domestic recharge and all public charging stations were assigned a uniform power level of 11 kW and a flat tariff of EUR 0.60/kWh. The walking time to access public charging stations was standardized to 5 min for all public charging events, regardless of whether the location was near the residence or at other destinations and no penalties were applied for improper parking at any public charging location.",
            "3.2. Case Study‚ÄîSimulation Results": "3.2.1. Aggregate and Individual Charging BehaviorAccording to the behavioral model results for our case study, the likelihood of recharging increases with the number of stops within the home-based roundtrip, reflecting the cumulative energy consumption. The trend is consistent across both analyzed scenarios, with a slightly higher probability in s2, where recharging at the destination is more frequent. This leads to lower initial SOC levels in subsequent chains compared to when recharging occurs at home, which is more common in s1.Figure 12presents the results of the first-level choice simulation for both scenarios, showing the frequency of recharging among chains with a recharge probability above 30%.Figure 12.Frequencies of ‚ÄúRecharge‚Äù choices from the MC simulation for all home-based chains with recharge probability >30%, compared to average modeled probabilities. s1 (left) and s2 (right).The outcomes are largely consistent between the two scenarios, except for the longest chains, where randomness has a greater impact due to the smaller sample size.The average probability of selecting ‚ÄúDestination‚Äù varies significantly between the two scenarios, highlighting the model‚Äôs sensitivity to assumptions regarding charging infrastructure availability (public and private), power levels, and costs (Figure 13).Figure 13.Frequency of ‚ÄúDestination‚Äù selections following the second-level choice MC simulation, compared to average modeled probabilities. s1 (left) and s2 (right).In s2, where tariffs and walking distances are equal for both destination and home, the model estimated an average ‚ÄúDestination‚Äù choice probability of approximately 50% among chains where the first-level choice was ‚ÄúRecharge.‚ÄùIn s1, where tariffs and walking distances favored home charging, the ‚ÄúDestination‚Äù choice probability dropped to 20% for chains with a single intermediate destination, reaching a maximum of 40% for chains with four destinations.The second-level simulation results (Figure 13) closely align with the average probability values, except for chain length categories with limited sample sizes. Notably, a larger deviation is observed between the simulated choice frequency and the average probability for the second-level choice than the first-level choice, primarily due to the smaller sample size.In both scenarios, a slight positive correlation is observed between first-level and second-level choice probabilities: the average probability of selecting ‚ÄúDestination‚Äù increases when the probability of choosing ‚ÄúRecharge‚Äù is higher. Conversely, when the probability of ‚ÄúNo Recharge‚Äù is higher, the likelihood of selecting ‚ÄúDestination‚Äù decreases.Table 10summarizes the aggregated results of the recharge model applied to the Rome case across the two infrastructure scenarios in terms of the following: average probability of the ‚ÄúRecharge‚Äù choice of all 456,832 chains (avg_p_yes), share of choice ‚ÄúRecharge‚Äù of the chains with a probability higher than 30% after MC simulation (% MC = yes, p_yes > 30%), average probability of the ‚ÄúDestination‚Äù choice of all 437,544 intermediate destinations (avg p_dest); average probability of the ‚ÄúDestination‚Äù choice of intermediate destinations when the MC first-level choice is ‚ÄúRecharge‚Äù (avg p_dest, MC = yes), and share of the ‚ÄúDestination‚Äù choice when MC first-level choice is ‚ÄúRecharge‚Äù (% MC = dest, MC = yes).Table 10.Main results of behavioral model application and MC simulations.Table 11andTable 12present aggregate simulation results based on battery capacity, respectively, at home and at other destinations. Scenarios and battery size comparisons are based on the average initial SOC, average final SOC, and average recharged energy.Table 11.Home recharging habits based on battery capacity.Table 12.Other destinations recharging habits based on battery capacity.Initial SOC levels for home charging are generally lower than those for destination charging, with average values of approximately 35% and 39%, respectively, across both scenarios.As battery capacity increases, the initial SOC at the start of charging tends to rise. Although this may appear counterintuitive‚Äîgiven that a 70 kWh battery at 40% SOC still offers substantial range‚Äîit is partially supported by the results inFigure 2, which show that a notable share of users chooses to recharge even with a high residual range relative to the next trip.Final SOC levels are typically higher for home charging than for destination charging (94% vs. 83%/76%), with a slight downward trend for larger batteries. Energy delivered during home charging is significantly greater than at destinations, increasing proportionally with battery capacity.Additionally, the average duration of charging stops ranges from approximately 15.6 h in s1 to 11.9 h in s2.Home charging, more prevalent in s1, benefits from longer stop durations‚Äînot only because home stops are inherently longer, but also because the behavioral model tends to favor shorter destination stops, which result in lower overall charging costs where tariffs are higher.As an example,Figure 14shows the trend of the battery SOC simulated for a car with a 40 kWh battery over more than 60 travel chains during the two-month analysis period. For this vehicle, s1 includes access to private home charging, which is unavailable by default in s2.Figure 14.SOC trend over the two-month analysis period for a vehicle with a 40 kWh battery. s1 (top) and s2 (down). Initial SOC at every stopover is shown, distinguishing among type of stop location: home (Hm), intermediate destination (Ot), and public recharge near home (Hs). Stopovers without any recharge event are depicted in green while stopovers with a recharge event are depicted in yellow for the SOC before the recharge and in red as for the SOC after the recharge. Stopovers ending a chain (at home or nearby) are evidenced in black.Going from s1 to s2 reveals a clear shift from home charging to charging at other destinations.In s1, the vehicle performs 12 charging events, 11 of which occur at home, almost always resulting in a full recharge. Only toward the end of the period does a full recharge occur at an intermediate destination, followed by a top-up (‚Äúbiberonage‚Äù) at another destination to avoid battery depletion after two energy-intensive chains.In s2, the total number of charging events increases to 15, resulting in a slightly higher final SOC than in s1. Seven of these charges occur at intermediate destinations, mostly as partial charges.Enlarging the vision to other individual samples, and also to varying battery size and recharging infrastructure availability and characteristics, it emerges that the behavioral model performs more realistically when charging tariffs discourage the ‚ÄúDestination‚Äù option, which otherwise tends to be selected too frequently.This finding suggests the need for adjustments to the model to limit unnecessary top-ups (‚Äúbiberonage‚Äù), which may not always be justified.In all analyzed cases, Scenario 2 shows an increase in the total number of charging events, along with a higher incidence of recharges at intermediate destinations‚Äîexceeding 50% for the vehicle with the smallest battery capacity.As previously observed in the overall results, this outcome is primarily due to the equalization of charging tariffs between the home and destination assumed in this scenario. Despite this tariff alignment, the total cost of charging tends to be higher at home, where the average amount of energy delivered is greater. This is driven by two concurrent factors: a lower initial SOC at the start of charging and a longer available time window to complete the recharge.In s1, both the total number of charging events and the share of those occurring at intermediate destinations decrease. In this scenario, the cost of charging at destinations is significantly higher than in s2 and, more importantly, higher than at home‚Äîeven when private home charging is unavailable.Charging costs at intermediate destinations rise in s1 due to both higher tariffs and the application of an additional fee for overstaying at the charging station beyond the required time (plus a 60 min tolerance).Overall, unit charging costs tend to converge across the two scenarios. s1 is penalized by higher destination tariffs but benefits, in some cases, from the presence of private home stations where lower rates apply compared to public home charging‚Äîthe only option available in s2.Previous graphs express the maximum detail of simulations, from which aggregate estimates on energy demand over time and space are derived, as shown in the subsequent paragraph. 3.2.2. Charging ProfilesDaily average of recharged energy (kWh) and delivered power (kW) required within each traffic zone per weekday has been computed considering outputs for the whole month of October 2022. For the sake of clarity, the same number of weekdays were considered to compute daily averages.In summary, for each weekday and a traffic zone, power profiles referred to the mean/maximum power. Within the same time interval, charging contributions can be of different power types, such as domestic (home), public near the user‚Äôs residence (House), or public near to other stay points (other).Analysis of full-electric private cars‚Äô charging demand during the week shows that from Wednesday to Friday the charging activity is higher compared to other days of the week, especially on Friday. Instead, the weekend is characterized by a drop, with Sunday being the day with the lowest charging activity. On the other hand, the days Monday and Tuesday show relatively high charging activity, indicating a gradual increase after the weekend.The charts shown inFigure 15andFigure 16illustrate the estimated hourly trend of average power demand across the entire study area on a representative Wednesday for both analysis scenarios.Figure 15.Average charging power demand across the metropolitan area of Rome on Wednesdays under analysis‚Äîs1.Figure 16.Average charging power demand across the whole metropolitan area of Rome on Wednesdays under analysis‚Äîs2.The above Figure illustrates the role of the different components of power demand in both scenarios. Charging at intermediate destinations exhibits a dual peak pattern‚Äîone in the early morning hours and another in the afternoon‚Äîwhereas home-based charging shows a pronounced peak during evening return hours and a secondary peak in the early afternoon, coinciding with a decline in charging activity at other destinations.In s2, power demand is more evenly distributed throughout the day. This is due to the increased cost competition between home-based charging and charging at alternative destinations. The latter tends to occur predominantly in the morning and afternoon, while the former remains concentrated in the evening.In both scenarios, there is a near-complete drop in power demand between 2:00 a.m. and 5:00 a.m. This observation highlights the potential for implementing load-shifting strategies through targeted pricing policies and/or smart charging techniques to optimize energy demand distribution over time.Figure 17presents the trend of absolute peak power values observed at different hours of the day throughout the entire month of analysis. In practice, the below histograms represent the envelop of the daily trends during the considered period. When referred to specific zones, as can be done by subdividing the territory into suitable sub-areas, these outputs are of critical importance for assessing the adequacy of the electrical grid supporting the charging infrastructure.Figure 17.Max. required power by hour in the analysis period‚Äîs1 (up) and s2 (down).The trend of peak power values over the analysis period qualitatively confirms the patterns observed in the average power profiles presented earlier. Comparing these two metrics provides valuable insights into developing strategies to manage electricity demand and supply.Based on the simulated power demand and assuming no congestion effects, the total installed power capacity required to meet charging needs‚Äîboth private and public‚Äîis estimated as follows:53,198 MW in s1;22,178 MW for private charging infrastructure;31,020 MW for public charging infrastructure;42,224 MW in s2, entirely allocated to public charging infrastructure.By aggregating individual charging behaviors, it is also possible to estimate the minimum number of charging points required in each zone to satisfy the simulated charging demand at any time during the analysis period, assuming no demand-side management strategies (e.g., smart charging). This enables a preliminary estimation of the investment requirements for both private and public charging infrastructure based on unit cost assumptions.",
            "3.2.1. Aggregate and Individual Charging Behavior": "According to the behavioral model results for our case study, the likelihood of recharging increases with the number of stops within the home-based roundtrip, reflecting the cumulative energy consumption. The trend is consistent across both analyzed scenarios, with a slightly higher probability in s2, where recharging at the destination is more frequent. This leads to lower initial SOC levels in subsequent chains compared to when recharging occurs at home, which is more common in s1. Figure 12presents the results of the first-level choice simulation for both scenarios, showing the frequency of recharging among chains with a recharge probability above 30%. Figure 12.Frequencies of ‚ÄúRecharge‚Äù choices from the MC simulation for all home-based chains with recharge probability >30%, compared to average modeled probabilities. s1 (left) and s2 (right). The outcomes are largely consistent between the two scenarios, except for the longest chains, where randomness has a greater impact due to the smaller sample size. The average probability of selecting ‚ÄúDestination‚Äù varies significantly between the two scenarios, highlighting the model‚Äôs sensitivity to assumptions regarding charging infrastructure availability (public and private), power levels, and costs (Figure 13). Figure 13.Frequency of ‚ÄúDestination‚Äù selections following the second-level choice MC simulation, compared to average modeled probabilities. s1 (left) and s2 (right). In s2, where tariffs and walking distances are equal for both destination and home, the model estimated an average ‚ÄúDestination‚Äù choice probability of approximately 50% among chains where the first-level choice was ‚ÄúRecharge.‚Äù In s1, where tariffs and walking distances favored home charging, the ‚ÄúDestination‚Äù choice probability dropped to 20% for chains with a single intermediate destination, reaching a maximum of 40% for chains with four destinations. The second-level simulation results (Figure 13) closely align with the average probability values, except for chain length categories with limited sample sizes. Notably, a larger deviation is observed between the simulated choice frequency and the average probability for the second-level choice than the first-level choice, primarily due to the smaller sample size. In both scenarios, a slight positive correlation is observed between first-level and second-level choice probabilities: the average probability of selecting ‚ÄúDestination‚Äù increases when the probability of choosing ‚ÄúRecharge‚Äù is higher. Conversely, when the probability of ‚ÄúNo Recharge‚Äù is higher, the likelihood of selecting ‚ÄúDestination‚Äù decreases. Table 10summarizes the aggregated results of the recharge model applied to the Rome case across the two infrastructure scenarios in terms of the following: average probability of the ‚ÄúRecharge‚Äù choice of all 456,832 chains (avg_p_yes), share of choice ‚ÄúRecharge‚Äù of the chains with a probability higher than 30% after MC simulation (% MC = yes, p_yes > 30%), average probability of the ‚ÄúDestination‚Äù choice of all 437,544 intermediate destinations (avg p_dest); average probability of the ‚ÄúDestination‚Äù choice of intermediate destinations when the MC first-level choice is ‚ÄúRecharge‚Äù (avg p_dest, MC = yes), and share of the ‚ÄúDestination‚Äù choice when MC first-level choice is ‚ÄúRecharge‚Äù (% MC = dest, MC = yes). Table 10.Main results of behavioral model application and MC simulations. Table 11andTable 12present aggregate simulation results based on battery capacity, respectively, at home and at other destinations. Scenarios and battery size comparisons are based on the average initial SOC, average final SOC, and average recharged energy. Table 11.Home recharging habits based on battery capacity. Table 12.Other destinations recharging habits based on battery capacity. Initial SOC levels for home charging are generally lower than those for destination charging, with average values of approximately 35% and 39%, respectively, across both scenarios. As battery capacity increases, the initial SOC at the start of charging tends to rise. Although this may appear counterintuitive‚Äîgiven that a 70 kWh battery at 40% SOC still offers substantial range‚Äîit is partially supported by the results inFigure 2, which show that a notable share of users chooses to recharge even with a high residual range relative to the next trip. Final SOC levels are typically higher for home charging than for destination charging (94% vs. 83%/76%), with a slight downward trend for larger batteries. Energy delivered during home charging is significantly greater than at destinations, increasing proportionally with battery capacity. Additionally, the average duration of charging stops ranges from approximately 15.6 h in s1 to 11.9 h in s2. Home charging, more prevalent in s1, benefits from longer stop durations‚Äînot only because home stops are inherently longer, but also because the behavioral model tends to favor shorter destination stops, which result in lower overall charging costs where tariffs are higher. As an example,Figure 14shows the trend of the battery SOC simulated for a car with a 40 kWh battery over more than 60 travel chains during the two-month analysis period. For this vehicle, s1 includes access to private home charging, which is unavailable by default in s2. Figure 14.SOC trend over the two-month analysis period for a vehicle with a 40 kWh battery. s1 (top) and s2 (down). Initial SOC at every stopover is shown, distinguishing among type of stop location: home (Hm), intermediate destination (Ot), and public recharge near home (Hs). Stopovers without any recharge event are depicted in green while stopovers with a recharge event are depicted in yellow for the SOC before the recharge and in red as for the SOC after the recharge. Stopovers ending a chain (at home or nearby) are evidenced in black. Going from s1 to s2 reveals a clear shift from home charging to charging at other destinations. In s1, the vehicle performs 12 charging events, 11 of which occur at home, almost always resulting in a full recharge. Only toward the end of the period does a full recharge occur at an intermediate destination, followed by a top-up (‚Äúbiberonage‚Äù) at another destination to avoid battery depletion after two energy-intensive chains. In s2, the total number of charging events increases to 15, resulting in a slightly higher final SOC than in s1. Seven of these charges occur at intermediate destinations, mostly as partial charges. Enlarging the vision to other individual samples, and also to varying battery size and recharging infrastructure availability and characteristics, it emerges that the behavioral model performs more realistically when charging tariffs discourage the ‚ÄúDestination‚Äù option, which otherwise tends to be selected too frequently. This finding suggests the need for adjustments to the model to limit unnecessary top-ups (‚Äúbiberonage‚Äù), which may not always be justified. In all analyzed cases, Scenario 2 shows an increase in the total number of charging events, along with a higher incidence of recharges at intermediate destinations‚Äîexceeding 50% for the vehicle with the smallest battery capacity. As previously observed in the overall results, this outcome is primarily due to the equalization of charging tariffs between the home and destination assumed in this scenario. Despite this tariff alignment, the total cost of charging tends to be higher at home, where the average amount of energy delivered is greater. This is driven by two concurrent factors: a lower initial SOC at the start of charging and a longer available time window to complete the recharge. In s1, both the total number of charging events and the share of those occurring at intermediate destinations decrease. In this scenario, the cost of charging at destinations is significantly higher than in s2 and, more importantly, higher than at home‚Äîeven when private home charging is unavailable. Charging costs at intermediate destinations rise in s1 due to both higher tariffs and the application of an additional fee for overstaying at the charging station beyond the required time (plus a 60 min tolerance). Overall, unit charging costs tend to converge across the two scenarios. s1 is penalized by higher destination tariffs but benefits, in some cases, from the presence of private home stations where lower rates apply compared to public home charging‚Äîthe only option available in s2. Previous graphs express the maximum detail of simulations, from which aggregate estimates on energy demand over time and space are derived, as shown in the subsequent paragraph.",
            "3.2.2. Charging Profiles": "Daily average of recharged energy (kWh) and delivered power (kW) required within each traffic zone per weekday has been computed considering outputs for the whole month of October 2022. For the sake of clarity, the same number of weekdays were considered to compute daily averages. In summary, for each weekday and a traffic zone, power profiles referred to the mean/maximum power. Within the same time interval, charging contributions can be of different power types, such as domestic (home), public near the user‚Äôs residence (House), or public near to other stay points (other). Analysis of full-electric private cars‚Äô charging demand during the week shows that from Wednesday to Friday the charging activity is higher compared to other days of the week, especially on Friday. Instead, the weekend is characterized by a drop, with Sunday being the day with the lowest charging activity. On the other hand, the days Monday and Tuesday show relatively high charging activity, indicating a gradual increase after the weekend. The charts shown inFigure 15andFigure 16illustrate the estimated hourly trend of average power demand across the entire study area on a representative Wednesday for both analysis scenarios. Figure 15.Average charging power demand across the metropolitan area of Rome on Wednesdays under analysis‚Äîs1. Figure 16.Average charging power demand across the whole metropolitan area of Rome on Wednesdays under analysis‚Äîs2. The above Figure illustrates the role of the different components of power demand in both scenarios. Charging at intermediate destinations exhibits a dual peak pattern‚Äîone in the early morning hours and another in the afternoon‚Äîwhereas home-based charging shows a pronounced peak during evening return hours and a secondary peak in the early afternoon, coinciding with a decline in charging activity at other destinations. In s2, power demand is more evenly distributed throughout the day. This is due to the increased cost competition between home-based charging and charging at alternative destinations. The latter tends to occur predominantly in the morning and afternoon, while the former remains concentrated in the evening. In both scenarios, there is a near-complete drop in power demand between 2:00 a.m. and 5:00 a.m. This observation highlights the potential for implementing load-shifting strategies through targeted pricing policies and/or smart charging techniques to optimize energy demand distribution over time. Figure 17presents the trend of absolute peak power values observed at different hours of the day throughout the entire month of analysis. In practice, the below histograms represent the envelop of the daily trends during the considered period. When referred to specific zones, as can be done by subdividing the territory into suitable sub-areas, these outputs are of critical importance for assessing the adequacy of the electrical grid supporting the charging infrastructure. Figure 17.Max. required power by hour in the analysis period‚Äîs1 (up) and s2 (down). The trend of peak power values over the analysis period qualitatively confirms the patterns observed in the average power profiles presented earlier. Comparing these two metrics provides valuable insights into developing strategies to manage electricity demand and supply. Based on the simulated power demand and assuming no congestion effects, the total installed power capacity required to meet charging needs‚Äîboth private and public‚Äîis estimated as follows: 53,198 MW in s1;22,178 MW for private charging infrastructure;31,020 MW for public charging infrastructure;42,224 MW in s2, entirely allocated to public charging infrastructure. By aggregating individual charging behaviors, it is also possible to estimate the minimum number of charging points required in each zone to satisfy the simulated charging demand at any time during the analysis period, assuming no demand-side management strategies (e.g., smart charging). This enables a preliminary estimation of the investment requirements for both private and public charging infrastructure based on unit cost assumptions.",
            "4. Discussion and Conclusions": "The methodology proposed is grounded in the use of FCD, which offers the distinct advantage of being applicable across diverse territorial contexts, if vehicle monitoring is sufficiently widespread. This monitoring is primarily driven by insurance-related purposes, as insurance companies benefit from tracking events that involve damage compensation, thereby enabling reductions in insurance premiums. In Italy, a significant proportion of vehicle owners have adopted this technology, resulting in a relatively high penetration rate of onboard monitoring devices‚Äîcurrently estimated at approximately 18% of all mandatory motor insurance policies [53]. In addition, the European Parliament, within the framework of the ‚ÄúVision Zero‚Äù strategy aimed at eliminating road fatalities by 2050, has recently enacted Regulation GSR 2019/2144. This regulation mandates the installation of On-Board Units in all vehicles registered from 2024 onwards, specifically for accident detection. Although access to monitoring data is restricted to competent authorities and only permitted in cases of serious accidents, the regulation is expected to further encourage the continuous monitoring of private mobility. This would significantly enhance the understanding of mobility dynamics and improve the effectiveness of policies promoting sustainable transport demand management at local, regional, and national levels. In this evolving landscape, the development of digital platforms capable of storing and processing FCD samples is accelerating. These platforms also provide advanced analytical and simulation tools that are highly valuable for transport system planners and operators. In this context, a web-based mobility platform PRIORITY (Platform for the tRansition to sustanInable zerO-caRbon mobilITY) is going to be developed by ENEA to support the mobility governance processes, oriented toward energy efficiency. This platform has the purpose of innovating the knowledge and evaluation processes of urban mobility by offering a relevant instrument to mobility managers to design Sustainable Urban Mobility Plans (SUMPs). Nonetheless, the use of FCD for analytical purposes presents several critical limitations. First and foremost, privacy regulations‚Äîspecifically EU Regulation 2016/679 (GDPR)‚Äîimpose strict constraints on the use of personal data that could directly or indirectly identify individuals. The European Data Protection Board (EDPB) guidelines require the implementation of specific safeguards such as anonymization and pseudonymization. While these measures are generally adopted by FCD providers, a recent requirement to continuously change vehicle ID code may significantly hinder the reconstruction of individual mobility patterns over extended timeframes. Another key concern is the statistical representativeness of the FCD sample relative to the target population, which must be rigorously validated prior to conducting any analysis. Furthermore, FCD is typically provided on a commercial basis, necessitating the allocation of a dedicated budget for its use in analytical methodologies. Ideally, this budget should also encompass access to enriched datasets beyond the standard FCD format, which typically categorizes vehicles into two broad groups: Light-Duty Vehicles (LDV) and Heavy-Duty Vehicles (HDV). Possible additional data‚Äîcompliant with privacy regula year of registration), predominant usage (private, corporate, and commercial), and the territorial scope of short-range circulation (e.g., inferred from the administrative area of the insurance contract). Such supplementary information would support the assignment of scenario variables related to vehicle size class, demographic characteristics of the primary user, and area of residence. Similarly, a more comprehensive and detailed knowledge base regarding the distribution and characteristics of urban charging infrastructure, particularly private charging points (individual or shared within residential buildings), could facilitate the definition of baseline scenarios for charging supply. These baselines would serve as reference points for evaluating potential infrastructure expansion, especially in geographic areas such as Italy, where the adoption of electric vehicles remains relatively modest. With respect to traction energy consumption estimation, the methodology employed is well-established and has long been utilized in the development of regional inventories for road transport energy use and emissions. However, it is noted that this model is suitable for large-scale or real-time applications, providing good results, since it requires fewer data input and enables fast processing [27,54]. The estimation of energy consumption attributable to cabin climate control (heating and cooling) is based on an original procedure developed by ENEA. This procedure currently relies on measurements taken from electric buses, with additional campaigns underway. A dedicated study focusing on passenger cars is planned, involving the development of curves that describe HVAC system power requirements as a function of ambient temperature and humidity based on experimental or simulated vehicle cabin conditions. These findings will be presented in future publications. Preliminary results from an analysis of HVAC consumption in the ATAC fleet in Rome indicate that, under the most severe climate change scenarios outlined by the IPCC for the remainder of this century, HVAC-related energy demand could significantly affect both charging requirements and range estimates for electric vehicles. In summer, peak HVAC energy demand could reach up to 40% of the total traction energy. These impacts may be partially mitigated by anticipated improvements in battery efficiency and charging technologies. In conclusion, despite the lack of certain key data required for scenario definition in the case study, the proposed methodology has demonstrated its effectiveness as a predictive tool for urban electric vehicle charging power demand across simulation scenarios, as illustrated by the charging profiles resulting in the two case studies examined (Figure 17). Through our procedure, each demand/supply scenario can be evaluated in terms of the hourly distribution of peak charging power demand at the level of individual urban zones or across broader areas, thereby informing potential upgrades both to the recharge infrastructure and electricity distribution network. Moreover, simulation results enable downstream analysis to estimate the financial investment required for public and private charging infrastructure, as well as the impact on charging costs for electric vehicle users. Finally, the potential for smart charging and Vehicle-to-Everything (V2X) applications can also be assessed. Future developments will include distinguishing corporate charging supply and evaluating the impact of potential queuing during peak demand periods. To better validate the proposed methodology, waiting for actual data from the field, another predictive behavioral module is now being developed using results of a Revealed Preferences (RP) survey. The new model estimates recharging probabilities through the analysis of the conditional probabilities statistically emerging from the revealed behavior of the electric vehicle users. The next research activities will provide a comparison between methodologies and results of the two analytical approaches, involving recent external research findings on the same subject."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/1996-1073/18/23/6370",
        "scraped_at": "2025-12-05 23:54:20"
    },
    {
        "title": "Assessing Surface Water Dynamics of Wetlands in Reclaimed Mining Areas in the Athabasca Oil Sands Region, Alberta, Canada, with Time-Varying Sentinel-1 SAR and Sentinel-2 Multi-Spectral Imagery",
        "authors": "byErik Biederstadt,Faramarz F. Samavati,Hannah Porter,Elizabeth GillisandJan J. H. Ciborowski",
        "journal": "Remote Sens.2025,17(23), 3927;https://doi.org/10.3390/rs17233927- 4 Dec 2025",
        "abstract": "Wetlands provide critical ecological and socio-economic benefits, covering approximately 45% of the Athabasca Oil Sands Region in Alberta, Canada. However, open-pit oil sand mining has led to widespread wetland loss. While reclamation efforts are ongoing, the development of effective wetland monitoring methods remain essential. This paper presents a novel approach to tracking wetland dynamics in reclaimed and reference landscapes using Sentinel-1 SAR and Sentinel-2 multispectral imagery. We assess surface water extent and emergent vegetation, validating our satellite-based measurements against high-resolution UAV-derived wetland area data (ùëÖ2=0.902R2=0.902). Our results reveal minor differences in intra-annual variability in wetland area between wetlands in reclaimed versus those in reference landscapes. Wetlands exhibit a positive log-linear relationship between maximum depth and variability in open-water area, a pattern that was consistent between landscape types. Intra- and interannual variability in spatial extent were both positively associated with wetland area. This paper introduces the first ground-truthed automated wetland monitoring approach for the region. These findings document the similarities in range of variation between wetlands developing in reclaimed and reference landscapes and provide a simple tool to support long-term monitoring to document the persistence of wetlands forming in reclaimed landscapes.",
        "keywords": ":surface water dynamics; boreal wetlands; reclaimed landscapes; remote sensing; synthetic aperture radar; Google Earth Enginesurface water dynamics;boreal wetlands;reclaimed landscapes;remote sensing;synthetic aperture radar;Google Earth Engine",
        "full_content": {
            "Abstract": "Wetlands provide critical ecological and socio-economic benefits, covering approximately 45% of the Athabasca Oil Sands Region in Alberta, Canada. However, open-pit oil sand mining has led to widespread wetland loss. While reclamation efforts are ongoing, the development of effective wetland monitoring methods remain essential. This paper presents a novel approach to tracking wetland dynamics in reclaimed and reference landscapes using Sentinel-1 SAR and Sentinel-2 multispectral imagery. We assess surface water extent and emergent vegetation, validating our satellite-based measurements against high-resolution UAV-derived wetland area data (ùëÖ2=0.902R2=0.902). Our results reveal minor differences in intra-annual variability in wetland area between wetlands in reclaimed versus those in reference landscapes. Wetlands exhibit a positive log-linear relationship between maximum depth and variability in open-water area, a pattern that was consistent between landscape types. Intra- and interannual variability in spatial extent were both positively associated with wetland area. This paper introduces the first ground-truthed automated wetland monitoring approach for the region. These findings document the similarities in range of variation between wetlands developing in reclaimed and reference landscapes and provide a simple tool to support long-term monitoring to document the persistence of wetlands forming in reclaimed landscapes. Keywords:surface water dynamics; boreal wetlands; reclaimed landscapes; remote sensing; synthetic aperture radar; Google Earth Enginesurface water dynamics;boreal wetlands;reclaimed landscapes;remote sensing;synthetic aperture radar;Google Earth Engine",
            "Share and Cite": "MDPI and ACS StyleBiederstadt, E.;                     Samavati, F.F.;                     Porter, H.;                     Gillis, E.;                     Ciborowski, J.J.H.    \n        Assessing Surface Water Dynamics of Wetlands in Reclaimed Mining Areas in the Athabasca Oil Sands Region, Alberta, Canada, with Time-Varying Sentinel-1 SAR and Sentinel-2 Multi-Spectral Imagery.Remote Sens.2025,17, 3927.\n    https://doi.org/10.3390/rs17233927AMA StyleBiederstadt E,                                 Samavati FF,                                 Porter H,                                 Gillis E,                                 Ciborowski JJH.        \n                Assessing Surface Water Dynamics of Wetlands in Reclaimed Mining Areas in the Athabasca Oil Sands Region, Alberta, Canada, with Time-Varying Sentinel-1 SAR and Sentinel-2 Multi-Spectral Imagery.Remote Sensing. 2025; 17(23):3927.\n        https://doi.org/10.3390/rs17233927Chicago/Turabian StyleBiederstadt, Erik,                                 Faramarz F. Samavati,                                 Hannah Porter,                                 Elizabeth Gillis,                                 and Jan J. H. Ciborowski.        \n                2025. \"Assessing Surface Water Dynamics of Wetlands in Reclaimed Mining Areas in the Athabasca Oil Sands Region, Alberta, Canada, with Time-Varying Sentinel-1 SAR and Sentinel-2 Multi-Spectral Imagery\"Remote Sensing17, no. 23: 3927.\n        https://doi.org/10.3390/rs17233927APA StyleBiederstadt, E.,                                 Samavati, F. F.,                                 Porter, H.,                                 Gillis, E.,                                 & Ciborowski, J. J. H.        \n        \n        (2025). Assessing Surface Water Dynamics of Wetlands in Reclaimed Mining Areas in the Athabasca Oil Sands Region, Alberta, Canada, with Time-Varying Sentinel-1 SAR and Sentinel-2 Multi-Spectral Imagery.Remote Sensing,17(23), 3927.\n        https://doi.org/10.3390/rs17233927 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further detailshere.",
            "Article Metrics": "YesCitationsNo citations were found for this article, but you may check onGoogle ScholarNoArticle Access StatisticsCreated with Highcharts 4.0.4Chart context menuArticle access statisticsArticle Views6. Dec0For more information on the journal statistics, clickhere.Multiple requests from the same IP address are counted as one view.",
            "Citations": "No citations were found for this article, but you may check onGoogle Scholar",
            "Article Access Statistics": "Created with Highcharts 4.0.4Chart context menuArticle access statisticsArticle Views6. Dec0 For more information on the journal statistics, click here . Multiple requests from the same IP address are counted as one view."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2072-4292/17/23/3927",
        "scraped_at": "2025-12-05 23:54:26"
    },
    {
        "title": "Improving Texture Recognition via Multi-Layer Feature Aggregation from Pre-Trained Vision Architectures",
        "authors": "byNikolay Neshov,Krasimir Tonchev,Ivaylo Bozhilov,Radostina PetkovaandAgata Manolova",
        "journal": "Electronics2025,14(23), 4779;https://doi.org/10.3390/electronics14234779- 4 Dec 2025",
        "abstract": "Texture recognition is a fundamental task in computer vision, with diverse applications in material sciences, medicine, and agriculture. The ability to analyze complex patterns in images has been greatly enhanced by advancements in Deep Neural Networks and Vision Transformers. To address the challenging nature of texture recognition, this paper investigates the performance of several pre-trained vision architectures for texture recognition, including both CNN- and transformer-based models. For each architecture, multi-level features are extracted from early, intermediate, and final layers, concatenated, and fed into a trainable Multi-Layer Perceptron (MLP) classifier. The architecture is thoroughly evaluated using five publicly available texture datasets, KTH-TIPS2-b, FMD, GTOS-Mobile, DTD, and Soil, with MLP hyperparameters determined through an exhaustive grid search on one of the datasets to ensure optimal performance. Extensive experiments highlight the comparative performance of each architecture and demonstrate that aggregating features from different hierarchical levels improves texture recognition in most cases, outperforming even architectures that require substantially higher computational resources. The study also shows the particular effectiveness of transformer-based models, such as BEiTv2, in achieving state-of-the-art results on four of the five examined datasets.Keywords:DTD;FMD;GTOS-Mobile;KTH-TIPS-2;Multi-Layer Perceptron;texture recognition;transformer architectures",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Texture is ubiquitous in natural images and serves as a crucial visual cue for various image analysis tasks, such as image segmentation, retrieval, and shape analysis from texture. In computer vision and image processing, texture classification is a fundamental problem, playing an essential role across a wide range of applications. These include medical image analysis [1], where texture recognition helps in the retrieval of medical images for accurate diagnosis. In remote sensing [2], texture analysis can be used to classify land types and detect environmental changes in satellite or aerial imagery. In agriculture [3], texture recognition aids in predicting soil texture, which plays a key role in crop management and land use planning. Object recognition [4] relies on texture to distinguish between objects, improving detection and classification in complex environments. As textures are integral to these applications, their accurate recognition significantly enhances the performance of automated systems in fields such as healthcare, environmental monitoring, and industrial applications. Architectures based on traditional Convolutional Neural Networks (CNNs) have been widely employed in image recognition tasks, significantly improving the analysis of material and object appearance through transfer learning on pre-trained models across large datasets [5,6,7]. These methods offer computational efficiency during inference, yet retraining the backbone remains computationally demanding. Techniques like Global Average Pooling (GAP) have been employed to aggregate features, providing robust representations resistant to spatial transformations [6,8]. However, these approaches often rely on large datasets for effective end-to-end training with deep CNN architectures. At the same time, CNNs may struggle with complex material patterns due to inherent spatial biases, lacking robustness to translation, rotation, and scale variations. Nevertheless, recent advancements in convolutional network design continue to push the boundaries of CNN-based models. Modern architectures such as ConvNeXt [9] demonstrate that carefully rethinking convolutional building blocks‚Äîinspired by insights from Transformer-based models‚Äîcan substantially enhance representational capacity and scalability without abandoning the efficiency advantages of convolution. These next-generation CNNs narrow the performance gap with vision transformers while retaining strong inductive biases, efficient inference, and compatibility with established transfer learning workflows for material and object recognition tasks. Vision Transformer (ViT) architectures offer a promising alternative to CNNs by effectively capturing global context in images. ViTs have begun to dominate the computer vision literature, challenging traditional CNNs, particularly in image classification tasks [10,11]. However, their lack of spatial inductive biases, which are crucial for capturing fine-grained local patterns typical in textures, limits their effectiveness. Despite these limitations, ViT-based models remain highly promising candidates for texture recognition tasks due to their strong capacity for modeling long-range dependencies and contextual relationships. Their ability to capture complex global structures suggests significant potential in scenarios where texture information is distributed across spatial regions. However, further investigation is needed to adapt and optimize transformer architectures for texture-focused applications, particularly in integrating local detail sensitivity with their global feature modeling strengths The motivation of this work is to investigate the potential of modern deep learning-based vision architectures for the challenging task of texture recognition. By leveraging their hierarchical feature extraction capabilities and integrating a trainable MLP classifier, this study explores a lightweight and transferable multi-layer feature aggregation strategy specifically tailored to texture analysis. The proposed method aims to harness the complementary representations captured at different depths of these architectures‚Äîfrom early low-level features to mid-level structural patterns and high-level semantic abstractions. By aggregating and combining these multi-level features through GAP and feature concatenation, the model enhances its discriminative capacity and adapts the representational power of both CNN- and transformer-based backbones to the unique complexities of texture recognition. The main contributions of the presented work are as follows: This work investigates various pre-trained vision architectures specifically for texture recognition tasks. By adding and training a MLP for classification, a comprehensive comparison of their performance is presented across five widely utilized datasets: KTH-TIPS2-b, FMD, GTOS-Mobile, DTD, and Soil. This comparative analysis highlights the strengths of different architectures and informs future choices in model selection for texture recognition.An architecture for texture recognition is proposed, effectively aggregating data representations from the extracted feature maps of various pre-trained vision models without the need for fine-tuning. In particular, we use features from the early layers of the vision architectures where the information is rich in visual cues (i.e., pixel-like), from the mid layer where geometric cues are more present, and from the output where the features are the most semantic rich. Such combination of different layer features can boost the need for more dedicated texture-related features. The proposed architecture also incorporates a trainable MLP as the classification head, with optimal hyperparameters determined through an exhaustive grid search on one of the selected datasets.The proposed architecture demonstrates the highest accuracy across four texture benchmarks, outperforming all state-of-the-art methods built on CNN backbones. The superiority holds not only against models with substantially fewer parameters, such as ResNet18 (12 M) and ResNet50 (26 M), but also against models with comparable computational complexity to ours (197 M), such as ConvNeXt-L (198 M), as well as more resource-intensive models like ConvNeXt-L-22k (198 M) and ConvNeXt-XL-22k (350 M). Additionally, insights into the model‚Äôs behavior are provided through confusion matrices, along with an ablation study examining the impact of different architectural components, further clarifying the advantages of the proposed method in texture recognition. The rest of the paper is organized as follows.Section 2offers an overview of related works in the field of texture recognition.Section 3outlines the proposed architecture and its implementation. InSection 4, the utilized datasets, experimental protocols, and the model selection process are discussed, along with comparisons to state-of-the-art methods. This section also presents confusion matrices and an ablation study on different architectural components. It also addresses the limitations of the proposed work. Finally,Section 5summarizes the key findings and contributions of the paper.",
            "2. Related Works": "Traditional methods for texture recognition have predominantly relied on manually crafted features designed to maintain invariance to various factors, such as scale, illumination, and translation. Among the earliest and most influential approaches is the Gray-Level Co-occurrence Matrix (GLCM) [12], which captures second-order statistical dependencies between pixel intensities to describe texture contrast, homogeneity, and entropy. Several prominent techniques for texture analysis include the Rotation Invariant Feature Transform (RIFT) [13], which utilizes histograms of gradient orientations, and the Bag-of-Visual-Words (BoVW) framework [13], which aggregates local texture patches into visual words. The Vector of Locally Aggregated Descriptors (VLAD) [14] and the Scale Invariant Feature Transform (SIFT) [15] are also widely used, with SIFT focusing on detecting local extrema in scale-space. Another classical family of methods employs Gabor Filter Banks (GFB) [16], which model the human visual system‚Äôs response to texture by analyzing images at multiple scales and orientations through band-pass filtering. Other notable methods include Local Binary Patterns (LBP) [17], which ensure invariance to grayscale and rotation. Recently, there has been a significant increase in the adoption of CNNs for texture feature extraction, based on their success in computer vision. This trend reflects the wider shift toward deep learning techniques in texture analysis. Notable algorithms include the Locality-Sensitive Coding Network (LSCNet) [18], the Deep Encoding Pooling Network (DEPNet) [19], and the Deep Texture Encoding Network (DeepTEN) [20]. These approaches have enhanced texture recognition by incorporating learned encodings into CNN architectures, resulting in improved performance across diverse datasets. Zhai et al. [6] developed MAPNet, a model for learning visual attributes in texture recognition that employs a multi-branch architecture for iterative attribute learning and spatially-adaptive GAP for feature aggregation. In their follow-up study [5], they introduced DSRNet, which includes a dependency learning module to identify spatial relationships among texture primitives and extract structural details. Peeples et al. [21] introduced HistRes, a texture classification network that combines traditional histogram features with deep learning techniques. By substituting the GAP layer, HistRes improves accuracy by directly extracting histogram information from the output feature map. Xu et al. [22] developed FENet, which employs hierarchical fractal analysis to identify the fractal characteristics of spatial arrangements found in CNN feature maps. Mao et al. [23] utilized the deep Residual Pooling Network (RPNet) for texture recognition, combining a residual encoding module that preserves spatial details with an aggregation module to generate orderless features. Chen et al. [7] introduced CLASSNet, a CNN module that employs Cross-Layer Aggregation to capture statistical self-similarity in texture images. This is achieved through a unique feature aggregation method using a differential box-counting pooling layer. Zhai et al. [24] introduced the Multiple Primitives and Attributes Perception (MPAP) network, which extracts features by combining bottom-up structural information with top-down attribute relationships in a cohesive multi-branch framework. Chen et al. [25] developed the Deep Tracing Pattern encoding Network (DTPNet), which analyzes feature maps from various backbone layers. This network encodes local patches using binary codes and combines them into a histogram-based global feature representation. Scabini et al. [26] developed Random encoding of Aggregated Deep Activation Maps (RADAM), a texture recognition method that uses a Randomized Autoencoder (RAE) to encode outputs from various depths of a pre-trained backbone. This approach generates a feature representation for a linear Support Vector Machine (SVM) without requiring backbone fine-tuning. Florindo et al. [27] proposed a fractal-based pooling method to enhance CNNs for texture recognition using differential box-counting fractal dimension at the last convolutional layer instead of traditional average pooling. Despite the superior robustness of ViT architectures compared to CNNs, especially in the presence of noise or image augmentation [28], their use in texture recognition remains limited. A comparative study on various Vision Transformers for feature extraction in texture analysis is detailed in [29]. This study evaluates the effectiveness of pretrained transformer architectures, using k-Nearest Neighbors (kNN), Linear Discriminant Analysis (LDA), and SVM for classification, achieving promising results. However, it does not address the utilization of intermediate feature maps extracted from the transformers or the possibility of using a MLP as a classification head, indicating a gap in the existing research. In the comparison of various backbones on ImageNet-1K classification presented in [30], the Swin Transformer achieved higher accuracy than the ViT. Building on these insights, the current work aims to explore various vision architectures for texture recognition, identify the most effective ones, and propose a method for combining features from different feature maps to effectively encode them for training a standard MLP as the classification head. Although various strategies for multi-layer feature fusion have been proposed in the literature (e.g., [31,32]), we show that even this simple approach of concatenating features from selected layers and training a standard MLP achieves competitive or superior performance.",
            "3. Proposed Method": "3.1. Overview of the Proposed MethodA block diagram illustrating the proposed method is shown inFigure 1. The core idea of the approach is to extract features from four intermediate layers corresponding to different stages of a transformer architecture. This allows the model to leverage multi-level contextual information, capturing both early-stage low-level features and deeper high-level representations. Incorporating features with varying contextual richness introduces diversity, enabling even the less semantically rich early features to contribute meaningful information.Figure 1.The figure illustrates the block diagram of the proposed architecture. Feature extraction is carried out using a pre-trained vision backbone, from which representations are collected at early, intermediate, and final stages. GAP is then applied to each feature map, and the resulting representations are concatenated into a single feature vector, which is subsequently fed into the classifier. The final Classification step includes a MLP layer withLhidden units, Batch Normalization, ReLU activation, and Dropout at a rate ofp. The parametersLandpare set to 64 and 0.2, respectively, as determined by a grid search detailed inSection 4.4.Each extracted feature undergoes GAP to reduce channel dimensionality and generate compact descriptors. These pooled features, which summarize information at different levels of abstraction, are then concatenated into a unified feature vector. To emphasize the discriminative power of the multi-stage representations themselves, the concatenated feature vector is passed through a lightweight MLP classifier. The MLP is intentionally chosen for its simplicity, ensuring that improvements in recognition performance are attributed to the proposed feature pooling and fusion strategy, rather than to additional complexity in the classifier.This design enables the model to exploit complementary information across transformer layers, resulting in superior performance for texture recognition tasks while preserving architectural clarity and interpretability.It generalizes across various transformer-based as well as non-transformer based architectures. Many of the evaluated transformer models (e.g., Swin Transformer, MaxViT) are organized into four stages, which motivates the selection of four feature extraction points as depicted on the figure.Each stage (Stage 1 to Stage 4) contains a specific number of transformer blocks, denoted asùëÅùëèùëñ,ùëñ=1,...,4Nbi,i=1,...,4, which varies depending on the chosen architecture. The application of GAP is also architecture-dependent, as some intermediate features are 2D spatial maps, while others are flattened sequences, requiring different pooling strategies. Description of each selected vision architectures and its adaptation to the proposed architecture follows in the next sections. 3.2. Swin TransformerIn our experiments, we evaluate the proposed architecture (Section 3.1) on the Swin Transformer [30]. Swin is a hierarchical vision transformer with four stages of windowed self-attention and progressively reduced spatial resolution via Patch Partition (stage 1) and Patch Merging (stages 2‚Äì4). Given an RGB input of sizeùêª√óùëä√ó3H√óW√ó3, stage-wise feature maps have the canonical shapes:ùêπ1ùêπ3‚àà‚Ñùùêª4√óùëä4√óùê∂,‚àà‚Ñùùêª16√óùëä16√ó4ùê∂,ùêπ2ùêπ4‚àà‚Ñùùêª8√óùëä8√ó2ùê∂,‚àà‚Ñùùêª32√óùëä32√ó8ùê∂,F1‚ààRH4√óW4√óC,F2‚ààRH8√óW8√ó2C,F3‚ààRH16√óW16√ó4C,F4‚ààRH32√óW32√ó8C,whereùê∂‚àà{96,96,128,192}C‚àà{96,96,128,192}for Swin-T/S/B/L, respectively.Each stage contains multiple Swin Transformer blocks, where each block consists of Layer Normalization (LN), Window-based Multi-head Self-Attention (W-MSA), Shifted Window-based Multi-head Self-Attention (SW-MSA), a MLP, and residual connections [30]. The first stage uses a linear embedding after patch partition, while stages 2‚Äì4 downsample via patch merging [30].In our implementation, we tap the outputs at the end of each stage (i.e., after stages 1‚Äì4). These tensors are denoted by{ùêπùëñ}4ùëñ=1{Fi}i=14and have shapes given above. GAP is applied channel-wise to eachùêπùëñFi, producing pooled vectorsùëìùëñ‚àà‚Ñùùëöùëñùê∂fi‚ààRmiCwithùëö1=1m1=1,ùëö2=2m2=2,ùëö3=4m3=4, andùëö4=8m4=8. Concatenating these vectors yields:ùëìconcat=[ùëì1,ùëì2,ùëì3,ùëì4]‚àà‚Ñù‚àë4ùëñ=1ùëöùëñùê∂=‚Ñù15ùê∂,fconcat=[f1,f2,f3,f4]‚ààR‚àëi=14miC=R15C,(1)which is fed into the MLP classifier described inSection 3.1. In the conducted experiments we use the Swin-L model withùê∂=192C=192. 3.3. Swin V2 TransformerSwinV2 [33] retains the four-stage hierarchical structure of Swin but replaces dot-product attention with Windowed Scaled Cosine Attention (W-SCA) and its variant Shifted Windowed Scaled Cosine Attention (SW-SCA), and further introduces log-spaced continuous relative position bias and post-normalization. These modifications improve training stability and scalability to larger model sizes. Each stage consists of repeated SwinV2 blocks (LN‚ÄìW-SCA/SW-SCA‚ÄìMLP with residual connections); stage 1 begins with a linear embedding after patch partition, and stages 2‚Äì4 downsample via patch merging.For feature aggregation we apply the same pooling and concatenation strategy described inSection 3.2, i.e., global average pooling of all stage outputs followed by concatenation into a15ùê∂15C-dimensional vector. In our experiments we use the SwinV2-B model withùê∂=128C=128, which yields a concatenated feature dimension of 1920. 3.4. ConvNeXtConvNeXt [9] is a modernized CNN that mirrors the hierarchical four-stage design of Swin-like transformers while remaining purely convolutional. Each stage stacks ConvNeXt blocks composed of depthwise convolutions, Layer Normalization (channel-last), a pointwise convolutional feed-forward with GELU activations, and residual connections. Architectural choices such as larger depthwise kernels, LayerNorm in place of BatchNorm, and reduced nonlinearity budget align ConvNeXt with transformer-era design while preserving CNN efficiency.We apply the same pooling-and-concatenation strategy as in Equation (1): global average pooling is applied to the outputs of all four stages, and the pooled vectors are concatenated into a15ùê∂15C-dimensional descriptor fed to the MLP head. In the conducted experiments we use ConvNeXt-Base (ùê∂=128C=128), resulting in a concatenated feature of dimension15ùê∂=192015C=1920. 3.5. MaxViTMaxViT [34] introduces a unified hierarchical architecture that integrates multi-axis attention with depthwise convolution in a stage-based design. An input image is first tokenized by a convolutional stem into non-overlapping patches, which form the input to the first stage. Subsequent stages progressively downsample the spatial resolution by a factor of 2 while expanding the channel dimension. Each stage consists of multiple MaxViT blocks, which combine Mobile Inverted Bottleneck Convolution (MBConv) with block-wise and grid-wise Multi-Head Self-Attention (MHSA), enabling both local and global context modeling.As with the previous backbones, we apply global average ppoling to the outputs of all four stages and concatenate the resulting vectors as in Equation (1). In our experiments we use MaxViT-Base withùê∂=128C=128, yielding a concatenated feature vector of size15ùê∂=192015C=1920, which is fed to the proposed classification head. 3.6. MambaOutMambaOut [35] is a hierarchical vision model composed of four stages. Except for the first, all stages apply downsampling together with channel expansion, followed by variable number of gated CNN blocks. Unlike the original Mamba [36], which integrates a State-Space Model (SSM) as a token mixer within the blocks, MambaOut discards SSMs and relies entirely on the gated CNNs, motivated by their strong performance on image classification. Specifically, each gated CNN block expands channels to split the feature map into a signal and a gate branch, applies a depthwise convolution to the signal, and recombines them through gated modulation.For feature aggregation, we follow the same GAP-based strategy as with the other backbones: global average pooling is applied to intermediate feature maps at three progressively deeper points in the network, yielding pooled vectors of size{2ùê∂,4ùê∂,6ùê∂}={256,512,768}{2C,4C,6C}={256,512,768}. Concatenation produces a descriptor of total dimension256+512+768=1536,256+512+768=1536,which serves as input to the MLP classifier. 3.7. DeiT3DeiT3 [11] is a ViT-style encoder: images are divided intoùëÅ=(ùêª/ùëÉ√óùëä/ùëÉ)N=(H/P√óW/P)patches, each linearly projected to widthC, with a learnable class token prepended. The sequence is processed byLTransformer blocks (LN‚ÄìMHSA‚ÄìMLP with residuals), augmented by stability refinements such as post-norm, LayerScale, and stochastic depth.For feature aggregation, we tap four increasingly deeper blocks, apply global average pooling over the patch tokens (excluding the class token), and obtain vectorsùëìùëò‚àà‚Ñùùê∂fk‚ààRC(ùëò=1‚Ä¶4k=1‚Ä¶4). The selection of four blocks was made to provide approximately uniform coverage across the network depth, capturing features from early, intermediate, and deep layers. This design choice maintains consistency with hierarchical architectures such as Swin, which naturally comprise four stages, and enables a fair comparison across different backbone types. A broader discussion on layer-selection strategies for non-hierarchical ViT-based models is provided inSection 4.5. Concatenation formsùëìconcat=[ùëì1,ùëì2,ùëì3,ùëì4]‚àà‚Ñù4ùê∂,fconcat=[f1,f2,f3,f4]‚ààR4C,which is passed to the MLP head. With DeiT3-Base (ùê∂=768C=768,ùêø=12L=12), this results in4ùê∂=30724C=3072-dimensional descriptors. 3.8. BEiTv2BEiTv2 [37] adopts the same ViT backbone design: an input is patch-embedded into a sequence ofNtokens of widthC, a class token is prepended, and the sequence is processed byLTransformer blocks (LN‚ÄìMHSA‚ÄìMLP with residuals). Unlike hierarchical backbones, the token resolution is fixed throughout.As with DeiT3, we extract outputs from four layers, pool over the token dimension to obtainùëìùëò‚àà‚Ñùùê∂fk‚ààRC, and concatenate them intoùëìconcat=[,ùëì1,ùëì2,ùëì3,ùëì4,]‚àà‚Ñù4ùê∂fconcat=[,f1,f2,f3,f4,]‚ààR4C. The choice of four layers balances the need to capture low-, mid-, and high-level features for discriminative texture recognition while avoiding unnecessary computational overhead. Using fewer than four layers was found insufficient to encode the full spectrum of relevant information, whereas using more layers yielded only marginal improvements. The rationale behind this layer-count choice and a systematic evaluation of alternative layer combinations are discussed inSection 4.5. Using the BEiTv2-Base configuration (ùê∂=768C=768,ùêø=12L=12), the aggregated feature has dimension4ùê∂=30724C=3072.",
            "3.1. Overview of the Proposed Method": "A block diagram illustrating the proposed method is shown inFigure 1. The core idea of the approach is to extract features from four intermediate layers corresponding to different stages of a transformer architecture. This allows the model to leverage multi-level contextual information, capturing both early-stage low-level features and deeper high-level representations. Incorporating features with varying contextual richness introduces diversity, enabling even the less semantically rich early features to contribute meaningful information. Figure 1.The figure illustrates the block diagram of the proposed architecture. Feature extraction is carried out using a pre-trained vision backbone, from which representations are collected at early, intermediate, and final stages. GAP is then applied to each feature map, and the resulting representations are concatenated into a single feature vector, which is subsequently fed into the classifier. The final Classification step includes a MLP layer withLhidden units, Batch Normalization, ReLU activation, and Dropout at a rate ofp. The parametersLandpare set to 64 and 0.2, respectively, as determined by a grid search detailed inSection 4.4. Each extracted feature undergoes GAP to reduce channel dimensionality and generate compact descriptors. These pooled features, which summarize information at different levels of abstraction, are then concatenated into a unified feature vector. To emphasize the discriminative power of the multi-stage representations themselves, the concatenated feature vector is passed through a lightweight MLP classifier. The MLP is intentionally chosen for its simplicity, ensuring that improvements in recognition performance are attributed to the proposed feature pooling and fusion strategy, rather than to additional complexity in the classifier. This design enables the model to exploit complementary information across transformer layers, resulting in superior performance for texture recognition tasks while preserving architectural clarity and interpretability. It generalizes across various transformer-based as well as non-transformer based architectures. Many of the evaluated transformer models (e.g., Swin Transformer, MaxViT) are organized into four stages, which motivates the selection of four feature extraction points as depicted on the figure. Each stage (Stage 1 to Stage 4) contains a specific number of transformer blocks, denoted asùëÅùëèùëñ,ùëñ=1,...,4Nbi,i=1,...,4, which varies depending on the chosen architecture. The application of GAP is also architecture-dependent, as some intermediate features are 2D spatial maps, while others are flattened sequences, requiring different pooling strategies. Description of each selected vision architectures and its adaptation to the proposed architecture follows in the next sections.",
            "3.2. Swin Transformer": "In our experiments, we evaluate the proposed architecture (Section 3.1) on the Swin Transformer [30]. Swin is a hierarchical vision transformer with four stages of windowed self-attention and progressively reduced spatial resolution via Patch Partition (stage 1) and Patch Merging (stages 2‚Äì4). Given an RGB input of sizeùêª√óùëä√ó3H√óW√ó3, stage-wise feature maps have the canonical shapes:ùêπ1ùêπ3‚àà‚Ñùùêª4√óùëä4√óùê∂,‚àà‚Ñùùêª16√óùëä16√ó4ùê∂,ùêπ2ùêπ4‚àà‚Ñùùêª8√óùëä8√ó2ùê∂,‚àà‚Ñùùêª32√óùëä32√ó8ùê∂,F1‚ààRH4√óW4√óC,F2‚ààRH8√óW8√ó2C,F3‚ààRH16√óW16√ó4C,F4‚ààRH32√óW32√ó8C,whereùê∂‚àà{96,96,128,192}C‚àà{96,96,128,192}for Swin-T/S/B/L, respectively. Each stage contains multiple Swin Transformer blocks, where each block consists of Layer Normalization (LN), Window-based Multi-head Self-Attention (W-MSA), Shifted Window-based Multi-head Self-Attention (SW-MSA), a MLP, and residual connections [30]. The first stage uses a linear embedding after patch partition, while stages 2‚Äì4 downsample via patch merging [30]. In our implementation, we tap the outputs at the end of each stage (i.e., after stages 1‚Äì4). These tensors are denoted by{ùêπùëñ}4ùëñ=1{Fi}i=14and have shapes given above. GAP is applied channel-wise to eachùêπùëñFi, producing pooled vectorsùëìùëñ‚àà‚Ñùùëöùëñùê∂fi‚ààRmiCwithùëö1=1m1=1,ùëö2=2m2=2,ùëö3=4m3=4, andùëö4=8m4=8. Concatenating these vectors yields:ùëìconcat=[ùëì1,ùëì2,ùëì3,ùëì4]‚àà‚Ñù‚àë4ùëñ=1ùëöùëñùê∂=‚Ñù15ùê∂,fconcat=[f1,f2,f3,f4]‚ààR‚àëi=14miC=R15C,(1)which is fed into the MLP classifier described inSection 3.1. In the conducted experiments we use the Swin-L model withùê∂=192C=192.",
            "3.3. Swin V2 Transformer": "SwinV2 [33] retains the four-stage hierarchical structure of Swin but replaces dot-product attention with Windowed Scaled Cosine Attention (W-SCA) and its variant Shifted Windowed Scaled Cosine Attention (SW-SCA), and further introduces log-spaced continuous relative position bias and post-normalization. These modifications improve training stability and scalability to larger model sizes. Each stage consists of repeated SwinV2 blocks (LN‚ÄìW-SCA/SW-SCA‚ÄìMLP with residual connections); stage 1 begins with a linear embedding after patch partition, and stages 2‚Äì4 downsample via patch merging. For feature aggregation we apply the same pooling and concatenation strategy described inSection 3.2, i.e., global average pooling of all stage outputs followed by concatenation into a15ùê∂15C-dimensional vector. In our experiments we use the SwinV2-B model withùê∂=128C=128, which yields a concatenated feature dimension of 1920.",
            "3.4. ConvNeXt": "ConvNeXt [9] is a modernized CNN that mirrors the hierarchical four-stage design of Swin-like transformers while remaining purely convolutional. Each stage stacks ConvNeXt blocks composed of depthwise convolutions, Layer Normalization (channel-last), a pointwise convolutional feed-forward with GELU activations, and residual connections. Architectural choices such as larger depthwise kernels, LayerNorm in place of BatchNorm, and reduced nonlinearity budget align ConvNeXt with transformer-era design while preserving CNN efficiency. We apply the same pooling-and-concatenation strategy as in Equation (1): global average pooling is applied to the outputs of all four stages, and the pooled vectors are concatenated into a15ùê∂15C-dimensional descriptor fed to the MLP head. In the conducted experiments we use ConvNeXt-Base (ùê∂=128C=128), resulting in a concatenated feature of dimension15ùê∂=192015C=1920.",
            "3.5. MaxViT": "MaxViT [34] introduces a unified hierarchical architecture that integrates multi-axis attention with depthwise convolution in a stage-based design. An input image is first tokenized by a convolutional stem into non-overlapping patches, which form the input to the first stage. Subsequent stages progressively downsample the spatial resolution by a factor of 2 while expanding the channel dimension. Each stage consists of multiple MaxViT blocks, which combine Mobile Inverted Bottleneck Convolution (MBConv) with block-wise and grid-wise Multi-Head Self-Attention (MHSA), enabling both local and global context modeling. As with the previous backbones, we apply global average ppoling to the outputs of all four stages and concatenate the resulting vectors as in Equation (1). In our experiments we use MaxViT-Base withùê∂=128C=128, yielding a concatenated feature vector of size15ùê∂=192015C=1920, which is fed to the proposed classification head.",
            "3.6. MambaOut": "MambaOut [35] is a hierarchical vision model composed of four stages. Except for the first, all stages apply downsampling together with channel expansion, followed by variable number of gated CNN blocks. Unlike the original Mamba [36], which integrates a State-Space Model (SSM) as a token mixer within the blocks, MambaOut discards SSMs and relies entirely on the gated CNNs, motivated by their strong performance on image classification. Specifically, each gated CNN block expands channels to split the feature map into a signal and a gate branch, applies a depthwise convolution to the signal, and recombines them through gated modulation. For feature aggregation, we follow the same GAP-based strategy as with the other backbones: global average pooling is applied to intermediate feature maps at three progressively deeper points in the network, yielding pooled vectors of size{2ùê∂,4ùê∂,6ùê∂}={256,512,768}{2C,4C,6C}={256,512,768}. Concatenation produces a descriptor of total dimension256+512+768=1536,256+512+768=1536,which serves as input to the MLP classifier.",
            "3.7. DeiT3": "DeiT3 [11] is a ViT-style encoder: images are divided intoùëÅ=(ùêª/ùëÉ√óùëä/ùëÉ)N=(H/P√óW/P)patches, each linearly projected to widthC, with a learnable class token prepended. The sequence is processed byLTransformer blocks (LN‚ÄìMHSA‚ÄìMLP with residuals), augmented by stability refinements such as post-norm, LayerScale, and stochastic depth. For feature aggregation, we tap four increasingly deeper blocks, apply global average pooling over the patch tokens (excluding the class token), and obtain vectorsùëìùëò‚àà‚Ñùùê∂fk‚ààRC(ùëò=1‚Ä¶4k=1‚Ä¶4). The selection of four blocks was made to provide approximately uniform coverage across the network depth, capturing features from early, intermediate, and deep layers. This design choice maintains consistency with hierarchical architectures such as Swin, which naturally comprise four stages, and enables a fair comparison across different backbone types. A broader discussion on layer-selection strategies for non-hierarchical ViT-based models is provided inSection 4.5. Concatenation formsùëìconcat=[ùëì1,ùëì2,ùëì3,ùëì4]‚àà‚Ñù4ùê∂,fconcat=[f1,f2,f3,f4]‚ààR4C,which is passed to the MLP head. With DeiT3-Base (ùê∂=768C=768,ùêø=12L=12), this results in4ùê∂=30724C=3072-dimensional descriptors.",
            "3.8. BEiTv2": "BEiTv2 [37] adopts the same ViT backbone design: an input is patch-embedded into a sequence ofNtokens of widthC, a class token is prepended, and the sequence is processed byLTransformer blocks (LN‚ÄìMHSA‚ÄìMLP with residuals). Unlike hierarchical backbones, the token resolution is fixed throughout. As with DeiT3, we extract outputs from four layers, pool over the token dimension to obtainùëìùëò‚àà‚Ñùùê∂fk‚ààRC, and concatenate them intoùëìconcat=[,ùëì1,ùëì2,ùëì3,ùëì4,]‚àà‚Ñù4ùê∂fconcat=[,f1,f2,f3,f4,]‚ààR4C. The choice of four layers balances the need to capture low-, mid-, and high-level features for discriminative texture recognition while avoiding unnecessary computational overhead. Using fewer than four layers was found insufficient to encode the full spectrum of relevant information, whereas using more layers yielded only marginal improvements. The rationale behind this layer-count choice and a systematic evaluation of alternative layer combinations are discussed inSection 4.5. Using the BEiTv2-Base configuration (ùê∂=768C=768,ùêø=12L=12), the aggregated feature has dimension4ùê∂=30724C=3072.",
            "4. Experiments": "This section presents the experimental evaluation of the proposed method for texture recognition. We begin by describing the implementation details, including benchmark datasets used for evaluation (Section 4.1) and training setup (Section 4.2). Next, we detail the model size selection procedure for hierarchical architectures such as Swin (Section 4.3), followed by the determination of optimal MLP hyperparameters through grid search (Section 4.4), which are considered representative for all models. For non-hierarchical transformer models, a systematic analysis of layer selection is conducted to identify the most effective combination of features for aggregation (Section 4.5). Subsequently, the performance of the proposed method, including feature aggregation and MLP, is compared against baseline models (i.e., using only the backbone with its final-layer features fed into a classification layer) (Section 4.6). Finally, the model considered most effective is evaluated with the proposed architecture against state-of-the-art approaches (Section 4.7). 4.1. DatasetsEvaluation is conducted on five benchmark datasets that capture different forms of textural information. Three of them, KTH TIPS2 b, the Flickr Material Dataset (FMD), and GTOS Mobile, represent material or object centered images that nevertheless exhibit distinctive texture patterns. The remaining two datasets, the Describable Textures Dataset (DTD) and the Soil dataset, contain attribute level texture categories. For the first four datasets, we follow the evaluation protocols described in [26] to ensure full comparability with previous works, while for the Soil dataset we apply the protocol specified in [38]. Below is a brief description of each dataset together with the corresponding training protocol.KTH-TIPS2-b[39]: Comprises 4752 images from 11 material categories, including ‚Äòaluminium_foil‚Äô, ‚Äòbrown_bread‚Äô, ‚Äòcorduroy‚Äô, ‚Äòwool‚Äô, etc. Evaluation follows a 4-fold cross-validation protocol with fixed splits, as in [24].FMD[40]: Contains 1000 images distributed across 10 categories such as ‚Äòfabric‚Äô, ‚Äòglass‚Äô, ‚Äòleather‚Äô, and ‚Äòwood‚Äô. Performance is evaluated using 10 repetitions of 10-fold cross-validation.GTOS-Mobile[19]: Includes 100,011 mobile phone images of 31 outdoor ground material categories such as ‚Äòasphalt‚Äô, ‚Äògrass‚Äô, ‚Äòpebble‚Äô, and ‚Äòsand‚Äô. The dataset provides a predefined single train/test split, consistent with prior work [5,6,7,25,41].DTD[42]: Comprises 5640 images labeled with 47 describable texture attributes (e.g., ‚Äòbumpy‚Äô, ‚Äòstriped‚Äô, ‚Äôdotted‚Äô). Following [26], evaluation is performed using the standard protocol of 10 random train/validation/test splits, and results are averaged over these runs.Augmented Soil (Soil)[38]: This dataset originally contains 829 soil images grouped into seven classes (for example ‚ÄòAlluvial Soil‚Äô, ‚ÄòBlack Soil‚Äô, ‚ÄòLaterite Soil‚Äô and others). The dataset is partitioned into 70% training, 15% validation, and 15% testing subsets. Following [38], the training set is augmented and expanded to 3316 images, while the validation and test sets remain at 182 and 178 images respectively. This corresponds to the augmented version of the Soil dataset introduced in [38]. For brevity, in the remainder of the manuscript we refer to this augmented variant simply as ‚ÄúSoil‚Äù. This dataset serves as an application-oriented benchmark and is used in the development of an AI-based tool that supports farmers in soil identification and crop selection by incorporating geological and environmental information.Figure 2illustrates challenging cases from GTOS-Mobile, where similar categories such as ‚Äôbrick‚Äô and ‚Äômetal_cover‚Äô may be confused due to visual overlap [43]. The selection of the Swin Transformer variant (Section 4.3) and the hyper-parameters for the MLP (L,p) determined on KTH-TIPS2-b (Section 4.4) are reused for FMD, GTOS-Mobile, DTD, and Soil.Figure 2.Example images from GTOS-Mobile. 4.2. Implementation DetailsThe proposed algorithm is implemented using PyTorch-GPU (v2.5.1) [44] on a system equipped with an Intel Xeon E5-2640 v3 CPU (Intel Corporation, Santa Clara, CA, USA; 2.60 GHz, 8 cores), 32 GB of RAM (Kingston, NY, USA), and an NVIDIA GeForce RTX 2080 Ti GPU (NVIDIA Corporation, Santa Clara, CA, USA). Backbone architectures are initialized with pre-trained weights from the PyTorch Image Models library ‚Äútimm‚Äù (v1.0.15) [45]. A batch size of 32 is used throughout training. The architecture is trained for 100 epochs using Adaptive Moment Estimation (ADAM) with momentum0.90.9and weight decay1√ó10‚àí41√ó10‚àí4, optimizing the cross-entropy loss. The learning rate is set to1√ó10‚àí31√ó10‚àí3.To ensure deterministic and fully reproducible experiments, all runs were executed using a fixed global random seed of 42, which serves as the base seed and is applied consistently across Python (v3.10.16), NumPy (v2.0.1), and PyTorch (v2.5.1) CPU and GPU operations. CUDA deterministic execution was enabled, benchmark mode was disabled, and deterministic algorithms were enforced. These settings guarantee that model initialization, data loading order, and all stochastic operations (e.g., shuffling and augmentation) remain fully reproducible.Regarding the dataset splits and the preparation of training, validation, and test sets, the following procedures were applied to ensure full reproducibility across all experiments:KTH-TIPS2-b [39], GTOS-Mobile [19], and DTD [42]: As mentioned inSection 4.1, the dataset splits for training, validation, and testing are used as provided by the original sources, with no additional random partitioning or fold generation applied.FMD [40]: Following the widely adopted protocol in [26], performance is evaluated using 10 repetitions of 10-fold cross-validation. For each repetition, a new deterministic seed is derived from the base seed by multiplying it with the repetition index. This seed is then used to initialize all random number generators, including Python (v3.10.16), NumPy (v2.0.1), PyTorch (v2.5.1) CPU and GPU before generating the fold assignment. This procedure ensures that each repetition has a unique fold configuration while maintaining full reproducibility.Soil [38]: As mentioned inSection 4.1, we use the augmented version of the Soil dataset provided by the authors [38], in which the training set is expanded via standard augmentation techniques while the validation and test sets remain fixed. The code used for splitting the data and performing the training set augmentation is available in the authors‚Äô repository [46].All preprocessing steps are applied consistently across datasets. For four of the datasets (KTH-TIPS2-b, FMD, GTOS-Mobile, and DTD), each input image is resized to224√ó224224√ó224pixels. For the Soil dataset, each training image is randomly cropped and resized to the same resolution, following the official preprocessing procedure provided by the dataset authors [46]. Finally, for all datasets, images are normalized using channel-wise mean values of (0.485, 0.456, 0.406) and standard deviations of (0.229, 0.224, 0.225).Regarding the backbone specification used in our experiments, we provide a detailed description of the pretrained checkpoints and the exact intermediate layers from which features are extracted. The following pretrained models from [45] are employed: Swin (‚Äòswin_large_patch4_window7_224‚Äô), SwinV2 (‚Äòswinv2_base_window12_192.ms_in22k‚Äô), ConvNeXt (‚Äòconvnext_base_in22ft1k‚Äô), MaxViT (‚Äòmaxvit_large_tf_224.in1k‚Äô), MambaOut (‚Äòmambaout_base_wide_rw‚Äô), DeiT3 (‚Äòdeit3_base_patch16_224.fb_in22k_ft_in1k‚Äô), and BEiTv2 (‚Äòbeitv2_base_patch16_224.in1k_ft_in22k_in1k‚Äô).For hierarchical architectures (Swin, SwinV2, ConvNeXt, MaxViT, MambaOut), the four intermediate layers available for these backbones are used, while for the remaining backbones (DeiT3 and BEiTv2), features are extracted from layers 2, 5, 8, and 11 according to the layer selection analysis presented inSection 4.5. 4.3. Model Size SelectionModel size selection is considered a preliminary step with the goal of selecting the size of the model that performs the best. From all architectures considered in this work, we select the Swin Transformer as the backbone for model size evaluation in texture classification tasks. Unlike DeiT3 and BEiTv2, Swin employs a hierarchical architecture with shifted-window self-attention. This design provides an effective trade-off between local feature modeling and global context aggregation, and it has demonstrated strong performance across various vision benchmarks while maintaining computational efficiency and scalability. It is also the earliest vision architecture compared to the remaining considered in this work. The configuration for model size evaluation is a Swin transformer for feature extraction, without its classification head. The classification layer is replaced with a set of units corresponding to the dataset‚Äôs classes, which are randomly initialized and fully trained for 150 epochs. The other training parameters are the same as those described inSection 4.2. For this analysis, only the KTH-TIPS2-b dataset is considered.Four primary Swin transformer architectures are commonly recognized for model selection: Swin-T, Swin-S, Swin-B, and Swin-L. Six variants from timm (v1.0.15) [45] were examined: Swin-T, Swin-S (pretrained on ImageNet-1k), Swin-B, Swin-L (pretrained on ImageNet-22k and fine-tuned on ImageNet-1k), as well as Swin-B-22k and Swin-L-22k (both pretrained on ImageNet-22k). The accuracy results, along with the number of parameters and Giga Multiply-Accumulate Operations (GMACs) for each backbone in this investigation, are summarized inTable 1.Table 1.Performance in terms of accuracy (%) of all Swin Transformer variants tested independently (with only the classification layer of the head replaced) on the KTH-TIPS2-b dataset. The best-performing backbone, along with its number of parameters, GMACs, and accuracy are highlighted in bold.The highest accuracy of 93.4% is achieved by the Swin-L model pretrained on ImageNet-22k and fine-tuned on ImageNet-1k, making it the preferred choice for the proposed architecture.The sizes of the remaining backbones were chosen to be comparable to that of the Swin model. The complete list of parameter counts and GMACs is as follows:Swin: 195 M parameters, 34.5 GMACs, pretrained on ImageNet-22k and fine-tuned on ImageNet-1k.SwinV2: 228.8 M parameters, 26.2 GMACs, pretrained on ImageNet-22k.ConvNeXt: 197.8 M parameters, 34.4 GMACs, pretrained on ImageNet-22k and fine-tuned on ImageNet-1k.MaxViT: 211.8 M parameters, 43.7 GMACs, pretrained on ImageNet-21k.MambaOut: 94.4 M parameters, 17.8 GMACs, pretrained on ImageNet-1k.DeiT3: 86.6 M parameters, 17.6 GMACs, pretrained on ImageNet-22k and fine-tuned on ImageNet-1k.BEiTv2: 86.5 M parameters, 17.6 GMACs, pretrained on ImageNet-22k and fine-tuned on ImageNet-1k.It should be noted that MambaOut, DeiT3, and BEiTv2 are approximately half the size of Swin. This choice was intentional: we included smaller models to reduce the risk of overestimating performance and instead provide a more conservative estimate. Moreover, for certain architectures, the implementations available in the timm library [45] do not offer variants with parameter counts closely matching the 195 M scale of Swin. 4.4. Grid Search for MLP ParametersUsing the selected Swin transformer model size, a grid search is conducted on the KTH-TIPS2-b dataset (fold 2) to determine the optimal number of hidden unitsLand dropout ratiopfor the classification step of the proposed architecture. The search explores values ofLfrom 32 to 256 in increments of 32, andpfrom 0.1 to 0.9 in increments of 0.1. The resulting plot is shown inFigure 3, showing that the highest accuracy of 92.51% is achieved withùêø=64L=64andùëù=0.2p=0.2. Therefore, these values for the classification head are used for all backbones.Figure 3.3D plot illustrating the grid search for optimal MLP hyper-parameters. 4.5. Analysis of Layer Selection for Non-Hierarchical ModelsNon-hierarchical transformer backbones, such as DeiT3 and BEiTv2, contain substantially more than four internal blocks, which makes the choice of layer sampling strategy non-trivial. To determine a principled configuration for extracting multi-level representations, we perform a dedicated analysis using BEiTv2 as a representative ViT-style architecture with a uniform depth structure. Several candidate layer sets were evaluated, each designed to reflect different assumptions about where texture-relevant information may reside within the transformer stack.The tested configurations include: Equidistant ([3, 6, 9, 12]), Shifted ([2, 5, 8, 11]), Early-heavy ([1, 2, 3, 4]), Late-heavy ([9, 10, 11, 12]), Middle ([4, 5, 6, 7]), Distributed ([1, 4, 7, 10]), and Center+end ([3, 6, 10, 12]). For each set, features from the selected blocks were pooled over the token dimension, concatenated, and used to train the MLP classifier following the parameters described inSection 4.4. The resulting performance for each candidate set on the KTH-TIPS2-b dataset is reported inTable 2, enabling a direct comparison of the effectiveness of the different layer combinations.Table 2.Best classification accuracy (%) on KTH-TIPS2-b for different layer selection strategies in non-hierarchical transformers. The best result is highlighted inbold.The analysis also revealed that sampling fewer than four blocks resulted in insufficient coverage of both local and global patterns, whereas using more than four blocks introduced additional computational overhead with only marginal accuracy gains. Among the tested configurations (seeTable 2), the Shifted set consistently yielded the strongest performance, providing a balanced representation of low-, mid-, and high-level texture cues. Based on this empirical evaluation, the Shifted set ([2, 5, 8, 11]) was selected as the default extraction configuration for the non-hierarchical transformer architectures. 4.6. Results Against BaselineThe proposed architecture was evaluated for each backbone using the datasets described inSection 4.1and following the RADAM protocol [26]. To assess its effectiveness, we compare the proposed method against baseline models that use the same backbone but rely solely on the features extracted from the final output layer. The results are reported inTable 3, where performance is assessed in terms of average classification accuracy (%) and standard deviation.Table 3.Evaluation of the proposed architecture using five datasets against baseline. The baseline using only final layer features is noted as No Aggr., while the proposed method using multi-layer aggregation is noted as Aggr. Accuracy (in %) is reported for all experiments. The better result, comparing No Aggr. with Aggr. for each experiment, is given in bold.For clarity, the table includes both configurations (i) using only the final-layer features of each backbone (denoted No Aggr., or baseline) and (ii) using the proposed feature aggregation from intermediate and final layers (denoted Aggr.). This allows us to directly evaluate the contribution of the aggregation strategy.The analysis of the results reveals that the benefits of feature aggregation are not uniform across backbones and datasets. In particular, the DeiT3 backbone consistently achieves better performance when using only the final-layer features, suggesting that its most descriptive representations for texture recognition are already concentrated at the output stage. In contrast, other backbones exhibit weaker final-layer representations, and thus benefit from the additional information captured in intermediate layers. Nevertheless, the performance of DeiT3 remains lower than that of most other backbones, indicating that although its most descriptive features are concentrated in the final output layer, this representation alone is insufficient compared to the richer feature sets provided by the alternative architectures.This behavior can be attributed to the non-hierarchical structure of DeiT3, which differs fundamentally from the hierarchical organization of architectures such as Swin or ConvNeXt. In DeiT3, most discriminative information is concentrated in the final transformer layers, whereas earlier layers primarily capture low-level patch embeddings with limited semantic value. Consequently, concatenating early and late representations may introduce redundancy or noise, slightly degrading performance. This observation suggests that future research should explore specialized aggregation schemes tailored for non-hierarchical architectures.A further trend can be observed on the GTOS-Mobile dataset, where most backbones perform better without aggregation. This may be attributed to the large size of the dataset combined with high variation and complexity.To draw conclusions regarding the effectiveness of the proposed method (Aggr.) compared to the baseline (No Aggr.), we employ a paired design [47]. Examination of the paired differences reveals the presence of clear outliers-most notably for DeiT3, where the Aggr. configuration consistently underperforms-casting doubt on the assumption of normality. Consequently, we predefine the Wilcoxon signed-rank test (one-sided,ùêª1H1: Feat. Aggr. > baseline) as our primary statistical analysis, given its robustness to both non-normal distributions and the influence of outliers [48]. For transparency, we also report a pairedt-test on the mean difference and a sign test on the direction of change. The results are summarized as follows:Wilcoxon signed-rank:The median improvement is +1.0 percentage point (pp), rank-biserial correlationùëü=0.36r=0.36(moderate). One-sidedùëù=0.041p=0.041(<0.05) meaning that Aggregation improvement is unlikely due to chance.Signed test:25 out of 35 positives, one-sided p=0.0052 (significant).Pairedt-test (mean effect):meanŒî=‚àí0.79Œî=‚àí0.79pp,ùë°(34)=‚àí0.72t(34)=‚àí0.72, one-sidedùëù=0.762p=0.762(not significant). The negative mean is driven by the large, consistent drops for DeiT3 across all five datasets.Using the paired, robust Wilcoxon signed-rank test, the feature aggregation shows a statistically significant typical-case improvement over the baseline across backbones and datasets (median‚âà+1.0‚âà+1.0pp,ùëù=0.041p=0.041). However, the mean improvement is not significant because DeiT3 exhibits large performance decrements with aggregation. Practically, aggregation can be expected to help in most backbone‚Äìdataset combinations, but DeiT3 is a notable exception that warrants configuration checks or exclusion. 4.7. Comparison with State-of-the-Art MethodsTable 4presents the classification accuracy (%) and standard deviation of the proposed architecture compared to several state-of-the-art methods for four of the datasets (KTH-TIPS2-b, FMD, GTOS-Mobile, and DTD).Table 4.Comparison of the proposed architecture‚Äôs performance in terms of accuracy (%) and standard deviation (¬±) against state-of-the-art methods. The backbones are organized into row blocks based on their computational capacity, with their corresponding number of parameters and GMACs. The highest results for each dataset within each block are highlighted in bold, while the overall best results, regardless of the backbone, are underlined. References in square brackets next to each method indicate the original source of the results, and if a second reference is listed, it denotes that the results were taken from the subsequent source.The evaluation protocols used for four of the datasets (KTH-TIPS2-b, FMD, GTOS-Mobile, and DTD) align with those adopted in RADAM [26] and several other studies. For the Soil dataset, the protocol from [38] was followed (see the setup and dataset descriptions inSection 4.1andSection 4.2). The methods inTable 4are grouped by backbone models and organized based on their computational capacity, indicated by the number of parameters and GMACs.The proposed method, using the BEiTv2 backbone, delivers the highest accuracy across four datasets: 96.8% on KTH-TIPS2-b, 96.7% on FMD, 91.8% on GTOS-Mobile, and 94.5% on Soil. However, on the DTD dataset, the accuracy of 82.0% is slightly lower than that of the RADAM ConvNeXt-L-22k (84.0%) and RADAM ConvNeXt-XL-22k (83.7%) models.For the Augmented Soil dataset, to the best of our knowledge, only one article [38] reports several evaluated methods based on fine-tuning. Among these, we compare only with the ViT-B/16 model, which achieved the highest reported accuracy of 91.0% [38]. Our BEiTv2-based method reaches 94.5%, representing a 3.5% improvement. It is also worth noting that the computational characteristics of our model, in terms of GMACs and number of parameters, are very close to those of ViT-B/16, making the comparison balanced and directly relevant.Regarding the other four datasets, it is worth emphasizing that the proposed BEiTv2-based model is considerably more compact, containing 86.5 million parameters and requiring 17.6 GMACs, compared to 229.8 million parameters and 34 GMACs for ConvNeXt-L-22k and 392.9 million parameters and 61 GMACs for ConvNeXt-XL-22k. Despite having roughly two to three times fewer parameters and computational cost, the proposed approach achieves comparable performance on DTD and surpasses all larger models on the other three benchmarks. This demonstrates the efficiency and competitiveness of the proposed feature aggregation strategy, showing that strong texture recognition performance can be achieved using pre-trained architectures, inter-layer feature aggregation, and a lightweight MLP classifier.In comparison, models based on smaller backbones such as ResNet18 and ResNet50 offer reasonable accuracy with significantly lower complexity. For instance, ResNet18-based methods like Fractal Pooling achieve 88.3% on KTH-TIPS2-b, while DTPNet reaches 85.7% on FMD and 87.0% on GTOS-Mobile. On DTD, the best-performing ResNet18 method (MPAP) attains 72.4%, using only 11.7 million parameters and 1.8 GMACs. Similarly, ResNet50-based approaches such as Fractal Pooling and MPAP achieve accuracies between 78.0% and 90.7% across datasets, with 25.6 million parameters and 4.1 GMACs, demonstrating a balance between accuracy and efficiency but still lagging behind the proposed method.While larger RADAM ConvNeXt models show competitive performance, they require substantially more computational resources. For example, RADAM ConvNeXt-L (197.8 M parameters, 34.4 GMACs) achieves 89.3% on KTH-TIPS2-b and FMD, 85.8% on GTOS-Mobile, and 77.4% on DTD, which are lower than the BEiTv2-based results across all datasets. RADAM ConvNeXt-L-22k reaches 91.3%, 95.2%, 87.3%, and 84.0% respectively, outperforming the proposed model only on DTD. Likewise, RADAM ConvNeXt-XL-22k, with 392.9 M parameters and 61 GMACs, attains 94.4%, 95.2%, and 90.2% on KTH-TIPS2-b, FMD, and GTOS-Mobile, while achieving 83.7% on DTD, again higher on this dataset but lower elsewhere.Overall, this comparison highlights that the proposed BEiTv2-based model achieves the best balance between accuracy and efficiency, outperforming all compared methods on four out of five datasets while using significantly fewer parameters and computational cost, although for the Soil dataset the parameter count and computational cost are comparable. 4.8. Detailed ResultsConfusion matrices:The effectiveness of the proposed architecture is further evaluated by using the best-performing backbone, i.e., BEiTv2, analyzing the confusion matrices and corresponding accuracies for the worst-case folds of KTH-TIPS2-b, FMD, and DTD, while GTOS-Mobile and Soil, both having a single test fold, are examined separately (seeFigure 4). In general, the confusion matrices show a strong diagonal pattern, reflecting the algorithm‚Äôs strong classification performance.Figure 4.Confusion matrices and accuracies obtained using the proposed architecture with the BEiTv2 backbone for each dataset. For KTH-TIPS2-b, FMD, and DTD, the results correspond to the lowest-accuracy folds, while for GTOS-Mobile and Soil, results are shown from its single available fold. Darker red indicates higher proportion of predictions in a cell.For the KTH-TIPS2-b dataset, the most frequent confusions are ‚Äòwool‚Äô or ‚Äòcorduroy‚Äô predicted as ‚Äòcotton‚Äô and ‚Äòcotton‚Äô misclassified as ‚Äòlinen‚Äô.In the FMD dataset, misclassifications include ‚Äòleather‚Äô predicted as ‚Äòfabric‚Äô or ‚Äòmetal‚Äô, ‚Äòglass‚Äô as ‚Äòfoliage‚Äô, ‚Äòpaper‚Äô as ‚Äòglass‚Äô, ‚Äòplastic‚Äô as ‚Äòglass‚Äô or ‚Äòmetal‚Äô, ‚Äòleather‚Äô as ‚Äòmetal‚Äô, ‚Äòmetal‚Äô as ‚Äòplastic‚Äô, and ‚Äòmetal‚Äô as ‚Äòplastic‚Äô. These mistakes are largely due to similar visual appearances among the classes.In the GTOS-Mobile dataset the most noticeable misclassifications are ‚Äòstone_cement‚Äô predicted as ‚Äòcement‚Äô, ‚Äòstone_brick‚Äô as ‚Äòbrick‚Äô, ‚Äòsmall_limestone‚Äô, ‚Äòstone_asphalt‚Äô, and ‚Äòstone_brick‚Äô confused with ‚Äòasphalt‚Äô, and ‚Äòpaint_cover‚Äô misclassified as ‚Äòmetal_cover‚Äô. As with the other datasets, most of these errors involve classes with similar characteristics.For the DTD dataset, the most prominent misclassification occurs for the blotchy class, which appears as the lightest (i.e., lowest-accuracy) square along the diagonal of the DTD confusion matrix. Representative errors include, but are not limited to, predicting ‚Äòblotchy‚Äô as ‚Äòstained‚Äô or ‚Äòpitted‚Äô, ‚Äòbanded‚Äô as ‚Äòlined‚Äô, ‚Äòdotted‚Äô as ‚Äòpolka-dotted‚Äô, and ‚Äòswirly‚Äô as ‚Äòspiralled‚Äô.Regarding the Soil dataset, the most notable confusion cases include ‚ÄòYellow Soil‚Äô predicted as ‚ÄòAlluvial Soil‚Äô, as well as Alluvial Soil as ‚ÄòLaterite Soil‚Äô or ‚ÄòMontian Soil‚Äô.Confusion cases:Figure 5presents several confusing cases encountered by the proposed architecture on the KTH-TIPS2-b, FMD, and GTOS-Mobile datasets, illustrating typical misclassifications and their visually similar predicted counterparts.Figure 6provides additional examples of analogous confusion patterns observed in the DTD and Soil datasets, following the same presentation structure and highlighting similar challenges in distinguishing between visually related categories. Most of these pairs of images, which reflect the incorrect classifications, correspond to the highlighted cells in the confusion matrices of the respective datasets previously shown inFigure 4.Figure 5.Examples of confusing material categories from the KTH-TIPS2-b, FMD, and GTOS-Mobile datasets using the proposed architecture with BEiTv2 backbone. The top section displays misclassified samples with their true labels, whereas the bottom section illustrates their incorrect predictions alongside visually similar samples from the predicted class.Figure 6.Examples of confusing material categories from the DTD and Soil datasets using the proposed architecture with a BEiTv2 backbone. The structure and interpretation of the figure follow the same convention as inFigure 5, where the upper section shows misclassified samples with their ground-truth labels, and the lower section presents the corresponding incorrect predictions along with visually similar samples from the predicted classes.Regarding the KTH-TIPS2-b dataset (see the top pairs of images inFigure 5), the most frequent confusions occur between visually similar categories such as ‚Äòwool‚Äô‚Äì‚Äòcotton‚Äô, ‚Äòcotton‚Äô‚Äì‚Äòlinen‚Äô, ‚Äòcorduroy‚Äô‚Äì‚Äòcotton‚Äô, ‚Äòbrown_bread‚Äô‚Äì‚Äòwhite_bread‚Äô, and ‚Äòcracker‚Äô‚Äì‚Äôcork‚Äô. These pairs share overlapping local texture patterns, including uniform weaves, coarse fiber structures, or porous surfaces, which make their appearance highly similar in isolated patches. As a result, the model often relies on subtle surface cues that are difficult to distinguish even for human observers. Near-duplicate textures across material types further increase ambiguity. For instance, ‚Äôlinen‚Äô and ‚Äôcotton‚Äô exhibit overlapping weave patterns that become almost indistinguishable under uniform lighting and medium-scale zoom. Similarly, irregular porous structures make brown and white bread, as well as cracker and cork, difficult to differentiate.For the FMD dataset (see the image pairs in the middle ofFigure 5), the most frequent misclassifications occur between material pairs that share strong visual overlap in global appearance rather than fine-grained texture cues, such as ‚Äòleather‚Äô‚Äì‚Äòfabric‚Äô, ‚Äòmetal‚Äô‚Äì‚Äòplastic‚Äô, ‚Äòpaper‚Äô‚Äì‚Äòglass‚Äô, ‚Äòleather‚Äô‚Äì‚Äòmetal‚Äô, and ‚Äòglass‚Äô‚Äì‚Äòcork‚Äô. Across these cases, the classifier appears to rely primarily on object-centric shape regularities and specular highlights instead of discriminative micro-texture detail. For example, smooth folds make ‚Äòleather‚Äô visually comparable to ‚Äòfabric‚Äô, while polished reflection patterns cause ‚Äòmetal‚Äô to closely resemble ‚Äòplastic‚Äô under strong illumination. Similarly, the ‚Äòpaper‚Äô lantern is mistakenly associated with a ‚Äòglass‚Äô marble due to their shared rounded shape, despite being structurally different as materials. The reflective golden surface of ‚Äòmetal‚Äô and its resemblance in contour to the ‚Äòleather‚Äô sample further encourages confusion. In the rightmost examples, both images depict leaf-shaped objects, even though one is glass and the other is cork. The model fails to separate them because the similarity in overall form dominates over material-specific cues. These findings suggest that the ImageNet-pretrained backbone used in our model is biased toward global shape and object-level features rather than texture-specific representations, which considerably contributes to the observed misclassification patterns. Future work that involves training or fine-tuning the backbone on large-scale texture- or material-oriented datasets could potentially enhance its sensitivity to fine-grained surface cues, ultimately leading to more reliable recognition performance and fewer shape-driven classification errors.Regarding the GTOS-Mobile dataset, the image pairs shown at the bottom ofFigure 5often exhibit highly similar visual characteristics, making it difficult for the model to distinguish between them accurately.The subtle variations in local texture and fine-grained patterns pose a significant challenge for correct classification. Specifically, pairs such as ‚Äòstone_cement‚Äô‚Äì‚Äòcement‚Äô, ‚Äòstone_asphalt‚Äô‚Äì‚Äòasphalt‚Äô, and ‚Äòsmile_limestone‚Äô‚Äì‚Äòasphalt‚Äô share almost identical color tones and surface appearance, making them particularly difficult to differentiate. In contrast, pairs like ‚Äòpaint_cover‚Äô‚Äì‚Äòmetal_cover‚Äô and ‚ÄòsandPaper‚Äô‚Äì‚Äòbrick‚Äô exhibit similarity primarily in their texture and structural patterns rather than color, which also leads to misclassification. Addressing such confusing cases may require improving the model‚Äôs sensitivity to subtle differences in both texture geometry and material reflectance, representing a promising direction for future research to further improve robustness and classification accuracy.For the DTD dataset (see the top pairs of images inFigure 6), the most frequent misclassifications occur between texture categories that share highly similar global structure: ‚Äòswirly‚Äô‚Äì‚Äòspiralled‚Äô, ‚Äòblotchy‚Äô‚Äì‚Äòstained‚Äô, ‚Äòblotchy‚Äô‚Äì‚Äòpitted‚Äô, ‚Äòbanded‚Äô‚Äì‚Äòlined‚Äô, and ‚Äòdotted‚Äô‚Äì‚Äòpolka-dotted‚Äô. In all cases, the classifier appears to rely predominantly on overall pattern layout, which often overwhelms the finer distinctions that define the official category labels. The ‚Äòswirly‚Äô‚Äì‚Äòspiralled‚Äô confusion stems from their almost identical rotational geometry: both depict concentric, radiating curves with comparable frequency and curvature, making the subtle differences in pattern uniformity difficult for the model to separate. In the ‚Äòblotchy‚Äô‚Äì‚Äòstained‚Äô‚Äì‚Äòpitted‚Äô examples, the surfaces display irregular, uneven textures with overlapping patch-like structures; the lack of distinctive boundaries between tonal blotches, stain marks, and shallow surface pits leads to strong cross-class similarity. The ‚Äòbanded‚Äô‚Äì‚Äòlined‚Äô pair highlights how challenging it is to discriminate between wide-band and narrow-line patterns when both present horizontal black‚Äìwhite striping. Because their global appearance is dominated by parallel high-contrast stripes, the classifier fails to capture the difference in stripe thickness that defines the two categories. Finally, ‚Äòdotted‚Äô samples are often misidentified as ‚Äòpolka-dotted‚Äô due to the presence of similarly shaped circular elements; despite the more regular spatial arrangement characteristic of ‚Äòpolka-dotted‚Äô, the approximate dot distribution in both images makes them visually difficult for the model to separate. Overall, these findings indicate that misclassifications in DTD primarily arise from subtle geometric distinctions within pattern families, where global repetition cues dominate over fine-scale texture characteristics.For the Soil dataset (see the bottom pairs of images inFigure 6), the misclassified examples mainly involve the pairs ‚ÄòYellow_Soil‚Äô‚Äì‚ÄòAlluvial_Soil‚Äô, ‚ÄòAlluvial_Soil‚Äô‚Äì‚ÄòLaterite_Soil‚Äô, ‚ÄòAlluvial_Soil‚Äô‚Äì‚ÄòMountain_Soil‚Äô, ‚ÄòLaterite_Soil‚Äô‚Äì‚ÄòRed_Soil‚Äô, and ‚ÄòMountain_Soil‚Äô‚Äì‚ÄòLaterite_Soil‚Äô. These errors arise from a combination of textural similarity and shape-related cues that appear in the images. The first two pairs, ‚ÄòYellow_Soil‚Äô‚Äì‚ÄòAlluvial_Soil‚Äô and ‚ÄòAlluvial_Soil‚Äô‚Äì‚ÄòLaterite_Soil‚Äô, show soil surfaces with very similar particle distribution and overall appearance. Their fine-grained textures and close color ranges make them difficult to distinguish even visually, which naturally leads the model to mix these categories. In the ‚ÄòAlluvial_Soil‚Äô‚Äì‚ÄòMountain_Soil‚Äô example, the ‚ÄòAlluvial_Soil‚Äô sample resembles a small hillside due to its smooth curved profile, encouraging the model to interpret it as a landscape rather than pure soil texture. A related effect appears in the ‚ÄòMountain_Soil‚Äô‚Äì‚ÄòLaterite_Soil‚Äô pair, where the ‚ÄòLaterite_Soil‚Äô image presents a mound-like form that mimics a mountain silhouette despite representing a different soil type. The ‚ÄòLaterite_Soil‚Äô‚Äì‚ÄòRed_Soil‚Äô pair illustrates a case dominated by texture and color similarity: both samples share reddish tones and medium-grain roughness, making the boundary between these classes visually subtle. Overall, these results show that misclassifications in the Soil dataset stem from a mixture of textural resemblance and global shape cues within the images. This further indicates that the ImageNet-pretrained backbone tends to emphasize scene-like structures over fine soil-surface characteristics, limiting its ability to resolve closely related soil types. 4.9. Ablation StudyThe ablation study of the proposed approach was performed using the Swin backbone on the KTH-TIPS2-b dataset, fold 2, with MLP hyperparameters optimized as described inSection 4.4. This configuration is considered sufficiently representative of the remaining backbones. The study examines partial aggregation of features from different intermediate layers, as well as the effect of removing individual classifier components such as dropout and batch normalization. The results are summarized inTable 5.Table 5.Examining the impact of individual components on accuracy (%) within the proposed method using the Swin backbone on KTH-TIPS2-b dataset, fold 2 (the best result is highlighted in bold). A checkmark indicates the presence of the corresponding operation/component, while an ‚Äò‚úó‚Äô indicates its absence.When partial aggregation is applied, excluding features from stage f1, the accuracy decreases to 91.16% for stages f2, f3, and f4, and 91.50% when only stages f3 and f4 are aggregated (drops of 1.35% and 1.01%, respectively). Both cases demonstrate the importance of including features from earlier stages, as they provide complementary information that enhances classification performance.The exclusion of batch normalization has the most drastic effect, reducing accuracy to 56.57% (a drop of near 36%). This underscores its critical role in stabilizing training, preventing exploding or vanishing gradients, and improving generalization. Batch normalization absence causes instability in the training process, leading to significant performance degradation.Removing ReLU activation results in a smaller decrease in accuracy to 90.40% (a drop of more than 2%). This suggests that although the Swin Transformer‚Äôs feature extraction remains effective, the lack of ReLU impairs the model‚Äôs ability to learn complex features, leading to a significant reduction in accuracy. However, this drop is less dramatic than the performance drop observed when batch normalization is excluded.Similarly, excluding dropout leads to an accuracy of 89.82% (a drop of nearly 2.7%). Dropout is a regularization technique that helps prevent overfitting by randomly dropping units during training. Without it, the model becomes more prone to memorizing the training data, which explains the observed performance decline.Finally, simplifying the classification head by using a single Fully Connected (FC) layer instead of a two-layer MLP results in an accuracy of 91.84%, a drop of nearly 0.7%. This highlights the impact of the MLP structure in optimizing feature utilization and enhancing classification performance. While the simplified FC structure reduces computational cost, it comes at the expense of classification accuracy, highlighting the importance of the two-layer MLP architecture in the proposed method. 4.10. Limitations of the Presented WorkWe indicate the following limitations of the proposed work:High Computational Resource Requirements:The method achieves superior results across four investigated datasets (KTH-TIPS2-b, FMD, GTOS-Mobile, and Soil), utilizing the BEiTv2 transformer, which is notably computationally intensive, compared to the older models such as ResNet-50. This high resource demand may restrict the method‚Äôs applicability in environments with limited computational capabilities. Future research could explore model compression techniques or the development of lightweight variants to enhance accessibility.Simple feature aggregation:While the proposed method relies on a straightforward feature aggregation strategy based on global average pooling and concatenation, such a simplistic design may significantly limit the model‚Äôs ability to fully exploit the rich, complementary information present across different feature hierarchies. More advanced integration strategies, e.g., gating mechanisms or cross-attention, could provide more effective means of selectively emphasizing informative features and modeling their interactions. Additionally, the absence of dedicated projection heads may hinder the proper alignment of heterogeneous representations within a shared feature space, further constraining the model‚Äôs discriminative potential.Suboptimal MLP Hyperparameters for Diverse Datasets:While the feature extraction component does not require training, the effective training of the MLP relies on carefully selected hyperparameters. The current approach identifies optimal hyperparameters for the KTH-TIPS2-b dataset, which are then applied to the other three datasets. However, these parameters may not be ideal for all datasets. Further research could involve conducting separate hyperparameter optimization for each dataset using grid search or advanced optimization algorithms, potentially leading to improved performance across varied texture classes.In summary, addressing these limitations through targeted research efforts could enhance the applicability and effectiveness of the proposed texture recognition method, ensuring its relevance in diverse practical scenarios.",
            "4.1. Datasets": "Evaluation is conducted on five benchmark datasets that capture different forms of textural information. Three of them, KTH TIPS2 b, the Flickr Material Dataset (FMD), and GTOS Mobile, represent material or object centered images that nevertheless exhibit distinctive texture patterns. The remaining two datasets, the Describable Textures Dataset (DTD) and the Soil dataset, contain attribute level texture categories. For the first four datasets, we follow the evaluation protocols described in [26] to ensure full comparability with previous works, while for the Soil dataset we apply the protocol specified in [38]. Below is a brief description of each dataset together with the corresponding training protocol. KTH-TIPS2-b[39]: Comprises 4752 images from 11 material categories, including ‚Äòaluminium_foil‚Äô, ‚Äòbrown_bread‚Äô, ‚Äòcorduroy‚Äô, ‚Äòwool‚Äô, etc. Evaluation follows a 4-fold cross-validation protocol with fixed splits, as in [24].FMD[40]: Contains 1000 images distributed across 10 categories such as ‚Äòfabric‚Äô, ‚Äòglass‚Äô, ‚Äòleather‚Äô, and ‚Äòwood‚Äô. Performance is evaluated using 10 repetitions of 10-fold cross-validation.GTOS-Mobile[19]: Includes 100,011 mobile phone images of 31 outdoor ground material categories such as ‚Äòasphalt‚Äô, ‚Äògrass‚Äô, ‚Äòpebble‚Äô, and ‚Äòsand‚Äô. The dataset provides a predefined single train/test split, consistent with prior work [5,6,7,25,41].DTD[42]: Comprises 5640 images labeled with 47 describable texture attributes (e.g., ‚Äòbumpy‚Äô, ‚Äòstriped‚Äô, ‚Äôdotted‚Äô). Following [26], evaluation is performed using the standard protocol of 10 random train/validation/test splits, and results are averaged over these runs.Augmented Soil (Soil)[38]: This dataset originally contains 829 soil images grouped into seven classes (for example ‚ÄòAlluvial Soil‚Äô, ‚ÄòBlack Soil‚Äô, ‚ÄòLaterite Soil‚Äô and others). The dataset is partitioned into 70% training, 15% validation, and 15% testing subsets. Following [38], the training set is augmented and expanded to 3316 images, while the validation and test sets remain at 182 and 178 images respectively. This corresponds to the augmented version of the Soil dataset introduced in [38]. For brevity, in the remainder of the manuscript we refer to this augmented variant simply as ‚ÄúSoil‚Äù. This dataset serves as an application-oriented benchmark and is used in the development of an AI-based tool that supports farmers in soil identification and crop selection by incorporating geological and environmental information. Figure 2illustrates challenging cases from GTOS-Mobile, where similar categories such as ‚Äôbrick‚Äô and ‚Äômetal_cover‚Äô may be confused due to visual overlap [43]. The selection of the Swin Transformer variant (Section 4.3) and the hyper-parameters for the MLP (L,p) determined on KTH-TIPS2-b (Section 4.4) are reused for FMD, GTOS-Mobile, DTD, and Soil. Figure 2.Example images from GTOS-Mobile.",
            "4.2. Implementation Details": "The proposed algorithm is implemented using PyTorch-GPU (v2.5.1) [44] on a system equipped with an Intel Xeon E5-2640 v3 CPU (Intel Corporation, Santa Clara, CA, USA; 2.60 GHz, 8 cores), 32 GB of RAM (Kingston, NY, USA), and an NVIDIA GeForce RTX 2080 Ti GPU (NVIDIA Corporation, Santa Clara, CA, USA). Backbone architectures are initialized with pre-trained weights from the PyTorch Image Models library ‚Äútimm‚Äù (v1.0.15) [45]. A batch size of 32 is used throughout training. The architecture is trained for 100 epochs using Adaptive Moment Estimation (ADAM) with momentum0.90.9and weight decay1√ó10‚àí41√ó10‚àí4, optimizing the cross-entropy loss. The learning rate is set to1√ó10‚àí31√ó10‚àí3. To ensure deterministic and fully reproducible experiments, all runs were executed using a fixed global random seed of 42, which serves as the base seed and is applied consistently across Python (v3.10.16), NumPy (v2.0.1), and PyTorch (v2.5.1) CPU and GPU operations. CUDA deterministic execution was enabled, benchmark mode was disabled, and deterministic algorithms were enforced. These settings guarantee that model initialization, data loading order, and all stochastic operations (e.g., shuffling and augmentation) remain fully reproducible. Regarding the dataset splits and the preparation of training, validation, and test sets, the following procedures were applied to ensure full reproducibility across all experiments: KTH-TIPS2-b [39], GTOS-Mobile [19], and DTD [42]: As mentioned inSection 4.1, the dataset splits for training, validation, and testing are used as provided by the original sources, with no additional random partitioning or fold generation applied.FMD [40]: Following the widely adopted protocol in [26], performance is evaluated using 10 repetitions of 10-fold cross-validation. For each repetition, a new deterministic seed is derived from the base seed by multiplying it with the repetition index. This seed is then used to initialize all random number generators, including Python (v3.10.16), NumPy (v2.0.1), PyTorch (v2.5.1) CPU and GPU before generating the fold assignment. This procedure ensures that each repetition has a unique fold configuration while maintaining full reproducibility.Soil [38]: As mentioned inSection 4.1, we use the augmented version of the Soil dataset provided by the authors [38], in which the training set is expanded via standard augmentation techniques while the validation and test sets remain fixed. The code used for splitting the data and performing the training set augmentation is available in the authors‚Äô repository [46]. All preprocessing steps are applied consistently across datasets. For four of the datasets (KTH-TIPS2-b, FMD, GTOS-Mobile, and DTD), each input image is resized to224√ó224224√ó224pixels. For the Soil dataset, each training image is randomly cropped and resized to the same resolution, following the official preprocessing procedure provided by the dataset authors [46]. Finally, for all datasets, images are normalized using channel-wise mean values of (0.485, 0.456, 0.406) and standard deviations of (0.229, 0.224, 0.225). Regarding the backbone specification used in our experiments, we provide a detailed description of the pretrained checkpoints and the exact intermediate layers from which features are extracted. The following pretrained models from [45] are employed: Swin (‚Äòswin_large_patch4_window7_224‚Äô), SwinV2 (‚Äòswinv2_base_window12_192.ms_in22k‚Äô), ConvNeXt (‚Äòconvnext_base_in22ft1k‚Äô), MaxViT (‚Äòmaxvit_large_tf_224.in1k‚Äô), MambaOut (‚Äòmambaout_base_wide_rw‚Äô), DeiT3 (‚Äòdeit3_base_patch16_224.fb_in22k_ft_in1k‚Äô), and BEiTv2 (‚Äòbeitv2_base_patch16_224.in1k_ft_in22k_in1k‚Äô). For hierarchical architectures (Swin, SwinV2, ConvNeXt, MaxViT, MambaOut), the four intermediate layers available for these backbones are used, while for the remaining backbones (DeiT3 and BEiTv2), features are extracted from layers 2, 5, 8, and 11 according to the layer selection analysis presented inSection 4.5.",
            "4.3. Model Size Selection": "Model size selection is considered a preliminary step with the goal of selecting the size of the model that performs the best. From all architectures considered in this work, we select the Swin Transformer as the backbone for model size evaluation in texture classification tasks. Unlike DeiT3 and BEiTv2, Swin employs a hierarchical architecture with shifted-window self-attention. This design provides an effective trade-off between local feature modeling and global context aggregation, and it has demonstrated strong performance across various vision benchmarks while maintaining computational efficiency and scalability. It is also the earliest vision architecture compared to the remaining considered in this work. The configuration for model size evaluation is a Swin transformer for feature extraction, without its classification head. The classification layer is replaced with a set of units corresponding to the dataset‚Äôs classes, which are randomly initialized and fully trained for 150 epochs. The other training parameters are the same as those described inSection 4.2. For this analysis, only the KTH-TIPS2-b dataset is considered. Four primary Swin transformer architectures are commonly recognized for model selection: Swin-T, Swin-S, Swin-B, and Swin-L. Six variants from timm (v1.0.15) [45] were examined: Swin-T, Swin-S (pretrained on ImageNet-1k), Swin-B, Swin-L (pretrained on ImageNet-22k and fine-tuned on ImageNet-1k), as well as Swin-B-22k and Swin-L-22k (both pretrained on ImageNet-22k). The accuracy results, along with the number of parameters and Giga Multiply-Accumulate Operations (GMACs) for each backbone in this investigation, are summarized inTable 1. Table 1.Performance in terms of accuracy (%) of all Swin Transformer variants tested independently (with only the classification layer of the head replaced) on the KTH-TIPS2-b dataset. The best-performing backbone, along with its number of parameters, GMACs, and accuracy are highlighted in bold. The highest accuracy of 93.4% is achieved by the Swin-L model pretrained on ImageNet-22k and fine-tuned on ImageNet-1k, making it the preferred choice for the proposed architecture. The sizes of the remaining backbones were chosen to be comparable to that of the Swin model. The complete list of parameter counts and GMACs is as follows: Swin: 195 M parameters, 34.5 GMACs, pretrained on ImageNet-22k and fine-tuned on ImageNet-1k.SwinV2: 228.8 M parameters, 26.2 GMACs, pretrained on ImageNet-22k.ConvNeXt: 197.8 M parameters, 34.4 GMACs, pretrained on ImageNet-22k and fine-tuned on ImageNet-1k.MaxViT: 211.8 M parameters, 43.7 GMACs, pretrained on ImageNet-21k.MambaOut: 94.4 M parameters, 17.8 GMACs, pretrained on ImageNet-1k.DeiT3: 86.6 M parameters, 17.6 GMACs, pretrained on ImageNet-22k and fine-tuned on ImageNet-1k.BEiTv2: 86.5 M parameters, 17.6 GMACs, pretrained on ImageNet-22k and fine-tuned on ImageNet-1k. It should be noted that MambaOut, DeiT3, and BEiTv2 are approximately half the size of Swin. This choice was intentional: we included smaller models to reduce the risk of overestimating performance and instead provide a more conservative estimate. Moreover, for certain architectures, the implementations available in the timm library [45] do not offer variants with parameter counts closely matching the 195 M scale of Swin.",
            "4.4. Grid Search for MLP Parameters": "Using the selected Swin transformer model size, a grid search is conducted on the KTH-TIPS2-b dataset (fold 2) to determine the optimal number of hidden unitsLand dropout ratiopfor the classification step of the proposed architecture. The search explores values ofLfrom 32 to 256 in increments of 32, andpfrom 0.1 to 0.9 in increments of 0.1. The resulting plot is shown inFigure 3, showing that the highest accuracy of 92.51% is achieved withùêø=64L=64andùëù=0.2p=0.2. Therefore, these values for the classification head are used for all backbones. Figure 3.3D plot illustrating the grid search for optimal MLP hyper-parameters.",
            "4.5. Analysis of Layer Selection for Non-Hierarchical Models": "Non-hierarchical transformer backbones, such as DeiT3 and BEiTv2, contain substantially more than four internal blocks, which makes the choice of layer sampling strategy non-trivial. To determine a principled configuration for extracting multi-level representations, we perform a dedicated analysis using BEiTv2 as a representative ViT-style architecture with a uniform depth structure. Several candidate layer sets were evaluated, each designed to reflect different assumptions about where texture-relevant information may reside within the transformer stack. The tested configurations include: Equidistant ([3, 6, 9, 12]), Shifted ([2, 5, 8, 11]), Early-heavy ([1, 2, 3, 4]), Late-heavy ([9, 10, 11, 12]), Middle ([4, 5, 6, 7]), Distributed ([1, 4, 7, 10]), and Center+end ([3, 6, 10, 12]). For each set, features from the selected blocks were pooled over the token dimension, concatenated, and used to train the MLP classifier following the parameters described inSection 4.4. The resulting performance for each candidate set on the KTH-TIPS2-b dataset is reported inTable 2, enabling a direct comparison of the effectiveness of the different layer combinations. Table 2.Best classification accuracy (%) on KTH-TIPS2-b for different layer selection strategies in non-hierarchical transformers. The best result is highlighted inbold. The analysis also revealed that sampling fewer than four blocks resulted in insufficient coverage of both local and global patterns, whereas using more than four blocks introduced additional computational overhead with only marginal accuracy gains. Among the tested configurations (seeTable 2), the Shifted set consistently yielded the strongest performance, providing a balanced representation of low-, mid-, and high-level texture cues. Based on this empirical evaluation, the Shifted set ([2, 5, 8, 11]) was selected as the default extraction configuration for the non-hierarchical transformer architectures.",
            "4.6. Results Against Baseline": "The proposed architecture was evaluated for each backbone using the datasets described inSection 4.1and following the RADAM protocol [26]. To assess its effectiveness, we compare the proposed method against baseline models that use the same backbone but rely solely on the features extracted from the final output layer. The results are reported inTable 3, where performance is assessed in terms of average classification accuracy (%) and standard deviation. Table 3.Evaluation of the proposed architecture using five datasets against baseline. The baseline using only final layer features is noted as No Aggr., while the proposed method using multi-layer aggregation is noted as Aggr. Accuracy (in %) is reported for all experiments. The better result, comparing No Aggr. with Aggr. for each experiment, is given in bold. For clarity, the table includes both configurations (i) using only the final-layer features of each backbone (denoted No Aggr., or baseline) and (ii) using the proposed feature aggregation from intermediate and final layers (denoted Aggr.). This allows us to directly evaluate the contribution of the aggregation strategy. The analysis of the results reveals that the benefits of feature aggregation are not uniform across backbones and datasets. In particular, the DeiT3 backbone consistently achieves better performance when using only the final-layer features, suggesting that its most descriptive representations for texture recognition are already concentrated at the output stage. In contrast, other backbones exhibit weaker final-layer representations, and thus benefit from the additional information captured in intermediate layers. Nevertheless, the performance of DeiT3 remains lower than that of most other backbones, indicating that although its most descriptive features are concentrated in the final output layer, this representation alone is insufficient compared to the richer feature sets provided by the alternative architectures. This behavior can be attributed to the non-hierarchical structure of DeiT3, which differs fundamentally from the hierarchical organization of architectures such as Swin or ConvNeXt. In DeiT3, most discriminative information is concentrated in the final transformer layers, whereas earlier layers primarily capture low-level patch embeddings with limited semantic value. Consequently, concatenating early and late representations may introduce redundancy or noise, slightly degrading performance. This observation suggests that future research should explore specialized aggregation schemes tailored for non-hierarchical architectures. A further trend can be observed on the GTOS-Mobile dataset, where most backbones perform better without aggregation. This may be attributed to the large size of the dataset combined with high variation and complexity. To draw conclusions regarding the effectiveness of the proposed method (Aggr.) compared to the baseline (No Aggr.), we employ a paired design [47]. Examination of the paired differences reveals the presence of clear outliers-most notably for DeiT3, where the Aggr. configuration consistently underperforms-casting doubt on the assumption of normality. Consequently, we predefine the Wilcoxon signed-rank test (one-sided,ùêª1H1: Feat. Aggr. > baseline) as our primary statistical analysis, given its robustness to both non-normal distributions and the influence of outliers [48]. For transparency, we also report a pairedt-test on the mean difference and a sign test on the direction of change. The results are summarized as follows: Wilcoxon signed-rank:The median improvement is +1.0 percentage point (pp), rank-biserial correlationùëü=0.36r=0.36(moderate). One-sidedùëù=0.041p=0.041(<0.05) meaning that Aggregation improvement is unlikely due to chance.Signed test:25 out of 35 positives, one-sided p=0.0052 (significant).Pairedt-test (mean effect):meanŒî=‚àí0.79Œî=‚àí0.79pp,ùë°(34)=‚àí0.72t(34)=‚àí0.72, one-sidedùëù=0.762p=0.762(not significant). The negative mean is driven by the large, consistent drops for DeiT3 across all five datasets. Using the paired, robust Wilcoxon signed-rank test, the feature aggregation shows a statistically significant typical-case improvement over the baseline across backbones and datasets (median‚âà+1.0‚âà+1.0pp,ùëù=0.041p=0.041). However, the mean improvement is not significant because DeiT3 exhibits large performance decrements with aggregation. Practically, aggregation can be expected to help in most backbone‚Äìdataset combinations, but DeiT3 is a notable exception that warrants configuration checks or exclusion.",
            "4.7. Comparison with State-of-the-Art Methods": "Table 4presents the classification accuracy (%) and standard deviation of the proposed architecture compared to several state-of-the-art methods for four of the datasets (KTH-TIPS2-b, FMD, GTOS-Mobile, and DTD). Table 4.Comparison of the proposed architecture‚Äôs performance in terms of accuracy (%) and standard deviation (¬±) against state-of-the-art methods. The backbones are organized into row blocks based on their computational capacity, with their corresponding number of parameters and GMACs. The highest results for each dataset within each block are highlighted in bold, while the overall best results, regardless of the backbone, are underlined. References in square brackets next to each method indicate the original source of the results, and if a second reference is listed, it denotes that the results were taken from the subsequent source. The evaluation protocols used for four of the datasets (KTH-TIPS2-b, FMD, GTOS-Mobile, and DTD) align with those adopted in RADAM [26] and several other studies. For the Soil dataset, the protocol from [38] was followed (see the setup and dataset descriptions inSection 4.1andSection 4.2). The methods inTable 4are grouped by backbone models and organized based on their computational capacity, indicated by the number of parameters and GMACs. The proposed method, using the BEiTv2 backbone, delivers the highest accuracy across four datasets: 96.8% on KTH-TIPS2-b, 96.7% on FMD, 91.8% on GTOS-Mobile, and 94.5% on Soil. However, on the DTD dataset, the accuracy of 82.0% is slightly lower than that of the RADAM ConvNeXt-L-22k (84.0%) and RADAM ConvNeXt-XL-22k (83.7%) models. For the Augmented Soil dataset, to the best of our knowledge, only one article [38] reports several evaluated methods based on fine-tuning. Among these, we compare only with the ViT-B/16 model, which achieved the highest reported accuracy of 91.0% [38]. Our BEiTv2-based method reaches 94.5%, representing a 3.5% improvement. It is also worth noting that the computational characteristics of our model, in terms of GMACs and number of parameters, are very close to those of ViT-B/16, making the comparison balanced and directly relevant. Regarding the other four datasets, it is worth emphasizing that the proposed BEiTv2-based model is considerably more compact, containing 86.5 million parameters and requiring 17.6 GMACs, compared to 229.8 million parameters and 34 GMACs for ConvNeXt-L-22k and 392.9 million parameters and 61 GMACs for ConvNeXt-XL-22k. Despite having roughly two to three times fewer parameters and computational cost, the proposed approach achieves comparable performance on DTD and surpasses all larger models on the other three benchmarks. This demonstrates the efficiency and competitiveness of the proposed feature aggregation strategy, showing that strong texture recognition performance can be achieved using pre-trained architectures, inter-layer feature aggregation, and a lightweight MLP classifier. In comparison, models based on smaller backbones such as ResNet18 and ResNet50 offer reasonable accuracy with significantly lower complexity. For instance, ResNet18-based methods like Fractal Pooling achieve 88.3% on KTH-TIPS2-b, while DTPNet reaches 85.7% on FMD and 87.0% on GTOS-Mobile. On DTD, the best-performing ResNet18 method (MPAP) attains 72.4%, using only 11.7 million parameters and 1.8 GMACs. Similarly, ResNet50-based approaches such as Fractal Pooling and MPAP achieve accuracies between 78.0% and 90.7% across datasets, with 25.6 million parameters and 4.1 GMACs, demonstrating a balance between accuracy and efficiency but still lagging behind the proposed method. While larger RADAM ConvNeXt models show competitive performance, they require substantially more computational resources. For example, RADAM ConvNeXt-L (197.8 M parameters, 34.4 GMACs) achieves 89.3% on KTH-TIPS2-b and FMD, 85.8% on GTOS-Mobile, and 77.4% on DTD, which are lower than the BEiTv2-based results across all datasets. RADAM ConvNeXt-L-22k reaches 91.3%, 95.2%, 87.3%, and 84.0% respectively, outperforming the proposed model only on DTD. Likewise, RADAM ConvNeXt-XL-22k, with 392.9 M parameters and 61 GMACs, attains 94.4%, 95.2%, and 90.2% on KTH-TIPS2-b, FMD, and GTOS-Mobile, while achieving 83.7% on DTD, again higher on this dataset but lower elsewhere. Overall, this comparison highlights that the proposed BEiTv2-based model achieves the best balance between accuracy and efficiency, outperforming all compared methods on four out of five datasets while using significantly fewer parameters and computational cost, although for the Soil dataset the parameter count and computational cost are comparable.",
            "4.8. Detailed Results": "Confusion matrices:The effectiveness of the proposed architecture is further evaluated by using the best-performing backbone, i.e., BEiTv2, analyzing the confusion matrices and corresponding accuracies for the worst-case folds of KTH-TIPS2-b, FMD, and DTD, while GTOS-Mobile and Soil, both having a single test fold, are examined separately (seeFigure 4). In general, the confusion matrices show a strong diagonal pattern, reflecting the algorithm‚Äôs strong classification performance. Figure 4.Confusion matrices and accuracies obtained using the proposed architecture with the BEiTv2 backbone for each dataset. For KTH-TIPS2-b, FMD, and DTD, the results correspond to the lowest-accuracy folds, while for GTOS-Mobile and Soil, results are shown from its single available fold. Darker red indicates higher proportion of predictions in a cell. For the KTH-TIPS2-b dataset, the most frequent confusions are ‚Äòwool‚Äô or ‚Äòcorduroy‚Äô predicted as ‚Äòcotton‚Äô and ‚Äòcotton‚Äô misclassified as ‚Äòlinen‚Äô. In the FMD dataset, misclassifications include ‚Äòleather‚Äô predicted as ‚Äòfabric‚Äô or ‚Äòmetal‚Äô, ‚Äòglass‚Äô as ‚Äòfoliage‚Äô, ‚Äòpaper‚Äô as ‚Äòglass‚Äô, ‚Äòplastic‚Äô as ‚Äòglass‚Äô or ‚Äòmetal‚Äô, ‚Äòleather‚Äô as ‚Äòmetal‚Äô, ‚Äòmetal‚Äô as ‚Äòplastic‚Äô, and ‚Äòmetal‚Äô as ‚Äòplastic‚Äô. These mistakes are largely due to similar visual appearances among the classes. In the GTOS-Mobile dataset the most noticeable misclassifications are ‚Äòstone_cement‚Äô predicted as ‚Äòcement‚Äô, ‚Äòstone_brick‚Äô as ‚Äòbrick‚Äô, ‚Äòsmall_limestone‚Äô, ‚Äòstone_asphalt‚Äô, and ‚Äòstone_brick‚Äô confused with ‚Äòasphalt‚Äô, and ‚Äòpaint_cover‚Äô misclassified as ‚Äòmetal_cover‚Äô. As with the other datasets, most of these errors involve classes with similar characteristics. For the DTD dataset, the most prominent misclassification occurs for the blotchy class, which appears as the lightest (i.e., lowest-accuracy) square along the diagonal of the DTD confusion matrix. Representative errors include, but are not limited to, predicting ‚Äòblotchy‚Äô as ‚Äòstained‚Äô or ‚Äòpitted‚Äô, ‚Äòbanded‚Äô as ‚Äòlined‚Äô, ‚Äòdotted‚Äô as ‚Äòpolka-dotted‚Äô, and ‚Äòswirly‚Äô as ‚Äòspiralled‚Äô. Regarding the Soil dataset, the most notable confusion cases include ‚ÄòYellow Soil‚Äô predicted as ‚ÄòAlluvial Soil‚Äô, as well as Alluvial Soil as ‚ÄòLaterite Soil‚Äô or ‚ÄòMontian Soil‚Äô. Confusion cases:Figure 5presents several confusing cases encountered by the proposed architecture on the KTH-TIPS2-b, FMD, and GTOS-Mobile datasets, illustrating typical misclassifications and their visually similar predicted counterparts.Figure 6provides additional examples of analogous confusion patterns observed in the DTD and Soil datasets, following the same presentation structure and highlighting similar challenges in distinguishing between visually related categories. Most of these pairs of images, which reflect the incorrect classifications, correspond to the highlighted cells in the confusion matrices of the respective datasets previously shown inFigure 4. Figure 5.Examples of confusing material categories from the KTH-TIPS2-b, FMD, and GTOS-Mobile datasets using the proposed architecture with BEiTv2 backbone. The top section displays misclassified samples with their true labels, whereas the bottom section illustrates their incorrect predictions alongside visually similar samples from the predicted class. Figure 6.Examples of confusing material categories from the DTD and Soil datasets using the proposed architecture with a BEiTv2 backbone. The structure and interpretation of the figure follow the same convention as inFigure 5, where the upper section shows misclassified samples with their ground-truth labels, and the lower section presents the corresponding incorrect predictions along with visually similar samples from the predicted classes. Regarding the KTH-TIPS2-b dataset (see the top pairs of images inFigure 5), the most frequent confusions occur between visually similar categories such as ‚Äòwool‚Äô‚Äì‚Äòcotton‚Äô, ‚Äòcotton‚Äô‚Äì‚Äòlinen‚Äô, ‚Äòcorduroy‚Äô‚Äì‚Äòcotton‚Äô, ‚Äòbrown_bread‚Äô‚Äì‚Äòwhite_bread‚Äô, and ‚Äòcracker‚Äô‚Äì‚Äôcork‚Äô. These pairs share overlapping local texture patterns, including uniform weaves, coarse fiber structures, or porous surfaces, which make their appearance highly similar in isolated patches. As a result, the model often relies on subtle surface cues that are difficult to distinguish even for human observers. Near-duplicate textures across material types further increase ambiguity. For instance, ‚Äôlinen‚Äô and ‚Äôcotton‚Äô exhibit overlapping weave patterns that become almost indistinguishable under uniform lighting and medium-scale zoom. Similarly, irregular porous structures make brown and white bread, as well as cracker and cork, difficult to differentiate. For the FMD dataset (see the image pairs in the middle ofFigure 5), the most frequent misclassifications occur between material pairs that share strong visual overlap in global appearance rather than fine-grained texture cues, such as ‚Äòleather‚Äô‚Äì‚Äòfabric‚Äô, ‚Äòmetal‚Äô‚Äì‚Äòplastic‚Äô, ‚Äòpaper‚Äô‚Äì‚Äòglass‚Äô, ‚Äòleather‚Äô‚Äì‚Äòmetal‚Äô, and ‚Äòglass‚Äô‚Äì‚Äòcork‚Äô. Across these cases, the classifier appears to rely primarily on object-centric shape regularities and specular highlights instead of discriminative micro-texture detail. For example, smooth folds make ‚Äòleather‚Äô visually comparable to ‚Äòfabric‚Äô, while polished reflection patterns cause ‚Äòmetal‚Äô to closely resemble ‚Äòplastic‚Äô under strong illumination. Similarly, the ‚Äòpaper‚Äô lantern is mistakenly associated with a ‚Äòglass‚Äô marble due to their shared rounded shape, despite being structurally different as materials. The reflective golden surface of ‚Äòmetal‚Äô and its resemblance in contour to the ‚Äòleather‚Äô sample further encourages confusion. In the rightmost examples, both images depict leaf-shaped objects, even though one is glass and the other is cork. The model fails to separate them because the similarity in overall form dominates over material-specific cues. These findings suggest that the ImageNet-pretrained backbone used in our model is biased toward global shape and object-level features rather than texture-specific representations, which considerably contributes to the observed misclassification patterns. Future work that involves training or fine-tuning the backbone on large-scale texture- or material-oriented datasets could potentially enhance its sensitivity to fine-grained surface cues, ultimately leading to more reliable recognition performance and fewer shape-driven classification errors. Regarding the GTOS-Mobile dataset, the image pairs shown at the bottom ofFigure 5often exhibit highly similar visual characteristics, making it difficult for the model to distinguish between them accurately. The subtle variations in local texture and fine-grained patterns pose a significant challenge for correct classification. Specifically, pairs such as ‚Äòstone_cement‚Äô‚Äì‚Äòcement‚Äô, ‚Äòstone_asphalt‚Äô‚Äì‚Äòasphalt‚Äô, and ‚Äòsmile_limestone‚Äô‚Äì‚Äòasphalt‚Äô share almost identical color tones and surface appearance, making them particularly difficult to differentiate. In contrast, pairs like ‚Äòpaint_cover‚Äô‚Äì‚Äòmetal_cover‚Äô and ‚ÄòsandPaper‚Äô‚Äì‚Äòbrick‚Äô exhibit similarity primarily in their texture and structural patterns rather than color, which also leads to misclassification. Addressing such confusing cases may require improving the model‚Äôs sensitivity to subtle differences in both texture geometry and material reflectance, representing a promising direction for future research to further improve robustness and classification accuracy. For the DTD dataset (see the top pairs of images inFigure 6), the most frequent misclassifications occur between texture categories that share highly similar global structure: ‚Äòswirly‚Äô‚Äì‚Äòspiralled‚Äô, ‚Äòblotchy‚Äô‚Äì‚Äòstained‚Äô, ‚Äòblotchy‚Äô‚Äì‚Äòpitted‚Äô, ‚Äòbanded‚Äô‚Äì‚Äòlined‚Äô, and ‚Äòdotted‚Äô‚Äì‚Äòpolka-dotted‚Äô. In all cases, the classifier appears to rely predominantly on overall pattern layout, which often overwhelms the finer distinctions that define the official category labels. The ‚Äòswirly‚Äô‚Äì‚Äòspiralled‚Äô confusion stems from their almost identical rotational geometry: both depict concentric, radiating curves with comparable frequency and curvature, making the subtle differences in pattern uniformity difficult for the model to separate. In the ‚Äòblotchy‚Äô‚Äì‚Äòstained‚Äô‚Äì‚Äòpitted‚Äô examples, the surfaces display irregular, uneven textures with overlapping patch-like structures; the lack of distinctive boundaries between tonal blotches, stain marks, and shallow surface pits leads to strong cross-class similarity. The ‚Äòbanded‚Äô‚Äì‚Äòlined‚Äô pair highlights how challenging it is to discriminate between wide-band and narrow-line patterns when both present horizontal black‚Äìwhite striping. Because their global appearance is dominated by parallel high-contrast stripes, the classifier fails to capture the difference in stripe thickness that defines the two categories. Finally, ‚Äòdotted‚Äô samples are often misidentified as ‚Äòpolka-dotted‚Äô due to the presence of similarly shaped circular elements; despite the more regular spatial arrangement characteristic of ‚Äòpolka-dotted‚Äô, the approximate dot distribution in both images makes them visually difficult for the model to separate. Overall, these findings indicate that misclassifications in DTD primarily arise from subtle geometric distinctions within pattern families, where global repetition cues dominate over fine-scale texture characteristics. For the Soil dataset (see the bottom pairs of images inFigure 6), the misclassified examples mainly involve the pairs ‚ÄòYellow_Soil‚Äô‚Äì‚ÄòAlluvial_Soil‚Äô, ‚ÄòAlluvial_Soil‚Äô‚Äì‚ÄòLaterite_Soil‚Äô, ‚ÄòAlluvial_Soil‚Äô‚Äì‚ÄòMountain_Soil‚Äô, ‚ÄòLaterite_Soil‚Äô‚Äì‚ÄòRed_Soil‚Äô, and ‚ÄòMountain_Soil‚Äô‚Äì‚ÄòLaterite_Soil‚Äô. These errors arise from a combination of textural similarity and shape-related cues that appear in the images. The first two pairs, ‚ÄòYellow_Soil‚Äô‚Äì‚ÄòAlluvial_Soil‚Äô and ‚ÄòAlluvial_Soil‚Äô‚Äì‚ÄòLaterite_Soil‚Äô, show soil surfaces with very similar particle distribution and overall appearance. Their fine-grained textures and close color ranges make them difficult to distinguish even visually, which naturally leads the model to mix these categories. In the ‚ÄòAlluvial_Soil‚Äô‚Äì‚ÄòMountain_Soil‚Äô example, the ‚ÄòAlluvial_Soil‚Äô sample resembles a small hillside due to its smooth curved profile, encouraging the model to interpret it as a landscape rather than pure soil texture. A related effect appears in the ‚ÄòMountain_Soil‚Äô‚Äì‚ÄòLaterite_Soil‚Äô pair, where the ‚ÄòLaterite_Soil‚Äô image presents a mound-like form that mimics a mountain silhouette despite representing a different soil type. The ‚ÄòLaterite_Soil‚Äô‚Äì‚ÄòRed_Soil‚Äô pair illustrates a case dominated by texture and color similarity: both samples share reddish tones and medium-grain roughness, making the boundary between these classes visually subtle. Overall, these results show that misclassifications in the Soil dataset stem from a mixture of textural resemblance and global shape cues within the images. This further indicates that the ImageNet-pretrained backbone tends to emphasize scene-like structures over fine soil-surface characteristics, limiting its ability to resolve closely related soil types.",
            "4.9. Ablation Study": "The ablation study of the proposed approach was performed using the Swin backbone on the KTH-TIPS2-b dataset, fold 2, with MLP hyperparameters optimized as described inSection 4.4. This configuration is considered sufficiently representative of the remaining backbones. The study examines partial aggregation of features from different intermediate layers, as well as the effect of removing individual classifier components such as dropout and batch normalization. The results are summarized inTable 5. Table 5.Examining the impact of individual components on accuracy (%) within the proposed method using the Swin backbone on KTH-TIPS2-b dataset, fold 2 (the best result is highlighted in bold). A checkmark indicates the presence of the corresponding operation/component, while an ‚Äò‚úó‚Äô indicates its absence. When partial aggregation is applied, excluding features from stage f1, the accuracy decreases to 91.16% for stages f2, f3, and f4, and 91.50% when only stages f3 and f4 are aggregated (drops of 1.35% and 1.01%, respectively). Both cases demonstrate the importance of including features from earlier stages, as they provide complementary information that enhances classification performance. The exclusion of batch normalization has the most drastic effect, reducing accuracy to 56.57% (a drop of near 36%). This underscores its critical role in stabilizing training, preventing exploding or vanishing gradients, and improving generalization. Batch normalization absence causes instability in the training process, leading to significant performance degradation. Removing ReLU activation results in a smaller decrease in accuracy to 90.40% (a drop of more than 2%). This suggests that although the Swin Transformer‚Äôs feature extraction remains effective, the lack of ReLU impairs the model‚Äôs ability to learn complex features, leading to a significant reduction in accuracy. However, this drop is less dramatic than the performance drop observed when batch normalization is excluded. Similarly, excluding dropout leads to an accuracy of 89.82% (a drop of nearly 2.7%). Dropout is a regularization technique that helps prevent overfitting by randomly dropping units during training. Without it, the model becomes more prone to memorizing the training data, which explains the observed performance decline. Finally, simplifying the classification head by using a single Fully Connected (FC) layer instead of a two-layer MLP results in an accuracy of 91.84%, a drop of nearly 0.7%. This highlights the impact of the MLP structure in optimizing feature utilization and enhancing classification performance. While the simplified FC structure reduces computational cost, it comes at the expense of classification accuracy, highlighting the importance of the two-layer MLP architecture in the proposed method.",
            "4.10. Limitations of the Presented Work": "We indicate the following limitations of the proposed work: High Computational Resource Requirements:The method achieves superior results across four investigated datasets (KTH-TIPS2-b, FMD, GTOS-Mobile, and Soil), utilizing the BEiTv2 transformer, which is notably computationally intensive, compared to the older models such as ResNet-50. This high resource demand may restrict the method‚Äôs applicability in environments with limited computational capabilities. Future research could explore model compression techniques or the development of lightweight variants to enhance accessibility.Simple feature aggregation:While the proposed method relies on a straightforward feature aggregation strategy based on global average pooling and concatenation, such a simplistic design may significantly limit the model‚Äôs ability to fully exploit the rich, complementary information present across different feature hierarchies. More advanced integration strategies, e.g., gating mechanisms or cross-attention, could provide more effective means of selectively emphasizing informative features and modeling their interactions. Additionally, the absence of dedicated projection heads may hinder the proper alignment of heterogeneous representations within a shared feature space, further constraining the model‚Äôs discriminative potential.Suboptimal MLP Hyperparameters for Diverse Datasets:While the feature extraction component does not require training, the effective training of the MLP relies on carefully selected hyperparameters. The current approach identifies optimal hyperparameters for the KTH-TIPS2-b dataset, which are then applied to the other three datasets. However, these parameters may not be ideal for all datasets. Further research could involve conducting separate hyperparameter optimization for each dataset using grid search or advanced optimization algorithms, potentially leading to improved performance across varied texture classes. In summary, addressing these limitations through targeted research efforts could enhance the applicability and effectiveness of the proposed texture recognition method, ensuring its relevance in diverse practical scenarios.",
            "5. Conclusions": "This paper presented an effective architecture for texture recognition that aggregates multi-level representations extracted from pre-trained vision backbones. By combining features from early, mid, and late layers and feeding the resulting vector into a lightweight MLP classifier, the approach demonstrates the strong value of multi-level features, particularly from BEiTv2, for texture analysis and outperforms state-of-the-art approaches on four benchmark datasets: KTH-TIPS2-b, FMD, GTOS-Mobile, and Soil. A key finding of this study is that final-layer features of ImageNet-22k-pretrained models are not sufficient for fine-grained texture discrimination. The ablation analysis shows that all evaluated backbones benefit from incorporating earlier hierarchical stages, which provide the fine local cues missing when relying solely on the final layer. The only exception is DeiT-3, where the baseline performs slightly better, although the overall evidence indicates that single-layer representations remain inadequate for robust texture analysis. The work introduces several contributions, including a unified comparison of pre-trained backbones on five texture datasets, an efficient architecture that exploits multi-level representations without fine-tuning, and new empirical evidence on how hierarchical features contribute to texture discrimination. The approach also has limitations: its strongest configuration relies on computationally demanding transformers such as BEiTv2, the feature aggregation mechanism is intentionally simple, and the MLP hyperparameters tuned on KTH-TIPS2-b may not generalize optimally. Qualitative analysis further indicates a persistent bias of ImageNet-22k-pretrained models toward global shape or object-level cues. These observations suggest several directions for future work. Efficient or compressed variants of the architecture could broaden its applicability in resource-limited settings. Advanced integration mechanisms, such as cross-attention, learned projection heads, or gated feature fusion, may improve alignment and interaction across layers, and an analysis of state-of-the-art intelligent fusion methods could further guide the development of effective feature aggregation strategies. Fine-tuning or pre-training on larger texture-oriented datasets could reduce the observed shape bias and enhance discrimination for challenging texture classes. Finally, incorporating process-aware datasets or controlled subsets with explicit intra-class variation would support more comprehensive robustness evaluations."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2079-9292/14/23/4779",
        "scraped_at": "2025-12-05 23:54:36"
    },
    {
        "title": "RepackDroid: An Efficient Detection Model for Repackaged Android Applications",
        "authors": "byTito LeadonandKarim Elish",
        "journal": "Information2025,16(12), 1075;https://doi.org/10.3390/info16121075- 4 Dec 2025",
        "abstract": "Repackaged Android applications pose a significant threat to mobile ecosystems, acting as common vectors for malware distribution and intellectual property infringement. Addressing the challenges of existing repackaging detection methods‚Äîsuch as scalability, reliance on app pairs, and high computational costs‚Äîthis paper presents a novel hybrid approach that combines supervised learning and symptom discovery. We develop a lightweight feature extraction and analysis framework that leverages only 20 discriminative features, including inter-component communication (ICC) patterns, sensitive API usage, permission profiles, and a structural anomaly metric derived from string offset order. Our experiments, conducted on 8441 Android applications sourced from the RePack dataset, demonstrate the effectiveness of our approach, achieving a maximum F1 score of 85.9% and recall of 98.8% using Support Vector Machines‚Äîoutperforming prior state-of-the-art models that utilized over 500 features. We also evaluate the standalone predictive power of AndroidSOO‚Äôs string offset order feature and highlight its value as a low-cost repackaging indicator. This work offers an accurate, efficient, and scalable alternative for automated detection of repackaged mobile applications in large-scale Android marketplaces.Keywords:Android security;repackaged apps;mobile malware;static analysis;machine learning",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "The rapid growth of the Android operating system has revolutionized the mobile landscape, powering more than three billion active devices worldwide¬†[1]. Its open-source nature and flexible distribution model have accelerated innovation, enabling developers to create diverse applications that reach global audiences. However, this openness also introduces significant security risks. One of the most prevalent and damaging threats is application repackaging‚Äîa process in which attackers clone legitimate apps, inject malicious payloads or unauthorized modifications, and redistribute them on third-party marketplaces. Repackaged apps present a dual threat to the mobile ecosystem. On the one hand, they act as effective vectors for malware distribution, facilitating data theft, spyware, ransomware, and other malicious activities. On the other hand, they lead to intellectual property infringement and loss of revenue for legitimate developers. Users who unknowingly install these cloned applications often suffer degraded device performance, intrusive advertisements, and serious privacy violations, which collectively erode trust in the Android ecosystem. Alarmingly, prior studies estimate that 86% of Android malware consists of repackaged applications¬†[2], and industry reports show that 77% of the top 50 apps in Google Play once had repackaged counterparts¬†[3]. The 2016 release of Pok√©mon GO provides a prominent example of how attackers exploit repackaging. Because the game was initially released in only a limited number of countries, third-party repackaged versions quickly emerged in excluded regions. Many of these clones contained hidden malware, resulting in devices overheating, slowing down, and displaying intrusive foreign-language advertisements. This case highlights a recurring cycle: adversaries exploit user demand for unavailable, older, or modified applications to bypass official distribution channels. Such incidents underscore the need for robust detection mechanisms that can identify malicious repackaged apps before they reach end users. Despite a decade of research, repackaging detection remains an open challenge due to the following obstacles: Scalability: Most similarity-based approaches¬†[4,5,6] rely on pairwise comparisons between candidate and original apps. While accurate in small-scale settings, these techniques suffer from prohibitive computational costs when applied to millions of applications in large marketplaces.Dependence on Original‚ÄìRepackaged Pairs: Many methods, such as¬†[4,6], require both the benign app and its repackaged counterpart for comparison, which is impractical in real-world settings where original apps may not be available.Feature Explosion: Some state-of-the-art approaches extract hundreds of features (e.g., APIs, permissions, control flow graphs), which leads to high-dimensional data, longer training times, and limited portability¬†[7].Obfuscation and Code Transformation: Attackers frequently employ obfuscation, reordering, and library injection to disguise malicious payloads, which can confuse both similarity-based and machine learning-based detectors.Dataset Limitations: The lack of large, high-quality benchmark datasets hindered fair evaluation and reproducibility of proposed methods. This paper introduces RepackDroid, a lightweight and effective detection framework that addresses the limitations of prior approaches by combining supervised learning with symptom discovery. Unlike prior works that rely on large feature sets or explicit app pairs¬†[4,5,6,7], RepackDroid extracts a compact set of only 20 discriminative features, spanning inter-component communication (ICC) patterns, permission structures, sensitive API usage, and structural anomalies such as string offset order. In summary, this work makes the following contributions. Novel Feature Extraction Framework: We design and implement a tool that automatically processes APKs into a compact set of features tailored for repackaging detection.Efficient Learning with Fewer Features: We demonstrate that a reduced feature space (20 features) can outperform prior models trained on more than 500 features, achieving improved recall and F1 scores while reducing computational overhead.Comprehensive Empirical Evaluation: Using 8441 Android applications from the RePack dataset¬†[8], we show that Support Vector Machines achieve up to 98.8% recall and 85.9% F1 score, outperforming state-of-the-art baselines. We also evaluate the predictive power of the string offset order anomaly, validating its role as a cost-effective indicator of repackaging. The remainder of this paper is structured as follows.Section 2reviews existing approaches for repackaged app detection, highlighting their strengths and limitations.Section 3describes our methodology.Section 4describes the dataset characterization and preprocessing.Section 5presents experimental results.Section 6discusses the implications of our findings.Section 7discusses threats to validity to our approach. Finally,Section 8concludes the paper and outlines directions for future work.",
            "2. Related Work": "In this section, we review prior work that focuses specifically on the detection of repackaged Android applications. While the broader Android security literature includes extensive research on general malware detection, such as systems that analyze permissions, API usage, control-flow patterns, or machine learning models designed to classify malicious applications¬†[9,10,11,12,13,14,15], these approaches do not target the unique characteristics of repackaging or rely on datasets that contain original‚Äìrepackaged app pairs. Our discussion therefore concentrates on techniques explicitly developed for identifying repackaged apps, including similarity-based comparison, runtime instrumentation, supervised learning, and symptom¬†discovery. Similarity Comparison Approaches.Similarity-based detection remains one of the earliest and most intuitive strategies. These methods compare suspected applications with their original counterparts to identify code or resource similarities. Tools such as DroidMOSS¬†[5], CodeMatch¬†[4], and RomaDroid¬†[6] employ opcode sequences, fuzzy hashing, or manifest-based features to capture resemblance between app pairs. Other systems like CloneSpot¬†[16] analyze metadata available in app marketplaces, such as titles, developer names, and descriptions. While often effective in small-scale scenarios, similarity-based approaches face three critical challenges: (1) they require access to both the original and repackaged apps, which is unrealistic at marketplace scale; (2) they incur high computational overhead due to pairwise comparisons; and (3) they are vulnerable to obfuscation, library injection, and class reordering. These limitations hinder their scalability in real-world app markets. Runtime Monitoring Approaches.Runtime-based solutions attempt to detect repackaging by executing apps or embedding markers. For example, AppInk¬†[17] embeds watermarking code into applications, while Nguyen et al.¬†[18] apply perceptual hashing to dynamic UI screenshots. Such approaches reduce dependence on static code features and can bypass some obfuscation. However, runtime-based solutions depend heavily on developer cooperation (e.g., embedding watermarks) or require controlled execution environments, making them less practical for large-scale, automated marketplace scanning. Machine Learning Approaches.Recent studies leverage supervised and unsupervised learning to address the shortcomings of similarity-based detection. For example, SCSdroid¬†[19] uses system call sequences, DR-Droid¬†[20] employs code heterogeneity analysis, and DroidMat¬†[21] clusters applications using features such as API calls and manifest declarations. These models alleviate the dependency on explicit app pairs and offer greater adaptability. Nevertheless, many of these methods rely on large and complex feature sets‚Äîsometimes exceeding several hundred dimensions¬†[7]. High-dimensional feature spaces increase computational cost, reduce interpretability, and make the models more difficult to scale in production environments. Symptom Discovery Approaches.Symptom discovery methods focus on identifying artifacts consistently introduced during the repackaging process. AndroidSOO¬†[22], for example, exploits anomalies in the string offset order of Dalvik executable (.dex) files, achieving high detection accuracy with minimal computational requirements. This approach avoids dependence on original‚Äìrepackaged pairs and resists certain obfuscation techniques. However, symptom-based methods often hinge on specific characteristics of repackaging tools. Skilled adversaries who manually recompile or use alternative tools may circumvent these detection mechanisms, limiting their generalizability. Our Approach.Building on these insights, our approach, RepackDroid, introduces a hybrid detection model that combines the strengths of supervised learning and symptom discovery. Unlike prior machine learning efforts that rely on hundreds of features, we reduce the feature space to only 20 carefully selected attributes spanning ICC patterns, permission usage, sensitive API calls, and structural anomalies. This design balances accuracy with computational efficiency, making the approach suitable for large-scale deployment. Furthermore, our work is among the first to validate the predictive power of string offset order in combination with machine learning, using the large-scale RePack dataset¬†[8].",
            "3. The RepackDroid Approach": "This section presents the methodology underlying RepackDroid, our proposed framework for detecting repackaged Android applications. We first outline the overall approach, followed by detailed descriptions of the dataset construction, feature extraction process, machine learning model implementation, and evaluation methodology. 3.1. Overview of ApproachRepackDroid combines static analysis of Android applications with machine learning and symptom discovery.Figure 1shows the workflow of the proposed supervised learning-based detection framework. The workflow consists of four key stages:Figure 1.Workflow of the proposed supervised learning-based detection framework. The pipeline includes APK decompilation, feature extraction, and classification using machine learning models.Dataset Preparation: We curate and preprocess applications from the RePack dataset¬†[8], ensuring that each sample is properly labeled as original or repackaged.APK Decompilation and Parsing: Applications are decompiled using established tools to expose manifest files, Dalvik executable (DEX) bytecode, and intermediate smali code representations.Feature Extraction: We derive 20 discriminative features from four categories‚Äîinter-component communication (ICC), permissions, sensitive API usage, and structural anomalies.Classification: Supervised learning models are trained and tested to predict whether an application is repackaged. The feature set includes AndroidSOO‚Äôs¬†[22] string offset order anomaly as a low-cost, structural indicator of repackaging. 3.2. Dataset ConstructionWe utilized the RePack dataset¬†[8], a benchmark repository containing over 15,000¬†pairs of original and repackaged Android applications collected from AndroZoo¬†[23]. Each app pair is associated with SHA-256 hashes and metadata, which we used to automatically label samples as ‚Äúoriginal‚Äù or ‚Äúrepackaged.‚Äù Applications were downloaded via the AndroZoo API using an authenticated key. The dataset was organized into two directories: originals and repacks. This structure facilitated efficient labeling and batch processing. Each APK was decompiled using Apktool¬†[24] and baksmali¬†[25], producing the following artifacts:AndroidManifest.xml: Containing package metadata, permissions, and component definitions.Classes.dex: Dalvik bytecode compiled from Java sources..smali files: Human-readable intermediate code representation from DEX disassembly.During preprocessing, we identified and removed redundant samples. Specifically, one original app was found to appear in 1676 pairs (11% of the dataset). Since many of the associated repackaged versions were nearly identical, we excluded these to avoid data skew. After filtering, the final dataset consisted of 8441 unique applications (6081 repackaged, 2360 original). 3.3. Feature ExtractionTo enable automated and scalable processing of applications, we developed a custom feature extraction tool implemented in a combination of Java SE 12 and Python 3.4. This tool performs APK decompilation, parsing, and feature preprocessing in preparation for machine learning analysis.The workflow of the feature extraction tool is summarized in Algorithm¬†1. It begins by downloading APKs from the RePack dataset¬†[8], decompiling them to access manifest and bytecode files, and parsing inter-component communication (ICC) tuples following the approach in Elish et al.¬†[26]. Additional features such as permissions, sensitive API counts¬†[15,27], and string offset order anomalies¬†[22] are then integrated into a consolidated dataset for supervised learning.3.3.1. APK\nDecompilation and Preservation of DEX FilesThe workflow starts by decompiling APK file using apktool. After decompilation, APK contains the following artifacts: a .smali directory, a manifest file (AndroidManifest.xml), and one or more classes.dex files. A custom batch script was created to manage the decompilation process. While apktool is effective for converting APKs into smali code, its default process often discards the classes.dex file during smali folder generation. Since our methodology required access to both smali code and the original DEX file‚Äîparticularly for AndroidSOO‚Äôs string offset order analysis¬†[22]‚Äîour batch script made an explicit copy of the classes.dex file before disassembly. This ensured consistent availability of both¬†representations.Algorithm 1Feature Extraction Workflow.Require:RePack dataset (SHA-256 hashes of original and repackaged APKs)Ensure:Consolidated CSV dataset with 20 extracted features1:foreach APK in {originals, repacks}do2:Decompile APK usingapktoolto obtainAndroidManifest.xmland.smalifiles3:Copyclasses.dexprior to disassembly to preserve original bytecode4:ParseAndroidManifest.xmlto extract permissions and metadata5:foreach.smalifile in APKdo6:Identify ICC methods (e.g.,startActivity,sendBroadcast)7:Construct ICC tuple ‚å©ICC Name, Source Component, Target Component, Communication Type‚å™8:Update counters: Android APIs, Java APIs, user actions9:end for10:Store results in per-app CSV:11:‚Äì App metadata (permissions, AndroidSOO offset anomaly, label)12:‚Äì ICC tuples and incremental feature counts13:end for14:Consolidate per-app CSVs into a unified dataset using Python (pandas):15:‚Äì Compute numerical features from ICC relationships16:‚Äì Count sensitive API occurrences17:‚Äì Merge metadata into final schema18:returnStructured dataset with 20 features for machine learning classification3.3.2. Parsing Inter-Component Communication (ICC)A primary function of our tool is to identify an application‚Äôs .smali directory and parse its inter-component communication (ICC) events. We adopted the four-tuple ICC representation introduced by Elish et al.¬†[26], which consists of:ICC Name: the method central to initializing the communication (e.g., startActivity(), sendBroadcast()).Source Component: the component from which the intent originates.Target Component: the intended recipient of the communication, which may not always be identifiable due to limitations of smali code analysis. In cases where the target cannot be resolved, it is assigned a null value.Type of Communication: whether the intent is internal (within the same app) or external (directed outside the app), typically inferred through manifest file analysis.An example ICC tuple is shown inTable 1. Each time an ICC name was discovered in the code, a tuple was generated and appended to an ArrayList, which was later exported into the application‚Äôs CSV file.Table 1.Snippet of ICC Representation from Application.csv.3.3.3. Metadata and Preprocessing OutputAnother key function of our tool is to prepare a CSV file for each application. First, the tool attempted to extract the app‚Äôs name from its AndroidManifest.xml file. The CSV schema included multiple categories of information:Single-row metadata‚Äîsuch as the permissions list, AndroidSOO output, and binary classification label (repackaged vs. original).Iteratively updated fields‚Äîsuch as counts of Android APIs, Java APIs, and user actions, which were incremented with each discovered ICC tuple.This two-layer design prevented fragmentation of app data across multiple files while ensuring that both global and local features were captured.3.3.4. Consolidation and Feature EngineeringOnce all applications in the ‚Äúoriginals‚Äù and ‚Äúrepacks‚Äù directories were processed, their individual CSV files were funneled into a single directory. The final step is to iterate through these CSVs to construct the final dataset. During this step, the tool:Computed new numerical features derived from ICC tuples and their relationships.Counted sensitive API occurrences, following established lists from Elish et al.¬†[15] and Tian et al.¬†[27].Transferred metadata fields from the first row of each CSV into the consolidated dataset.The resulting dataset provided a structured and compact feature set, ready for building and training the machine learning classification models. 3.4. Feature Set DesignThe effectiveness of any machine learning approach for repackaged app detection depends critically on the quality of its feature representation. We designed a compact feature set of 20 attributes, grouped into four categories: inter-component communication (ICC) features, permissions features, sensitive API features, and structural anomaly feature. While these categories have been individually explored in prior work, to the best of our knowledge, their collective integration into a single lightweight model for repackaged app detection has not been reported in the literature. The complete set of features extracted for our model is summarized inTable 2, along with their descriptions and categorizations.Table 2.Description of the extracted features employed for repackaged app classification.3.4.1. ICC FeaturesInter-component communication (ICC) reflects how activities, services, and broadcast receivers interact within an application. Following the four-tuple ICC representation of Elish et al.¬†[26], we extract computable features that capture both the volume and structure of communications. These include the frequency of each ICC type, the ratio of internal versus external communications, the most common source and target components, and the aggregate component frequency.As illustrated inTable 3, ICC method calls such as startActivity() or sendBroadcast() serve as anchors for constructing these features. Because ICC governs the way Android components collaborate, deviations in these patterns often reveal artifacts of repackaging. Thus, ICC metrics provide both application uniqueness and cross-sample comparability for classification.Table 3.Categorizations of ICC Methods.3.4.2. Permissions FeaturesApplication permissions, declared in AndroidManifest.xml, represent another critical dimension of security analysis. Permissions are both easy to extract and highly informative, since many malicious repackaged apps request excessive or unnecessary access. Leveraging the Android documentation¬†[28], we categorize permissions into normal (low risk), dangerous (high risk), and signature (granted only to apps signed with the same certificate).From these categories, we compute the total number of permissions, the count of each type, and a riskiness score defined as the ratio of dangerous to total permissions. Prior work has shown that malware families often exhibit consistent permission usage patterns¬†[29], making these features highly predictive for repackaging detection.3.4.3. Sensitive API FeaturesWhereas permissions describe declared intent, sensitive APIs capture operational behavior. Using the curated API lists from Elish et al.¬†[15] and Tian et al.¬†[27], we measure the frequency of Android-specific and Java-specific API calls across each application. Additional derived metrics include:Total API count (sum of Android and Java APIs).API-per-component ratio, estimating the average intensity of API usage.User action counts, measuring the frequency of user-triggered interactions.User action‚Äìper-component ratio, normalizing these interactions by application size.For sensitive APIs, we include illustrative Android and Java API calls that are frequently associated with high-risk behavior. Examples include telephony and messaging interfaces (e.g.,SmsManager.sendTextMessage,TelephonyManager.getDeviceId), system-level operations (e.g.,Runtime.exec), and network or data-exfiltration APIs (e.g.,HttpURLConnection.connect,Socket.write). These examples clarify how the selected API categories capture behavioral traits that are commonly modified or introduced during repackaging.For user-triggered interactions, we highlight representative callbacks such asonClick,onTouchEvent, andstartActivityForResult, which indicate execution paths initiated through user input. Because injected or altered malicious payloads frequently leverage these interaction points, these features provide additional discriminative value for distinguishing repackaged from original applications.Together, these features characterize the ‚Äúmuscular system‚Äù of an application‚Äôs behavior, complementing the ICC and permissions features. Repackaging often alters these patterns due to injected functionality or code restructuring.3.4.4. Structural Anomaly Feature: String Offset OrderThe final feature category introduces a symptom discovery dimension based on structural anomalies. Specifically, we include the String Offset Order (SOO) feature, derived from AndroidSOO [22]. According to the Dalvik executable specification, string identifiers in DEX files should appear in alphabetical order; however, when an application is repackaged using tools such as apktool, this ordering is often disrupted.We encode SOO as a binary feature (‚Äúin order‚Äù vs. ‚Äúout of order‚Äù). Although simple, prior work has shown it to be a powerful low-cost indicator of tampering [22].",
            "3.1. Overview of Approach": "RepackDroid combines static analysis of Android applications with machine learning and symptom discovery.Figure 1shows the workflow of the proposed supervised learning-based detection framework. The workflow consists of four key stages: Figure 1.Workflow of the proposed supervised learning-based detection framework. The pipeline includes APK decompilation, feature extraction, and classification using machine learning models. Dataset Preparation: We curate and preprocess applications from the RePack dataset¬†[8], ensuring that each sample is properly labeled as original or repackaged.APK Decompilation and Parsing: Applications are decompiled using established tools to expose manifest files, Dalvik executable (DEX) bytecode, and intermediate smali code representations.Feature Extraction: We derive 20 discriminative features from four categories‚Äîinter-component communication (ICC), permissions, sensitive API usage, and structural anomalies.Classification: Supervised learning models are trained and tested to predict whether an application is repackaged. The feature set includes AndroidSOO‚Äôs¬†[22] string offset order anomaly as a low-cost, structural indicator of repackaging.",
            "3.2. Dataset Construction": "We utilized the RePack dataset¬†[8], a benchmark repository containing over 15,000¬†pairs of original and repackaged Android applications collected from AndroZoo¬†[23]. Each app pair is associated with SHA-256 hashes and metadata, which we used to automatically label samples as ‚Äúoriginal‚Äù or ‚Äúrepackaged.‚Äù Applications were downloaded via the AndroZoo API using an authenticated key. The dataset was organized into two directories: originals and repacks. This structure facilitated efficient labeling and batch processing. Each APK was decompiled using Apktool¬†[24] and baksmali¬†[25], producing the following artifacts: AndroidManifest.xml: Containing package metadata, permissions, and component definitions.Classes.dex: Dalvik bytecode compiled from Java sources..smali files: Human-readable intermediate code representation from DEX disassembly. During preprocessing, we identified and removed redundant samples. Specifically, one original app was found to appear in 1676 pairs (11% of the dataset). Since many of the associated repackaged versions were nearly identical, we excluded these to avoid data skew. After filtering, the final dataset consisted of 8441 unique applications (6081 repackaged, 2360 original).",
            "3.3. Feature Extraction": "To enable automated and scalable processing of applications, we developed a custom feature extraction tool implemented in a combination of Java SE 12 and Python 3.4. This tool performs APK decompilation, parsing, and feature preprocessing in preparation for machine learning analysis. The workflow of the feature extraction tool is summarized in Algorithm¬†1. It begins by downloading APKs from the RePack dataset¬†[8], decompiling them to access manifest and bytecode files, and parsing inter-component communication (ICC) tuples following the approach in Elish et al.¬†[26]. Additional features such as permissions, sensitive API counts¬†[15,27], and string offset order anomalies¬†[22] are then integrated into a consolidated dataset for supervised learning. 3.3.1. APK\nDecompilation and Preservation of DEX FilesThe workflow starts by decompiling APK file using apktool. After decompilation, APK contains the following artifacts: a .smali directory, a manifest file (AndroidManifest.xml), and one or more classes.dex files. A custom batch script was created to manage the decompilation process. While apktool is effective for converting APKs into smali code, its default process often discards the classes.dex file during smali folder generation. Since our methodology required access to both smali code and the original DEX file‚Äîparticularly for AndroidSOO‚Äôs string offset order analysis¬†[22]‚Äîour batch script made an explicit copy of the classes.dex file before disassembly. This ensured consistent availability of both¬†representations.Algorithm 1Feature Extraction Workflow.Require:RePack dataset (SHA-256 hashes of original and repackaged APKs)Ensure:Consolidated CSV dataset with 20 extracted features1:foreach APK in {originals, repacks}do2:Decompile APK usingapktoolto obtainAndroidManifest.xmland.smalifiles3:Copyclasses.dexprior to disassembly to preserve original bytecode4:ParseAndroidManifest.xmlto extract permissions and metadata5:foreach.smalifile in APKdo6:Identify ICC methods (e.g.,startActivity,sendBroadcast)7:Construct ICC tuple ‚å©ICC Name, Source Component, Target Component, Communication Type‚å™8:Update counters: Android APIs, Java APIs, user actions9:end for10:Store results in per-app CSV:11:‚Äì App metadata (permissions, AndroidSOO offset anomaly, label)12:‚Äì ICC tuples and incremental feature counts13:end for14:Consolidate per-app CSVs into a unified dataset using Python (pandas):15:‚Äì Compute numerical features from ICC relationships16:‚Äì Count sensitive API occurrences17:‚Äì Merge metadata into final schema18:returnStructured dataset with 20 features for machine learning classification 3.3.2. Parsing Inter-Component Communication (ICC)A primary function of our tool is to identify an application‚Äôs .smali directory and parse its inter-component communication (ICC) events. We adopted the four-tuple ICC representation introduced by Elish et al.¬†[26], which consists of:ICC Name: the method central to initializing the communication (e.g., startActivity(), sendBroadcast()).Source Component: the component from which the intent originates.Target Component: the intended recipient of the communication, which may not always be identifiable due to limitations of smali code analysis. In cases where the target cannot be resolved, it is assigned a null value.Type of Communication: whether the intent is internal (within the same app) or external (directed outside the app), typically inferred through manifest file analysis.An example ICC tuple is shown inTable 1. Each time an ICC name was discovered in the code, a tuple was generated and appended to an ArrayList, which was later exported into the application‚Äôs CSV file.Table 1.Snippet of ICC Representation from Application.csv. 3.3.3. Metadata and Preprocessing OutputAnother key function of our tool is to prepare a CSV file for each application. First, the tool attempted to extract the app‚Äôs name from its AndroidManifest.xml file. The CSV schema included multiple categories of information:Single-row metadata‚Äîsuch as the permissions list, AndroidSOO output, and binary classification label (repackaged vs. original).Iteratively updated fields‚Äîsuch as counts of Android APIs, Java APIs, and user actions, which were incremented with each discovered ICC tuple.This two-layer design prevented fragmentation of app data across multiple files while ensuring that both global and local features were captured. 3.3.4. Consolidation and Feature EngineeringOnce all applications in the ‚Äúoriginals‚Äù and ‚Äúrepacks‚Äù directories were processed, their individual CSV files were funneled into a single directory. The final step is to iterate through these CSVs to construct the final dataset. During this step, the tool:Computed new numerical features derived from ICC tuples and their relationships.Counted sensitive API occurrences, following established lists from Elish et al.¬†[15] and Tian et al.¬†[27].Transferred metadata fields from the first row of each CSV into the consolidated dataset.The resulting dataset provided a structured and compact feature set, ready for building and training the machine learning classification models.",
            "3.3.1. APK\nDecompilation and Preservation of DEX Files": "The workflow starts by decompiling APK file using apktool. After decompilation, APK contains the following artifacts: a .smali directory, a manifest file (AndroidManifest.xml), and one or more classes.dex files. A custom batch script was created to manage the decompilation process. While apktool is effective for converting APKs into smali code, its default process often discards the classes.dex file during smali folder generation. Since our methodology required access to both smali code and the original DEX file‚Äîparticularly for AndroidSOO‚Äôs string offset order analysis¬†[22]‚Äîour batch script made an explicit copy of the classes.dex file before disassembly. This ensured consistent availability of both¬†representations.Algorithm 1Feature Extraction Workflow.Require:RePack dataset (SHA-256 hashes of original and repackaged APKs)Ensure:Consolidated CSV dataset with 20 extracted features1:foreach APK in {originals, repacks}do2:Decompile APK usingapktoolto obtainAndroidManifest.xmland.smalifiles3:Copyclasses.dexprior to disassembly to preserve original bytecode4:ParseAndroidManifest.xmlto extract permissions and metadata5:foreach.smalifile in APKdo6:Identify ICC methods (e.g.,startActivity,sendBroadcast)7:Construct ICC tuple ‚å©ICC Name, Source Component, Target Component, Communication Type‚å™8:Update counters: Android APIs, Java APIs, user actions9:end for10:Store results in per-app CSV:11:‚Äì App metadata (permissions, AndroidSOO offset anomaly, label)12:‚Äì ICC tuples and incremental feature counts13:end for14:Consolidate per-app CSVs into a unified dataset using Python (pandas):15:‚Äì Compute numerical features from ICC relationships16:‚Äì Count sensitive API occurrences17:‚Äì Merge metadata into final schema18:returnStructured dataset with 20 features for machine learning classification",
            "3.3.2. Parsing Inter-Component Communication (ICC)": "A primary function of our tool is to identify an application‚Äôs .smali directory and parse its inter-component communication (ICC) events. We adopted the four-tuple ICC representation introduced by Elish et al.¬†[26], which consists of: ICC Name: the method central to initializing the communication (e.g., startActivity(), sendBroadcast()).Source Component: the component from which the intent originates.Target Component: the intended recipient of the communication, which may not always be identifiable due to limitations of smali code analysis. In cases where the target cannot be resolved, it is assigned a null value.Type of Communication: whether the intent is internal (within the same app) or external (directed outside the app), typically inferred through manifest file analysis. An example ICC tuple is shown inTable 1. Each time an ICC name was discovered in the code, a tuple was generated and appended to an ArrayList, which was later exported into the application‚Äôs CSV file. Table 1.Snippet of ICC Representation from Application.csv.",
            "3.3.3. Metadata and Preprocessing Output": "Another key function of our tool is to prepare a CSV file for each application. First, the tool attempted to extract the app‚Äôs name from its AndroidManifest.xml file. The CSV schema included multiple categories of information: Single-row metadata‚Äîsuch as the permissions list, AndroidSOO output, and binary classification label (repackaged vs. original).Iteratively updated fields‚Äîsuch as counts of Android APIs, Java APIs, and user actions, which were incremented with each discovered ICC tuple. This two-layer design prevented fragmentation of app data across multiple files while ensuring that both global and local features were captured.",
            "3.3.4. Consolidation and Feature Engineering": "Once all applications in the ‚Äúoriginals‚Äù and ‚Äúrepacks‚Äù directories were processed, their individual CSV files were funneled into a single directory. The final step is to iterate through these CSVs to construct the final dataset. During this step, the tool: Computed new numerical features derived from ICC tuples and their relationships.Counted sensitive API occurrences, following established lists from Elish et al.¬†[15] and Tian et al.¬†[27].Transferred metadata fields from the first row of each CSV into the consolidated dataset. The resulting dataset provided a structured and compact feature set, ready for building and training the machine learning classification models.",
            "3.4. Feature Set Design": "The effectiveness of any machine learning approach for repackaged app detection depends critically on the quality of its feature representation. We designed a compact feature set of 20 attributes, grouped into four categories: inter-component communication (ICC) features, permissions features, sensitive API features, and structural anomaly feature. While these categories have been individually explored in prior work, to the best of our knowledge, their collective integration into a single lightweight model for repackaged app detection has not been reported in the literature. The complete set of features extracted for our model is summarized inTable 2, along with their descriptions and categorizations. Table 2.Description of the extracted features employed for repackaged app classification. 3.4.1. ICC FeaturesInter-component communication (ICC) reflects how activities, services, and broadcast receivers interact within an application. Following the four-tuple ICC representation of Elish et al.¬†[26], we extract computable features that capture both the volume and structure of communications. These include the frequency of each ICC type, the ratio of internal versus external communications, the most common source and target components, and the aggregate component frequency.As illustrated inTable 3, ICC method calls such as startActivity() or sendBroadcast() serve as anchors for constructing these features. Because ICC governs the way Android components collaborate, deviations in these patterns often reveal artifacts of repackaging. Thus, ICC metrics provide both application uniqueness and cross-sample comparability for classification.Table 3.Categorizations of ICC Methods. 3.4.2. Permissions FeaturesApplication permissions, declared in AndroidManifest.xml, represent another critical dimension of security analysis. Permissions are both easy to extract and highly informative, since many malicious repackaged apps request excessive or unnecessary access. Leveraging the Android documentation¬†[28], we categorize permissions into normal (low risk), dangerous (high risk), and signature (granted only to apps signed with the same certificate).From these categories, we compute the total number of permissions, the count of each type, and a riskiness score defined as the ratio of dangerous to total permissions. Prior work has shown that malware families often exhibit consistent permission usage patterns¬†[29], making these features highly predictive for repackaging detection. 3.4.3. Sensitive API FeaturesWhereas permissions describe declared intent, sensitive APIs capture operational behavior. Using the curated API lists from Elish et al.¬†[15] and Tian et al.¬†[27], we measure the frequency of Android-specific and Java-specific API calls across each application. Additional derived metrics include:Total API count (sum of Android and Java APIs).API-per-component ratio, estimating the average intensity of API usage.User action counts, measuring the frequency of user-triggered interactions.User action‚Äìper-component ratio, normalizing these interactions by application size.For sensitive APIs, we include illustrative Android and Java API calls that are frequently associated with high-risk behavior. Examples include telephony and messaging interfaces (e.g.,SmsManager.sendTextMessage,TelephonyManager.getDeviceId), system-level operations (e.g.,Runtime.exec), and network or data-exfiltration APIs (e.g.,HttpURLConnection.connect,Socket.write). These examples clarify how the selected API categories capture behavioral traits that are commonly modified or introduced during repackaging.For user-triggered interactions, we highlight representative callbacks such asonClick,onTouchEvent, andstartActivityForResult, which indicate execution paths initiated through user input. Because injected or altered malicious payloads frequently leverage these interaction points, these features provide additional discriminative value for distinguishing repackaged from original applications.Together, these features characterize the ‚Äúmuscular system‚Äù of an application‚Äôs behavior, complementing the ICC and permissions features. Repackaging often alters these patterns due to injected functionality or code restructuring. 3.4.4. Structural Anomaly Feature: String Offset OrderThe final feature category introduces a symptom discovery dimension based on structural anomalies. Specifically, we include the String Offset Order (SOO) feature, derived from AndroidSOO [22]. According to the Dalvik executable specification, string identifiers in DEX files should appear in alphabetical order; however, when an application is repackaged using tools such as apktool, this ordering is often disrupted.We encode SOO as a binary feature (‚Äúin order‚Äù vs. ‚Äúout of order‚Äù). Although simple, prior work has shown it to be a powerful low-cost indicator of tampering [22].",
            "3.4.1. ICC Features": "Inter-component communication (ICC) reflects how activities, services, and broadcast receivers interact within an application. Following the four-tuple ICC representation of Elish et al.¬†[26], we extract computable features that capture both the volume and structure of communications. These include the frequency of each ICC type, the ratio of internal versus external communications, the most common source and target components, and the aggregate component frequency. As illustrated inTable 3, ICC method calls such as startActivity() or sendBroadcast() serve as anchors for constructing these features. Because ICC governs the way Android components collaborate, deviations in these patterns often reveal artifacts of repackaging. Thus, ICC metrics provide both application uniqueness and cross-sample comparability for classification. Table 3.Categorizations of ICC Methods.",
            "3.4.2. Permissions Features": "Application permissions, declared in AndroidManifest.xml, represent another critical dimension of security analysis. Permissions are both easy to extract and highly informative, since many malicious repackaged apps request excessive or unnecessary access. Leveraging the Android documentation¬†[28], we categorize permissions into normal (low risk), dangerous (high risk), and signature (granted only to apps signed with the same certificate). From these categories, we compute the total number of permissions, the count of each type, and a riskiness score defined as the ratio of dangerous to total permissions. Prior work has shown that malware families often exhibit consistent permission usage patterns¬†[29], making these features highly predictive for repackaging detection.",
            "3.4.3. Sensitive API Features": "Whereas permissions describe declared intent, sensitive APIs capture operational behavior. Using the curated API lists from Elish et al.¬†[15] and Tian et al.¬†[27], we measure the frequency of Android-specific and Java-specific API calls across each application. Additional derived metrics include: Total API count (sum of Android and Java APIs).API-per-component ratio, estimating the average intensity of API usage.User action counts, measuring the frequency of user-triggered interactions.User action‚Äìper-component ratio, normalizing these interactions by application size. For sensitive APIs, we include illustrative Android and Java API calls that are frequently associated with high-risk behavior. Examples include telephony and messaging interfaces (e.g.,SmsManager.sendTextMessage,TelephonyManager.getDeviceId), system-level operations (e.g.,Runtime.exec), and network or data-exfiltration APIs (e.g.,HttpURLConnection.connect,Socket.write). These examples clarify how the selected API categories capture behavioral traits that are commonly modified or introduced during repackaging. For user-triggered interactions, we highlight representative callbacks such asonClick,onTouchEvent, andstartActivityForResult, which indicate execution paths initiated through user input. Because injected or altered malicious payloads frequently leverage these interaction points, these features provide additional discriminative value for distinguishing repackaged from original applications. Together, these features characterize the ‚Äúmuscular system‚Äù of an application‚Äôs behavior, complementing the ICC and permissions features. Repackaging often alters these patterns due to injected functionality or code restructuring.",
            "3.4.4. Structural Anomaly Feature: String Offset Order": "The final feature category introduces a symptom discovery dimension based on structural anomalies. Specifically, we include the String Offset Order (SOO) feature, derived from AndroidSOO [22]. According to the Dalvik executable specification, string identifiers in DEX files should appear in alphabetical order; however, when an application is repackaged using tools such as apktool, this ordering is often disrupted. We encode SOO as a binary feature (‚Äúin order‚Äù vs. ‚Äúout of order‚Äù). Although simple, prior work has shown it to be a powerful low-cost indicator of tampering [22].",
            "4. Dataset Characterization and Preprocessing": "Before training machine learning models, it is critical to understand the statistical properties of the dataset. Such analysis not only provides insight into the relationships among features but also guides preprocessing decisions to reduce redundancy, balance feature contributions, and filter noisy or duplicated samples. We conducted three complementary analyses: feature correlation analysis, distributional assessment, and outlier detection. 4.1. Correlation AnalysisA correlation matrix was computed across all extracted features to evaluate their interrelationships. As visualized inFigure 2, the resulting heatmap provides a global view of how features relate to one another across the 9524 Android applications initially analyzed. This visualization confirms that certain attributes are naturally correlated. For example, userActionCount exhibits strong positive correlation with both component frequencies and API counts, reflecting the fact that larger applications tend to have more code, more components, and more user-triggered events.Figure 2.Correlation heatmap of the extracted dataset features. The visualization highlights relationships among ICC, permissions, sensitive APIs, and structural anomaly features across studied Android applications, with color intensity indicating the strength and direction of pairwise correlations.The correlation map also validates the logical groupings introduced inSection 3.4(ICC, permissions, APIs, and structural anomalies). Features within the same category cluster together, appearing as distinct quadrilateral regions in the heatmap (e.g., the block spanning BroadcastReceiverOccurrenceFrequency to ExternalOccurrence).To avoid feature redundancy, we applied dimensionality reduction based on these correlations. Within the permission-related features, only RiskRatePerPerms was retained, as it provided the strongest correlation with the classification label. Similarly, within the API-related features, totalApiCount was chosen to represent the group of highly correlated metrics (javaApiCount, androidApiCount, and totalApiCount). This pruning preserved informativeness while lowering model complexity. 4.2. Distributional AnalysisFeature distributions were then examined through histograms, shown inFigure 3. These visualizations highlight the range and skew of each attribute across the dataset. Results indicate that the majority of applications are relatively small in scale, characterized by lower component counts, fewer permissions, and limited API usage.Figure 3.Histograms of the extracted dataset features. The plots illustrate the distribution of ICC, permissions, sensitive APIs, and structural anomaly attributes across studied Android applications, highlighting the predominance of smaller apps with relatively low feature counts and the presence of a minority of larger, more complex outliers.Conversely, a smaller subset of applications exhibited disproportionately higher values, with totalApiCount reaching 800‚Äì1000 and userActionCount exceeding 100 events. In contrast, smaller apps typically contained fewer than 200 API calls and 25 user actions. This observation suggests a long-tailed distribution: while most apps are lightweight, a minority are significantly more complex, with feature values four to five times greater. 4.3. Outlier Detection and FilteringFinally, we examined dataset composition for potential outliers and redundancies. Analysis of the RePack dataset revealed that one original application appeared in 1676 repackaged pairs, accounting for roughly 11% of the entire dataset. While in principle each repackaged variant could be distinct, feature extraction showed that many of these samples were nearly identical, providing little additional training value.To prevent the model from being biased by such duplicates, these redundant samples were excluded wherever identified. After filtering, the dataset size was reduced from 9524 to 8441 applications, yielding a cleaner and more representative corpus for experimentation.",
            "4.1. Correlation Analysis": "A correlation matrix was computed across all extracted features to evaluate their interrelationships. As visualized inFigure 2, the resulting heatmap provides a global view of how features relate to one another across the 9524 Android applications initially analyzed. This visualization confirms that certain attributes are naturally correlated. For example, userActionCount exhibits strong positive correlation with both component frequencies and API counts, reflecting the fact that larger applications tend to have more code, more components, and more user-triggered events. Figure 2.Correlation heatmap of the extracted dataset features. The visualization highlights relationships among ICC, permissions, sensitive APIs, and structural anomaly features across studied Android applications, with color intensity indicating the strength and direction of pairwise correlations. The correlation map also validates the logical groupings introduced inSection 3.4(ICC, permissions, APIs, and structural anomalies). Features within the same category cluster together, appearing as distinct quadrilateral regions in the heatmap (e.g., the block spanning BroadcastReceiverOccurrenceFrequency to ExternalOccurrence). To avoid feature redundancy, we applied dimensionality reduction based on these correlations. Within the permission-related features, only RiskRatePerPerms was retained, as it provided the strongest correlation with the classification label. Similarly, within the API-related features, totalApiCount was chosen to represent the group of highly correlated metrics (javaApiCount, androidApiCount, and totalApiCount). This pruning preserved informativeness while lowering model complexity.",
            "4.2. Distributional Analysis": "Feature distributions were then examined through histograms, shown inFigure 3. These visualizations highlight the range and skew of each attribute across the dataset. Results indicate that the majority of applications are relatively small in scale, characterized by lower component counts, fewer permissions, and limited API usage. Figure 3.Histograms of the extracted dataset features. The plots illustrate the distribution of ICC, permissions, sensitive APIs, and structural anomaly attributes across studied Android applications, highlighting the predominance of smaller apps with relatively low feature counts and the presence of a minority of larger, more complex outliers. Conversely, a smaller subset of applications exhibited disproportionately higher values, with totalApiCount reaching 800‚Äì1000 and userActionCount exceeding 100 events. In contrast, smaller apps typically contained fewer than 200 API calls and 25 user actions. This observation suggests a long-tailed distribution: while most apps are lightweight, a minority are significantly more complex, with feature values four to five times greater.",
            "4.3. Outlier Detection and Filtering": "Finally, we examined dataset composition for potential outliers and redundancies. Analysis of the RePack dataset revealed that one original application appeared in 1676 repackaged pairs, accounting for roughly 11% of the entire dataset. While in principle each repackaged variant could be distinct, feature extraction showed that many of these samples were nearly identical, providing little additional training value. To prevent the model from being biased by such duplicates, these redundant samples were excluded wherever identified. After filtering, the dataset size was reduced from 9524 to 8441 applications, yielding a cleaner and more representative corpus for experimentation.",
            "5. Evaluation Results": "This section presents the results of our experimental evaluation. We begin with a summary of the dataset used for training and testing, followed by a description of evaluation metrics. We then report classification results across several machine learning algorithms and compare our approach with state-of-the-art techniques from the literature. 5.1. Dataset Summary and PreprocessingAfter filtering redundant entries as described inSection 4.3, the final dataset contained 8441 Android applications, of which 6081 were repackaged and 2360 were original. This distribution indicates a significant class imbalance, with approximately 72% of samples belonging to the repackaged class.To address the imbalance, we applied the Synthetic Minority Oversampling Technique (SMOTE) [30], which generates synthetic minority-class samples based on feature-space interpolation of nearest neighbors. Unlike simple duplication, SMOTE increases diversity within the minority class and reduces overfitting, thereby improving generalization. For all experiments, the dataset was split into 80% training and 20% testing partitions.To assess the scalability of the proposed framework, we measured the execution time of the feature extraction pipeline across the full set of 8441 apps. Our Java-based feature extraction tool processed decompiled APK directories at an average rate of approximately 0.9 min per app, using standard workstation (Intel i7-12700K CPU, 32 GB RAM), to complete extraction of ICC tuples, sensitive API counts, permission metrics, and the SOO feature. These results empirically support the scalability claims of our approach and demonstrate that the proposed lightweight feature set enables efficient analysis even across large collections of apps. We acknowledge, however, that decompilation remains the dominant cost, and future work will explore parallelized APK processing and incremental parsing techniques to further reduce execution time. 5.2. Evaluation MetricsWe employed four widely adopted evaluation metrics for classification: precision, recall, F1-score, and precision‚Äìrecall area under the curve (PR-AUC).Precision=ùëáùëÉùëáùëÉ+ùêπùëÉPrecision=TPTP+FPRecall=ùëáùëÉùëáùëÉ+ùêπùëÅRecall=TPTP+FNF1=2‚àóùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ‚àóùëÖùëíùëêùëéùëôùëôùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ+ùëÖùëíùëêùëéùëôùëô=2‚àóùëáùëÉ2‚àóùëáùëÉ+ùêπùëÉ+ùêπùëÅF1=2‚àóPrecision‚àóRecallPrecision+Recall=2‚àóTP2‚àóTP+FP+FNPrecision measures the proportion of predicted repackaged apps that were correct, whereas recall measures the proportion of actual repackaged apps that were identified. F1-score balances these two metrics by computing their harmonic mean.Given the imbalanced dataset, we also emphasized PR-AUC, which summarizes classifier performance across multiple thresholds. Branco et al. [31] recommend PR-AUC as a more reliable measure than ROC-AUC in imbalanced domains. 5.3. Model PerformanceWe trained and evaluated multiple supervised learning algorithms using the scikit-learn library, including Support Vector Machine (SVM), k-Nearest Neighbors (kNN), Gaussian Naive Bayes, Logistic Regression, Random Forest, Decision Tree, and a simple Feedforward Neural Network. Performance results are reported inTable 4. The findings indicate that:Table 4.Performance metrics of supervised learning algorithms on our dataset. Precision, recall, F1-score, and precision‚Äìrecall area under the curve (PR-AUC) are reported for each classifier.Neural Network achieved the best recall (100%), successfully identifying all repackaged samples in the test set.SVM achieved the highest F1-score (85.9%), offering the best balance between precision and recall.Logistic Regression achieved the highest PR-AUC (87.8%), demonstrating stable performance across thresholds.These outcomes are further visualized inFigure 4, which shows the precision‚Äìrecall curves for the tested algorithms. Both KNN and Random Forest exhibited particularly strong PR curves, indicating robust trade-offs between false positives and false negatives.Figure 4.Precision‚Äìrecall curves for the evaluated algorithms on our dataset dataset. The curves illustrate the trade-off between precision and recall across thresholds, with area under the curve (PR-AUC) values highlighting overall detection performance.Cross-Validation and Statistical Robustness.To strengthen the reliability of the comparative evaluation, we conducted a 5-fold cross-validation experiment on the final feature set. For each classifier, performance was averaged across folds, and standard deviations were computed to quantify variability. The cross-validation results were consistent with those obtained from the original 80/20 train‚Äìtest split, confirming that the performance differences observed among classifiers are stable rather than artifacts of a single partition. As expected, Support Vector Machines and k-Nearest Neighbors continued to yield the strongest recall and F1-score profiles, with standard deviations typically below 0.02 across folds.",
            "5.1. Dataset Summary and Preprocessing": "After filtering redundant entries as described inSection 4.3, the final dataset contained 8441 Android applications, of which 6081 were repackaged and 2360 were original. This distribution indicates a significant class imbalance, with approximately 72% of samples belonging to the repackaged class. To address the imbalance, we applied the Synthetic Minority Oversampling Technique (SMOTE) [30], which generates synthetic minority-class samples based on feature-space interpolation of nearest neighbors. Unlike simple duplication, SMOTE increases diversity within the minority class and reduces overfitting, thereby improving generalization. For all experiments, the dataset was split into 80% training and 20% testing partitions. To assess the scalability of the proposed framework, we measured the execution time of the feature extraction pipeline across the full set of 8441 apps. Our Java-based feature extraction tool processed decompiled APK directories at an average rate of approximately 0.9 min per app, using standard workstation (Intel i7-12700K CPU, 32 GB RAM), to complete extraction of ICC tuples, sensitive API counts, permission metrics, and the SOO feature. These results empirically support the scalability claims of our approach and demonstrate that the proposed lightweight feature set enables efficient analysis even across large collections of apps. We acknowledge, however, that decompilation remains the dominant cost, and future work will explore parallelized APK processing and incremental parsing techniques to further reduce execution time.",
            "5.2. Evaluation Metrics": "We employed four widely adopted evaluation metrics for classification: precision, recall, F1-score, and precision‚Äìrecall area under the curve (PR-AUC).Precision=ùëáùëÉùëáùëÉ+ùêπùëÉPrecision=TPTP+FPRecall=ùëáùëÉùëáùëÉ+ùêπùëÅRecall=TPTP+FNF1=2‚àóùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ‚àóùëÖùëíùëêùëéùëôùëôùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ+ùëÖùëíùëêùëéùëôùëô=2‚àóùëáùëÉ2‚àóùëáùëÉ+ùêπùëÉ+ùêπùëÅF1=2‚àóPrecision‚àóRecallPrecision+Recall=2‚àóTP2‚àóTP+FP+FN Precision measures the proportion of predicted repackaged apps that were correct, whereas recall measures the proportion of actual repackaged apps that were identified. F1-score balances these two metrics by computing their harmonic mean. Given the imbalanced dataset, we also emphasized PR-AUC, which summarizes classifier performance across multiple thresholds. Branco et al. [31] recommend PR-AUC as a more reliable measure than ROC-AUC in imbalanced domains.",
            "5.3. Model Performance": "We trained and evaluated multiple supervised learning algorithms using the scikit-learn library, including Support Vector Machine (SVM), k-Nearest Neighbors (kNN), Gaussian Naive Bayes, Logistic Regression, Random Forest, Decision Tree, and a simple Feedforward Neural Network. Performance results are reported inTable 4. The findings indicate that: Table 4.Performance metrics of supervised learning algorithms on our dataset. Precision, recall, F1-score, and precision‚Äìrecall area under the curve (PR-AUC) are reported for each classifier. Neural Network achieved the best recall (100%), successfully identifying all repackaged samples in the test set.SVM achieved the highest F1-score (85.9%), offering the best balance between precision and recall.Logistic Regression achieved the highest PR-AUC (87.8%), demonstrating stable performance across thresholds. These outcomes are further visualized inFigure 4, which shows the precision‚Äìrecall curves for the tested algorithms. Both KNN and Random Forest exhibited particularly strong PR curves, indicating robust trade-offs between false positives and false negatives. Figure 4.Precision‚Äìrecall curves for the evaluated algorithms on our dataset dataset. The curves illustrate the trade-off between precision and recall across thresholds, with area under the curve (PR-AUC) values highlighting overall detection performance. Cross-Validation and Statistical Robustness.To strengthen the reliability of the comparative evaluation, we conducted a 5-fold cross-validation experiment on the final feature set. For each classifier, performance was averaged across folds, and standard deviations were computed to quantify variability. The cross-validation results were consistent with those obtained from the original 80/20 train‚Äìtest split, confirming that the performance differences observed among classifiers are stable rather than artifacts of a single partition. As expected, Support Vector Machines and k-Nearest Neighbors continued to yield the strongest recall and F1-score profiles, with standard deviations typically below 0.02 across folds.",
            "6. Discussion": "6.1. Comparative Analysis with State-of-the-ArtWe summarize the most prominent state-of-the-art techniques for repackaged app detection inTable 5. Some of these techniques are not open-sourced and it is hard to re-implement or test them, or reproduce their results. For comparison, we highlight the approach type, number of features used, evaluation scale, and detection performance of each method.Table 5.Summary of representative state-of-the-art techniques for repackaged app detection.Our approach, RepackDroid, achieved recall = 98.8% and F1 = 85.9% while using only 20 features, though precision was somewhat lower (72.0%). This trade-off underscores the complementary strengths of the two approaches. Li et al.‚Äôs model [7] prioritizes precision through a large, feature-rich representation, whereas RepackDroid emphasizes recall and efficiency, making it more appropriate for marketplace-scale deployment where missing a repackaged app (false negative) is more damaging than mistakenly flagging a legitimate one (false positive).When compared with similarity-based detection methods, our system demonstrates a different performance profile. Similarity-based tools such as CodeMatch [4] and RomaDroid [6] often achieve precision scores above 90%, substantially higher than our average of 73.4% across eight algorithms. However, as discussed inSection 2, these methods depend on pairwise comparisons between original and candidate apps. While effective on small datasets, this approach does not scale to millions of applications due to the combinatorial explosion of comparisons. Indeed, most similarity-based studies evaluate their tools on relatively small test sets, limiting their applicability to real-world markets.In contrast, our method trades some precision for scalability, recall, and efficiency, which are essential characteristics for practical deployment in large-scale app stores. By reducing the feature space and eliminating the need for app pairs, RepackDroid provides a lightweight yet robust alternative to both similarity-based and high-dimensional supervised learning approaches. 6.2. Role of String Offset Order in DetectionA key distinguishing aspect of our framework is the ability to achieve competitive performance with only 20 features, compared to the 521 features employed by Li et al. [7]. This reduction is not merely a matter of computational efficiency‚Äîit also demonstrates that carefully selected, discriminative features can capture the essence of repackaging behavior without requiring high-dimensional representations.Among the features incorporated in our model, the String Offset Order (SOO) anomaly proposed by Gonzalez et al. [22] via the AndroidSOO tool emerged as particularly impactful. SOO exploits the structural property of Dalvik executable files, where string identifiers should appear in alphabetical order according to the DEX specification. When an app is repackaged using tools such as apktool, this ordering is often disrupted, leaving a reliable ‚Äúsymptom‚Äù of tampering.Our experiments confirmed the utility of SOO when integrated into a broader supervised learning model. As shown inTable 6, the SOO feature alone demonstrates meaningful predictive power in distinguishing repackaged from original applications. More importantly, its combination with ICC, permissions, and sensitive API features improved overall accuracy and recall compared to models that excluded it. This aligns with the prediction made in Gonzalez et al.‚Äôs original work [22], but our contribution provides a stronger empirical validation: unlike their initial study, we evaluated SOO using the RePack dataset, which is specifically curated for repackaged application detection.Table 6.Performance metrics of the String Offset Order (SOO) feature. The results demonstrate the predictive capability of SOO as a standalone symptom discovery indicator for repackaged application detection.To the best of our knowledge, this represents the first systematic assessment of SOO within a dataset explicitly designed for repackaging research. Previous work on SOO could not confirm its robustness in this context, as suitable benchmark datasets were not yet available. By demonstrating its effectiveness at scale, our study extends the relevance of SOO beyond a theoretical proof-of-concept to a practical, lightweight feature in modern repackaging detection pipelines.This finding has two important implications. First, it highlights the value of symptom discovery approaches, which exploit structural artifacts left behind during repackaging. Second, it suggests that future work could explore additional low-cost structural features that may complement SOO, thereby reducing reliance on more computationally intensive feature sets. In particular, fine-tuning SOO thresholds, combining it with advanced ICC metrics, or applying it to more recent repackaging datasets could further improve both robustness and generalizability.Our post-hoc examination of misclassified original applications indicates that false positives typically arise from a small subset of features whose distributions overlap between large, complex original apps and repackaged variants. In particular, high API usage counts and elevated external ICC frequencies occasionally mirror patterns introduced during repackaging, especially in multifunctional or advertisement-heavy legitimate applications. We confirmed that these cases are not attributable to noise in the SOO feature; rather, they stem from behavioral and structural characteristics naturally present in certain original applications. Although removing these features reduces false positives, the resulting models exhibit a substantial decline in recall and overall F1-score. Because large-scale marketplace analysis prioritizes minimizing false negatives over marginal reductions in false positives, we retain these features but now explicitly discuss this trade-off inSection 6. Future work will explore adaptive feature weighting, calibration techniques, and ensemble-based strategies to further mitigate false positives without compromising recall. 6.3. Summary of Results and Main FindingsThe experimental evaluation demonstrates that our proposed approach, RepackDroid, offers a lightweight yet effective solution for detecting repackaged Android applications. Several key findings emerge from the results:6.3.1. Competitive Performance with Fewer FeaturesDespite relying on only 20 features, our framework achieved an F1-score of 85.9% and a recall of 98.8% using k-Nearest Neighbors and Support Vector Machines, respectively. This performance surpasses or rivals models that use hundreds of features [7]. The implication is that feature quality can outweigh feature quantity when features are carefully chosen to reflect structural and behavioral properties of Android applications.6.3.2. High Recall Prioritized over PrecisionA notable outcome is that our system emphasizes recall over precision. For large-scale app marketplaces, this trade-off is strategically valuable: missing a repackaged application (false negative) poses a greater risk to users and developers than mistakenly flagging an original app (false positive). By maximizing recall, our approach ensures that potentially harmful repackaged apps are rarely overlooked, leaving false positives to be handled by downstream verification processes.6.3.3. Algorithm-Specific StrengthsOur evaluation across multiple classifiers revealed algorithm-specific trade-offs:Decision Tree achieved the highest precision (77.6%).Neural Network reached perfect recall (100%) but at the expense of lower precision.Logistic Regression yielded the strongest PR-AUC (87.8%), reflecting stability across thresholds.Support Vector Machine provided the best overall balance with the highest F1-score (85.9%). This diversity suggests that ensemble methods, combining strengths of different algorithms, could further enhance detection performance in future work.6.3.4. Validation of Symptom DiscoveryIn addition to behavioral and structural features (ICC, permissions, APIs), our study validates the role of symptom discovery, specifically the String Offset Order anomaly. Its inclusion significantly enhanced classification performance, confirming that structural artifacts of repackaging can serve as powerful, low-cost indicators when combined with supervised learning.6.3.5. Broader ImplicationsOverall, the findings suggest that lightweight, symptom-enhanced machine learning models can provide scalable, accurate, and efficient alternatives to existing repackaging detection techniques. This contributes directly to the pressing need for marketplace-ready solutions capable of handling high volumes of Android applications without sacrificing detection accuracy.",
            "6.1. Comparative Analysis with State-of-the-Art": "We summarize the most prominent state-of-the-art techniques for repackaged app detection inTable 5. Some of these techniques are not open-sourced and it is hard to re-implement or test them, or reproduce their results. For comparison, we highlight the approach type, number of features used, evaluation scale, and detection performance of each method. Table 5.Summary of representative state-of-the-art techniques for repackaged app detection. Our approach, RepackDroid, achieved recall = 98.8% and F1 = 85.9% while using only 20 features, though precision was somewhat lower (72.0%). This trade-off underscores the complementary strengths of the two approaches. Li et al.‚Äôs model [7] prioritizes precision through a large, feature-rich representation, whereas RepackDroid emphasizes recall and efficiency, making it more appropriate for marketplace-scale deployment where missing a repackaged app (false negative) is more damaging than mistakenly flagging a legitimate one (false positive). When compared with similarity-based detection methods, our system demonstrates a different performance profile. Similarity-based tools such as CodeMatch [4] and RomaDroid [6] often achieve precision scores above 90%, substantially higher than our average of 73.4% across eight algorithms. However, as discussed inSection 2, these methods depend on pairwise comparisons between original and candidate apps. While effective on small datasets, this approach does not scale to millions of applications due to the combinatorial explosion of comparisons. Indeed, most similarity-based studies evaluate their tools on relatively small test sets, limiting their applicability to real-world markets. In contrast, our method trades some precision for scalability, recall, and efficiency, which are essential characteristics for practical deployment in large-scale app stores. By reducing the feature space and eliminating the need for app pairs, RepackDroid provides a lightweight yet robust alternative to both similarity-based and high-dimensional supervised learning approaches.",
            "6.2. Role of String Offset Order in Detection": "A key distinguishing aspect of our framework is the ability to achieve competitive performance with only 20 features, compared to the 521 features employed by Li et al. [7]. This reduction is not merely a matter of computational efficiency‚Äîit also demonstrates that carefully selected, discriminative features can capture the essence of repackaging behavior without requiring high-dimensional representations. Among the features incorporated in our model, the String Offset Order (SOO) anomaly proposed by Gonzalez et al. [22] via the AndroidSOO tool emerged as particularly impactful. SOO exploits the structural property of Dalvik executable files, where string identifiers should appear in alphabetical order according to the DEX specification. When an app is repackaged using tools such as apktool, this ordering is often disrupted, leaving a reliable ‚Äúsymptom‚Äù of tampering. Our experiments confirmed the utility of SOO when integrated into a broader supervised learning model. As shown inTable 6, the SOO feature alone demonstrates meaningful predictive power in distinguishing repackaged from original applications. More importantly, its combination with ICC, permissions, and sensitive API features improved overall accuracy and recall compared to models that excluded it. This aligns with the prediction made in Gonzalez et al.‚Äôs original work [22], but our contribution provides a stronger empirical validation: unlike their initial study, we evaluated SOO using the RePack dataset, which is specifically curated for repackaged application detection. Table 6.Performance metrics of the String Offset Order (SOO) feature. The results demonstrate the predictive capability of SOO as a standalone symptom discovery indicator for repackaged application detection. To the best of our knowledge, this represents the first systematic assessment of SOO within a dataset explicitly designed for repackaging research. Previous work on SOO could not confirm its robustness in this context, as suitable benchmark datasets were not yet available. By demonstrating its effectiveness at scale, our study extends the relevance of SOO beyond a theoretical proof-of-concept to a practical, lightweight feature in modern repackaging detection pipelines. This finding has two important implications. First, it highlights the value of symptom discovery approaches, which exploit structural artifacts left behind during repackaging. Second, it suggests that future work could explore additional low-cost structural features that may complement SOO, thereby reducing reliance on more computationally intensive feature sets. In particular, fine-tuning SOO thresholds, combining it with advanced ICC metrics, or applying it to more recent repackaging datasets could further improve both robustness and generalizability. Our post-hoc examination of misclassified original applications indicates that false positives typically arise from a small subset of features whose distributions overlap between large, complex original apps and repackaged variants. In particular, high API usage counts and elevated external ICC frequencies occasionally mirror patterns introduced during repackaging, especially in multifunctional or advertisement-heavy legitimate applications. We confirmed that these cases are not attributable to noise in the SOO feature; rather, they stem from behavioral and structural characteristics naturally present in certain original applications. Although removing these features reduces false positives, the resulting models exhibit a substantial decline in recall and overall F1-score. Because large-scale marketplace analysis prioritizes minimizing false negatives over marginal reductions in false positives, we retain these features but now explicitly discuss this trade-off inSection 6. Future work will explore adaptive feature weighting, calibration techniques, and ensemble-based strategies to further mitigate false positives without compromising recall.",
            "6.3. Summary of Results and Main Findings": "The experimental evaluation demonstrates that our proposed approach, RepackDroid, offers a lightweight yet effective solution for detecting repackaged Android applications. Several key findings emerge from the results: 6.3.1. Competitive Performance with Fewer FeaturesDespite relying on only 20 features, our framework achieved an F1-score of 85.9% and a recall of 98.8% using k-Nearest Neighbors and Support Vector Machines, respectively. This performance surpasses or rivals models that use hundreds of features [7]. The implication is that feature quality can outweigh feature quantity when features are carefully chosen to reflect structural and behavioral properties of Android applications. 6.3.2. High Recall Prioritized over PrecisionA notable outcome is that our system emphasizes recall over precision. For large-scale app marketplaces, this trade-off is strategically valuable: missing a repackaged application (false negative) poses a greater risk to users and developers than mistakenly flagging an original app (false positive). By maximizing recall, our approach ensures that potentially harmful repackaged apps are rarely overlooked, leaving false positives to be handled by downstream verification processes. 6.3.3. Algorithm-Specific StrengthsOur evaluation across multiple classifiers revealed algorithm-specific trade-offs:Decision Tree achieved the highest precision (77.6%).Neural Network reached perfect recall (100%) but at the expense of lower precision.Logistic Regression yielded the strongest PR-AUC (87.8%), reflecting stability across thresholds.Support Vector Machine provided the best overall balance with the highest F1-score (85.9%). This diversity suggests that ensemble methods, combining strengths of different algorithms, could further enhance detection performance in future work. 6.3.4. Validation of Symptom DiscoveryIn addition to behavioral and structural features (ICC, permissions, APIs), our study validates the role of symptom discovery, specifically the String Offset Order anomaly. Its inclusion significantly enhanced classification performance, confirming that structural artifacts of repackaging can serve as powerful, low-cost indicators when combined with supervised learning. 6.3.5. Broader ImplicationsOverall, the findings suggest that lightweight, symptom-enhanced machine learning models can provide scalable, accurate, and efficient alternatives to existing repackaging detection techniques. This contributes directly to the pressing need for marketplace-ready solutions capable of handling high volumes of Android applications without sacrificing detection accuracy.",
            "6.3.1. Competitive Performance with Fewer Features": "Despite relying on only 20 features, our framework achieved an F1-score of 85.9% and a recall of 98.8% using k-Nearest Neighbors and Support Vector Machines, respectively. This performance surpasses or rivals models that use hundreds of features [7]. The implication is that feature quality can outweigh feature quantity when features are carefully chosen to reflect structural and behavioral properties of Android applications.",
            "6.3.2. High Recall Prioritized over Precision": "A notable outcome is that our system emphasizes recall over precision. For large-scale app marketplaces, this trade-off is strategically valuable: missing a repackaged application (false negative) poses a greater risk to users and developers than mistakenly flagging an original app (false positive). By maximizing recall, our approach ensures that potentially harmful repackaged apps are rarely overlooked, leaving false positives to be handled by downstream verification processes.",
            "6.3.3. Algorithm-Specific Strengths": "Our evaluation across multiple classifiers revealed algorithm-specific trade-offs: Decision Tree achieved the highest precision (77.6%).Neural Network reached perfect recall (100%) but at the expense of lower precision.Logistic Regression yielded the strongest PR-AUC (87.8%), reflecting stability across thresholds.Support Vector Machine provided the best overall balance with the highest F1-score (85.9%). This diversity suggests that ensemble methods, combining strengths of different algorithms, could further enhance detection performance in future work.",
            "6.3.4. Validation of Symptom Discovery": "In addition to behavioral and structural features (ICC, permissions, APIs), our study validates the role of symptom discovery, specifically the String Offset Order anomaly. Its inclusion significantly enhanced classification performance, confirming that structural artifacts of repackaging can serve as powerful, low-cost indicators when combined with supervised learning.",
            "6.3.5. Broader Implications": "Overall, the findings suggest that lightweight, symptom-enhanced machine learning models can provide scalable, accurate, and efficient alternatives to existing repackaging detection techniques. This contributes directly to the pressing need for marketplace-ready solutions capable of handling high volumes of Android applications without sacrificing detection accuracy.",
            "7. Threats to Validity": "Although the proposed framework demonstrates strong empirical performance, several factors may affect the interpretation and generalizability of the results. Internal validity. The effectiveness of the proposed framework depends on the completeness and accuracy of static feature extraction. Because ICC patterns, API counts, and manifest attributes are obtained from decompiled smali code, the resulting features may be affected by code transformations, multi-dex partitioning, identifier renaming, encrypted strings, and reflection-based intent construction. Such factors may lead to partial or incomplete extraction of ICC tuples and API usage counts. These challenges stem from the inherent limitations of static analysis and similarly affect prior repackaged-app detection approaches that rely on smali or DEX-level artifacts. Construct validity. Class imbalance in the RePack dataset was handled using SMOTE during training. Because oversampling modifies the minority-class distribution, there is a risk of generating synthetic feature vectors that fall between regions of the feature space that do not occur naturally. This is particularly relevant for aggregate count-based metrics. However, the SOO feature is binary and preserved exactly, and ICC/API features represent frequency-based abstractions that remain meaningful under moderate interpolation. The test set retains the original class distribution, ensuring that evaluation metrics reflect performance on real applications. External validity. The evaluation is performed entirely on the RePack dataset, which is the only publicly available corpus containing verified original‚Äìrepackaged application pairs. While this dataset is appropriate for assessing repackaging-specific structural deviations, it does not include systematically obfuscated or reflection-heavy variants representing more advanced repackaging techniques. Datasets commonly used for general malware detection, such as DREBIN [33] or AMD [34], lack original‚Äìrepackaged labels and therefore cannot support meaningful assessment of repackaging detection. As a result, generalizability to heavily obfuscated or dynamically loaded applications remains an open direction. Methodological validity. The comparison of supervised learning algorithms is based on an 80/20 split supplemented with 5-fold cross-validation to assess performance stability. Cross-validation reduces sensitivity to a single data partition, but differences among heterogeneous classifiers are not subjected to formal statistical hypothesis tests, which may limit interpretability of small performance variations. The study focuses on evaluating a lightweight feature set tailored to repackaging detection rather than benchmarking against deep-learning-based malware detectors, which operate under different learning objectives and datasets. This methodological choice aligns the evaluation with the specific task of detecting repackaged applications rather than general malware classification.",
            "8. Conclusions and Future Work": "In this paper, we presented RepackDroid, a supervised learning-based framework for detecting repackaged Android applications. Our approach demonstrates that accurate and scalable detection can be achieved with a compact feature set, overcoming the limitations of prior work that relied on extensive feature engineering or computationally expensive similarity comparisons. By leveraging only 20 carefully selected features, including inter-component communication (ICC) metrics, permission and API usage, and the structural anomaly of String Offset Order (SOO), we achieved competitive results compared to state-of-the-art methods that use more than 500 features. Our evaluation on the RePack dataset confirmed that RepackDroid provides strong detection performance, with a maximum F1-score of 0.859 and recall of 98.8%. These findings highlight the effectiveness of lightweight features, especially SOO, as practical indicators of repackaging. To the best of our knowledge, this is the first study to systematically assess SOO within a dataset specifically curated for repackaging research, thereby extending its relevance from a proof-of-concept indicator to a validated component of an integrated detection framework. Beyond raw performance, the results demonstrate important trade-offs. While our average precision was lower than that of similarity-based approaches, our system emphasizes recall and efficiency, which are crucial for real-world deployment in large-scale marketplaces where false negatives‚Äîmissed repackaged apps‚Äîpose the greatest risk. The lightweight nature of our framework makes it suitable for integration into continuous app vetting pipelines. There are opportunities for improvement. Future work should explore additional low-cost structural symptoms of repackaging, refine ICC and API-based features, and investigate ensemble models that combine the strengths of different classifiers. Expanding evaluations to larger and more diverse datasets, including recently released applications and obfuscated variants, will also be essential to confirm the generalizability of our findings."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2078-2489/16/12/1075",
        "scraped_at": "2025-12-05 23:54:44"
    },
    {
        "title": "Predicting the Lifespan of Twisted String Actuators Using Empirical and Hybrid Machine Learning Approaches",
        "authors": "byHai Nguyen,Chanthol EangandSeungjae Lee",
        "journal": "Sensors2025,25(23), 7387; https://doi.org/10.3390/s25237387 (registering¬†DOI) - 4 Dec 2025",
        "abstract": "Predicting the fatigue lifespan of Twisted String Actuators (TSAs) is essential for improving the reliability of robotic and mechanical systems that rely on flexible transmission mechanisms. Traditional empirical approaches based on regression or Weibull distribution analysis have provided useful approximations, yet they often struggle to capture nonlinear dependencies and stochastic influences inherent to real-world fatigue behavior. This study introduces and compares four machine learning (ML) models‚ÄîLinear Regression, Random Forest, XGBoost, and Gaussian Process Regression (GPR)‚Äîfor predicting TSA lifespan under varying weight (W), number of strings (N), and diameter (D) conditions. Building upon this comparison, a hybrid physics-guided model is proposed by integrating an empirical fatigue life equation with an XGBoost residual-correction model. Experimental data collected from repetitive actuation tests (144 valid samples) served as the basis for training and validation. The hybrid model achieved an R2= 0.9856, RMSE = 5299.47 cycles, and MAE = 3329.67 cycles, outperforming standalone ML models in cross-validation consistency (CV R2= 0.9752). The results demonstrate that physics-informed learning yields superior interpretability and generalization even in limited-data regimes. These findings highlight the potential of hybrid empirical‚ÄìML modeling for component life prediction in robotic actuation systems, where experimental fatigue data are scarce and operating conditions vary.Keywords:twisted string actuator (TSA);fatigue life prediction;physics-guided machine learning;hybrid XGBoost;reliability modeling;uncertainty quantification;predictive maintenance",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Twisted String Actuators (TSAs) have emerged as one of the most promising solutions for compact, lightweight, and high-force robotic actuation. By converting rotational motion into linear displacement through the twisting of flexible strings or cables, TSAs achieve remarkable mechanical efficiency, generating high output forces relative to their small mass and volume [1,2]. This unique principle enables the design of efficient transmission systems with advantages such as compactness, mechanical simplicity, energy-efficient, and high force-to-weight ratio, making them ideal for wearable robotics, assistive devices, and soft actuation systems [3,4]. Recent research highlights the versatility of TSAs across diverse engineering domains. For instance, adjustable-offset TSA mechanisms have been developed to provide variable transmission ratios and precise motion control [3], while haptic force-rendering TSAs enable realistic feedback in immersive human‚Äìmachine interfaces [4]. Similarly, fabric-integrated wearable gloves employing TSA actuation have shown potential in rehabilitation and manual handling tasks, offering compactness without compromising strength [5,6]. At larger scales, the NASA Ames Research Center explored impedance-controlled TSA architectures for tensegrity robots, where lightweight, compliant actuation systems are essential for safe planetary exploration and adaptive locomotion [7]. More recently, Zheng et al. [8] applied a fully data-driven modeling strategy to twisted string mechanisms for anthropomorphic robotic hands, demonstrating that nonlinear actuator behaviors can be captured through hybrid AI-based representations while maintaining mechanical compliance and control precision. These developments illustrate the growing convergence between physical actuation design and intelligent modeling paradigms. Despite these advances, the fatigue life and durability of TSAs remain a major limitation for long-term deployment. Each twist‚Äìuntwist cycle induces torsional and bending stresses that gradually degrade the wire material, eventually leading to micro-fracture and failure. The degradation rate is influenced by multiple interdependent factors, including applied load, string diameter, number of wires, material properties, and frictional losses, as well as environmental conditions such as temperature and humidity. These nonlinear interactions render lifespan prediction a highly complex problem, yet accurate prediction is crucial for ensuring reliability, safety, and predictive maintenance in robotic and assistive systems. Historically, empirical and statistical life prediction models, such as those based on Weibull distributions or regression analysis, have been employed to model mechanical fatigue. These methods offer clear interpretability and strong physical grounding but often rely on simplifying assumptions, such as linearity or independence among variables. While effective for isolated parameters, such formulations cannot fully capture the nonlinear coupling effects observed in twisted string mechanisms, where geometric deformation, internal friction, and microstructural wear interact dynamically over time. Consequently, empirical models tend to underperform when extrapolated beyond calibrated experimental conditions. In recent years, machine learning (ML) has gained attention as a powerful tool for nonlinear regression and multivariate modeling in mechanical systems. Algorithms such as Random Forest (RF), Extreme Gradient Boosting (XGBoost), and Gaussian Process Regression (GPR) can learn complex mappings between input parameters and observed fatigue outcomes, even from small datasets. However, purely data-driven methods are often criticized as ‚Äúblack-box‚Äù models, offering high predictive accuracy at the expense of physical interpretability. For safety-critical or high-reliability applications, such as TSA-driven exoskeletons and space robotics, this lack of interpretability poses a significant barrier to practical adoption. Recent industrial studies have also demonstrated the promise of interpretable hybrid AI frameworks for reliability estimation and equipment lifespan prediction [9], reinforcing the need for models that combine transparency with predictive strength. To address these challenges, this study proposes a hybrid physics-guided machine learning framework that integrates empirical knowledge with data-driven learning. The approach combines a Weibull-based empirical model, which encodes deterministic physical relationships with an XGBoost residual learner, which models unaccounted nonlinear deviations. This Hybrid Empirical‚ÄìXGBoost model aims to achieve three primary goals: Enhance predictive accuracy by learning residual deviations beyond empirical approximations;Maintain physical interpretability through embedded physics-based priors;Improve generalization in small-sample experimental contexts typical of actuator testing. The framework is evaluated using a dataset of 144 TSA fatigue life samples under varied load and geometry conditions. Five modeling paradigms are systematically compared: (i) Linear Regression (LR), (ii) Random Forest (RF), (iii) XGBoost (XGB), (iv) Gaussian Process Regression (GPR), and (v) the proposed hybrid empirical‚ÄìXGBoost model. The major contributions of this study are summarized as follows: Dataset Development: Establishment of a consolidated experimental dataset for TSA fatigue life prediction covering diverse loading and geometric conditions.Modeling Framework: Development and comparative evaluation of five predictive approaches, including a hybrid physics-guided architecture combining empirical and ML components.Interpretability and Practical Deployment: Implementation of a Python-based graphical user interface (GUI) enabling real-time prediction, visualization, and parameter sensitivity analysis for engineering applications.",
            "2. Related Works": "Understanding the fatigue behavior and lifespan of Twisted String Actuators (TSAs) requires integrating insights from multiple research domains, ranging from traditional empirical fatigue modeling to data-driven machine learning (ML) prediction, and hybrid physics-informed modeling frameworks. This section reviews prior literature in these areas, with emphasis on methodologies that have influenced the development of our hybrid empirical‚Äìmachine learning approach, which includes exploring traditional empirical and statistical models of fatigue life, especially Weibull-based reliability analysis; expanding on Linear Regression (LR) and interpretable modeling techniques used in mechanical reliability. Then, we will cover ensemble-based ML models, namely Random Forest (RF) and Extreme Gradient Boosting (XGBoost), followed by discussing Gaussian Process Regression (GPR) as a Bayesian framework for uncertainty quantification. Finally, we review physics-informed and hybrid approaches that unify empirical modeling with modern ML algorithms, providing the theoretical foundation for our hybrid TSA lifespan predictor. 2.1. Empirical and Statistical Approaches to Mechanical FatigueEmpirical reliability analysis has long been the foundation of fatigue life prediction for mechanical systems. Conventional regression models approximate deterministic relationships between operating load, cycle count, and geometry but generally rely on simplified functional assumptions, linear or power‚Äìlaw relations, that fail under complex, nonlinear fatigue mechanisms.To capture the stochastic nature of material failure, the Weibull distribution became the dominant probabilistic tool. It describes time-to-failure as a statistical process defined by shape and scale parameters that quantify the likelihood and spread of failure events. Jes√∫s et al. [10] validated the Weibull distribution for mechanical fatigue modeling, demonstrating its flexibility in characterizing lifetime dispersion. Fakoor et al. [11] introduced parameter modifications to improve predictive precision, and Wang et al. [12] extended the technique to mechanical-part fatigue analysis. Kundu et al. [13] applied a Weibull-based accelerated-life regression for remaining useful life (RUL) prediction under multiple loads, while Herzog [14] merged Weibull reliability metrics with neural networks for bearing-life estimation. Mohammad et al. [15] linked Weibull distributions with acoustic-emission features to forecast fatigue in SAE-1045 steel.Although Weibull regression excels at handling randomness and providing interpretability, it is constrained by its parametric simplicity. In TSA systems, where lifespan depends jointly on weight (W), diameter (D), and number of strings (N), traditional models often fail to describe nonlinear cross-effects or manufacturing variability. Consequently, modern studies increasingly adopt machine learning techniques capable of capturing these higher-order dependencies directly from data. 2.2. Linear Regression and Interpretable ModelingBefore the rise of nonlinear ensemble algorithms, Linear Regression (LR) was the primary modeling tool for mechanical reliability and predictive maintenance tasks. Its enduring appeal lies in its simplicity, transparency, and strong interpretability, making it ideal for understanding how individual variables affect system lifespan.Recent works continue to highlight the importance of LR in engineering diagnostics. Yƒ±ldƒ±rƒ±m et al. [16] demonstrated LR‚Äôs reliability in NASA C-MAPSS datasets for predictive maintenance, establishing its effectiveness in baseline life estimation for turbine systems. Lajos H√∂fler [17] assessed LR‚Äôs regression accuracy using sensor data in chemical systems, showing that linear models can yield strong predictive performance when features are physically meaningful. Kruschel et al. [18] challenged the traditional notion that interpretability must trade off against accuracy, presenting linear and generalized additive models that achieved near‚Äìblack-box performance in structured data environments. Similarly, Long et al. [19] emphasized the synergy between interpretable ML and sensor fusion for real-time industrial health monitoring.Beyond interpretability, LR serves as a benchmark for evaluating nonlinear learners such as Random Forest (RF) and XGBoost. Christos et al. [20] used polynomial and Linear Regression for wind-turbine energy prediction, and Schaeffer et al. [21] analyzed high-dimensional LR stability. Additionally, Schauer et al. [22] introduced hardware-efficient implementations of interpretable ML models for sensor-based diagnostics, improving the practicality of regression-based reliability prediction.In TSA lifespan prediction, LR provides a critical baseline. While it cannot represent nonlinear degradation mechanisms or material fatigue saturation, it enables feature-level analysis, identifying which design variables (e.g., load or diameter) most influence actuator endurance. This interpretability makes it invaluable for model validation and physical understanding prior to adopting more complex ML frameworks. 2.3. Ensemble Learning: Random Forest and XGBoostEnsemble methods such as Random Forest (RF) and Extreme Gradient Boosting (XGBoost) have revolutionized data-driven fatigue modeling by combining interpretability with strong nonlinear representation.RF constructs an ensemble of decision trees trained on random feature subsets, mitigating overfitting and enabling robust feature importance analysis. Fatima et al. [23] and Imani et al. [24] demonstrated RF‚Äôs efficiency across mechanical prediction tasks, while Suenaga et al. [25] validated its superiority over shallow neural networks for structural performance estimation. Wohlwend [26] and Hamidou et al. [27] highlighted RF‚Äôs consistent accuracy in fault diagnosis and anomaly detection. Yang et al. [28] applied RF and XGBoost to machine-failure prediction, confirming their generalization ability under imbalanced datasets.Building upon RF, XGBoost employs iterative gradient boosting to minimize residual errors with strong regularization. The primary distinction between the two methods lies in how decision trees are aggregated. RF builds trees in parallel, while XGBoost constructs them sequentially to optimize performance [29]. This distinction allows XGBoost to achieve higher predictive precision, particularly for small and noisy datasets such as TSA fatigue experiments.XGBoost extends this paradigm through boosted gradient optimization, sequentially reducing residual errors to achieve high precision even on small datasets, like the 144-sample TSA dataset used in this study. Its built-in regularization (L1/L2) prevents overfitting, while its scalability enables fast convergence. RF and XGBoost together represent a balance between explainability, efficiency, and predictive accuracy, making them ideal candidates for small-scale experimental fatigue data. 2.4. Gaussian Process Regression and Uncertainty QuantificationGaussian Process Regression (GPR) provides a probabilistic framework for nonlinear modeling, explicitly representing both predicted means and uncertainties. Unlike deterministic learners, GPR outputs confidence intervals, an essential feature for safety-critical mechanical systems, where risk-aware decision-making is vital.Li et al. [30] introduced GPR as an intuitive framework for modeling both predictions and associated confidence intervals, while Li et al. [31] implemented deep Bayesian GPR for medical uncertainty estimation. Manfredi [32] and Bilionis et al. [33] developed hybrid polynomial chaos‚ÄìGPR models to enhance Bayesian sensitivity analysis, enabling uncertainty propagation in mechanical systems. Oakley [34] provided a foundational overview of GPR for engineers, illustrating its practical utility in uncertainty-aware regression.In TSA fatigue modeling, where datasets are limited, GPR‚Äôs nonparametric nature makes it highly suitable for capturing smooth nonlinear relationships and generating credible uncertainty intervals. This capability enhances trust in predictions, essential for mission-critical actuators or robotic systems. 2.5. Physics-Informed and Hybrid Modeling FrameworksWhile pure data-driven ML models often achieve high accuracy, they lack physical interpretability, hindering their acceptance in engineering applications. Conversely, empirical models are interpretable but limited in adaptability. To reconcile these paradigms, recent studies have proposed hybrid and physics-informed learning frameworks that integrate domain knowledge into ML architectures.Raissi et al. [35] pioneered the Physics-Informed Neural Network (PINN), integrating differential equations directly into the training process. Farea et al. [36] and Zhang et al. [37] advanced this concept by embedding physical constraints into neural architectures. SoftServe [38] emphasized the growing industrial relevance of PINNs, describing their role in merging AI and engineering physics, while Luo et al. [39] reviewed its broad applicability across engineering PDE problems. Wesselkamp et al. [40] extended these ideas into Process-Informed Neural Networks (PrINNs), demonstrating improved inference under scientific consistency.Beyond PINNs, hybrid empirical‚ÄìML regression models have gained traction in mechanical reliability and fatigue life prediction. In these frameworks, an analytical or empirical model provides a physics-consistent baseline, while an ML model learns the residual deviations caused by nonlinear effects, manufacturing variability, or microstructural changes. This approach retains interpretability while improving adaptability to complex real-world patterns.The present study builds on this emerging paradigm by designing a hybrid framework tailored specifically to Twisted String Actuators. Unlike generic hybrid models, our approach integrates a TSA-specific empirical fatigue life equation derived from torsional and geometric mechanics and constrains the machine learning component (XGBoost) to operate on physically meaningful variables such as weight, diameter, and wire configuration. This targeted integration enables the model to capture global degradation trends through the empirical component while allowing XGBoost to represent localized nonlinear effects that empirical models alone cannot describe.Despite significant advancements across empirical, interpretable, ensemble, and hybrid paradigms, predicting TSA fatigue life remains challenging due to limited datasets, nonlinear parameter coupling, and material variability. Conventional Weibull or linear models cannot capture these cross-effects, while purely ML-based methods often lose physical meaning. The need, therefore, is for a lightweight hybrid model that leverages empirical domain knowledge and the nonlinear flexibility of ML, precisely the direction taken in this study.",
            "2.1. Empirical and Statistical Approaches to Mechanical Fatigue": "Empirical reliability analysis has long been the foundation of fatigue life prediction for mechanical systems. Conventional regression models approximate deterministic relationships between operating load, cycle count, and geometry but generally rely on simplified functional assumptions, linear or power‚Äìlaw relations, that fail under complex, nonlinear fatigue mechanisms. To capture the stochastic nature of material failure, the Weibull distribution became the dominant probabilistic tool. It describes time-to-failure as a statistical process defined by shape and scale parameters that quantify the likelihood and spread of failure events. Jes√∫s et al. [10] validated the Weibull distribution for mechanical fatigue modeling, demonstrating its flexibility in characterizing lifetime dispersion. Fakoor et al. [11] introduced parameter modifications to improve predictive precision, and Wang et al. [12] extended the technique to mechanical-part fatigue analysis. Kundu et al. [13] applied a Weibull-based accelerated-life regression for remaining useful life (RUL) prediction under multiple loads, while Herzog [14] merged Weibull reliability metrics with neural networks for bearing-life estimation. Mohammad et al. [15] linked Weibull distributions with acoustic-emission features to forecast fatigue in SAE-1045 steel. Although Weibull regression excels at handling randomness and providing interpretability, it is constrained by its parametric simplicity. In TSA systems, where lifespan depends jointly on weight (W), diameter (D), and number of strings (N), traditional models often fail to describe nonlinear cross-effects or manufacturing variability. Consequently, modern studies increasingly adopt machine learning techniques capable of capturing these higher-order dependencies directly from data.",
            "2.2. Linear Regression and Interpretable Modeling": "Before the rise of nonlinear ensemble algorithms, Linear Regression (LR) was the primary modeling tool for mechanical reliability and predictive maintenance tasks. Its enduring appeal lies in its simplicity, transparency, and strong interpretability, making it ideal for understanding how individual variables affect system lifespan. Recent works continue to highlight the importance of LR in engineering diagnostics. Yƒ±ldƒ±rƒ±m et al. [16] demonstrated LR‚Äôs reliability in NASA C-MAPSS datasets for predictive maintenance, establishing its effectiveness in baseline life estimation for turbine systems. Lajos H√∂fler [17] assessed LR‚Äôs regression accuracy using sensor data in chemical systems, showing that linear models can yield strong predictive performance when features are physically meaningful. Kruschel et al. [18] challenged the traditional notion that interpretability must trade off against accuracy, presenting linear and generalized additive models that achieved near‚Äìblack-box performance in structured data environments. Similarly, Long et al. [19] emphasized the synergy between interpretable ML and sensor fusion for real-time industrial health monitoring. Beyond interpretability, LR serves as a benchmark for evaluating nonlinear learners such as Random Forest (RF) and XGBoost. Christos et al. [20] used polynomial and Linear Regression for wind-turbine energy prediction, and Schaeffer et al. [21] analyzed high-dimensional LR stability. Additionally, Schauer et al. [22] introduced hardware-efficient implementations of interpretable ML models for sensor-based diagnostics, improving the practicality of regression-based reliability prediction. In TSA lifespan prediction, LR provides a critical baseline. While it cannot represent nonlinear degradation mechanisms or material fatigue saturation, it enables feature-level analysis, identifying which design variables (e.g., load or diameter) most influence actuator endurance. This interpretability makes it invaluable for model validation and physical understanding prior to adopting more complex ML frameworks.",
            "2.3. Ensemble Learning: Random Forest and XGBoost": "Ensemble methods such as Random Forest (RF) and Extreme Gradient Boosting (XGBoost) have revolutionized data-driven fatigue modeling by combining interpretability with strong nonlinear representation. RF constructs an ensemble of decision trees trained on random feature subsets, mitigating overfitting and enabling robust feature importance analysis. Fatima et al. [23] and Imani et al. [24] demonstrated RF‚Äôs efficiency across mechanical prediction tasks, while Suenaga et al. [25] validated its superiority over shallow neural networks for structural performance estimation. Wohlwend [26] and Hamidou et al. [27] highlighted RF‚Äôs consistent accuracy in fault diagnosis and anomaly detection. Yang et al. [28] applied RF and XGBoost to machine-failure prediction, confirming their generalization ability under imbalanced datasets. Building upon RF, XGBoost employs iterative gradient boosting to minimize residual errors with strong regularization. The primary distinction between the two methods lies in how decision trees are aggregated. RF builds trees in parallel, while XGBoost constructs them sequentially to optimize performance [29]. This distinction allows XGBoost to achieve higher predictive precision, particularly for small and noisy datasets such as TSA fatigue experiments. XGBoost extends this paradigm through boosted gradient optimization, sequentially reducing residual errors to achieve high precision even on small datasets, like the 144-sample TSA dataset used in this study. Its built-in regularization (L1/L2) prevents overfitting, while its scalability enables fast convergence. RF and XGBoost together represent a balance between explainability, efficiency, and predictive accuracy, making them ideal candidates for small-scale experimental fatigue data.",
            "2.4. Gaussian Process Regression and Uncertainty Quantification": "Gaussian Process Regression (GPR) provides a probabilistic framework for nonlinear modeling, explicitly representing both predicted means and uncertainties. Unlike deterministic learners, GPR outputs confidence intervals, an essential feature for safety-critical mechanical systems, where risk-aware decision-making is vital. Li et al. [30] introduced GPR as an intuitive framework for modeling both predictions and associated confidence intervals, while Li et al. [31] implemented deep Bayesian GPR for medical uncertainty estimation. Manfredi [32] and Bilionis et al. [33] developed hybrid polynomial chaos‚ÄìGPR models to enhance Bayesian sensitivity analysis, enabling uncertainty propagation in mechanical systems. Oakley [34] provided a foundational overview of GPR for engineers, illustrating its practical utility in uncertainty-aware regression. In TSA fatigue modeling, where datasets are limited, GPR‚Äôs nonparametric nature makes it highly suitable for capturing smooth nonlinear relationships and generating credible uncertainty intervals. This capability enhances trust in predictions, essential for mission-critical actuators or robotic systems.",
            "2.5. Physics-Informed and Hybrid Modeling Frameworks": "While pure data-driven ML models often achieve high accuracy, they lack physical interpretability, hindering their acceptance in engineering applications. Conversely, empirical models are interpretable but limited in adaptability. To reconcile these paradigms, recent studies have proposed hybrid and physics-informed learning frameworks that integrate domain knowledge into ML architectures. Raissi et al. [35] pioneered the Physics-Informed Neural Network (PINN), integrating differential equations directly into the training process. Farea et al. [36] and Zhang et al. [37] advanced this concept by embedding physical constraints into neural architectures. SoftServe [38] emphasized the growing industrial relevance of PINNs, describing their role in merging AI and engineering physics, while Luo et al. [39] reviewed its broad applicability across engineering PDE problems. Wesselkamp et al. [40] extended these ideas into Process-Informed Neural Networks (PrINNs), demonstrating improved inference under scientific consistency. Beyond PINNs, hybrid empirical‚ÄìML regression models have gained traction in mechanical reliability and fatigue life prediction. In these frameworks, an analytical or empirical model provides a physics-consistent baseline, while an ML model learns the residual deviations caused by nonlinear effects, manufacturing variability, or microstructural changes. This approach retains interpretability while improving adaptability to complex real-world patterns. The present study builds on this emerging paradigm by designing a hybrid framework tailored specifically to Twisted String Actuators. Unlike generic hybrid models, our approach integrates a TSA-specific empirical fatigue life equation derived from torsional and geometric mechanics and constrains the machine learning component (XGBoost) to operate on physically meaningful variables such as weight, diameter, and wire configuration. This targeted integration enables the model to capture global degradation trends through the empirical component while allowing XGBoost to represent localized nonlinear effects that empirical models alone cannot describe. Despite significant advancements across empirical, interpretable, ensemble, and hybrid paradigms, predicting TSA fatigue life remains challenging due to limited datasets, nonlinear parameter coupling, and material variability. Conventional Weibull or linear models cannot capture these cross-effects, while purely ML-based methods often lose physical meaning. The need, therefore, is for a lightweight hybrid model that leverages empirical domain knowledge and the nonlinear flexibility of ML, precisely the direction taken in this study.",
            "3. Methodology": "This section describes the dataset structure, preprocessing steps, modeling procedures, and evaluation protocols used to predict the fatigue lifespan of Twisted String Actuators (TSAs). The methodological workflow integrates empirical modeling, machine learning (ML) algorithms, and a hybrid physics-guided approach, culminating in an interactive GUI for model testing and validation. Figure 1illustrates a schematic overview of the full modeling pipeline used in this study. The process begins with experimental data collection of five key parameters related the wires, which are weight (W), diameter (D), number of wires (N), cross-sectional area (A) and lifespan, or number of execution (cycles) until the wire break, followed by preprocessing and log transformation for variance stabilization. The empirical Weibull-based model establishes a deterministic baseline, while four machine learning models (Linear Regression, Random Forest, XGBoost, and Gaussian Process Regression) provide nonlinear, data-driven predictions. The hybrid Empirical + Residual XGBoost model integrates both physics and data learning. Final evaluation and visualization are performed through a Python-based GUI. Figure 1.Methodology workflow. 3.1. Dataset and Experimental Setup3.1.1. Experimental DesignTwisted String Actuators (TSAs) were subjected to cyclic twisting‚Äìuntwisting operations under controlled laboratory conditions until complete wire fracture occurred. Each actuator specimen consisted of multiple steel or nylon wires connected to a rotary motor applying constant tension through a load cell. The cycles to failure were recorded automatically through a digital counter linked to the actuator control system. Environmental conditions, such as temperature (~25 ¬∞C), humidity (~50%), and rotational speed (150 rpm), were maintained constant to minimize uncontrolled variance.All twisted string specimens were fabricated using LIROS D-Pro SK78, which originates from LIROS Ropes/Karlskron, Bavaria, Germany, a high-performance rope constructed from 100% Dyneema¬ÆSK78 fibers. This material is manufactured under strict industrial quality-control processes, including a proprietary heat-stretch stabilization procedure that ensures consistent breaking strength, elongation characteristics, density, and surface finish across production batches. To minimize material variability, all samples were sourced from the same manufacturing lot (or adjacent batch) and were subjected to visual inspection and basic pre-tension checks before testing. Although minor microstructural differences may remain, the standardized nature of SK78 rope and these pre-test screening procedures justify the assumption of material uniformity for the purpose of fatigue life modeling.Each experimental configuration corresponded to a unique combination of applied weight (W), string diameter (D), and number of wires (N). The resulting fatigue life (C) values were measured in cycles until mechanical failure, reflecting the actuator‚Äôs endurance under repetitive mechanical stress. A total of 144 valid experimental observations were retained after removing incomplete or corrupted entries, representing a balanced mix of load and geometry variations. This dataset forms the basis for both empirical regression and machine learning model training.Although the dataset consists of 144 fatigue life samples, this scale is typical for Twisted String Actuator testing, where each experiment requires thousands of cycles and ends in destructive failure. Consequently, data collection is inherently slow, resource-intensive, and physically constrained. To mitigate risks associated with small datasets, the modeling pipeline incorporates repeated 5-fold cross-validation, nested hyperparameter tuning, and a hybrid regression structure in which the empirical component captures global degradation trends while the machine learning component learns only localized nonlinear residuals. These measures collectively reduce variance and improve robustness despite the limited sample size.The experimental setup used for fatigue testing of the Twisted String Actuators is shown inFigure 2. The test bench consists of a servo-motor‚Äìdriven rotary system connected to a string-mounting frame, a load unit with a cylinder for tension application, and a displacement sensor (LVDT) for real-time elongation tracking. The entire assembly integrates a control panel, data-acquisition monitor, and computer system for automatic recording of cycles to failure, ensuring reproducible test conditions. This apparatus enabled precise control of loading parameters (W, D, N) and accurate detection of fracture points across all 144 experimental runs.Figure 2.Experimental setup for Twisted String Actuator (TSA) fatigue testing.3.1.2. VariablesInput Variables:‚ó¶Weight (W): 15‚Äì25 kg applied tension.‚ó¶Diameter (D): 1.0‚Äì2.0 mm string thickness.‚ó¶Number of Strings (N): 2‚Äì7 parallel wires.‚ó¶Cross-Sectional Area (A): computed asA=œÄ(D/2)2A=œÄ(D/2)2Output Variable:‚ó¶Cycles to Failure (C): total number of actuation cycles until wire fracture.All of variables are summarized inTable 1.Table 1.Summary of experimental variables.3.1.3. Data Acquisition and OrganizationRaw measurements were recorded in structured CSV blocks corresponding to three load categories (15, 20, 25 kg). Within each load block, multiple-diameter string count combinations were tested to capture multivariate behavior. Data integrity was verified through duplicate checks and physical inspection logs. The dataset was programmatically parsed using Python (pandas) into a unified data frame for subsequent preprocessing and analysis.3.1.4. Data PreprocessingTo ensure statistical consistency and model convergence, preprocessing steps were applied as follows:Cleaning: Invalid entries such as zero-cycle measurements, corrupted logs, or nonsensical values were removed. These errors originated from counter resets, missing LVDT frames, or automatic safety stops and did not represent meaningful physical noise. Normal experimental noise from sensors was preserved. NaN values primarily resulted from intermittent sensor malfunctions, including incomplete cycle-count logs and temporary LVDT readout interruptions.Feature Engineering: The cross-sectional area was computed and added as a predictor.Normalization: Input features were standardized to zero mean and unit variance.Transformation: The output variable (Cycles to Failure) was log-transformed (log(1+y)log(1+y)) to stabilize variance and reduce heteroscedasticity.Train‚ÄìTest Split: An 80/20 split ensured balanced representation of parameter ranges across training and validation sets.This workflow was implemented in Python 3.13 using NumPy, pandas, and scikit-learn, allowing reproducible data handling and model evaluation. 3.2. Empirical Baseline ModelPrior to machine learning training, an empirical life prediction equation was used as the baseline. The model was derived from Weibull distribution-based life analysis and multiple regression fitting, capturing the influence of weight, number of strings, and diameter on lifespan. The formulation captures the primary physical dependencies between actuator geometry and applied load:Cemp=ùëé√óùê∑ùëè√óùëÅùëê√óùëä‚àíùëë,Cemp=a√óDb√óNc√óW‚àíd,(1)whereùê∂empCempare the predicted cycles to failure,D is the string diameter,N is the number of parallel wires,W is the applied weight, anda, b, c, d are regression coefficients estimated via least-squares fitting on the log-transformed data.This empirical approach provides a deterministic interpretation of fatigue behavior: lifespan increases with diameter and string count but decreases with applied load. Although highly interpretable, the model cannot fully represent nonlinear interactions or manufacturing variability. The empirical model serves two key purposes:Benchmarking: Provides a deterministic reference for evaluating the gain achieved by ML models.Hybrid Prior: Acts as a physics-based prior in the hybrid model, constraining learning to physically meaningful domains. 3.3. Machine Learning ModelsFour machine learning algorithms were implemented and benchmarked against the empirical model: Linear Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and Gaussian Process Regression (GPR). All models used identical training and validation splits to ensure comparability. The entire workflow was executed in Python using scikit-learn and XGBoost packages.The LR model assumes additive linear relationships and serves as a transparent statistical benchmark. It estimates a weighted combination of features to predict fatigue life, enabling straightforward physical interpretation of coefficients. While limited in flexibility, LR is essential for verifying the directionality and proportional influence of each factor, ensuring dataset consistency before applying more complex learners.RF constructs an ensemble of decorrelated decision trees using bootstrap aggregation, effectively capturing nonlinear interactions. Each tree contributes to the ensemble‚Äôs final output via averaging, which mitigates variance and enhances robustness. Feature importance derived from Gini impurity consistently identified diameter and number of strings as dominant predictors, followed by cross-sectional area and load. However, the model displayed mild overfitting due to the limited dataset, prompting the inclusion of gradient-based regularized methods.XGBoost iteratively constructs trees that correct the residuals of previous ones, using gradient optimization and built-in regularization (L1/L2 penalties). This allows precise modeling of nonlinear dependencies between load, geometry, and fatigue lifespan. Optimal hyperparameters determined via GridSearchCV were n_estimators = 300, learning_rate = 0.05, max_depth = 4, subsample = 0.9, colsample_bytree = 0.9. This configuration yielded stable convergence with limited data, achieving high R2scores and low error variance under 5-fold cross-validation.The GPR model provides a probabilistic approach for both prediction and uncertainty quantification. Using a composite kernel comprising a Radial Basis Function (RBF) and WhiteKernel, it models smooth nonlinear trends while accounting for observation noise. Beyond accuracy, GPR yields confidence intervals for each prediction, an essential property for reliability-critical applications such as robotic actuation and aerospace mechanisms.All models were trained using the log-transformed target and evaluated on the test set via R2, RMSE, and MAE metrics. Cross-validation (5-fold) was applied to measure stability and generalization. 3.4. Hybrid Physics-Guided XGBoost Model3.4.1. Hybrid Design PhilosophyTo combine the interpretability of physics-based modeling with the flexibility of data-driven learning, a hybrid model was developed that embeds the empirical fatigue life equation within a residual learning structure. The hybrid approach adheres to the principle of Physics-Guided Machine Learning (PGML), constraining data-driven optimization through deterministic scientific laws to preserve physical plausibility.The proposed hybrid framework combines the physics-based empirical model with a data-driven residual learner using XGBoost. The approach follows a two-stage process:Stage 1‚ÄîEmpirical Prediction: the deterministic model computes the baseline lifespan (CempCemp) for each sample based on known physical relationships between L, D, and N.Stage 2‚ÄîResidual Learning: an XGBoost regressor is trained to predict the residual error (Œîùê∂=Cobs‚àíCempŒîC=Cobs‚àíCemp), capturing systematic nonlinearities unexplained by the baseline equation.The final prediction is thus computed as follows:Chybrid=Cemp+fXGB(W,D,N,A)Chybrid=Cemp+fXGB(W,D,N,A)(2)wherefXGBfXGBrepresents the learned correction function.This additive correction structure ensures that the model respects empirical monotonic trends (e.g., lifespan decreases with increasing load) while capturing subtle nonlinear deviations due to microstructural or tribological effects.Unlike previously reported hybrid frameworks that simply combine an analytical model with a residual learning regressor, the hybrid design used in this study incorporates several TSA-specific innovations. The empirical component is derived directly from the torsional deformation mechanics of twisted string transmission, enabling it to capture the global trend of fatigue degradation that arises from geometric shortening and load-dependent friction. The XGBoost component is intentionally constrained to learn only the localized nonlinear deviations associated with micro-slippage, strand compaction, surface friction evolution, and material hysteresis, which are behaviors that are highly characteristic of TSA actuation but not modeled in conventional fatigue equations. By structuring the hybrid model such that the empirical term governs the physically consistent prediction trend while XGBoost refines only the complex residual patterns, the proposed method ensures both interpretability and enhanced predictive accuracy. This TSA-specific formulation differentiates our approach from existing additive hybrid frameworks and provides a tailored solution for accurately modeling twisted string fatigue behavior.3.4.2. Implementation and AdvantagesThe hybrid model was implemented in Python (scikit-learn + XGBoost) and serialized as a portable .pkl file for real-time deployment.Advantages include the following:Higher Predictive Fidelity: Learns nonlinear effects beyond analytical limits.Physical Coherence: Retains deterministic relationships to prevent unphysical extrapolation.Data Efficiency: Achieves strong accuracy with small datasets due to empirical priors.Interpretability: Enables feature importance analysis to identify dominant fatigue drivers.This structure mirrors modern trends in physics-informed AI, bridging the gap between classical reliability theory and adaptive learning systems. 3.5. Graphical User Interface (GUI) ImplementationA Tkinter-based GUI was developed to enable real-time testing and visualization. It serves as both an educational and practical interface for engineers to interact with the trained models. The interface allows users to perform the following:Input experimental or hypothetical actuator parameters (W, D, N, A).Instantly compute fatigue life predictions from all five models (Empirical, LR, RF, XGB, GPR, Hybrid).Visualize comparative outputs, feature importances, and predicted lifespans.Export results for design optimization or validation reports.Internally, the GUI loads serialized model files and executes predictions concurrently, ensuring consistency with Python-based results. This tool not only provides an accessible visualization platform for research validation but also serves as a design support interface for actuator development and educational demonstrations in robotic systems engineering. 3.6. Evaluation MetricsModel performance was quantified using four complementary statistical metrics: the coefficient of determination (R2), root mean square error (RMSE), mean absolute error (MAE), and cross-validated R2(CV R2).Coefficient of Determination (R2): R2measures how much of the variance in the observed lifespan values is explained by the model. It is defined asùëÖ2=1‚àí‚àëùëñ(ùë¶ùëñ‚àíùë¶ÃÇùëñ)2‚àëùëñ(ùë¶ùëñ‚àíùë¶‚àí‚àí)2R2=1‚àí‚àëi(yi‚àíy^i)2‚àëi(yi‚àíyÀâ)2(3)whereùë¶ùëñyiandùë¶ÃÇùëñy^idenote actual and predicted values, andùë¶‚àí‚àíyÀâis the sample mean. AnùëÖ2R2close to 1 indicates strong explanatory power; a negative value implies the model performs worse than a constant-mean predictor.RMSE (Root Mean Square Error): RMSE evaluates the magnitude of prediction errors and penalizes large deviations more severely.ùëÄùëÜùê∏=1ùëõ‚àëùëñ(ùë¶ùëñ‚àíùë¶ÃÇùëñ)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àöMSE=1n‚àëi(yi‚àíy^i)2(4)Expressed in the same units as the target (cycles), it reflects the typical error scale and overall model precision.MAE (Mean Absolute Error): MAE provides the average absolute deviation:MAE=1n‚àëi‚à£yi‚àíyÃÇi‚à£MAE=1n‚àëi‚à£yi‚àíy^i‚à£(5)It is less sensitive to outliers than RMSE and therefore complements it by capturing typical prediction bias in engineering terms.Cross-Validated R2: To assess model generalization, 5-fold cross-validation was performed. The dataset was divided into five equal folds; four folds were used for training and one for testing in each iteration. The average R2across all folds gives the cross-validated R2:CVR2=1k‚àëkj=1R2jCVR2=1k‚àëj=1kRj2(6)A small gap between training R2and CV R2indicates low overfitting and robust predictive consistency across parameter variations.Collectively, these four metrics provide a balanced evaluation: R2assesses explained variance, RMSE and MAE quantify absolute prediction accuracy, and CV R2measures generalization. Predictions were evaluated in log scale and then back-transformed to the real scale usingexp(y)‚àí1expy‚àí1for physical interpretability. This dual-scale evaluation ensures numerical stability during training while maintaining engineering relevance in the reported results.This methodological framework integrates physics-based reliability knowledge with modern machine learning techniques. The combination of empirical formulation, four distinct ML models, and a hybrid XGBoost architecture provides a comprehensive evaluation of TSA lifespan prediction strategies. Through data preprocessing, model training, and interactive testing, the approach establishes a robust foundation for engineering-grade lifespan estimation and sets the stage for the comparative analysis presented inSection 4.",
            "3.1. Dataset and Experimental Setup": "3.1.1. Experimental DesignTwisted String Actuators (TSAs) were subjected to cyclic twisting‚Äìuntwisting operations under controlled laboratory conditions until complete wire fracture occurred. Each actuator specimen consisted of multiple steel or nylon wires connected to a rotary motor applying constant tension through a load cell. The cycles to failure were recorded automatically through a digital counter linked to the actuator control system. Environmental conditions, such as temperature (~25 ¬∞C), humidity (~50%), and rotational speed (150 rpm), were maintained constant to minimize uncontrolled variance.All twisted string specimens were fabricated using LIROS D-Pro SK78, which originates from LIROS Ropes/Karlskron, Bavaria, Germany, a high-performance rope constructed from 100% Dyneema¬ÆSK78 fibers. This material is manufactured under strict industrial quality-control processes, including a proprietary heat-stretch stabilization procedure that ensures consistent breaking strength, elongation characteristics, density, and surface finish across production batches. To minimize material variability, all samples were sourced from the same manufacturing lot (or adjacent batch) and were subjected to visual inspection and basic pre-tension checks before testing. Although minor microstructural differences may remain, the standardized nature of SK78 rope and these pre-test screening procedures justify the assumption of material uniformity for the purpose of fatigue life modeling.Each experimental configuration corresponded to a unique combination of applied weight (W), string diameter (D), and number of wires (N). The resulting fatigue life (C) values were measured in cycles until mechanical failure, reflecting the actuator‚Äôs endurance under repetitive mechanical stress. A total of 144 valid experimental observations were retained after removing incomplete or corrupted entries, representing a balanced mix of load and geometry variations. This dataset forms the basis for both empirical regression and machine learning model training.Although the dataset consists of 144 fatigue life samples, this scale is typical for Twisted String Actuator testing, where each experiment requires thousands of cycles and ends in destructive failure. Consequently, data collection is inherently slow, resource-intensive, and physically constrained. To mitigate risks associated with small datasets, the modeling pipeline incorporates repeated 5-fold cross-validation, nested hyperparameter tuning, and a hybrid regression structure in which the empirical component captures global degradation trends while the machine learning component learns only localized nonlinear residuals. These measures collectively reduce variance and improve robustness despite the limited sample size.The experimental setup used for fatigue testing of the Twisted String Actuators is shown inFigure 2. The test bench consists of a servo-motor‚Äìdriven rotary system connected to a string-mounting frame, a load unit with a cylinder for tension application, and a displacement sensor (LVDT) for real-time elongation tracking. The entire assembly integrates a control panel, data-acquisition monitor, and computer system for automatic recording of cycles to failure, ensuring reproducible test conditions. This apparatus enabled precise control of loading parameters (W, D, N) and accurate detection of fracture points across all 144 experimental runs.Figure 2.Experimental setup for Twisted String Actuator (TSA) fatigue testing. 3.1.2. VariablesInput Variables:‚ó¶Weight (W): 15‚Äì25 kg applied tension.‚ó¶Diameter (D): 1.0‚Äì2.0 mm string thickness.‚ó¶Number of Strings (N): 2‚Äì7 parallel wires.‚ó¶Cross-Sectional Area (A): computed asA=œÄ(D/2)2A=œÄ(D/2)2Output Variable:‚ó¶Cycles to Failure (C): total number of actuation cycles until wire fracture.All of variables are summarized inTable 1.Table 1.Summary of experimental variables. 3.1.3. Data Acquisition and OrganizationRaw measurements were recorded in structured CSV blocks corresponding to three load categories (15, 20, 25 kg). Within each load block, multiple-diameter string count combinations were tested to capture multivariate behavior. Data integrity was verified through duplicate checks and physical inspection logs. The dataset was programmatically parsed using Python (pandas) into a unified data frame for subsequent preprocessing and analysis. 3.1.4. Data PreprocessingTo ensure statistical consistency and model convergence, preprocessing steps were applied as follows:Cleaning: Invalid entries such as zero-cycle measurements, corrupted logs, or nonsensical values were removed. These errors originated from counter resets, missing LVDT frames, or automatic safety stops and did not represent meaningful physical noise. Normal experimental noise from sensors was preserved. NaN values primarily resulted from intermittent sensor malfunctions, including incomplete cycle-count logs and temporary LVDT readout interruptions.Feature Engineering: The cross-sectional area was computed and added as a predictor.Normalization: Input features were standardized to zero mean and unit variance.Transformation: The output variable (Cycles to Failure) was log-transformed (log(1+y)log(1+y)) to stabilize variance and reduce heteroscedasticity.Train‚ÄìTest Split: An 80/20 split ensured balanced representation of parameter ranges across training and validation sets.This workflow was implemented in Python 3.13 using NumPy, pandas, and scikit-learn, allowing reproducible data handling and model evaluation.",
            "3.1.1. Experimental Design": "Twisted String Actuators (TSAs) were subjected to cyclic twisting‚Äìuntwisting operations under controlled laboratory conditions until complete wire fracture occurred. Each actuator specimen consisted of multiple steel or nylon wires connected to a rotary motor applying constant tension through a load cell. The cycles to failure were recorded automatically through a digital counter linked to the actuator control system. Environmental conditions, such as temperature (~25 ¬∞C), humidity (~50%), and rotational speed (150 rpm), were maintained constant to minimize uncontrolled variance. All twisted string specimens were fabricated using LIROS D-Pro SK78, which originates from LIROS Ropes/Karlskron, Bavaria, Germany, a high-performance rope constructed from 100% Dyneema¬ÆSK78 fibers. This material is manufactured under strict industrial quality-control processes, including a proprietary heat-stretch stabilization procedure that ensures consistent breaking strength, elongation characteristics, density, and surface finish across production batches. To minimize material variability, all samples were sourced from the same manufacturing lot (or adjacent batch) and were subjected to visual inspection and basic pre-tension checks before testing. Although minor microstructural differences may remain, the standardized nature of SK78 rope and these pre-test screening procedures justify the assumption of material uniformity for the purpose of fatigue life modeling. Each experimental configuration corresponded to a unique combination of applied weight (W), string diameter (D), and number of wires (N). The resulting fatigue life (C) values were measured in cycles until mechanical failure, reflecting the actuator‚Äôs endurance under repetitive mechanical stress. A total of 144 valid experimental observations were retained after removing incomplete or corrupted entries, representing a balanced mix of load and geometry variations. This dataset forms the basis for both empirical regression and machine learning model training. Although the dataset consists of 144 fatigue life samples, this scale is typical for Twisted String Actuator testing, where each experiment requires thousands of cycles and ends in destructive failure. Consequently, data collection is inherently slow, resource-intensive, and physically constrained. To mitigate risks associated with small datasets, the modeling pipeline incorporates repeated 5-fold cross-validation, nested hyperparameter tuning, and a hybrid regression structure in which the empirical component captures global degradation trends while the machine learning component learns only localized nonlinear residuals. These measures collectively reduce variance and improve robustness despite the limited sample size. The experimental setup used for fatigue testing of the Twisted String Actuators is shown inFigure 2. The test bench consists of a servo-motor‚Äìdriven rotary system connected to a string-mounting frame, a load unit with a cylinder for tension application, and a displacement sensor (LVDT) for real-time elongation tracking. The entire assembly integrates a control panel, data-acquisition monitor, and computer system for automatic recording of cycles to failure, ensuring reproducible test conditions. This apparatus enabled precise control of loading parameters (W, D, N) and accurate detection of fracture points across all 144 experimental runs. Figure 2.Experimental setup for Twisted String Actuator (TSA) fatigue testing.",
            "3.1.2. Variables": "Input Variables:‚ó¶Weight (W): 15‚Äì25 kg applied tension.‚ó¶Diameter (D): 1.0‚Äì2.0 mm string thickness.‚ó¶Number of Strings (N): 2‚Äì7 parallel wires.‚ó¶Cross-Sectional Area (A): computed asA=œÄ(D/2)2A=œÄ(D/2)2Output Variable:‚ó¶Cycles to Failure (C): total number of actuation cycles until wire fracture. All of variables are summarized inTable 1. Table 1.Summary of experimental variables.",
            "3.1.3. Data Acquisition and Organization": "Raw measurements were recorded in structured CSV blocks corresponding to three load categories (15, 20, 25 kg). Within each load block, multiple-diameter string count combinations were tested to capture multivariate behavior. Data integrity was verified through duplicate checks and physical inspection logs. The dataset was programmatically parsed using Python (pandas) into a unified data frame for subsequent preprocessing and analysis.",
            "3.1.4. Data Preprocessing": "To ensure statistical consistency and model convergence, preprocessing steps were applied as follows: Cleaning: Invalid entries such as zero-cycle measurements, corrupted logs, or nonsensical values were removed. These errors originated from counter resets, missing LVDT frames, or automatic safety stops and did not represent meaningful physical noise. Normal experimental noise from sensors was preserved. NaN values primarily resulted from intermittent sensor malfunctions, including incomplete cycle-count logs and temporary LVDT readout interruptions.Feature Engineering: The cross-sectional area was computed and added as a predictor.Normalization: Input features were standardized to zero mean and unit variance.Transformation: The output variable (Cycles to Failure) was log-transformed (log(1+y)log(1+y)) to stabilize variance and reduce heteroscedasticity.Train‚ÄìTest Split: An 80/20 split ensured balanced representation of parameter ranges across training and validation sets. This workflow was implemented in Python 3.13 using NumPy, pandas, and scikit-learn, allowing reproducible data handling and model evaluation.",
            "3.2. Empirical Baseline Model": "Prior to machine learning training, an empirical life prediction equation was used as the baseline. The model was derived from Weibull distribution-based life analysis and multiple regression fitting, capturing the influence of weight, number of strings, and diameter on lifespan. The formulation captures the primary physical dependencies between actuator geometry and applied load:Cemp=ùëé√óùê∑ùëè√óùëÅùëê√óùëä‚àíùëë,Cemp=a√óDb√óNc√óW‚àíd,(1)where ùê∂empCempare the predicted cycles to failure,D is the string diameter,N is the number of parallel wires,W is the applied weight, anda, b, c, d are regression coefficients estimated via least-squares fitting on the log-transformed data. This empirical approach provides a deterministic interpretation of fatigue behavior: lifespan increases with diameter and string count but decreases with applied load. Although highly interpretable, the model cannot fully represent nonlinear interactions or manufacturing variability. The empirical model serves two key purposes: Benchmarking: Provides a deterministic reference for evaluating the gain achieved by ML models.Hybrid Prior: Acts as a physics-based prior in the hybrid model, constraining learning to physically meaningful domains.",
            "3.3. Machine Learning Models": "Four machine learning algorithms were implemented and benchmarked against the empirical model: Linear Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and Gaussian Process Regression (GPR). All models used identical training and validation splits to ensure comparability. The entire workflow was executed in Python using scikit-learn and XGBoost packages. The LR model assumes additive linear relationships and serves as a transparent statistical benchmark. It estimates a weighted combination of features to predict fatigue life, enabling straightforward physical interpretation of coefficients. While limited in flexibility, LR is essential for verifying the directionality and proportional influence of each factor, ensuring dataset consistency before applying more complex learners. RF constructs an ensemble of decorrelated decision trees using bootstrap aggregation, effectively capturing nonlinear interactions. Each tree contributes to the ensemble‚Äôs final output via averaging, which mitigates variance and enhances robustness. Feature importance derived from Gini impurity consistently identified diameter and number of strings as dominant predictors, followed by cross-sectional area and load. However, the model displayed mild overfitting due to the limited dataset, prompting the inclusion of gradient-based regularized methods. XGBoost iteratively constructs trees that correct the residuals of previous ones, using gradient optimization and built-in regularization (L1/L2 penalties). This allows precise modeling of nonlinear dependencies between load, geometry, and fatigue lifespan. Optimal hyperparameters determined via GridSearchCV were n_estimators = 300, learning_rate = 0.05, max_depth = 4, subsample = 0.9, colsample_bytree = 0.9. This configuration yielded stable convergence with limited data, achieving high R2scores and low error variance under 5-fold cross-validation. The GPR model provides a probabilistic approach for both prediction and uncertainty quantification. Using a composite kernel comprising a Radial Basis Function (RBF) and WhiteKernel, it models smooth nonlinear trends while accounting for observation noise. Beyond accuracy, GPR yields confidence intervals for each prediction, an essential property for reliability-critical applications such as robotic actuation and aerospace mechanisms. All models were trained using the log-transformed target and evaluated on the test set via R2, RMSE, and MAE metrics. Cross-validation (5-fold) was applied to measure stability and generalization.",
            "3.4. Hybrid Physics-Guided XGBoost Model": "3.4.1. Hybrid Design PhilosophyTo combine the interpretability of physics-based modeling with the flexibility of data-driven learning, a hybrid model was developed that embeds the empirical fatigue life equation within a residual learning structure. The hybrid approach adheres to the principle of Physics-Guided Machine Learning (PGML), constraining data-driven optimization through deterministic scientific laws to preserve physical plausibility.The proposed hybrid framework combines the physics-based empirical model with a data-driven residual learner using XGBoost. The approach follows a two-stage process:Stage 1‚ÄîEmpirical Prediction: the deterministic model computes the baseline lifespan (CempCemp) for each sample based on known physical relationships between L, D, and N.Stage 2‚ÄîResidual Learning: an XGBoost regressor is trained to predict the residual error (Œîùê∂=Cobs‚àíCempŒîC=Cobs‚àíCemp), capturing systematic nonlinearities unexplained by the baseline equation.The final prediction is thus computed as follows:Chybrid=Cemp+fXGB(W,D,N,A)Chybrid=Cemp+fXGB(W,D,N,A)(2)wherefXGBfXGBrepresents the learned correction function.This additive correction structure ensures that the model respects empirical monotonic trends (e.g., lifespan decreases with increasing load) while capturing subtle nonlinear deviations due to microstructural or tribological effects.Unlike previously reported hybrid frameworks that simply combine an analytical model with a residual learning regressor, the hybrid design used in this study incorporates several TSA-specific innovations. The empirical component is derived directly from the torsional deformation mechanics of twisted string transmission, enabling it to capture the global trend of fatigue degradation that arises from geometric shortening and load-dependent friction. The XGBoost component is intentionally constrained to learn only the localized nonlinear deviations associated with micro-slippage, strand compaction, surface friction evolution, and material hysteresis, which are behaviors that are highly characteristic of TSA actuation but not modeled in conventional fatigue equations. By structuring the hybrid model such that the empirical term governs the physically consistent prediction trend while XGBoost refines only the complex residual patterns, the proposed method ensures both interpretability and enhanced predictive accuracy. This TSA-specific formulation differentiates our approach from existing additive hybrid frameworks and provides a tailored solution for accurately modeling twisted string fatigue behavior. 3.4.2. Implementation and AdvantagesThe hybrid model was implemented in Python (scikit-learn + XGBoost) and serialized as a portable .pkl file for real-time deployment.Advantages include the following:Higher Predictive Fidelity: Learns nonlinear effects beyond analytical limits.Physical Coherence: Retains deterministic relationships to prevent unphysical extrapolation.Data Efficiency: Achieves strong accuracy with small datasets due to empirical priors.Interpretability: Enables feature importance analysis to identify dominant fatigue drivers.This structure mirrors modern trends in physics-informed AI, bridging the gap between classical reliability theory and adaptive learning systems.",
            "3.4.1. Hybrid Design Philosophy": "To combine the interpretability of physics-based modeling with the flexibility of data-driven learning, a hybrid model was developed that embeds the empirical fatigue life equation within a residual learning structure. The hybrid approach adheres to the principle of Physics-Guided Machine Learning (PGML), constraining data-driven optimization through deterministic scientific laws to preserve physical plausibility. The proposed hybrid framework combines the physics-based empirical model with a data-driven residual learner using XGBoost. The approach follows a two-stage process: Stage 1‚ÄîEmpirical Prediction: the deterministic model computes the baseline lifespan (CempCemp) for each sample based on known physical relationships between L, D, and N.Stage 2‚ÄîResidual Learning: an XGBoost regressor is trained to predict the residual error (Œîùê∂=Cobs‚àíCempŒîC=Cobs‚àíCemp), capturing systematic nonlinearities unexplained by the baseline equation. The final prediction is thus computed as follows:Chybrid=Cemp+fXGB(W,D,N,A)Chybrid=Cemp+fXGB(W,D,N,A)(2)wherefXGBfXGBrepresents the learned correction function. This additive correction structure ensures that the model respects empirical monotonic trends (e.g., lifespan decreases with increasing load) while capturing subtle nonlinear deviations due to microstructural or tribological effects. Unlike previously reported hybrid frameworks that simply combine an analytical model with a residual learning regressor, the hybrid design used in this study incorporates several TSA-specific innovations. The empirical component is derived directly from the torsional deformation mechanics of twisted string transmission, enabling it to capture the global trend of fatigue degradation that arises from geometric shortening and load-dependent friction. The XGBoost component is intentionally constrained to learn only the localized nonlinear deviations associated with micro-slippage, strand compaction, surface friction evolution, and material hysteresis, which are behaviors that are highly characteristic of TSA actuation but not modeled in conventional fatigue equations. By structuring the hybrid model such that the empirical term governs the physically consistent prediction trend while XGBoost refines only the complex residual patterns, the proposed method ensures both interpretability and enhanced predictive accuracy. This TSA-specific formulation differentiates our approach from existing additive hybrid frameworks and provides a tailored solution for accurately modeling twisted string fatigue behavior.",
            "3.4.2. Implementation and Advantages": "The hybrid model was implemented in Python (scikit-learn + XGBoost) and serialized as a portable .pkl file for real-time deployment. Advantages include the following: Higher Predictive Fidelity: Learns nonlinear effects beyond analytical limits.Physical Coherence: Retains deterministic relationships to prevent unphysical extrapolation.Data Efficiency: Achieves strong accuracy with small datasets due to empirical priors.Interpretability: Enables feature importance analysis to identify dominant fatigue drivers. This structure mirrors modern trends in physics-informed AI, bridging the gap between classical reliability theory and adaptive learning systems.",
            "3.5. Graphical User Interface (GUI) Implementation": "A Tkinter-based GUI was developed to enable real-time testing and visualization. It serves as both an educational and practical interface for engineers to interact with the trained models. The interface allows users to perform the following: Input experimental or hypothetical actuator parameters (W, D, N, A).Instantly compute fatigue life predictions from all five models (Empirical, LR, RF, XGB, GPR, Hybrid).Visualize comparative outputs, feature importances, and predicted lifespans.Export results for design optimization or validation reports. Internally, the GUI loads serialized model files and executes predictions concurrently, ensuring consistency with Python-based results. This tool not only provides an accessible visualization platform for research validation but also serves as a design support interface for actuator development and educational demonstrations in robotic systems engineering.",
            "3.6. Evaluation Metrics": "Model performance was quantified using four complementary statistical metrics: the coefficient of determination (R2), root mean square error (RMSE), mean absolute error (MAE), and cross-validated R2(CV R2). Coefficient of Determination (R2): R2measures how much of the variance in the observed lifespan values is explained by the model. It is defined as ùëÖ2=1‚àí‚àëùëñ(ùë¶ùëñ‚àíùë¶ÃÇùëñ)2‚àëùëñ(ùë¶ùëñ‚àíùë¶‚àí‚àí)2R2=1‚àí‚àëi(yi‚àíy^i)2‚àëi(yi‚àíyÀâ)2(3)whereùë¶ùëñyiandùë¶ÃÇùëñy^idenote actual and predicted values, andùë¶‚àí‚àíyÀâis the sample mean. AnùëÖ2R2close to 1 indicates strong explanatory power; a negative value implies the model performs worse than a constant-mean predictor. RMSE (Root Mean Square Error): RMSE evaluates the magnitude of prediction errors and penalizes large deviations more severely. ùëÄùëÜùê∏=1ùëõ‚àëùëñ(ùë¶ùëñ‚àíùë¶ÃÇùëñ)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àöMSE=1n‚àëi(yi‚àíy^i)2(4) Expressed in the same units as the target (cycles), it reflects the typical error scale and overall model precision. MAE (Mean Absolute Error): MAE provides the average absolute deviation: MAE=1n‚àëi‚à£yi‚àíyÃÇi‚à£MAE=1n‚àëi‚à£yi‚àíy^i‚à£(5) It is less sensitive to outliers than RMSE and therefore complements it by capturing typical prediction bias in engineering terms. Cross-Validated R2: To assess model generalization, 5-fold cross-validation was performed. The dataset was divided into five equal folds; four folds were used for training and one for testing in each iteration. The average R2across all folds gives the cross-validated R2: CVR2=1k‚àëkj=1R2jCVR2=1k‚àëj=1kRj2(6) A small gap between training R2and CV R2indicates low overfitting and robust predictive consistency across parameter variations. Collectively, these four metrics provide a balanced evaluation: R2assesses explained variance, RMSE and MAE quantify absolute prediction accuracy, and CV R2measures generalization. Predictions were evaluated in log scale and then back-transformed to the real scale usingexp(y)‚àí1expy‚àí1for physical interpretability. This dual-scale evaluation ensures numerical stability during training while maintaining engineering relevance in the reported results. This methodological framework integrates physics-based reliability knowledge with modern machine learning techniques. The combination of empirical formulation, four distinct ML models, and a hybrid XGBoost architecture provides a comprehensive evaluation of TSA lifespan prediction strategies. Through data preprocessing, model training, and interactive testing, the approach establishes a robust foundation for engineering-grade lifespan estimation and sets the stage for the comparative analysis presented inSection 4.",
            "4. Results": "This section presents and analyzes the predictive performance of all models developed in this study: the empirical baseline, four individual machine learning (ML) models (Linear Regression, Random Forest, XGBoost, and Gaussian Process Regression), and the proposed Hybrid Empirical‚ÄìXGBoost model. Each model was evaluated on identical datasets using consistent metrics: R2, RMSE, MAE, and cross-validated R2, ensuring fair comparison. Results are discussed both quantitatively (based on statistical metrics) and qualitatively (based on interpretability, stability, and alignment with physical principles). The final subsection discusses the graphical user interface (GUI) and its role in validating model predictions interactively. 4.1. Linear Regression (LR)Linear Regression represents the most interpretable but least flexible modeling paradigm. Assuming additive and proportional relationships among variables (W, D, N, A), it serves as a conceptual baseline for identifying dominant fatigue factors before introducing nonlinear learners.The LR model achieved R2= 0.945, RMSE ‚âà 13,519 cycles, and MAE ‚âà 12,830 cycles on the real-scale dataset. Despite its modest quantitative performance, it provided strong qualitative validation of physical trends.Coefficient analysis revealed the following:Weight (W): negative coefficient, confirming that higher load shortens actuator lifespan.Diameter (D), Number of Strings (N), and Cross-sectional Area (A): positive coefficients, consistent with enhanced structural stiffness and load distribution.These tendencies verify that the experimental dataset follows expected fatigue-mechanics behavior. However, the residuals exhibited a clear nonlinear pattern, particularly at the extremes of applied load and diameter, where the LR model systematically under- or over-predicted fatigue cycles. This bias arises from the model‚Äôs inability to capture multiplicative or interaction effects. For instance, the coupling between load and cross-sectional area, or diminishing returns in durability as diameter increases beyond ~1.8 mm.Consequently, while LR is useful for diagnostic interpretation and sanity checking, its predictive accuracy is insufficient for practical design use, motivating the adoption of nonlinear ensemble models. 4.2. Random Forest (RF)The Random Forest ensemble markedly improved performance, achieving R2= 0.982, RMSE = 8218 cycles, and MAE = 7170 cycles, thus reducing error magnitude by roughly 40% compared with the LR baseline. By aggregating multiple decorrelated decision trees, RF effectively modeled nonlinear dependencies while maintaining moderate interpretability through feature importance metrics.Feature importance ranking shows that Diameter (0.57) > Number of Wires (0.27) > Cross-sectional Area (0.14) > Weight (0.03). This hierarchy mirrors physical intuition: geometric parameters dominate fatigue endurance, while external load, though inversely related, contributes less explanatory variance due to its limited tested range (15‚Äì25 kg).Nevertheless, cross-validation yielded a slightly negative mean R2(‚àí0.19), signaling mild overfitting, a consequence of RF‚Äôs tendency to memorize limited training patterns when sample size is small (n = 144). Despite this limitation, the RF model demonstrated that nonlinear, nonparametric methods substantially outperform linear approaches for TSA data, confirming the existence of higher-order feature interactions that cannot be expressed through traditional regression. 4.3. Extreme Gradient Boosting (XGBoost)Among the standalone ML models, XGBoost exhibited the strongest predictive capability. The optimized configuration (n_estimators = 300, learning_rate = 0.05, max_depth = 4, subsample = 0.9, colsample_bytree = 0.9) delivered R2= 0.987, RMSE ‚âà 8000 cycles, MAE ‚âà 5430 cycles, and a cross-validated R2= 0.25, indicating stable generalization.Feature importance shows that Diameter (0.49) > Weight (0.21) > Area (0.17) > Number of Wires (0.13). Compared to RF, XGBoost assigned higher significance to load, reflecting its superior ability to resolve subtle degradation patterns driven by tension amplitude.Performance gains stem from XGBoost‚Äôs gradient-based residual minimization, which incrementally refines weak learners to reduce systematic bias. This mechanism enables the algorithm to model diminishing marginal returns, e.g., the plateauing effect where increases in wire count or diameter yield progressively smaller gains in lifespan, a phenomenon well-known in cable fatigue.The model also effectively captured localized irregularities caused by string friction, surface roughness, and micro-slip, factors that introduce stochastic variation in fatigue cycles. Furthermore, its moderate transparency (via feature importance and partial-dependence plots) made XGBoost a suitable backbone for hybridization with physics-based priors.Overall, the XGBoost results confirm that a regularized, gradient-driven ensemble can approach empirical-model accuracy while preserving adaptability and robustness. 4.4. Gaussian Process Regression (GPR)Gaussian Process Regression delivered competitive accuracy with R2= 0.983 and RMSE ‚âà 9000 cycles, performing slightly below XGBoost but offering unique insights through uncertainty quantification. Unlike tree-based ensembles, GPR provides probabilistic predictions, returning both a mean estimate and a standard deviation for each input configuration.The implemented kernel structure:ùëò(ùë•ùëñ,ùë•ùëó)=ùúé2exp‚éõ‚éù‚éú‚éú‚éú‚àí‚à•ùë•ùëñ‚àíùë•ùëó‚à•22ùëô2‚éû‚é†‚éü‚éü‚éü+ùúé2ùëõùõøùëñùëók(xi,xj)=œÉ2exp(‚àí‚à•xi‚àíxj‚à•22l2)+œÉn2Œ¥ij(7)(RBF + White Noise kernel) enabled smooth interpolation between observed data points while accounting for measurement noise. The average 1 œÉ uncertainty band (~¬±10% of predicted life) encompassed 96% of all test samples, demonstrating credible probabilistic calibration. Such reliability estimates are essential for safety-critical systems where conservative design margins are required.Although slightly less precise in point predictions than XGBoost, GPR‚Äôs interpretive strength lies in expressing confidence intervals that quantify prediction reliability, a capability absent in purely deterministic models. In mechanical-reliability contexts, this makes GPR particularly useful for early-stage design screening, where understanding prediction variance is as important as the mean estimate.The findings also highlight complementarity between methods: while XGBoost maximizes predictive accuracy, GPR maximizes predictive trustworthiness, a property leveraged in future probabilistic hybrid model extensions. 4.5. Hybrid Physics-Guided XGBoost ModelThe Hybrid Empirical‚ÄìXGBoost framework combines the deterministic precision of empirical modeling with the nonlinear flexibility of machine learning. By embedding the empirical fatigue life equation as a physics-based prior and using XGBoost to learn residual deviations, the hybrid approach achieved the best overall performance among all tested paradigms: R2= 0.986, RMSE = 5299 cycles, MAE = 3329 cycles, and cross-validated R2= 0.975. This corresponds to a ~35% reduction in RMSE relative to Random Forest and ~20% improvement over standalone XGBoost, confirming enhanced generalization and stability.Residual feature importance analysis shows Weight (0.38) > Diameter (0.26) > Cross-sectional Area (0.23) > Number of Wires (0.13). The shift in dominance from geometry in standalone models to load in the hybrid residual learner indicates that the empirical component adequately modeled geometric effects, leaving the residual correction to emphasize load-dependent degradation mechanisms such as frictional heating, torsional stress accumulation, and local strand deformation.Physically, the hybrid architecture preserves monotonicity: predicted lifespan always decreases with increasing W, ensuring physics-consistent extrapolation beyond the training domain. Simultaneously, the data-driven residual term captures fine-scale nonlinearities caused by material heterogeneity, imperfect string alignment, or internal wear dynamics.Error-distribution analysis further supports these observations. Residuals were symmetrically centered around zero with no evident heteroscedasticity, confirming the hybrid model‚Äôs well-calibrated performance across the full parameter range. Compared with purely empirical or purely data-driven approaches, this hybrid model achieved the best compromise between accuracy, interpretability, and physical realism.From an engineering standpoint, the hybrid method is also computationally efficient: training converged within seconds, and inference latency was negligible, facilitating real-time integration into design or maintenance software. The resulting predictor thus provides a practically deployable, scientifically grounded tool for estimating actuator endurance in robotics and soft-mechanism applications. 4.6. Comparative SummaryThe comparative performance of all six predictive approaches: Empirical, Linear Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), Gaussian Process Regression (GPR), and the Hybrid Empirical‚ÄìXGBoost model is summarized below. Each model was trained and evaluated on identical datasets using the same metrics (R2, RMSE, MAE, and cross-validated R2) to ensure fairness in comparison.Table 2highlights the performance of all models with all metrics mentioned above.Table 2.Performance comparison among all models for TSA lifespan prediction.4.6.1. Model Level ComparisonFigure 3presents a bar-chart comparison of determination coefficients across all models. The Hybrid Empirical‚ÄìXGBoost model achieved near-empirical accuracy (R2‚âà 0.986) while maintaining strong generalization. Linear Regression underperformed due to its inability to capture nonlinear dependencies, confirming the necessity of ensemble and hybrid techniques for multivariate mechanical systems.Figure 3.Model performance comparison (R2).Figure 4illustrates both RMSE and MAE for each model in real-scale cycles. The hybrid model simultaneously minimized both error metrics, reducing error variance by approximately 35% compared with Random Forest and about 20% relative to standalone XGBoost. In contrast, Linear Regression exhibited the largest bias and variance, highlighting the limitations of purely linear assumptions when dealing with complex fatigue interactions.Figure 4.Model error comparison (RMSE vs. MAE).As shown inFigure 3andFigure 4, the hybrid model consistently ranked highest in overall performance and stability. While the empirical model achieved the highest R2value within its calibrated range, its deterministic nature limits adaptability to unseen configurations. The hybrid model, by contrast, preserved the underlying physical relationships, lifespan decreasing with greater W (load) and increasing with D and N (diameter and wire count), while flexibly capturing nonlinear cross-effects not representable in analytical formulations.4.6.2. Generalization and Error BehaviorFigure 5demonstrates generalization stability through cross-validated R2scores. The Hybrid model maintains a CV-R2‚âà 0.975, outperforming other algorithms that showed pronounced overfitting or instability under limited-sample conditions. This result confirms that incorporating physical priors helps constrain the learning process and enhances robustness against data scarcity.Figure 5.Cross-validation R2illustrating model generalization ability.Figure 6visualizes residual error distributions across models. The Hybrid and XGBoost curves are symmetrically centered around zero, indicating minimal systematic bias and homoscedastic variance. In contrast, LR and RF display broader, skewed residuals that imply under- or overestimation trends at higher load regimes.Figure 6.Residual error analysis visualizes residual distributions.These findings verify that the hybrid approach balances deterministic consistency and stochastic flexibility: the empirical prior preserves physical monotonicity, while the residual learner adapts to unmodeled nonlinearities such as frictional heating, micro-slip, and strand hysteresis. Together, they enable reliable extrapolation beyond the experimental range, an ability typically absents in purely data-driven systems.4.6.3. Hybrid Model Insights and Predictive VisualizationFigure 7highlights variable importance within the hybrid model, offering a clearer view of how different physical parameters influence TSA fatigue behavior. Weight (‚âà0.37 importance) emerges as the dominant contributor because external load directly governs the torsional‚Äìaxial energy input to the cable. Higher loads increase strand-to-strand friction, local heating, and micro-slippage, which are all factors that accelerate fatigue. This strong sensitivity is consistent with established TSA mechanics, where load is the primary driver of internal energy dissipation.Figure 7.Feature importance in the Hybrid Empirical‚ÄìXGBoost model.Diameter and Cross-Sectional Area exhibit the next highest importance because they determine how stress is distributed across the cable. Larger diameters reduce the torsional strain per unit fiber and mitigate compaction-induced damage, thereby extending lifespan. Their relatively high importance (‚âà0.23‚Äì0.26) reflects this protective effect and captures nonlinear interactions between geometry and frictional losses.The Number of Wires contributes less to the prediction (‚âà0.13 importance), which is expected because wire count influences stiffness and friction pathways but plays a secondary role compared with load magnitude or geometric scaling. Although it affects deformation kinematics, its impact on fatigue progression is subtler and more dependent on configuration-specific effects.Together, these values illustrate that the hybrid model not only fits the data statistically but also aligns with the known physical mechanisms governing TSA degradation, strengthening the interpretability and reliability of the proposed framework.Beyond aggregated feature importance scores, additional analysis reveals how the hybrid model captures local and nonlinear dependencies relevant to TSA fatigue progression. Examination of the model‚Äôs residual structure shows that increases in load produce disproportionately large reductions in predicted lifespan when combined with small diameters, indicating a strong interaction between external tension and geometric stiffness. This nonlinear coupling is consistent with physical expectations: under high loads, thinner strings experience higher torsional strain per fiber and accelerated friction-induced wear.Local sensitivity analysis further indicates that the empirical component captures the monotonic global trend (lifespan decreases with load and increases with geometry), while the XGBoost residual learner corrects localized deviations caused by strand compaction, hysteresis, and micro-slippage. For example, when diameter increases from 1.2 mm to 1.5 mm at moderate loads, the residual correction remains small, but under high-load conditions the model introduces a compensatory residual reflecting the onset of stress concentration and nonlinear friction effects.The model also exhibits stable behavior across the domain boundaries. No prediction inversions or physically implausible trends were observed, suggesting that the hybrid structure constrains potential model bias. By forcing the ML component to learn only residuals‚Äînot the full mapping‚Äîthe architecture avoids overfitting to sparse regions of the dataset and reduces the risk of bias toward high-variance samples.Together, these observations demonstrate that the hybrid model captures the key mechanical interactions governing TSA fatigue while maintaining consistent, physically meaningful predictive behavior. This deeper interpretive analysis shows that the model is not simply fitting statistical patterns but learning corrections that align with known degradation mechanisms.From a computational perspective, the hybrid model is highly scalable. The XGBoost residual learner has approximately O (n log n) training complexity and constant-time inference once the trees are constructed. Therefore, increasing the dataset size does not introduce exponential computational cost. Real-time prediction (<10 ms on a standard laptop CPU) remains feasible even for significantly larger datasets or embedded microcontroller deployment.To further validate real-world usability, the hybrid model was integrated into a graphical user interface (GUI) for interactive TSA lifespan estimation. Three representative test samples which were chosen to span the experimental design space were used to evaluate the predictive consistency of the trained model under unseen input conditions. These correspond to the feature sets shown inTable 3.Table 3.Sample features used to test prediction accuracy.These samples were identical to those used in the Python test block within the hybrid model pipeline (tsa_cycle_predictor_hybrid.pkl) for post-training validation. Each case was processed using the empirical baseline and the hybrid residual-correction model, and compared against actual fatigue data from the TSA dataset.Figure 8compares predicted lifespans from all models against measured experimental data for representative samples. The black bars represent experimentally observed lifespans (ground truth), while colored bars correspond to the model predictions: Linear Regression (LR), Random Forest (RF), XGBoost (XGB), Gaussian Process Regression (GPR), and the Hybrid model.Figure 8.Comparison between actual and model-predicted TSA lifespans for selected samples.The Hybrid model exhibits the closest agreement to the measured data in all three test cases, with deviations consistently within ¬±5% of actual values. This high-fidelity prediction confirms that the hybrid architecture generalizes effectively beyond its training domain, preserving the empirical model‚Äôs physical consistency while capturing nonlinearities introduced by load‚Äìgeometry interactions.Furthermore, these test samples were later embedded in the GUI environment to enable real-time model demonstration and sensitivity analysis (Section 4.7). When users input parameters (load, diameter, number of wires, and area), the trained hybrid model instantly computes lifespan predictions, visualizing empirical, residual, and total predicted outputs for engineering decision support.In summary, the Hybrid Empirical‚ÄìXGBoost architecture delivers the optimal balance between accuracy, interpretability, and generalization. It surpasses standalone ML and empirical models across all statistical and visual evaluations. These results validate the hybrid paradigm as a scalable, physics-consistent framework for fatigue life prediction in small-sample actuator systems. 4.7. GUI Validation and VisualizationTo enhance the practical usability of the proposed models, a Tkinter-based graphical user interface (GUI) was developed to enable users to input custom actuator parameters and instantly compare predicted lifespans across all models. This GUI bridges data analytics and engineering design, allowing real-time interaction with the empirical, ML, and hybrid models.The GUI is organized around three elements:Input Panel: Users enter applied load W (kg), string diameter D (mm), and number of wires N. The interface automatically computes cross-sectional area (ùê¥=ùëÅ¬∑ùúã¬∑ùê∑2/4A=N¬∑œÄ¬∑D2/4, mm2) and displays it read-only to avoid transcription errors. Preset selectors (Sample 1‚Äì3) provide one-click access to the three representative samples used throughout this study (seeFigure 9for the preset interface).Computation Engine: The active model tab determines which prediction algorithm is executed. The empirical lifetime follows this equation:C(N,W,D)=k√óùëÅ1.222√óùëä‚àí1.326√óùê∑2.509CN,W,D=k√óN1.222√óW‚àí1.326√óD2.509(8)whereùëò=2.541√ó105k=2.541√ó105(9)is the empirical constant.Moreover, the hybrid and machine learning tabs invoke the corresponding trained .pkl regressors.Output Display: The resulting lifespan is presented with thousands separators for readability (e.g., 72,006 cycles). Each model tab clearly labels its source (Empirical, Hybrid, Linear Regression, Random Forest, XGBoost, or GPR), allowing direct visual comparison of predictions under identical input conditions.Figure 9.Tkinter-based GUI for TSA lifespan prediction.Figure 9.Tkinter-based GUI for TSA lifespan prediction.The interface was developed in Python using the Tkinter framework and organized into three modular layers: (1) an input‚Äìcontrol panel, (2) a hybrid-model computation engine, and (3) an output visualization module. The computation engine loads the trained empirical, machine learning, and hybrid regressors from serialized .pkl files and executes predictions in under 10 ms on a standard laptop CPU (Intel i5), ensuring real-time responsiveness.To ensure physically meaningful predictions, the GUI performs type checks and range validation on all inputs. Invalid entries (e.g., negative weights, diameters outside 1‚Äì2 mm, non-integer wire counts, undefined values) are blocked, and users are notified via error dialogs. The interface also issues warnings for inputs that, while valid, fall outside the recommended domain of the calibrated models.Together, these implementation details demonstrate that the GUI is more than a high-level visualization tool: it is a functional, validated interface that operationalizes the proposed hybrid model for practical design and diagnostic tasks.Figure 9provides an overview of the GUI layout, showing input boxes, preset selector, and model tabs.Figure 10illustrates the dropdown preset system that ensures consistent input handling and automatic computation of A.Figure 10.Preset selector and auto-computed area field. Dropdown presets (Sample 1‚Äì3) automatically populate W, D, N and calculate cross-sectional area, minimizing input error.Figure 11a‚Äìf displays the prediction panels for all six models under the same operating condition (Sample 2: W = 20 kg, D = 1.5 mm, N = 4). The hybrid model (Figure 10predicts a lifespan of 74,088 cycles, representing a +2.89% improvement relative to the empirical baseline (72,006 cycles), while maintaining physically consistent trends.Figure 11.Model-specific prediction results displayed through the GUI. (a) Empirical model, (b) Hybrid Empirical + Residual, (c) Linear Regression, (d) Random Forest, (e) XGBoost, and (f) Gaussian Process Regression. All examples correspond to Sample 2 (W = 20 kg, D = 1.5 mm, N = 4).Regression testing verified numerical equivalence between GUI outputs and direct Python evaluations, with absolute differences = 0 for all tested samples. The GUI thus provides a validated, lightweight engineering tool for real-time TSA life estimation, supporting both design exploration and classroom demonstration. By combining visualization and data analytics, the GUI bridges experimental and computational domains, facilitating both laboratory validation and industry deployment of the proposed framework.All Python source codes, trained models, and datasets used in this study are openly available in the project‚Äôs GitHub (2.41.0) repository (Appendix A).",
            "4.1. Linear Regression (LR)": "Linear Regression represents the most interpretable but least flexible modeling paradigm. Assuming additive and proportional relationships among variables (W, D, N, A), it serves as a conceptual baseline for identifying dominant fatigue factors before introducing nonlinear learners. The LR model achieved R2= 0.945, RMSE ‚âà 13,519 cycles, and MAE ‚âà 12,830 cycles on the real-scale dataset. Despite its modest quantitative performance, it provided strong qualitative validation of physical trends. Coefficient analysis revealed the following: Weight (W): negative coefficient, confirming that higher load shortens actuator lifespan.Diameter (D), Number of Strings (N), and Cross-sectional Area (A): positive coefficients, consistent with enhanced structural stiffness and load distribution. These tendencies verify that the experimental dataset follows expected fatigue-mechanics behavior. However, the residuals exhibited a clear nonlinear pattern, particularly at the extremes of applied load and diameter, where the LR model systematically under- or over-predicted fatigue cycles. This bias arises from the model‚Äôs inability to capture multiplicative or interaction effects. For instance, the coupling between load and cross-sectional area, or diminishing returns in durability as diameter increases beyond ~1.8 mm. Consequently, while LR is useful for diagnostic interpretation and sanity checking, its predictive accuracy is insufficient for practical design use, motivating the adoption of nonlinear ensemble models.",
            "4.2. Random Forest (RF)": "The Random Forest ensemble markedly improved performance, achieving R2= 0.982, RMSE = 8218 cycles, and MAE = 7170 cycles, thus reducing error magnitude by roughly 40% compared with the LR baseline. By aggregating multiple decorrelated decision trees, RF effectively modeled nonlinear dependencies while maintaining moderate interpretability through feature importance metrics. Feature importance ranking shows that Diameter (0.57) > Number of Wires (0.27) > Cross-sectional Area (0.14) > Weight (0.03). This hierarchy mirrors physical intuition: geometric parameters dominate fatigue endurance, while external load, though inversely related, contributes less explanatory variance due to its limited tested range (15‚Äì25 kg). Nevertheless, cross-validation yielded a slightly negative mean R2(‚àí0.19), signaling mild overfitting, a consequence of RF‚Äôs tendency to memorize limited training patterns when sample size is small (n = 144). Despite this limitation, the RF model demonstrated that nonlinear, nonparametric methods substantially outperform linear approaches for TSA data, confirming the existence of higher-order feature interactions that cannot be expressed through traditional regression.",
            "4.3. Extreme Gradient Boosting (XGBoost)": "Among the standalone ML models, XGBoost exhibited the strongest predictive capability. The optimized configuration (n_estimators = 300, learning_rate = 0.05, max_depth = 4, subsample = 0.9, colsample_bytree = 0.9) delivered R2= 0.987, RMSE ‚âà 8000 cycles, MAE ‚âà 5430 cycles, and a cross-validated R2= 0.25, indicating stable generalization. Feature importance shows that Diameter (0.49) > Weight (0.21) > Area (0.17) > Number of Wires (0.13). Compared to RF, XGBoost assigned higher significance to load, reflecting its superior ability to resolve subtle degradation patterns driven by tension amplitude. Performance gains stem from XGBoost‚Äôs gradient-based residual minimization, which incrementally refines weak learners to reduce systematic bias. This mechanism enables the algorithm to model diminishing marginal returns, e.g., the plateauing effect where increases in wire count or diameter yield progressively smaller gains in lifespan, a phenomenon well-known in cable fatigue. The model also effectively captured localized irregularities caused by string friction, surface roughness, and micro-slip, factors that introduce stochastic variation in fatigue cycles. Furthermore, its moderate transparency (via feature importance and partial-dependence plots) made XGBoost a suitable backbone for hybridization with physics-based priors. Overall, the XGBoost results confirm that a regularized, gradient-driven ensemble can approach empirical-model accuracy while preserving adaptability and robustness.",
            "4.4. Gaussian Process Regression (GPR)": "Gaussian Process Regression delivered competitive accuracy with R2= 0.983 and RMSE ‚âà 9000 cycles, performing slightly below XGBoost but offering unique insights through uncertainty quantification. Unlike tree-based ensembles, GPR provides probabilistic predictions, returning both a mean estimate and a standard deviation for each input configuration. The implemented kernel structure:ùëò(ùë•ùëñ,ùë•ùëó)=ùúé2exp‚éõ‚éù‚éú‚éú‚éú‚àí‚à•ùë•ùëñ‚àíùë•ùëó‚à•22ùëô2‚éû‚é†‚éü‚éü‚éü+ùúé2ùëõùõøùëñùëók(xi,xj)=œÉ2exp(‚àí‚à•xi‚àíxj‚à•22l2)+œÉn2Œ¥ij(7) (RBF + White Noise kernel) enabled smooth interpolation between observed data points while accounting for measurement noise. The average 1 œÉ uncertainty band (~¬±10% of predicted life) encompassed 96% of all test samples, demonstrating credible probabilistic calibration. Such reliability estimates are essential for safety-critical systems where conservative design margins are required. Although slightly less precise in point predictions than XGBoost, GPR‚Äôs interpretive strength lies in expressing confidence intervals that quantify prediction reliability, a capability absent in purely deterministic models. In mechanical-reliability contexts, this makes GPR particularly useful for early-stage design screening, where understanding prediction variance is as important as the mean estimate. The findings also highlight complementarity between methods: while XGBoost maximizes predictive accuracy, GPR maximizes predictive trustworthiness, a property leveraged in future probabilistic hybrid model extensions.",
            "4.5. Hybrid Physics-Guided XGBoost Model": "The Hybrid Empirical‚ÄìXGBoost framework combines the deterministic precision of empirical modeling with the nonlinear flexibility of machine learning. By embedding the empirical fatigue life equation as a physics-based prior and using XGBoost to learn residual deviations, the hybrid approach achieved the best overall performance among all tested paradigms: R2= 0.986, RMSE = 5299 cycles, MAE = 3329 cycles, and cross-validated R2= 0.975. This corresponds to a ~35% reduction in RMSE relative to Random Forest and ~20% improvement over standalone XGBoost, confirming enhanced generalization and stability. Residual feature importance analysis shows Weight (0.38) > Diameter (0.26) > Cross-sectional Area (0.23) > Number of Wires (0.13). The shift in dominance from geometry in standalone models to load in the hybrid residual learner indicates that the empirical component adequately modeled geometric effects, leaving the residual correction to emphasize load-dependent degradation mechanisms such as frictional heating, torsional stress accumulation, and local strand deformation. Physically, the hybrid architecture preserves monotonicity: predicted lifespan always decreases with increasing W, ensuring physics-consistent extrapolation beyond the training domain. Simultaneously, the data-driven residual term captures fine-scale nonlinearities caused by material heterogeneity, imperfect string alignment, or internal wear dynamics. Error-distribution analysis further supports these observations. Residuals were symmetrically centered around zero with no evident heteroscedasticity, confirming the hybrid model‚Äôs well-calibrated performance across the full parameter range. Compared with purely empirical or purely data-driven approaches, this hybrid model achieved the best compromise between accuracy, interpretability, and physical realism. From an engineering standpoint, the hybrid method is also computationally efficient: training converged within seconds, and inference latency was negligible, facilitating real-time integration into design or maintenance software. The resulting predictor thus provides a practically deployable, scientifically grounded tool for estimating actuator endurance in robotics and soft-mechanism applications.",
            "4.6. Comparative Summary": "The comparative performance of all six predictive approaches: Empirical, Linear Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), Gaussian Process Regression (GPR), and the Hybrid Empirical‚ÄìXGBoost model is summarized below. Each model was trained and evaluated on identical datasets using the same metrics (R2, RMSE, MAE, and cross-validated R2) to ensure fairness in comparison.Table 2highlights the performance of all models with all metrics mentioned above. Table 2.Performance comparison among all models for TSA lifespan prediction. 4.6.1. Model Level ComparisonFigure 3presents a bar-chart comparison of determination coefficients across all models. The Hybrid Empirical‚ÄìXGBoost model achieved near-empirical accuracy (R2‚âà 0.986) while maintaining strong generalization. Linear Regression underperformed due to its inability to capture nonlinear dependencies, confirming the necessity of ensemble and hybrid techniques for multivariate mechanical systems.Figure 3.Model performance comparison (R2).Figure 4illustrates both RMSE and MAE for each model in real-scale cycles. The hybrid model simultaneously minimized both error metrics, reducing error variance by approximately 35% compared with Random Forest and about 20% relative to standalone XGBoost. In contrast, Linear Regression exhibited the largest bias and variance, highlighting the limitations of purely linear assumptions when dealing with complex fatigue interactions.Figure 4.Model error comparison (RMSE vs. MAE).As shown inFigure 3andFigure 4, the hybrid model consistently ranked highest in overall performance and stability. While the empirical model achieved the highest R2value within its calibrated range, its deterministic nature limits adaptability to unseen configurations. The hybrid model, by contrast, preserved the underlying physical relationships, lifespan decreasing with greater W (load) and increasing with D and N (diameter and wire count), while flexibly capturing nonlinear cross-effects not representable in analytical formulations. 4.6.2. Generalization and Error BehaviorFigure 5demonstrates generalization stability through cross-validated R2scores. The Hybrid model maintains a CV-R2‚âà 0.975, outperforming other algorithms that showed pronounced overfitting or instability under limited-sample conditions. This result confirms that incorporating physical priors helps constrain the learning process and enhances robustness against data scarcity.Figure 5.Cross-validation R2illustrating model generalization ability.Figure 6visualizes residual error distributions across models. The Hybrid and XGBoost curves are symmetrically centered around zero, indicating minimal systematic bias and homoscedastic variance. In contrast, LR and RF display broader, skewed residuals that imply under- or overestimation trends at higher load regimes.Figure 6.Residual error analysis visualizes residual distributions.These findings verify that the hybrid approach balances deterministic consistency and stochastic flexibility: the empirical prior preserves physical monotonicity, while the residual learner adapts to unmodeled nonlinearities such as frictional heating, micro-slip, and strand hysteresis. Together, they enable reliable extrapolation beyond the experimental range, an ability typically absents in purely data-driven systems. 4.6.3. Hybrid Model Insights and Predictive VisualizationFigure 7highlights variable importance within the hybrid model, offering a clearer view of how different physical parameters influence TSA fatigue behavior. Weight (‚âà0.37 importance) emerges as the dominant contributor because external load directly governs the torsional‚Äìaxial energy input to the cable. Higher loads increase strand-to-strand friction, local heating, and micro-slippage, which are all factors that accelerate fatigue. This strong sensitivity is consistent with established TSA mechanics, where load is the primary driver of internal energy dissipation.Figure 7.Feature importance in the Hybrid Empirical‚ÄìXGBoost model.Diameter and Cross-Sectional Area exhibit the next highest importance because they determine how stress is distributed across the cable. Larger diameters reduce the torsional strain per unit fiber and mitigate compaction-induced damage, thereby extending lifespan. Their relatively high importance (‚âà0.23‚Äì0.26) reflects this protective effect and captures nonlinear interactions between geometry and frictional losses.The Number of Wires contributes less to the prediction (‚âà0.13 importance), which is expected because wire count influences stiffness and friction pathways but plays a secondary role compared with load magnitude or geometric scaling. Although it affects deformation kinematics, its impact on fatigue progression is subtler and more dependent on configuration-specific effects.Together, these values illustrate that the hybrid model not only fits the data statistically but also aligns with the known physical mechanisms governing TSA degradation, strengthening the interpretability and reliability of the proposed framework.Beyond aggregated feature importance scores, additional analysis reveals how the hybrid model captures local and nonlinear dependencies relevant to TSA fatigue progression. Examination of the model‚Äôs residual structure shows that increases in load produce disproportionately large reductions in predicted lifespan when combined with small diameters, indicating a strong interaction between external tension and geometric stiffness. This nonlinear coupling is consistent with physical expectations: under high loads, thinner strings experience higher torsional strain per fiber and accelerated friction-induced wear.Local sensitivity analysis further indicates that the empirical component captures the monotonic global trend (lifespan decreases with load and increases with geometry), while the XGBoost residual learner corrects localized deviations caused by strand compaction, hysteresis, and micro-slippage. For example, when diameter increases from 1.2 mm to 1.5 mm at moderate loads, the residual correction remains small, but under high-load conditions the model introduces a compensatory residual reflecting the onset of stress concentration and nonlinear friction effects.The model also exhibits stable behavior across the domain boundaries. No prediction inversions or physically implausible trends were observed, suggesting that the hybrid structure constrains potential model bias. By forcing the ML component to learn only residuals‚Äînot the full mapping‚Äîthe architecture avoids overfitting to sparse regions of the dataset and reduces the risk of bias toward high-variance samples.Together, these observations demonstrate that the hybrid model captures the key mechanical interactions governing TSA fatigue while maintaining consistent, physically meaningful predictive behavior. This deeper interpretive analysis shows that the model is not simply fitting statistical patterns but learning corrections that align with known degradation mechanisms.From a computational perspective, the hybrid model is highly scalable. The XGBoost residual learner has approximately O (n log n) training complexity and constant-time inference once the trees are constructed. Therefore, increasing the dataset size does not introduce exponential computational cost. Real-time prediction (<10 ms on a standard laptop CPU) remains feasible even for significantly larger datasets or embedded microcontroller deployment.To further validate real-world usability, the hybrid model was integrated into a graphical user interface (GUI) for interactive TSA lifespan estimation. Three representative test samples which were chosen to span the experimental design space were used to evaluate the predictive consistency of the trained model under unseen input conditions. These correspond to the feature sets shown inTable 3.Table 3.Sample features used to test prediction accuracy.These samples were identical to those used in the Python test block within the hybrid model pipeline (tsa_cycle_predictor_hybrid.pkl) for post-training validation. Each case was processed using the empirical baseline and the hybrid residual-correction model, and compared against actual fatigue data from the TSA dataset.Figure 8compares predicted lifespans from all models against measured experimental data for representative samples. The black bars represent experimentally observed lifespans (ground truth), while colored bars correspond to the model predictions: Linear Regression (LR), Random Forest (RF), XGBoost (XGB), Gaussian Process Regression (GPR), and the Hybrid model.Figure 8.Comparison between actual and model-predicted TSA lifespans for selected samples.The Hybrid model exhibits the closest agreement to the measured data in all three test cases, with deviations consistently within ¬±5% of actual values. This high-fidelity prediction confirms that the hybrid architecture generalizes effectively beyond its training domain, preserving the empirical model‚Äôs physical consistency while capturing nonlinearities introduced by load‚Äìgeometry interactions.Furthermore, these test samples were later embedded in the GUI environment to enable real-time model demonstration and sensitivity analysis (Section 4.7). When users input parameters (load, diameter, number of wires, and area), the trained hybrid model instantly computes lifespan predictions, visualizing empirical, residual, and total predicted outputs for engineering decision support.In summary, the Hybrid Empirical‚ÄìXGBoost architecture delivers the optimal balance between accuracy, interpretability, and generalization. It surpasses standalone ML and empirical models across all statistical and visual evaluations. These results validate the hybrid paradigm as a scalable, physics-consistent framework for fatigue life prediction in small-sample actuator systems.",
            "4.6.1. Model Level Comparison": "Figure 3presents a bar-chart comparison of determination coefficients across all models. The Hybrid Empirical‚ÄìXGBoost model achieved near-empirical accuracy (R2‚âà 0.986) while maintaining strong generalization. Linear Regression underperformed due to its inability to capture nonlinear dependencies, confirming the necessity of ensemble and hybrid techniques for multivariate mechanical systems. Figure 3.Model performance comparison (R2). Figure 4illustrates both RMSE and MAE for each model in real-scale cycles. The hybrid model simultaneously minimized both error metrics, reducing error variance by approximately 35% compared with Random Forest and about 20% relative to standalone XGBoost. In contrast, Linear Regression exhibited the largest bias and variance, highlighting the limitations of purely linear assumptions when dealing with complex fatigue interactions. Figure 4.Model error comparison (RMSE vs. MAE). As shown inFigure 3andFigure 4, the hybrid model consistently ranked highest in overall performance and stability. While the empirical model achieved the highest R2value within its calibrated range, its deterministic nature limits adaptability to unseen configurations. The hybrid model, by contrast, preserved the underlying physical relationships, lifespan decreasing with greater W (load) and increasing with D and N (diameter and wire count), while flexibly capturing nonlinear cross-effects not representable in analytical formulations.",
            "4.6.2. Generalization and Error Behavior": "Figure 5demonstrates generalization stability through cross-validated R2scores. The Hybrid model maintains a CV-R2‚âà 0.975, outperforming other algorithms that showed pronounced overfitting or instability under limited-sample conditions. This result confirms that incorporating physical priors helps constrain the learning process and enhances robustness against data scarcity. Figure 5.Cross-validation R2illustrating model generalization ability. Figure 6visualizes residual error distributions across models. The Hybrid and XGBoost curves are symmetrically centered around zero, indicating minimal systematic bias and homoscedastic variance. In contrast, LR and RF display broader, skewed residuals that imply under- or overestimation trends at higher load regimes. Figure 6.Residual error analysis visualizes residual distributions. These findings verify that the hybrid approach balances deterministic consistency and stochastic flexibility: the empirical prior preserves physical monotonicity, while the residual learner adapts to unmodeled nonlinearities such as frictional heating, micro-slip, and strand hysteresis. Together, they enable reliable extrapolation beyond the experimental range, an ability typically absents in purely data-driven systems.",
            "4.6.3. Hybrid Model Insights and Predictive Visualization": "Figure 7highlights variable importance within the hybrid model, offering a clearer view of how different physical parameters influence TSA fatigue behavior. Weight (‚âà0.37 importance) emerges as the dominant contributor because external load directly governs the torsional‚Äìaxial energy input to the cable. Higher loads increase strand-to-strand friction, local heating, and micro-slippage, which are all factors that accelerate fatigue. This strong sensitivity is consistent with established TSA mechanics, where load is the primary driver of internal energy dissipation. Figure 7.Feature importance in the Hybrid Empirical‚ÄìXGBoost model. Diameter and Cross-Sectional Area exhibit the next highest importance because they determine how stress is distributed across the cable. Larger diameters reduce the torsional strain per unit fiber and mitigate compaction-induced damage, thereby extending lifespan. Their relatively high importance (‚âà0.23‚Äì0.26) reflects this protective effect and captures nonlinear interactions between geometry and frictional losses. The Number of Wires contributes less to the prediction (‚âà0.13 importance), which is expected because wire count influences stiffness and friction pathways but plays a secondary role compared with load magnitude or geometric scaling. Although it affects deformation kinematics, its impact on fatigue progression is subtler and more dependent on configuration-specific effects. Together, these values illustrate that the hybrid model not only fits the data statistically but also aligns with the known physical mechanisms governing TSA degradation, strengthening the interpretability and reliability of the proposed framework. Beyond aggregated feature importance scores, additional analysis reveals how the hybrid model captures local and nonlinear dependencies relevant to TSA fatigue progression. Examination of the model‚Äôs residual structure shows that increases in load produce disproportionately large reductions in predicted lifespan when combined with small diameters, indicating a strong interaction between external tension and geometric stiffness. This nonlinear coupling is consistent with physical expectations: under high loads, thinner strings experience higher torsional strain per fiber and accelerated friction-induced wear. Local sensitivity analysis further indicates that the empirical component captures the monotonic global trend (lifespan decreases with load and increases with geometry), while the XGBoost residual learner corrects localized deviations caused by strand compaction, hysteresis, and micro-slippage. For example, when diameter increases from 1.2 mm to 1.5 mm at moderate loads, the residual correction remains small, but under high-load conditions the model introduces a compensatory residual reflecting the onset of stress concentration and nonlinear friction effects. The model also exhibits stable behavior across the domain boundaries. No prediction inversions or physically implausible trends were observed, suggesting that the hybrid structure constrains potential model bias. By forcing the ML component to learn only residuals‚Äînot the full mapping‚Äîthe architecture avoids overfitting to sparse regions of the dataset and reduces the risk of bias toward high-variance samples. Together, these observations demonstrate that the hybrid model captures the key mechanical interactions governing TSA fatigue while maintaining consistent, physically meaningful predictive behavior. This deeper interpretive analysis shows that the model is not simply fitting statistical patterns but learning corrections that align with known degradation mechanisms. From a computational perspective, the hybrid model is highly scalable. The XGBoost residual learner has approximately O (n log n) training complexity and constant-time inference once the trees are constructed. Therefore, increasing the dataset size does not introduce exponential computational cost. Real-time prediction (<10 ms on a standard laptop CPU) remains feasible even for significantly larger datasets or embedded microcontroller deployment. To further validate real-world usability, the hybrid model was integrated into a graphical user interface (GUI) for interactive TSA lifespan estimation. Three representative test samples which were chosen to span the experimental design space were used to evaluate the predictive consistency of the trained model under unseen input conditions. These correspond to the feature sets shown inTable 3. Table 3.Sample features used to test prediction accuracy. These samples were identical to those used in the Python test block within the hybrid model pipeline (tsa_cycle_predictor_hybrid.pkl) for post-training validation. Each case was processed using the empirical baseline and the hybrid residual-correction model, and compared against actual fatigue data from the TSA dataset. Figure 8compares predicted lifespans from all models against measured experimental data for representative samples. The black bars represent experimentally observed lifespans (ground truth), while colored bars correspond to the model predictions: Linear Regression (LR), Random Forest (RF), XGBoost (XGB), Gaussian Process Regression (GPR), and the Hybrid model. Figure 8.Comparison between actual and model-predicted TSA lifespans for selected samples. The Hybrid model exhibits the closest agreement to the measured data in all three test cases, with deviations consistently within ¬±5% of actual values. This high-fidelity prediction confirms that the hybrid architecture generalizes effectively beyond its training domain, preserving the empirical model‚Äôs physical consistency while capturing nonlinearities introduced by load‚Äìgeometry interactions. Furthermore, these test samples were later embedded in the GUI environment to enable real-time model demonstration and sensitivity analysis (Section 4.7). When users input parameters (load, diameter, number of wires, and area), the trained hybrid model instantly computes lifespan predictions, visualizing empirical, residual, and total predicted outputs for engineering decision support. In summary, the Hybrid Empirical‚ÄìXGBoost architecture delivers the optimal balance between accuracy, interpretability, and generalization. It surpasses standalone ML and empirical models across all statistical and visual evaluations. These results validate the hybrid paradigm as a scalable, physics-consistent framework for fatigue life prediction in small-sample actuator systems.",
            "4.7. GUI Validation and Visualization": "To enhance the practical usability of the proposed models, a Tkinter-based graphical user interface (GUI) was developed to enable users to input custom actuator parameters and instantly compare predicted lifespans across all models. This GUI bridges data analytics and engineering design, allowing real-time interaction with the empirical, ML, and hybrid models. The GUI is organized around three elements: Input Panel: Users enter applied load W (kg), string diameter D (mm), and number of wires N. The interface automatically computes cross-sectional area (ùê¥=ùëÅ¬∑ùúã¬∑ùê∑2/4A=N¬∑œÄ¬∑D2/4, mm2) and displays it read-only to avoid transcription errors. Preset selectors (Sample 1‚Äì3) provide one-click access to the three representative samples used throughout this study (seeFigure 9for the preset interface).Computation Engine: The active model tab determines which prediction algorithm is executed. The empirical lifetime follows this equation: C(N,W,D)=k√óùëÅ1.222√óùëä‚àí1.326√óùê∑2.509CN,W,D=k√óN1.222√óW‚àí1.326√óD2.509(8)whereùëò=2.541√ó105k=2.541√ó105(9)is the empirical constant. Moreover, the hybrid and machine learning tabs invoke the corresponding trained .pkl regressors. Output Display: The resulting lifespan is presented with thousands separators for readability (e.g., 72,006 cycles). Each model tab clearly labels its source (Empirical, Hybrid, Linear Regression, Random Forest, XGBoost, or GPR), allowing direct visual comparison of predictions under identical input conditions. Figure 9.Tkinter-based GUI for TSA lifespan prediction.Figure 9.Tkinter-based GUI for TSA lifespan prediction. The interface was developed in Python using the Tkinter framework and organized into three modular layers: (1) an input‚Äìcontrol panel, (2) a hybrid-model computation engine, and (3) an output visualization module. The computation engine loads the trained empirical, machine learning, and hybrid regressors from serialized .pkl files and executes predictions in under 10 ms on a standard laptop CPU (Intel i5), ensuring real-time responsiveness. To ensure physically meaningful predictions, the GUI performs type checks and range validation on all inputs. Invalid entries (e.g., negative weights, diameters outside 1‚Äì2 mm, non-integer wire counts, undefined values) are blocked, and users are notified via error dialogs. The interface also issues warnings for inputs that, while valid, fall outside the recommended domain of the calibrated models. Together, these implementation details demonstrate that the GUI is more than a high-level visualization tool: it is a functional, validated interface that operationalizes the proposed hybrid model for practical design and diagnostic tasks. Figure 9provides an overview of the GUI layout, showing input boxes, preset selector, and model tabs.Figure 10illustrates the dropdown preset system that ensures consistent input handling and automatic computation of A. Figure 10.Preset selector and auto-computed area field. Dropdown presets (Sample 1‚Äì3) automatically populate W, D, N and calculate cross-sectional area, minimizing input error. Figure 11a‚Äìf displays the prediction panels for all six models under the same operating condition (Sample 2: W = 20 kg, D = 1.5 mm, N = 4). The hybrid model (Figure 10predicts a lifespan of 74,088 cycles, representing a +2.89% improvement relative to the empirical baseline (72,006 cycles), while maintaining physically consistent trends. Figure 11.Model-specific prediction results displayed through the GUI. (a) Empirical model, (b) Hybrid Empirical + Residual, (c) Linear Regression, (d) Random Forest, (e) XGBoost, and (f) Gaussian Process Regression. All examples correspond to Sample 2 (W = 20 kg, D = 1.5 mm, N = 4). Regression testing verified numerical equivalence between GUI outputs and direct Python evaluations, with absolute differences = 0 for all tested samples. The GUI thus provides a validated, lightweight engineering tool for real-time TSA life estimation, supporting both design exploration and classroom demonstration. By combining visualization and data analytics, the GUI bridges experimental and computational domains, facilitating both laboratory validation and industry deployment of the proposed framework. All Python source codes, trained models, and datasets used in this study are openly available in the project‚Äôs GitHub (2.41.0) repository (Appendix A).",
            "5. Discussion": "The collective results confirm that embedding physics knowledge within machine learning frameworks significantly enhances both predictive accuracy and scientific interpretability for small-sample mechanical systems such as TSAs. 5.1. Quantitative InsightsWhile the empirical model achieved the highest nominal R2within the calibrated range (‚âà0.987), its performance was largely the result of over-tuning to laboratory-specific conditions. Under unseen inputs or modified geometries, its error variance increased sharply, illustrating limited generalizability. By contrast, the hybrid Empirical‚ÄìXGBoost model maintained R2‚âà 0.986 and cross-validated R2‚âà 0.975 even under domain shift, reflecting robust generalization and data efficiency.Quantitatively, the hybrid approach reduced prediction-error variance by ~35% relative to Random Forest and ~20% relative to standalone XGBoost, while preserving monotonic physical behavior across all input domains. It also produced the most stable residual distribution (standard deviation ‚âà 0.07 in log scale), indicating well-balanced bias‚Äìvariance trade-off.The joint analysis of R2, RMSE, MAE, and CV R2confirms that the hybrid empirical‚ÄìXGBoost model not only fits the training data closely but also generalizes reliably. The small discrepancy between R2(0.986) and CV R2(0.975) indicates minimal overfitting, while low RMSE and MAE values demonstrate precise quantitative agreement with experimental measurements.The strong cross-validated performance of the hybrid model arises from the complementary roles of its empirical and machine learning components. The empirical TSA fatigue equation captures the dominant global degradation trend driven by weight, geometry, and torsional mechanics, providing a physically grounded baseline prediction. XGBoost then focuses exclusively on learning the residual nonlinearities‚Äîsuch as friction-induced hysteresis, strand compaction, and material micro-slippage‚Äîthat the empirical model alone cannot represent. Because the ML component does not need to learn the entire mapping from scratch, but only the local deviations from the physical law, the model exhibits lower variance, improved stability under domain shift, and reduced risk of overfitting compared with standalone ML models. This residual learning structure explains why the hybrid approach achieves a significantly higher cross-validated R2than Random Forest, Gaussian Process Regression, or standalone XGBoost. 5.2. Methodological InterpretationFrom a modeling perspective, the hybrid architecture captures residual nonlinearities, such as the interaction between load and diameter, that traditional Weibull or regression formulations cannot express. By embedding the empirical equation as a deterministic prior, the learning process is constrained within physically plausible limits. Consequently, lifespan predictions remain monotonic (decreasing with increasing load and increasing with diameter and string count) even when extrapolated beyond training ranges.Residual inspection showed that major corrections occurred in two regions:Low-diameter (<1.2 mm) samples where frictional losses caused premature failure and empirical predictions overestimated lifespan.High-weight (>22 kg) conditions where material plasticity accelerated degradation beyond Weibull-based expectations.The residual learner successfully captured these nonlinear behaviors, reducing systematic bias and improving real-world reliability. 5.3. Engineering and Scientific SignificanceBeyond numerical accuracy, the hybrid model offers critical engineering benefits:Interpretability: Predictions can be traced to physically meaningful relationships, enhancing trust in design decisions.Reusability: Requires only minor re-calibration for new materials or geometries, reducing experimental burden.Robustness: Performs reliably even with limited data, making it suitable for early-stage product development and prototype testing.Scalability: Although no real-world IoT deployment was performed, the hybrid model is computationally lightweight (<10 ms inference time) and small in memory footprint, making it suitable for microcontroller-based predictive maintenance systems. 5.4. Comparison with Prior StudiesThe observed advantages align with findings by [4,30,35], who demonstrated that incorporating physical constraints into learning algorithms produces more trustworthy and generalizable predictors. Like these physics-informed neural networks (PINNs), the proposed hybrid regression balances empirical law fidelity with statistical flexibility, but with significantly lower computational complexity and easier interpretability. 5.5. Practical Implications and Future DirectionsFor TSA systems, where acquiring large fatigue datasets is time-consuming and costly, the proposed method offers a scalable alternative for lifespan forecasting and predictive maintenance. The GUI developed in this study represents a practical engineering contribution that operationalizes the hybrid model in a deployable form. Unlike prior TSA prediction studies that present models only in analytical or code-based formats, the GUI allows users to obtain real-time lifespan predictions without programming expertise. It includes built-in parameter validation, structured input ranges based on physical constraints, and instant visualization of predicted fatigue trends. The interface enables researchers, engineers, and system designers to perform rapid what-if analysis, explore safe operating regions, and evaluate the impact of weight, geometry, and wire configuration on actuator lifespan. By providing an accessible tool that implements the full hybrid modeling pipeline, this study translates its methodological innovation into a functional platform suitable for laboratory use and early-stage product development.Future extensions will explore probabilistic hybrid frameworks that combine the current empirical‚ÄìXGBoost structure with Gaussian process residuals to yield uncertainty-aware predictions. Integration with real-time sensor streams and edge-computing devices could enable online health monitoring for TSA-based robots and tendon-driven mechanisms.In summary, the proposed physics-guided hybrid architecture demonstrates that the fusion of empirical knowledge and machine learning is a powerful, data-efficient, and interpretable approach for mechanical lifespan prediction. It bridges the long-standing gap between domain theory and data science, establishing a foundation for next-generation AI-driven reliability engineering in robotic actuation systems.",
            "5.1. Quantitative Insights": "While the empirical model achieved the highest nominal R2within the calibrated range (‚âà0.987), its performance was largely the result of over-tuning to laboratory-specific conditions. Under unseen inputs or modified geometries, its error variance increased sharply, illustrating limited generalizability. By contrast, the hybrid Empirical‚ÄìXGBoost model maintained R2‚âà 0.986 and cross-validated R2‚âà 0.975 even under domain shift, reflecting robust generalization and data efficiency. Quantitatively, the hybrid approach reduced prediction-error variance by ~35% relative to Random Forest and ~20% relative to standalone XGBoost, while preserving monotonic physical behavior across all input domains. It also produced the most stable residual distribution (standard deviation ‚âà 0.07 in log scale), indicating well-balanced bias‚Äìvariance trade-off. The joint analysis of R2, RMSE, MAE, and CV R2confirms that the hybrid empirical‚ÄìXGBoost model not only fits the training data closely but also generalizes reliably. The small discrepancy between R2(0.986) and CV R2(0.975) indicates minimal overfitting, while low RMSE and MAE values demonstrate precise quantitative agreement with experimental measurements. The strong cross-validated performance of the hybrid model arises from the complementary roles of its empirical and machine learning components. The empirical TSA fatigue equation captures the dominant global degradation trend driven by weight, geometry, and torsional mechanics, providing a physically grounded baseline prediction. XGBoost then focuses exclusively on learning the residual nonlinearities‚Äîsuch as friction-induced hysteresis, strand compaction, and material micro-slippage‚Äîthat the empirical model alone cannot represent. Because the ML component does not need to learn the entire mapping from scratch, but only the local deviations from the physical law, the model exhibits lower variance, improved stability under domain shift, and reduced risk of overfitting compared with standalone ML models. This residual learning structure explains why the hybrid approach achieves a significantly higher cross-validated R2than Random Forest, Gaussian Process Regression, or standalone XGBoost.",
            "5.2. Methodological Interpretation": "From a modeling perspective, the hybrid architecture captures residual nonlinearities, such as the interaction between load and diameter, that traditional Weibull or regression formulations cannot express. By embedding the empirical equation as a deterministic prior, the learning process is constrained within physically plausible limits. Consequently, lifespan predictions remain monotonic (decreasing with increasing load and increasing with diameter and string count) even when extrapolated beyond training ranges. Residual inspection showed that major corrections occurred in two regions: Low-diameter (<1.2 mm) samples where frictional losses caused premature failure and empirical predictions overestimated lifespan.High-weight (>22 kg) conditions where material plasticity accelerated degradation beyond Weibull-based expectations. The residual learner successfully captured these nonlinear behaviors, reducing systematic bias and improving real-world reliability.",
            "5.3. Engineering and Scientific Significance": "Beyond numerical accuracy, the hybrid model offers critical engineering benefits: Interpretability: Predictions can be traced to physically meaningful relationships, enhancing trust in design decisions.Reusability: Requires only minor re-calibration for new materials or geometries, reducing experimental burden.Robustness: Performs reliably even with limited data, making it suitable for early-stage product development and prototype testing.Scalability: Although no real-world IoT deployment was performed, the hybrid model is computationally lightweight (<10 ms inference time) and small in memory footprint, making it suitable for microcontroller-based predictive maintenance systems.",
            "5.4. Comparison with Prior Studies": "The observed advantages align with findings by [4,30,35], who demonstrated that incorporating physical constraints into learning algorithms produces more trustworthy and generalizable predictors. Like these physics-informed neural networks (PINNs), the proposed hybrid regression balances empirical law fidelity with statistical flexibility, but with significantly lower computational complexity and easier interpretability.",
            "5.5. Practical Implications and Future Directions": "For TSA systems, where acquiring large fatigue datasets is time-consuming and costly, the proposed method offers a scalable alternative for lifespan forecasting and predictive maintenance. The GUI developed in this study represents a practical engineering contribution that operationalizes the hybrid model in a deployable form. Unlike prior TSA prediction studies that present models only in analytical or code-based formats, the GUI allows users to obtain real-time lifespan predictions without programming expertise. It includes built-in parameter validation, structured input ranges based on physical constraints, and instant visualization of predicted fatigue trends. The interface enables researchers, engineers, and system designers to perform rapid what-if analysis, explore safe operating regions, and evaluate the impact of weight, geometry, and wire configuration on actuator lifespan. By providing an accessible tool that implements the full hybrid modeling pipeline, this study translates its methodological innovation into a functional platform suitable for laboratory use and early-stage product development. Future extensions will explore probabilistic hybrid frameworks that combine the current empirical‚ÄìXGBoost structure with Gaussian process residuals to yield uncertainty-aware predictions. Integration with real-time sensor streams and edge-computing devices could enable online health monitoring for TSA-based robots and tendon-driven mechanisms. In summary, the proposed physics-guided hybrid architecture demonstrates that the fusion of empirical knowledge and machine learning is a powerful, data-efficient, and interpretable approach for mechanical lifespan prediction. It bridges the long-standing gap between domain theory and data science, establishing a foundation for next-generation AI-driven reliability engineering in robotic actuation systems.",
            "6. Conclusions": "This study presented a comprehensive hybrid modeling framework for predicting the fatigue lifespan of Twisted String Actuators (TSAs) by integrating empirical reliability formulations with modern machine learning (ML) techniques. Using 144 experimental samples across varying weights, diameters, and wire counts, the research systematically compared empirical, data-driven, and hybrid methods to identify an optimal balance between predictive accuracy, interpretability, and generalization. The empirical model, derived from Weibull-based regression, effectively represented deterministic relationships among weight (W), diameter (D), and number of wires (N), reflecting established mechanical reliability principles. However, it was unable to capture nonlinear or stochastic deviations inherent to real-world fatigue phenomena. In contrast, the ML models: Linear Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and Gaussian Process Regression (GPR) learned complex relationships directly from the data, substantially improving prediction fidelity, especially under variable geometric and loading conditions. Among these, XGBoost achieved the best standalone performance but the Hybrid Empirical‚ÄìXGBoost model delivered the most balanced and robust performance overall. By embedding the empirical equation as a physics prior and learning only residual deviations, the hybrid framework preserved monotonic physical trends while capturing nonlinearities caused by frictional losses, micro-slip, and material wear. This synergy between physical knowledge and data adaptability produced predictions that were both accurate and physically interpretable, a crucial property for safety-critical actuator design. The hybrid framework introduced here demonstrates that combining deterministic physical laws with data-driven residual learning provides a scalable solution for low-data or high-cost experimental systems such as TSAs, microactuators, and cable-driven robotic mechanisms. From an engineering perspective, the model offers a low-complexity, high-accuracy predictive tool that can be easily retrained for different actuator geometries and load conditions. The accompanying graphical user interface (GUI) operationalizes this framework, enabling real-time prediction, model comparison, and visualization of fatigue lifespans. This direct translation from computational research to an interactive engineering tool facilitates rapid parametric exploration, supports design optimization, and provides an accessible demonstration of hybrid intelligence principles for experimental validation. From a broader scientific standpoint, this work contributes to the emerging discipline of physics-informed machine learning (PIML) by bridging the gap between deterministic modeling and statistical inference. The results affirm that incorporating empirical constraints within machine learning can yield trustworthy, explainable, and generalizable predictive systems, which is a major step forward in data-efficient mechanical reliability analysis. The hybrid empirical‚ÄìML paradigm presented here offers a blueprint for future intelligent design tools, enabling accurate, interpretable lifespan estimation across multiple engineering domains including robotics, aerospace, and soft-actuator systems. Despite its promising outcomes, several limitations remain that guide future refinement: Dataset Size: The model was trained on 144 experimental samples. Although sufficient for regression analysis, expanding the dataset would improve model generalization and robustness against outliers.Material Uniformity: All specimens were assumed to share identical material properties and surface finish. Real-world variability in coatings, humidity, and friction could influence actual fatigue behavior.Environmental Simplification: Experiments were performed under static laboratory conditions. Effects of temperature, dynamic load variation, and vibration were not modeled but are relevant to field applications.Model Structure Simplification: The hybrid architecture currently uses an additive correction formùê∂hybrid=ùê∂emp+ùëìXGBChybrid=Cemp+fXGB. Future variants could explore multiplicative or embedded formulations to better represent coupled deterministic‚Äìstochastic interactions.Temporal Staticity: The model treats fatigue life prediction as a static regression task. Incorporating cycle-by-cycle degradation monitoring would enable temporal forecasting and continuous health assessment. Recognizing these constraints is essential for safe extrapolation and for advancing hybrid reliability modeling in complex systems. Building on the present findings, several research avenues are proposed as follows: Expanded Dataset Collection: Conduct broader experiments encompassing diverse materials, geometries, and environmental conditions to enhance model robustness and cover additional failure mechanisms.Physics-Informed Neural Networks (PINNs): Extend the hybrid framework using deep-learning architectures with embedded physical constraints to improve scalability and model continuity across parameter domains.Probabilistic and Bayesian Hybrids: Combine residual learning with Gaussian process or Bayesian ensembles to provide uncertainty-aware lifespan predictions critical for safety assurance.Real-Time Predictive Maintenance: Integrate the hybrid model into IoT-enabled monitoring systems for online health estimation, allowing in situ lifespan tracking of TSA-based robots and actuators.Multiphysics Integration: Incorporate additional physical variables such as friction coefficient, strain rate, and temperature into empirical components to enable multiphysics hybrid simulations.Cross-Domain Application: Adapt the methodology to other cable-driven and tendon-based systems, e.g., continuum robots, prosthetics, and exoskeletal joints, where fatigue and wear prediction remain key reliability challenges. In conclusion, this research demonstrates that physics-guided hybrid modeling constitutes a powerful, interpretable, and data-efficient strategy for predicting the lifespan of Twisted String Actuators. By merging empirical understanding with machine learning adaptability, the proposed Hybrid Empirical‚ÄìXGBoost model achieves high predictive accuracy while preserving the physical credibility vital for engineering trust. This framework bridges the long-standing divide between theory-driven and data-driven paradigms, paving the way for a new generation of hybrid intelligent systems in mechanical design and predictive maintenance. Beyond TSAs, the presented approach sets a foundation for future AI-driven reliability engineering, where domain knowledge and data analytics coexist synergistically to enhance real-world performance, safety, and sustainability."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/1424-8220/25/23/7387",
        "scraped_at": "2025-12-05 23:54:53"
    },
    {
        "title": "Empathy by Design: Reframing the Empathy Gap Between AI and Humans in Mental Health Chatbots",
        "authors": "byAlastair HowcroftandHolly Blake",
        "journal": "Information2025,16(12), 1074;https://doi.org/10.3390/info16121074- 4 Dec 2025",
        "abstract": "Artificial intelligence (AI) chatbots are now embedded across therapeutic contexts, from the United Kingdom‚Äôs National Health Service (NHS) Talking Therapies to widely used platforms like ChatGPT. Whether welcomed or not, these systems are increasingly used for both patient care and everyday support, sometimes even replacing human contact. Their capacity to convey empathy strongly influences how people experience and benefit from them. However, current systems often create an ‚ÄúAI empathy gap‚Äù, where interactions feel impersonal and superficial compared to those with human practitioners. This paper, presented as a critical narrative review, cautiously challenges the prevailing narrative that empathy is a uniquely human skill that AI cannot replicate. We argue this belief can stem from an unfair comparison: evaluating generic AIs against an idealised human practitioner. We reframe capabilities seen as exclusively human, such as building bonds through long-term memory and personalisation, not as insurmountable barriers but as concrete design targets. We also discuss the critical architectural and privacy trade-offs between cloud and on-device (edge) solutions. Accordingly, we propose a conceptual framework to meet these targets. It integrates three key technologies: Retrieval-Augmented Generation (RAG) for long-term memory; feedback-driven adaptation for real-time emotional tuning; and lightweight adapter modules for personalised conversational styles. This framework provides a path toward systems that users perceive as genuinely empathic, rather than ones that merely mimic supportive language. While AI cannot experience emotional empathy, it can model cognitive empathy and simulate affective and compassionate responses in coordinated ways at the behavioural level. However, because these systems lack conscious, autonomous ‚Äòhelping‚Äô intentions, these design advancements must be considered alongside careful ethical and regulatory safeguards.Keywords:artificial intelligence;chatbots;mental health;mental health chatbots;empathy;artificial empathy;conversational agents;large language models;digital mental health;affective computing",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "1.1. BackgroundEmpathy‚Äîthe ability to recognise another‚Äôs emotions and respond in a manner sensitive to the context‚Äîis typically divided into three types:cognitive empathy(understanding another‚Äôs perspective),affective empathy(sharing their feelings), andcompassionate empathy(motivation to help) [1]. This capacity is widely recognised for its critical role in enhancing well-being and alleviating psychological distress [2,3,4], particularly in mental health settings [5,6]. Conversely, invalidating or unemphatic communication has been linked to heightened negative affect and poorer outcomes, including harm [7,8]. When this capability is implemented in artificial intelligence (AI) systems, it is often termedartificial or computational empathy[9,10]. While there is ongoing debate, including philosophical and phenomenological views that reserve ‚Äúreal‚Äù empathy for claims about consciousness, inner feeling, or moral depth [11], our focus is on how empathy can be implemented or simulated across these three commonly described components as part of an integrated whole. AI can arguably implement cognitive empathy through perspective modelling, which does not require experiencing emotions, whereas emotional and compassionate empathy are more plausibly seen as simulated, since AI systems lack conscious, autonomous caring intentions or consciously experienced feeling states.As AI chatbots spread across therapeutic contexts, from clinical services to their use in social robots and mobile apps, delivering interactions that are perceived as empathic becomes a central design challenge. For example, Limbic Access, an AI chatbot used to augment clinical intake and assessment in NHS (National Health Service) Talking Therapies has, according to the company, delivered assessments to nearly 400,000 patients across the United Kingdom (UK) [12]. Mental health conditions are often treated (and diagnosed) through language, making them an especially suitable domain for large language models (LLMs), which process unstructured text as both input and output [13]. Recent advances in natural language processing (NLP) and generative AI have enabled the development of systems capable of delivering increasingly human-like conversations and forming connections through personalised dialogue [14], with evidence of improvements in psychological outcomes such as reduced anxiety and stress [15,16]. Beyond these clinical gains, computational empathy can also offer interactional benefits, such as clearer understanding of user intent, allowing systems to better infer what individuals hope to achieve and provide more contextually appropriate support. These advances mark a substantial shift from early rule-based systems to modern chatbots that engage in more coherent, context-aware, and emotionally attuned exchanges.Despite these advancements, a recurring narrative is that human interactions are intrinsically more empathic than AI, with theTopol Review, a UK government report, claiming that ‚Äúempathy and compassion‚Äù are ‚Äúessential human skill[s] that AI cannot replicate‚Äù [17]. Many head-to-head comparisons of perceived empathy currently use generic LLMs such as ChatGPT as the AI arm, not purpose-built therapeutic agents [18,19]. While these models can generate empathic-sounding language, such responses can often feel superficial. Users may detect deeper, subtler signals of empathy during human interactions, signals that go beyond comforting phrases [20]. Reflecting this, Scholich et al. [21] found that therapists conveyed empathy through probing questions and sensitivity to nonverbal cues, whereas chatbots tended to fall back on reassurance and generic advice, leading the authors to conclude that current systems cannot match the empathic depth required in sensitive situations. However, much current debate compares AI to an idealised human practitioner while evaluating AI by its generic implementations, which creates an asymmetry in expectations [22]. Moreover, many capabilities typically considered uniquely human‚Äîsuch as longitudinal rapport, memory for personal details, and sensitivity to non-verbal cues‚Äîcan be engineered through long-term memory architectures, adaptive personalisation, and multimodal affect recognition. We therefore treat these not as categorical barriers but as concrete design targets, provided they are implemented with safeguards to prevent over-attachment, mitigate deception, and protect vulnerable users.This paper therefore reframes the so-called ‚ÄúAI empathy gap‚Äù [23] as a problem of design and evaluation rather than an inherent technological shortfall. In text-based interactions, LLMs have already been rated as more empathic than clinicians in several studies, and they can outperform humans on standard emotional-intelligence tests [24,25]. Meanwhile, automatic emotion recognition from facial, vocal, and textual cues is now at or near human-level performance [26]. Where AI falls short, this typically reflects current design constraints rather than fundamental limitations. 1.2. Scope and Approach of This Critical Narrative ReviewWithin this context, we present a critical narrative review and conceptual framework to guide the design of future systems. A critical narrative review synthesises literature through an explicit interpretive lens, drawing together empirical, conceptual, and theoretical work to produce an integrated, theory-driven account of a field [27]. This approach complements systematic work that quantitatively synthesises evidence on perceived empathy in comparisons between AI chatbots and human healthcare practitioners [24]. Rather than providing an exhaustive systematic synthesis, our goal is to interpret and reframe this literature through a design-oriented lens. In this paper, we adopt this approach by combining our interpretive position with an iterative search of empirical, conceptual, and technical literature on mental health chatbots, computational empathy, and conversational systems. Because narrative reviews involve iterative cycles of searching, analysis, and interpretation rather than exhaustive systematic screening [27], we used purposive web-based searches, citation chaining, and the authors‚Äô expertise to identify relevant literature. Drawing on this approach, the paper proceeds in three steps. First, we review the current state of chatbots to establish what current systems already achieve. Second, we outline the limitations that sustain the perception of an empathy gap, focusing on personalisation, memory, and real-time adaptation. Third, we propose a conceptual framework that integrates retrieval-augmented memory, feedback-driven adaptation, and optional adapter-based style control. We argue that the so-called empathy gap is contingent, not categorical, with differences inperceivedempathy arising from current design and evaluation choices, not from properties that only humans can possess.",
            "1.1. Background": "Empathy‚Äîthe ability to recognise another‚Äôs emotions and respond in a manner sensitive to the context‚Äîis typically divided into three types:cognitive empathy(understanding another‚Äôs perspective),affective empathy(sharing their feelings), andcompassionate empathy(motivation to help) [1]. This capacity is widely recognised for its critical role in enhancing well-being and alleviating psychological distress [2,3,4], particularly in mental health settings [5,6]. Conversely, invalidating or unemphatic communication has been linked to heightened negative affect and poorer outcomes, including harm [7,8]. When this capability is implemented in artificial intelligence (AI) systems, it is often termedartificial or computational empathy[9,10]. While there is ongoing debate, including philosophical and phenomenological views that reserve ‚Äúreal‚Äù empathy for claims about consciousness, inner feeling, or moral depth [11], our focus is on how empathy can be implemented or simulated across these three commonly described components as part of an integrated whole. AI can arguably implement cognitive empathy through perspective modelling, which does not require experiencing emotions, whereas emotional and compassionate empathy are more plausibly seen as simulated, since AI systems lack conscious, autonomous caring intentions or consciously experienced feeling states. As AI chatbots spread across therapeutic contexts, from clinical services to their use in social robots and mobile apps, delivering interactions that are perceived as empathic becomes a central design challenge. For example, Limbic Access, an AI chatbot used to augment clinical intake and assessment in NHS (National Health Service) Talking Therapies has, according to the company, delivered assessments to nearly 400,000 patients across the United Kingdom (UK) [12]. Mental health conditions are often treated (and diagnosed) through language, making them an especially suitable domain for large language models (LLMs), which process unstructured text as both input and output [13]. Recent advances in natural language processing (NLP) and generative AI have enabled the development of systems capable of delivering increasingly human-like conversations and forming connections through personalised dialogue [14], with evidence of improvements in psychological outcomes such as reduced anxiety and stress [15,16]. Beyond these clinical gains, computational empathy can also offer interactional benefits, such as clearer understanding of user intent, allowing systems to better infer what individuals hope to achieve and provide more contextually appropriate support. These advances mark a substantial shift from early rule-based systems to modern chatbots that engage in more coherent, context-aware, and emotionally attuned exchanges. Despite these advancements, a recurring narrative is that human interactions are intrinsically more empathic than AI, with theTopol Review, a UK government report, claiming that ‚Äúempathy and compassion‚Äù are ‚Äúessential human skill[s] that AI cannot replicate‚Äù [17]. Many head-to-head comparisons of perceived empathy currently use generic LLMs such as ChatGPT as the AI arm, not purpose-built therapeutic agents [18,19]. While these models can generate empathic-sounding language, such responses can often feel superficial. Users may detect deeper, subtler signals of empathy during human interactions, signals that go beyond comforting phrases [20]. Reflecting this, Scholich et al. [21] found that therapists conveyed empathy through probing questions and sensitivity to nonverbal cues, whereas chatbots tended to fall back on reassurance and generic advice, leading the authors to conclude that current systems cannot match the empathic depth required in sensitive situations. However, much current debate compares AI to an idealised human practitioner while evaluating AI by its generic implementations, which creates an asymmetry in expectations [22]. Moreover, many capabilities typically considered uniquely human‚Äîsuch as longitudinal rapport, memory for personal details, and sensitivity to non-verbal cues‚Äîcan be engineered through long-term memory architectures, adaptive personalisation, and multimodal affect recognition. We therefore treat these not as categorical barriers but as concrete design targets, provided they are implemented with safeguards to prevent over-attachment, mitigate deception, and protect vulnerable users. This paper therefore reframes the so-called ‚ÄúAI empathy gap‚Äù [23] as a problem of design and evaluation rather than an inherent technological shortfall. In text-based interactions, LLMs have already been rated as more empathic than clinicians in several studies, and they can outperform humans on standard emotional-intelligence tests [24,25]. Meanwhile, automatic emotion recognition from facial, vocal, and textual cues is now at or near human-level performance [26]. Where AI falls short, this typically reflects current design constraints rather than fundamental limitations.",
            "1.2. Scope and Approach of This Critical Narrative Review": "Within this context, we present a critical narrative review and conceptual framework to guide the design of future systems. A critical narrative review synthesises literature through an explicit interpretive lens, drawing together empirical, conceptual, and theoretical work to produce an integrated, theory-driven account of a field [27]. This approach complements systematic work that quantitatively synthesises evidence on perceived empathy in comparisons between AI chatbots and human healthcare practitioners [24]. Rather than providing an exhaustive systematic synthesis, our goal is to interpret and reframe this literature through a design-oriented lens. In this paper, we adopt this approach by combining our interpretive position with an iterative search of empirical, conceptual, and technical literature on mental health chatbots, computational empathy, and conversational systems. Because narrative reviews involve iterative cycles of searching, analysis, and interpretation rather than exhaustive systematic screening [27], we used purposive web-based searches, citation chaining, and the authors‚Äô expertise to identify relevant literature. Drawing on this approach, the paper proceeds in three steps. First, we review the current state of chatbots to establish what current systems already achieve. Second, we outline the limitations that sustain the perception of an empathy gap, focusing on personalisation, memory, and real-time adaptation. Third, we propose a conceptual framework that integrates retrieval-augmented memory, feedback-driven adaptation, and optional adapter-based style control. We argue that the so-called empathy gap is contingent, not categorical, with differences inperceivedempathy arising from current design and evaluation choices, not from properties that only humans can possess.",
            "2. Review of Current Systems and Approaches": "2.1. Early Chatbots and Contemporary DeploymentsEarly mental health chatbots were often rule-based. A well-known example is ELIZA, developed in the 1960s, with the aim of simulating a psychotherapist [28]. It used simple pattern matching to rephrase the user‚Äôs input. Contemporary systems are much more sophisticated, mixing decision-tree style conversation paths with machine learning. Woebot, a widely publicised and clinically evaluated mental health chatbot [29], illustrates this hybrid approach. In practice, the chatbot combined scripted dialogues with limited NLP methods (like keyword spotting) to manage brief check-ins and branched CBT (cognitive behavioural therapy) exercises, which allowed safe and predictable interactions but limited flexibility. Empathic responses came from pre-scripted options rather than being generated afresh, which constrained the system‚Äôs capacity for adaptive, person-specific empathy. The company ultimately retired its direct-to-consumer app on 30 June 2025, citing the cost and uncertainty of United States Food and Drug Administration (FDA) marketing authorisation as it considered shifting toward more sophisticated large-language-model features [30].Wysa illustrates the current state of purpose-built therapeutic chatbots through its dual-product strategy. The Wysa Digital Referral Assistant, deployed in NHS Talking Therapies, supports intake and triage using structured dialogue and simple NLP rather than LLMs. This design makes responses predictable and easier to validate for clinical safety, enabling its classification as a Class I medical device and a conditional recommendation from the National Institute for Health and Care Excellence (NICE) for use during a three-year evidence-generation period [31]. Despite the limitations of a retrieval-based system, a mixed-methods study found that users formed a bond with the Wysa chatbot at levels comparable to those with human therapists [32]. However, limitations were apparent, with users expressing frustration over its frequent misunderstandings, irrelevant responses, and repetitiveness, which we can largely attribute to it not having an LLM‚Äôs ability to generate new content and process unstructured text. The company‚Äôs consumer app, Wysa+, does use LLMs with guardrails, but this version is not NHS-approved [33], likely reflecting the regulatory challenges in validating the safety and predictability of generative models. By contrast, Limbic Access integrates an LLM constrained by a proprietary control layer, allowing it to secure Class IIa certification and be deployed within NHS Talking Therapies. Together, these examples highlight the trade-off between limiting conversational flexibility to satisfy regulatory requirements and using LLMs with safeguards to deliver more adaptive and human-like empathy. 2.2. General-Purpose Large Language ModelsGeneral-purpose LLMs are also playing an increasingly prominent role in mental health support, even though they were not originally designed and validated for this purpose. OpenAI‚Äôs analysis of ChatGPT found that many people use it for comfort and therapeutic support [34], and participants in some studies have even rated ChatGPT as more empathic than clinicians [24].LLMs achieve human-like empathy through training that captures emotional and social patterns in language. They are first pretrained on vast text corpora containing natural examples of emotional expression and social dialogue. This allows them to internalise patterns such as ‚ÄúI‚Äôm sorry to hear‚Ä¶‚Äù even without explicit emotional objectives. In effect, pretraining instils a baseline understanding of human language that includes emotional nuance After pretraining, LLMs undergo supervised fine-tuning on narrower datasets to refine their style and improve performance on specific tasks [35]. For empathy, this often means training on example conversations labelled for compassionate responses. While supervised tuning provides examples, Reinforcement Learning from Human Feedback (RLHF) is used to align the model‚Äôs behaviour to human preferences on empathy and optimise the linguistic patterns of supportive communication learned during pre-training [36]. Through this process, models are systematically optimised to generate responses that human raters prefer, effectively ‚Äòpunishing‚Äô outputs perceived as unhelpful or cold and rewarding those that signal warmth and understanding.Figure 1illustrates this process.Figure 1.Pipeline of training and aligning a language model: from pretraining on large text corpora, to supervised fine-tuning on labelled dialogues, to RLHF for helpful, safe, and empathic behaviour.This process manifests in several key linguistic features. For instance, Small et al. [37] found that ChatGPT-4 tended to reflect users‚Äô own wording back to them, along with more affiliative expressions and positive words, resulting in a 61.5% higher positive polarity score in its replies compared to clinicians (0.21 vs. 0.13). This matters because positively framed messages tend to increase perceived empathy and trustworthiness [38]. The length of LLM replies is also an important factor, as longer responses correlate with higher perceived empathy [24]. In a direct evaluation, ChatGPT averaged 211 words per reply versus 52 for clinicians; this difference in length may help explain why its answers were rated as more empathic, in addition to also being rated as higher quality [39]. Clinicians‚Äô replies, by contrast, can often appear rushed [40] due to short appointment times and competing demands: one study found patients were interrupted after a median of only 11 s [41]. Such brevity may contribute to a broader problem reported by many people with lived experience of mental illness, who describe feeling devalued, dismissed, and dehumanised by health professionals, for example, when spoken to in stigmatising or demeaning ways [42]. LLMs, on the other hand, are trained to mirror supportive language and can feel less judgemental [43] and they are not subject to the same time pressures. However, they can reflect biases from their training data [44], and they are not a substitute for professional care or crisis services. 2.3. Social RobotsMany conversational systems are now embodied in physical robots that are being used to promote social engagement, reduce loneliness, and support emotional well-being [45,46]. Embodiment adds channels such as touch, gesture, gaze, and vocal prosody, but these cues will feel hollow if the dialogue is generic. The framework proposed below is therefore a prerequisite for integrated multimodal empathy in social robots. Addressing empathic conversation first creates a foundation for adding nonverbal behaviours that feel coherent and personal.Table 1synthesises the capabilities of rule-based and generative AI systems against the benchmark of an idealised human practitioner. While we have demonstrated how AI chatbots can already convey empathy in ways comparable to, and sometimes exceeding, human practitioners, important limitations remain, that go beyond linguistic markers.Section 3therefore examines the key technological barriers.Table 1.Comparison of rule-based chatbots, typical generative AI chatbots, and idealised human practitioners across key dimensions relevant to perceived empathy, flexibility, memory, safety, and overall consistency.",
            "2.1. Early Chatbots and Contemporary Deployments": "Early mental health chatbots were often rule-based. A well-known example is ELIZA, developed in the 1960s, with the aim of simulating a psychotherapist [28]. It used simple pattern matching to rephrase the user‚Äôs input. Contemporary systems are much more sophisticated, mixing decision-tree style conversation paths with machine learning. Woebot, a widely publicised and clinically evaluated mental health chatbot [29], illustrates this hybrid approach. In practice, the chatbot combined scripted dialogues with limited NLP methods (like keyword spotting) to manage brief check-ins and branched CBT (cognitive behavioural therapy) exercises, which allowed safe and predictable interactions but limited flexibility. Empathic responses came from pre-scripted options rather than being generated afresh, which constrained the system‚Äôs capacity for adaptive, person-specific empathy. The company ultimately retired its direct-to-consumer app on 30 June 2025, citing the cost and uncertainty of United States Food and Drug Administration (FDA) marketing authorisation as it considered shifting toward more sophisticated large-language-model features [30]. Wysa illustrates the current state of purpose-built therapeutic chatbots through its dual-product strategy. The Wysa Digital Referral Assistant, deployed in NHS Talking Therapies, supports intake and triage using structured dialogue and simple NLP rather than LLMs. This design makes responses predictable and easier to validate for clinical safety, enabling its classification as a Class I medical device and a conditional recommendation from the National Institute for Health and Care Excellence (NICE) for use during a three-year evidence-generation period [31]. Despite the limitations of a retrieval-based system, a mixed-methods study found that users formed a bond with the Wysa chatbot at levels comparable to those with human therapists [32]. However, limitations were apparent, with users expressing frustration over its frequent misunderstandings, irrelevant responses, and repetitiveness, which we can largely attribute to it not having an LLM‚Äôs ability to generate new content and process unstructured text. The company‚Äôs consumer app, Wysa+, does use LLMs with guardrails, but this version is not NHS-approved [33], likely reflecting the regulatory challenges in validating the safety and predictability of generative models. By contrast, Limbic Access integrates an LLM constrained by a proprietary control layer, allowing it to secure Class IIa certification and be deployed within NHS Talking Therapies. Together, these examples highlight the trade-off between limiting conversational flexibility to satisfy regulatory requirements and using LLMs with safeguards to deliver more adaptive and human-like empathy.",
            "2.2. General-Purpose Large Language Models": "General-purpose LLMs are also playing an increasingly prominent role in mental health support, even though they were not originally designed and validated for this purpose. OpenAI‚Äôs analysis of ChatGPT found that many people use it for comfort and therapeutic support [34], and participants in some studies have even rated ChatGPT as more empathic than clinicians [24]. LLMs achieve human-like empathy through training that captures emotional and social patterns in language. They are first pretrained on vast text corpora containing natural examples of emotional expression and social dialogue. This allows them to internalise patterns such as ‚ÄúI‚Äôm sorry to hear‚Ä¶‚Äù even without explicit emotional objectives. In effect, pretraining instils a baseline understanding of human language that includes emotional nuance After pretraining, LLMs undergo supervised fine-tuning on narrower datasets to refine their style and improve performance on specific tasks [35]. For empathy, this often means training on example conversations labelled for compassionate responses. While supervised tuning provides examples, Reinforcement Learning from Human Feedback (RLHF) is used to align the model‚Äôs behaviour to human preferences on empathy and optimise the linguistic patterns of supportive communication learned during pre-training [36]. Through this process, models are systematically optimised to generate responses that human raters prefer, effectively ‚Äòpunishing‚Äô outputs perceived as unhelpful or cold and rewarding those that signal warmth and understanding.Figure 1illustrates this process. Figure 1.Pipeline of training and aligning a language model: from pretraining on large text corpora, to supervised fine-tuning on labelled dialogues, to RLHF for helpful, safe, and empathic behaviour. This process manifests in several key linguistic features. For instance, Small et al. [37] found that ChatGPT-4 tended to reflect users‚Äô own wording back to them, along with more affiliative expressions and positive words, resulting in a 61.5% higher positive polarity score in its replies compared to clinicians (0.21 vs. 0.13). This matters because positively framed messages tend to increase perceived empathy and trustworthiness [38]. The length of LLM replies is also an important factor, as longer responses correlate with higher perceived empathy [24]. In a direct evaluation, ChatGPT averaged 211 words per reply versus 52 for clinicians; this difference in length may help explain why its answers were rated as more empathic, in addition to also being rated as higher quality [39]. Clinicians‚Äô replies, by contrast, can often appear rushed [40] due to short appointment times and competing demands: one study found patients were interrupted after a median of only 11 s [41]. Such brevity may contribute to a broader problem reported by many people with lived experience of mental illness, who describe feeling devalued, dismissed, and dehumanised by health professionals, for example, when spoken to in stigmatising or demeaning ways [42]. LLMs, on the other hand, are trained to mirror supportive language and can feel less judgemental [43] and they are not subject to the same time pressures. However, they can reflect biases from their training data [44], and they are not a substitute for professional care or crisis services.",
            "2.3. Social Robots": "Many conversational systems are now embodied in physical robots that are being used to promote social engagement, reduce loneliness, and support emotional well-being [45,46]. Embodiment adds channels such as touch, gesture, gaze, and vocal prosody, but these cues will feel hollow if the dialogue is generic. The framework proposed below is therefore a prerequisite for integrated multimodal empathy in social robots. Addressing empathic conversation first creates a foundation for adding nonverbal behaviours that feel coherent and personal. Table 1synthesises the capabilities of rule-based and generative AI systems against the benchmark of an idealised human practitioner. While we have demonstrated how AI chatbots can already convey empathy in ways comparable to, and sometimes exceeding, human practitioners, important limitations remain, that go beyond linguistic markers.Section 3therefore examines the key technological barriers. Table 1.Comparison of rule-based chatbots, typical generative AI chatbots, and idealised human practitioners across key dimensions relevant to perceived empathy, flexibility, memory, safety, and overall consistency.",
            "3. Limitations of Current Approaches: Connectivity, Personalisation and Real-Time Adaptation": "3.1. Shallow and Hollow PersonalisationMany chatbots still operate as ‚Äúone-size-fits-all‚Äù systems, with limited capacity to learn from individual users or adapt style to user preferences. Reviews of mental-health chatbots describe a heavy reliance on scripted, rule-based flows, which can lead to generic interactions [47]. Even for advanced LLMs, developers acknowledge the need for more individualisation. Sam Altman, CEO of OpenAI, remarked their goal for ChatGPT is ‚Äúmore per-user customisation of model personality‚Äù, since users vary widely in whether they prefer concise factual replies, more emotionally expressive language, or a warmer conversational tone [48]. This highlights how personalisation remains a key challenge, much as therapists must adjust their approach to the unique needs of each client. Truly adaptive systems (those that remember personal details, adjust communication style, and modify therapeutic strategies based on what works for a specific user) are largely absent. This is an issue because personalisation (tailoring the conversation to an individual‚Äôs needs and context, like referencing past experiences) is highly valued in therapy, as it is known to be a key factor in improving adherence and outcomes [49]. Current systems are apt at using many empathic words and phrases (‚ÄúI‚Äôm sorry to hear that‚Ä¶‚Äù), but there is a difference between ‚Äúempathic-sounding‚Äù text and actually making the userfeelempathically understood. Liu et al. [20] highlight how this results in some users perceiving chatbots as less genuinely empathic than humans. Interestingly, automated analyses of chatbot-based conversations show no large difference in the linguistic markers of empathy between bots and humans, implying chatbots can say the ‚Äúright things‚Äù, yet users can still feel something is missing. This suggests that many current systems rely on superficial cues to signal empathy; they can sprinkle apologetic or sympathetic phrases, but often lack the deeper consistency, insight, or warmth that users subconsciously look for. Achieving genuine-feeling empathy on par with humans requires more than surface-level responses; it requires the chatbot to truly engage with the user‚Äôs experience in a nuanced and personal way, which current systems fail to do. 3.2. Limited Memory and Context LengthMost chatbots still operate within limited context windows. Even though some newer models can process hundreds of thousands of tokens at once (a token is just a small text unit, roughly the size of a short word or part of a word) [50], the window is still finite and tied to a single session. As a result, the AI can ‚Äúforget‚Äù earlier parts of a conversation, especially in prolonged chats or across sessions. Important nuances, such as a deeply personal remark made earlier by the user, can be lost. Without a long-term memory of interactions, the chatbot cannot reliably follow up in a human-like way. Users may find themselves re-explaining things, which is frustrating, breaks the empathic rapport, and weakens the ‚Äúillusion‚Äù of genuineness. Some systems now try to address this. For example, ChatGPT supportssaved memories, where concise facts about a user can persist across chats and be reintroduced into future conversations [51]. This improves continuity but is still not the same as an open-ended, human-like memory. More generally, retrieval-based approaches are used to reinsert important context without overwhelming model capacity, and this retrieve-and-inject pattern is likely to be central to more empathic systems. 3.3. Rigidity in Emotional AdaptationCurrent chatbots often employ a fixed empathy style that does not evolve within conversations or persist across them. Kang et al. [52] found that these systems tend to rely on a narrow set of ‚Äúsafe‚Äù strategies, most often generic reassurance (‚ÄúI‚Äôm sorry you‚Äôre going through this‚Äù), and struggle to transition toward more active forms of help or deeper exploration when the situation requires it. This rigidity contrasts with human supporters, who naturally shift between listening, problem-solving, or even humour based on continuous assessment of the other person‚Äôs needs. This limitation stems from multiple factors. In part, it reflects a design emphasis on safety, causing chatbots to default to risk-averse responses. But as Scholich et al. [21] reinforce, the challenge goes beyond safety constraints; therapists emphasise that effective adaptation depends on reading nonverbal cues‚Äîfacial expressions, tone of voice, or subtle signals like sarcasm‚Äîthat inform moment-to-moment adjustments. When lacking this capability, chatbots can remain locked into generalised response patterns. 3.4. Cloud ArchitecturesMost contemporary mental health chatbots, operate on a cloud-based architecture, requiring an internet connection and the transmission of user data to a central server. This design inherently introduces privacy risks. Reviews of mental health apps consistently found weak data protections amongst apps, which eroded user trust and engagement [53]. This erosion of trust is problematic, since patient trust correlates strongly and positively with perceived clinician empathy [54]. Aside from trust, there are legal and regulatory consequences, e.g., if developers fail to comply with laws like the General Data Protection Regulation (GDPR). Dependence on connectivity also creates fragility. A lost connection can abruptly shatter the illusion of presence, revealing an error message that undermines the agent‚Äôs perceived reliability and empathy. Furthermore, if a chatbot were to use multimodal signals (e.g., facial expressions), effective interaction requires very low latency, yet continuously streaming sensitive data from users‚Äô devices to the cloud is both ethically and technically problematic. These factors make standard cloud-based architectures difficult to reconcile with the demands of empathic mental health support, particularly when multi-modal.",
            "3.1. Shallow and Hollow Personalisation": "Many chatbots still operate as ‚Äúone-size-fits-all‚Äù systems, with limited capacity to learn from individual users or adapt style to user preferences. Reviews of mental-health chatbots describe a heavy reliance on scripted, rule-based flows, which can lead to generic interactions [47]. Even for advanced LLMs, developers acknowledge the need for more individualisation. Sam Altman, CEO of OpenAI, remarked their goal for ChatGPT is ‚Äúmore per-user customisation of model personality‚Äù, since users vary widely in whether they prefer concise factual replies, more emotionally expressive language, or a warmer conversational tone [48]. This highlights how personalisation remains a key challenge, much as therapists must adjust their approach to the unique needs of each client. Truly adaptive systems (those that remember personal details, adjust communication style, and modify therapeutic strategies based on what works for a specific user) are largely absent. This is an issue because personalisation (tailoring the conversation to an individual‚Äôs needs and context, like referencing past experiences) is highly valued in therapy, as it is known to be a key factor in improving adherence and outcomes [49]. Current systems are apt at using many empathic words and phrases (‚ÄúI‚Äôm sorry to hear that‚Ä¶‚Äù), but there is a difference between ‚Äúempathic-sounding‚Äù text and actually making the userfeelempathically understood. Liu et al. [20] highlight how this results in some users perceiving chatbots as less genuinely empathic than humans. Interestingly, automated analyses of chatbot-based conversations show no large difference in the linguistic markers of empathy between bots and humans, implying chatbots can say the ‚Äúright things‚Äù, yet users can still feel something is missing. This suggests that many current systems rely on superficial cues to signal empathy; they can sprinkle apologetic or sympathetic phrases, but often lack the deeper consistency, insight, or warmth that users subconsciously look for. Achieving genuine-feeling empathy on par with humans requires more than surface-level responses; it requires the chatbot to truly engage with the user‚Äôs experience in a nuanced and personal way, which current systems fail to do.",
            "3.2. Limited Memory and Context Length": "Most chatbots still operate within limited context windows. Even though some newer models can process hundreds of thousands of tokens at once (a token is just a small text unit, roughly the size of a short word or part of a word) [50], the window is still finite and tied to a single session. As a result, the AI can ‚Äúforget‚Äù earlier parts of a conversation, especially in prolonged chats or across sessions. Important nuances, such as a deeply personal remark made earlier by the user, can be lost. Without a long-term memory of interactions, the chatbot cannot reliably follow up in a human-like way. Users may find themselves re-explaining things, which is frustrating, breaks the empathic rapport, and weakens the ‚Äúillusion‚Äù of genuineness. Some systems now try to address this. For example, ChatGPT supportssaved memories, where concise facts about a user can persist across chats and be reintroduced into future conversations [51]. This improves continuity but is still not the same as an open-ended, human-like memory. More generally, retrieval-based approaches are used to reinsert important context without overwhelming model capacity, and this retrieve-and-inject pattern is likely to be central to more empathic systems.",
            "3.3. Rigidity in Emotional Adaptation": "Current chatbots often employ a fixed empathy style that does not evolve within conversations or persist across them. Kang et al. [52] found that these systems tend to rely on a narrow set of ‚Äúsafe‚Äù strategies, most often generic reassurance (‚ÄúI‚Äôm sorry you‚Äôre going through this‚Äù), and struggle to transition toward more active forms of help or deeper exploration when the situation requires it. This rigidity contrasts with human supporters, who naturally shift between listening, problem-solving, or even humour based on continuous assessment of the other person‚Äôs needs. This limitation stems from multiple factors. In part, it reflects a design emphasis on safety, causing chatbots to default to risk-averse responses. But as Scholich et al. [21] reinforce, the challenge goes beyond safety constraints; therapists emphasise that effective adaptation depends on reading nonverbal cues‚Äîfacial expressions, tone of voice, or subtle signals like sarcasm‚Äîthat inform moment-to-moment adjustments. When lacking this capability, chatbots can remain locked into generalised response patterns.",
            "3.4. Cloud Architectures": "Most contemporary mental health chatbots, operate on a cloud-based architecture, requiring an internet connection and the transmission of user data to a central server. This design inherently introduces privacy risks. Reviews of mental health apps consistently found weak data protections amongst apps, which eroded user trust and engagement [53]. This erosion of trust is problematic, since patient trust correlates strongly and positively with perceived clinician empathy [54]. Aside from trust, there are legal and regulatory consequences, e.g., if developers fail to comply with laws like the General Data Protection Regulation (GDPR). Dependence on connectivity also creates fragility. A lost connection can abruptly shatter the illusion of presence, revealing an error message that undermines the agent‚Äôs perceived reliability and empathy. Furthermore, if a chatbot were to use multimodal signals (e.g., facial expressions), effective interaction requires very low latency, yet continuously streaming sensitive data from users‚Äô devices to the cloud is both ethically and technically problematic. These factors make standard cloud-based architectures difficult to reconcile with the demands of empathic mental health support, particularly when multi-modal.",
            "4. Future Directions: A Conceptual Framework for Adaptive Empathy": "To bridge the gap between the current state of AI empathy and its potential, we propose a conceptual framework to guide future implementations, built on three capabilities: Personalised Memory, Dynamic Adaptation, and Stylistic Flexibility. This framework directly addresses the identified shortfalls‚Äîshallow personalisation, limited memory, and emotional rigidity‚Äîby treating human-like empathy not as an unattainable trait but as a set of concrete engineering challenges. 4.1. Building a Personal Profile for Each UserContinuity is important for trust and feeling safe [55]. People feel understood when prior concerns are remembered and revisited, which in turn builds trust. We propose that an empathic chatbot should gradually build and update a personal model of each user, similar to how a skilled empathic human practitioner recalls important details about an individual over time. Rather than treating every interaction as isolated, as is typically done, the chatbot could track individual preferences, sensitivities, personalities, and conversation styles, and adjust its communication accordingly based not only on the current input but also by actively drawing on past conversations and stored personal information. In practice, this could involve creating a lightweight ‚Äúprofile‚Äù for each user, where key patterns (such as emotional triggers, preferred tones, and major life events) are stored (with consent) and appropriately referenced during conversations Each time the chatbot generates a reply, it should retrieve relevant information from the user‚Äôs profile to guide its response‚Äîa process known as Retrieval-Augmented Generation (RAG)‚Äîensuring the interaction feels more personal and contextually appropriate. A lightweight retrieval layer can support this process by comparing the user‚Äôs current message to their stored profile information and pulling out the most relevant details based on meaning, ensuring the chatbot‚Äôs replies feel more personal without overwhelming the model‚Äôs active context. To make this work, the system would need a way to connect new messages with past information. This kind of memory is possible with existing technology. For example, a model like MiniLMv2 can turn each message into a compact ‚Äúfingerprint‚Äù of its meaning, while tools such as FAISS or Chroma can act as efficient filing systems that store and search these fingerprints [56,57,58]. Together, they would allow a chatbot to recognise when new messages connect to past ones and bring that context into the reply. In practice, this means the system could remember a user once mentioned loving golden retrievers and later connect that detail when the user says they are thinking of getting a pet, making the interaction feel more personal and continuous. To illustrate this difference in practice,Table 2compares responses to a real-world user query adapted from the Reddit communityr/asktherapists. The comparison highlights how a standard generative model offers generic reassurance, whereas applying the proposed framework‚Äîspecifically the RAG component‚Äîenables the system to perform ‚Äòreality testing‚Äô by retrieving specific evidence of the user‚Äôs past success.Table 2.Comparison of a standard generative AI response versus the proposed Retrieval-Augmented Generation (RAG) framework applied to a workplace anxiety vignette. 4.2. Adjusting Responses Dynamically Based on User FeedbackEmpathy is an interactive process that involves recognising, interpreting, and responding to another‚Äôs verbal and non-verbal cues [59]. A therapist demonstrates empathy by noticing a client‚Äôs change in tone, a hesitation, or a shift in engagement, and adapting their response accordingly. For conversational agents, this could extend to a ‚Äúmatch, then steer‚Äù principle, where the system first mirrors the user‚Äôs current tone to build rapport, then gently nudges the exchange toward a more constructive emotional state (for example calming visible frustration or re-engaging a disengaged user). User outcomes can also be logged over time, for example, noting whether someone responds positively to humour or prefers concise, directive advice, and then used to adjust the chatbot‚Äôs behaviour. While this resembles reinforcement learning (RL), it does not require updating model weights. Instead, behaviour can be adapted dynamically through prompt manipulation guided by user feedback. Simple mechanisms like thumbs-up or thumbs-down can provide basic signals, but these are crude compared to real interaction. A more natural method would be to observe how a user‚Äôs affect changes after each response, noting whether they become more positive or negative following a chatbot reply. Because short-term variations in facial expression or vocal tone can be noisy, signals can be averaged over short time windows (for example, 30 s) to reflect more persistent emotional states rather than brief fluctuations. This smoothed signal, combined with text-based sentiment, could provide a more robust estimate of how effective the chatbot‚Äôs current conversational style is.We can also watch basic engagement cues, for example, gaze alignment with the camera or long pauses, since therapists often notice when someone seems distant. Prior work in multimodal virtual humans has shown that facial, gaze, and vocal features can be tracked during mental health conversations to keep interactions natural [60]. Toolkits like OpenFace 2.0 and on-device Software Development Kits (SDKs) such as those developed by BLUESKEYE AI already support fast, lightweight affect recognition on mobile and desktop devices, processing facial expressions and gaze locally, with some SDKs also incorporating vocal features, without needing cloud servers [61,62]. This demonstrates that real-time multimodal loops can run efficiently on mobile hardware, adding a ‚Äúprivacy-preserving‚Äù edge layer for adapting chatbot responses by keeping sensitive data local. This layer could be augmented with other on-device AI functionalities to create a more comprehensive edge architecture (Figure 2).Figure 2.Real-time facial affect recognition (via BLUESKEYE AI) running locally on a MacBook. The system tracks facial landmarks (left) to calculate Action Units (bottom-left), which are then mapped onto a Valence-Arousal circumplex (right) to classify the user‚Äôs emotional state (currently detecting ‚ÄúSad‚Äù). 4.3. Potential for True Reinforcement Learning with Local ModelsThe adaptive methods described above work by adjusting prompts and retrieving user-specific context, without altering the underlying model. This raises the question of whether ‚Äútrue‚Äù RL, in the machine learning sense, could be applied instead. In principle, if a local model such as LLaMA 3 were used (rather than a closed API model like GPT-5), user feedback could directly fine-tune the model‚Äôs internal weights so that, over time, it learns what users collectively experience as empathic. This kind of group-level training could be valuable for tailoring the chatbot to the needs of a specific target population, since the model would gradually align with what that community finds empathic for them. However, this approach comes with major limitations. First, it requires large amounts of data and secure infrastructure for collecting and transmitting feedback. Second, even with sufficient data, fine-tuning produces a general model that reflects average user preferences rather than personalising responses for individuals. Finally, the cost of fine-tuning even a ‚Äúsmall‚Äù model with billions of parameters makes it impractical to train separate models for each user. For these reasons, individual-level adaptation is likely better achieved by maintaining external user profiles and memory stores, which guide the chatbot‚Äôs responses dynamically without retraining the base model. 4.4. Exploring Adapter-Based PersonalisationAs a practical middle ground between relying solely on external memory with prompt-based control and implementing full RL, a promising approach could be to use lightweight adapter modules on a fixed, fine-tuned base model. These modules, such as those enabled by Low-Rank Adaptation (LoRA), add a small number of trainable parameters without changing the model‚Äôs weight [63]. During operation (a process known as inference), the large base model remains unchanged; instead, specific adapters can be loaded on-demand to steer the conversational style. For example, making it more direct and action-oriented, or more reflective and nurturing, all without the significant computational cost of retraining the full model. While creating a unique adapter for every user does not scale effectively due to management overhead, a more realistic approach is micro-stratification. This involves maintaining a small, curated library of adapters designed for user cohorts that share common preferences or therapeutic needs. Adapter could be combined with retrieval-augmented generation (RAG). In this combined architecture, RAG could govern what is said by injecting relevant, user-specific facts into the context, while adapters shape how it is said by modulating the model‚Äôs stylistic tendencies. In practice, a service could maintain a versioned set of pre-validated adapters; for example, ‚ÄòSupportive‚Äô, ‚ÄòDirective‚Äô, ‚ÄòBrief‚Äô, or ‚ÄòExpansive‚Äô, and then select amongst them based on recent feedback signals or a user‚Äôs declared preferences. This method keeps operational complexity modest, preserves a clean audit trail (since both the base model and the adapters are fixed and versioned), and offers finer stylistic control than prompting alone, while avoiding the substantial data and infrastructure burden of continuous, population-level RL. 4.5. Edge ArchitecturesLooking beyond the framework‚Äôs core capabilities, the choice between cloud and edge computing is important for trust and reliability. In response to the privacy and accessibility issues of cloud-based mental health chatbots, an edge model, where the chatbot runs entirely on the user‚Äôs device, offers a compelling alternative. It works offline and processes conversations locally, so sensitive data does not leave the device, and network latency is removed. When systems rely on many signals, for example, text, gaze, and facial affect, low end-to-end latency is critical for timely adaptation; performing these inferences locally can help by avoiding round-trip delays. However, if the local hardware is underpowered, there can be significant computational latency, making interactions slower despite being on-device. The practical value is therefore mixed, since some users will prioritise privacy while others will notice reduced speed or capability until hardware and model efficiency improve. A primary challenge is the computational demand of modern LLMs, which often exceeds consumer mobile capacity. Despite these hurdles, some applications use a full edge approach, such as Layla, a commercial offline chatbot that downloads a large model file and runs quantized Llama-2 variants on mobile CPUs [64]. The result preserves privacy, but reasoning depth and response speed remain lower than leading cloud systems.Figure 3provides a high-level overview of the conceptual framework.Figure 3.Conceptual Model of Adaptive, Multimodal Chatbot System. 4.6. Challenges and RisksDespite this potential, the move toward genuinely adaptive, empathic AI introduces new and heightened risks that demand careful consideration, especially in mental health contexts, where clinical systems operate under strict regulatory oversight such as NHS governance frameworks. As increasingly capable systems combine long-term memory, adaptive personalisation, and finely tuned empathic language, some interactions may start to feel indistinguishable from those with a caring human practitioner. This increasing realism raises important questions about how clearly users understand that empathy is being provided by an AI system, and whether some individuals might place more trust or emotional weight on these systems than intended. Because our proposed framework aims to support more coherent and contextually attuned empathic behaviour empathy at the behavioural level, safeguards should be built in from the outset to promote transparency, appropriate use, and protection of vulnerable users. Current systems often rely on scripted dialogues precisely to mitigate harm (i.e., they are deterministic); our framework, by design, moves away from this model. These problems include well-documented issues like AI hallucinations [65], and the potential for sycophancy, where a chatbot, in trying to validate the user‚Äôs feelings, may reinforce harmful thoughts or behaviours without appropriate challenge [66,67]. Creating an empathic chatbot increases this risk, as its supportive tone can unintentionally affirm delusions or unsafe choices [68]. While human therapists also make mistakes, errors from AI systems often attract greater scrutiny and may be more severe, underscoring the need for rigorous testing, escalation protocols, and strong governance. These challenges are compounded by technical and ethical hurdles, especially concerning data privacy, user consent, and the substantial computational demands of personalisation.At a broader level, further societal risks emerge. Increasing sophistication raises the danger of user over-reliance, potentially discouraging individuals from seeking professional medical support. The capacity for users to form strong emotional bonds with empathic systems also presents risks of commercial exploitation. If high levels of perceived empathy and supportiveness can be achieved, such capabilities could be repurposed beyond therapeutic contexts‚Äîfor instance, in romantic or companion-oriented applications‚Äîcreating risks of dependency and social withdrawal. Reports from users of theReplikachatbot illustrate these concerns, describing romantic attachment and even grief when updates altered the bot‚Äôs behaviour [69].However, these challenges do not constitute an argument against developing computational empathy but rather highlight the need for clear regulatory and ethical frameworks. Regardless of differing perspectives on its role, AI is going to be increasingly used in patient care‚Äîsometimes even replacing human contact‚Äîand people are already seeking emotional support from such systems [24]. Refusing to design for empathy would therefore be less responsible than doing so transparently and safely. Failing to imbue these systems with empathy could, at best, mean forgoing key benefits associated with human empathy, such as adherence and satisfaction, and at worst, risk causing patient distress through interactions perceived as cold or unsupportive [3].",
            "4.1. Building a Personal Profile for Each User": "Continuity is important for trust and feeling safe [55]. People feel understood when prior concerns are remembered and revisited, which in turn builds trust. We propose that an empathic chatbot should gradually build and update a personal model of each user, similar to how a skilled empathic human practitioner recalls important details about an individual over time. Rather than treating every interaction as isolated, as is typically done, the chatbot could track individual preferences, sensitivities, personalities, and conversation styles, and adjust its communication accordingly based not only on the current input but also by actively drawing on past conversations and stored personal information. In practice, this could involve creating a lightweight ‚Äúprofile‚Äù for each user, where key patterns (such as emotional triggers, preferred tones, and major life events) are stored (with consent) and appropriately referenced during conversations Each time the chatbot generates a reply, it should retrieve relevant information from the user‚Äôs profile to guide its response‚Äîa process known as Retrieval-Augmented Generation (RAG)‚Äîensuring the interaction feels more personal and contextually appropriate. A lightweight retrieval layer can support this process by comparing the user‚Äôs current message to their stored profile information and pulling out the most relevant details based on meaning, ensuring the chatbot‚Äôs replies feel more personal without overwhelming the model‚Äôs active context. To make this work, the system would need a way to connect new messages with past information. This kind of memory is possible with existing technology. For example, a model like MiniLMv2 can turn each message into a compact ‚Äúfingerprint‚Äù of its meaning, while tools such as FAISS or Chroma can act as efficient filing systems that store and search these fingerprints [56,57,58]. Together, they would allow a chatbot to recognise when new messages connect to past ones and bring that context into the reply. In practice, this means the system could remember a user once mentioned loving golden retrievers and later connect that detail when the user says they are thinking of getting a pet, making the interaction feel more personal and continuous. To illustrate this difference in practice,Table 2compares responses to a real-world user query adapted from the Reddit communityr/asktherapists. The comparison highlights how a standard generative model offers generic reassurance, whereas applying the proposed framework‚Äîspecifically the RAG component‚Äîenables the system to perform ‚Äòreality testing‚Äô by retrieving specific evidence of the user‚Äôs past success. Table 2.Comparison of a standard generative AI response versus the proposed Retrieval-Augmented Generation (RAG) framework applied to a workplace anxiety vignette.",
            "4.2. Adjusting Responses Dynamically Based on User Feedback": "Empathy is an interactive process that involves recognising, interpreting, and responding to another‚Äôs verbal and non-verbal cues [59]. A therapist demonstrates empathy by noticing a client‚Äôs change in tone, a hesitation, or a shift in engagement, and adapting their response accordingly. For conversational agents, this could extend to a ‚Äúmatch, then steer‚Äù principle, where the system first mirrors the user‚Äôs current tone to build rapport, then gently nudges the exchange toward a more constructive emotional state (for example calming visible frustration or re-engaging a disengaged user). User outcomes can also be logged over time, for example, noting whether someone responds positively to humour or prefers concise, directive advice, and then used to adjust the chatbot‚Äôs behaviour. While this resembles reinforcement learning (RL), it does not require updating model weights. Instead, behaviour can be adapted dynamically through prompt manipulation guided by user feedback. Simple mechanisms like thumbs-up or thumbs-down can provide basic signals, but these are crude compared to real interaction. A more natural method would be to observe how a user‚Äôs affect changes after each response, noting whether they become more positive or negative following a chatbot reply. Because short-term variations in facial expression or vocal tone can be noisy, signals can be averaged over short time windows (for example, 30 s) to reflect more persistent emotional states rather than brief fluctuations. This smoothed signal, combined with text-based sentiment, could provide a more robust estimate of how effective the chatbot‚Äôs current conversational style is. We can also watch basic engagement cues, for example, gaze alignment with the camera or long pauses, since therapists often notice when someone seems distant. Prior work in multimodal virtual humans has shown that facial, gaze, and vocal features can be tracked during mental health conversations to keep interactions natural [60]. Toolkits like OpenFace 2.0 and on-device Software Development Kits (SDKs) such as those developed by BLUESKEYE AI already support fast, lightweight affect recognition on mobile and desktop devices, processing facial expressions and gaze locally, with some SDKs also incorporating vocal features, without needing cloud servers [61,62]. This demonstrates that real-time multimodal loops can run efficiently on mobile hardware, adding a ‚Äúprivacy-preserving‚Äù edge layer for adapting chatbot responses by keeping sensitive data local. This layer could be augmented with other on-device AI functionalities to create a more comprehensive edge architecture (Figure 2). Figure 2.Real-time facial affect recognition (via BLUESKEYE AI) running locally on a MacBook. The system tracks facial landmarks (left) to calculate Action Units (bottom-left), which are then mapped onto a Valence-Arousal circumplex (right) to classify the user‚Äôs emotional state (currently detecting ‚ÄúSad‚Äù).",
            "4.3. Potential for True Reinforcement Learning with Local Models": "The adaptive methods described above work by adjusting prompts and retrieving user-specific context, without altering the underlying model. This raises the question of whether ‚Äútrue‚Äù RL, in the machine learning sense, could be applied instead. In principle, if a local model such as LLaMA 3 were used (rather than a closed API model like GPT-5), user feedback could directly fine-tune the model‚Äôs internal weights so that, over time, it learns what users collectively experience as empathic. This kind of group-level training could be valuable for tailoring the chatbot to the needs of a specific target population, since the model would gradually align with what that community finds empathic for them. However, this approach comes with major limitations. First, it requires large amounts of data and secure infrastructure for collecting and transmitting feedback. Second, even with sufficient data, fine-tuning produces a general model that reflects average user preferences rather than personalising responses for individuals. Finally, the cost of fine-tuning even a ‚Äúsmall‚Äù model with billions of parameters makes it impractical to train separate models for each user. For these reasons, individual-level adaptation is likely better achieved by maintaining external user profiles and memory stores, which guide the chatbot‚Äôs responses dynamically without retraining the base model.",
            "4.4. Exploring Adapter-Based Personalisation": "As a practical middle ground between relying solely on external memory with prompt-based control and implementing full RL, a promising approach could be to use lightweight adapter modules on a fixed, fine-tuned base model. These modules, such as those enabled by Low-Rank Adaptation (LoRA), add a small number of trainable parameters without changing the model‚Äôs weight [63]. During operation (a process known as inference), the large base model remains unchanged; instead, specific adapters can be loaded on-demand to steer the conversational style. For example, making it more direct and action-oriented, or more reflective and nurturing, all without the significant computational cost of retraining the full model. While creating a unique adapter for every user does not scale effectively due to management overhead, a more realistic approach is micro-stratification. This involves maintaining a small, curated library of adapters designed for user cohorts that share common preferences or therapeutic needs. Adapter could be combined with retrieval-augmented generation (RAG). In this combined architecture, RAG could govern what is said by injecting relevant, user-specific facts into the context, while adapters shape how it is said by modulating the model‚Äôs stylistic tendencies. In practice, a service could maintain a versioned set of pre-validated adapters; for example, ‚ÄòSupportive‚Äô, ‚ÄòDirective‚Äô, ‚ÄòBrief‚Äô, or ‚ÄòExpansive‚Äô, and then select amongst them based on recent feedback signals or a user‚Äôs declared preferences. This method keeps operational complexity modest, preserves a clean audit trail (since both the base model and the adapters are fixed and versioned), and offers finer stylistic control than prompting alone, while avoiding the substantial data and infrastructure burden of continuous, population-level RL.",
            "4.5. Edge Architectures": "Looking beyond the framework‚Äôs core capabilities, the choice between cloud and edge computing is important for trust and reliability. In response to the privacy and accessibility issues of cloud-based mental health chatbots, an edge model, where the chatbot runs entirely on the user‚Äôs device, offers a compelling alternative. It works offline and processes conversations locally, so sensitive data does not leave the device, and network latency is removed. When systems rely on many signals, for example, text, gaze, and facial affect, low end-to-end latency is critical for timely adaptation; performing these inferences locally can help by avoiding round-trip delays. However, if the local hardware is underpowered, there can be significant computational latency, making interactions slower despite being on-device. The practical value is therefore mixed, since some users will prioritise privacy while others will notice reduced speed or capability until hardware and model efficiency improve. A primary challenge is the computational demand of modern LLMs, which often exceeds consumer mobile capacity. Despite these hurdles, some applications use a full edge approach, such as Layla, a commercial offline chatbot that downloads a large model file and runs quantized Llama-2 variants on mobile CPUs [64]. The result preserves privacy, but reasoning depth and response speed remain lower than leading cloud systems. Figure 3provides a high-level overview of the conceptual framework. Figure 3.Conceptual Model of Adaptive, Multimodal Chatbot System.",
            "4.6. Challenges and Risks": "Despite this potential, the move toward genuinely adaptive, empathic AI introduces new and heightened risks that demand careful consideration, especially in mental health contexts, where clinical systems operate under strict regulatory oversight such as NHS governance frameworks. As increasingly capable systems combine long-term memory, adaptive personalisation, and finely tuned empathic language, some interactions may start to feel indistinguishable from those with a caring human practitioner. This increasing realism raises important questions about how clearly users understand that empathy is being provided by an AI system, and whether some individuals might place more trust or emotional weight on these systems than intended. Because our proposed framework aims to support more coherent and contextually attuned empathic behaviour empathy at the behavioural level, safeguards should be built in from the outset to promote transparency, appropriate use, and protection of vulnerable users. Current systems often rely on scripted dialogues precisely to mitigate harm (i.e., they are deterministic); our framework, by design, moves away from this model. These problems include well-documented issues like AI hallucinations [65], and the potential for sycophancy, where a chatbot, in trying to validate the user‚Äôs feelings, may reinforce harmful thoughts or behaviours without appropriate challenge [66,67]. Creating an empathic chatbot increases this risk, as its supportive tone can unintentionally affirm delusions or unsafe choices [68]. While human therapists also make mistakes, errors from AI systems often attract greater scrutiny and may be more severe, underscoring the need for rigorous testing, escalation protocols, and strong governance. These challenges are compounded by technical and ethical hurdles, especially concerning data privacy, user consent, and the substantial computational demands of personalisation. At a broader level, further societal risks emerge. Increasing sophistication raises the danger of user over-reliance, potentially discouraging individuals from seeking professional medical support. The capacity for users to form strong emotional bonds with empathic systems also presents risks of commercial exploitation. If high levels of perceived empathy and supportiveness can be achieved, such capabilities could be repurposed beyond therapeutic contexts‚Äîfor instance, in romantic or companion-oriented applications‚Äîcreating risks of dependency and social withdrawal. Reports from users of theReplikachatbot illustrate these concerns, describing romantic attachment and even grief when updates altered the bot‚Äôs behaviour [69]. However, these challenges do not constitute an argument against developing computational empathy but rather highlight the need for clear regulatory and ethical frameworks. Regardless of differing perspectives on its role, AI is going to be increasingly used in patient care‚Äîsometimes even replacing human contact‚Äîand people are already seeking emotional support from such systems [24]. Refusing to design for empathy would therefore be less responsible than doing so transparently and safely. Failing to imbue these systems with empathy could, at best, mean forgoing key benefits associated with human empathy, such as adherence and satisfaction, and at worst, risk causing patient distress through interactions perceived as cold or unsupportive [3].",
            "5. Discussion": "5.1. Implications for Care DeliveryThis paper has argued that the perceived ‚Äúempathy gap‚Äù in AI is not a fundamental technological limitation but a design challenge that can be addressed through a more sophisticated architectural approach. The proposed framework‚Äîintegrating retrieval-augmented long-term memory, feedback-driven adaptation, and modular style control‚Äîoffers a concrete roadmap for developing AI chatbots that can deliver more personalised, consistent, and emotionally attuned interactions. The implications of such a development, particularly within the current mental health landscape, are significant. Rather than replacing traditional therapy, these systems could serve as complementary tools that offer immediate, empathic support during times when human help may not be available. This is especially relevant given the persistent shortage of mental health professionals and the rising demand for accessible care around the clock [70]. In this environment, an AI tool that offers immediate, empathic support could serve as a vital stopgap, providing continuity and engagement when human help is unavailable. Research shows that more than one-third of individuals diagnosed with mental health conditions in primary care receive no formal treatment [71]. For this significant cohort, AI chatbots designed with our proposed framework could help fill a critical support vacuum, offering a safe and scalable way to engage in empathic conversations. 5.2. Felt Versus Performed EmpathyWhen discussing AI and empathy, it is important to acknowledge the ongoing debate over whether empathy is uniquely human, something AI might imitate convincingly but cannot truly ‚Äúfeel‚Äù [72]. Yet, in clinical practice, human empathy is often a performed competence through learned behaviours rather than genuinely experiencing each patient‚Äôs emotions [73]. While many practitioners do exhibit authentic compassion, the continual emotional demands of empathising with patients can naturally lead to compassion fatigue, which is prevalent across healthcare professions [74,75]. Compassion fatigue, a form of emotional and physical exhaustion arising from prolonged exposure to others‚Äô suffering, can diminish empathic presence and reduce the quality of care. A well-designed AI, built on the principles outlined here, could consistently replicate thebehavioursof empathy without such constraints. From the user‚Äôs perspective, the critical factor is whether they feel understood and supported, irrespective of whether empathy arises from genuine emotion or well-executed, learned behaviours. From this standpoint, AI-driven empathic interactions could closely parallel much of what humans routinely deliver in practice. However, the more effective an AI is at simulating support, the greater the risk of fostering an unhealthy over-dependency that prevents users from seeking professional help. A primary challenge, therefore, is to harness AI‚Äôs consistency without undermining the essential contribution of human clinicians.",
            "5.1. Implications for Care Delivery": "This paper has argued that the perceived ‚Äúempathy gap‚Äù in AI is not a fundamental technological limitation but a design challenge that can be addressed through a more sophisticated architectural approach. The proposed framework‚Äîintegrating retrieval-augmented long-term memory, feedback-driven adaptation, and modular style control‚Äîoffers a concrete roadmap for developing AI chatbots that can deliver more personalised, consistent, and emotionally attuned interactions. The implications of such a development, particularly within the current mental health landscape, are significant. Rather than replacing traditional therapy, these systems could serve as complementary tools that offer immediate, empathic support during times when human help may not be available. This is especially relevant given the persistent shortage of mental health professionals and the rising demand for accessible care around the clock [70]. In this environment, an AI tool that offers immediate, empathic support could serve as a vital stopgap, providing continuity and engagement when human help is unavailable. Research shows that more than one-third of individuals diagnosed with mental health conditions in primary care receive no formal treatment [71]. For this significant cohort, AI chatbots designed with our proposed framework could help fill a critical support vacuum, offering a safe and scalable way to engage in empathic conversations.",
            "5.2. Felt Versus Performed Empathy": "When discussing AI and empathy, it is important to acknowledge the ongoing debate over whether empathy is uniquely human, something AI might imitate convincingly but cannot truly ‚Äúfeel‚Äù [72]. Yet, in clinical practice, human empathy is often a performed competence through learned behaviours rather than genuinely experiencing each patient‚Äôs emotions [73]. While many practitioners do exhibit authentic compassion, the continual emotional demands of empathising with patients can naturally lead to compassion fatigue, which is prevalent across healthcare professions [74,75]. Compassion fatigue, a form of emotional and physical exhaustion arising from prolonged exposure to others‚Äô suffering, can diminish empathic presence and reduce the quality of care. A well-designed AI, built on the principles outlined here, could consistently replicate thebehavioursof empathy without such constraints. From the user‚Äôs perspective, the critical factor is whether they feel understood and supported, irrespective of whether empathy arises from genuine emotion or well-executed, learned behaviours. From this standpoint, AI-driven empathic interactions could closely parallel much of what humans routinely deliver in practice. However, the more effective an AI is at simulating support, the greater the risk of fostering an unhealthy over-dependency that prevents users from seeking professional help. A primary challenge, therefore, is to harness AI‚Äôs consistency without undermining the essential contribution of human clinicians.",
            "6. Conclusions": "In conclusion, the narrative of an inherent and unbridgeable ‚ÄúAI empathy gap‚Äù is a misleading oversimplification. As this paper has argued, the perceived shortfall in empathy between AI and human practitioners is not a fundamental technological limitation but rather a reflection of current design and evaluation constraints. By reframing empathy as a set of observable, replicable behaviours rather than an exclusively human trait, we can approach it as a concrete engineering challenge. Within this scope, our contribution is a critical narrative synthesis and a conceptual framework intended to inform and structure future system implementations. The proposed conceptual framework‚Äîintegrating retrieval-augmented memory for deep personalisation, feedback-driven loops for dynamic adaptation, and adapter-based modules for stylistic control‚Äîprovides a roadmap for developing AI systems that can listen, remember, and adapt in ways that make users feel genuinely understood. Empathic AI chatbots offer great potential to enhance healthcare service delivery, improve access to care, and support personalised treatment plans. However, this path comes with serious technical and ethical challenges. Largely, concerns about authenticity, data privacy, algorithmic bias and accountability‚Äîwhich may undermine trust and the therapeutic relationship [76]. Progress must therefore be careful and responsible, focusing on safety, transparency, and human oversight. Ultimately, the goal is to ensure that effective, empathic support is accessible to everyone, whether delivered by a human or a sufficiently advanced and safe AI. Future research should test each component of the framework, both on its own and in combination, for example, via a factorial study, to measure perceived empathy and benefits (while ensuring safeguards are in place)."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2078-2489/16/12/1074",
        "scraped_at": "2025-12-05 23:55:00"
    },
    {
        "title": "An Improved TOPSIS Method Using Fermatean Fuzzy Sets for Techno-Economic Evaluation of Multi-Type Power Sources",
        "authors": "byLun Ye,Jichuan Li,Shengjie Yang,Lei Jiang,Jing LiaoandBinkun Xu",
        "journal": "Electronics2025,14(23), 4770;https://doi.org/10.3390/electronics14234770- 4 Dec 2025",
        "abstract": "Scientific planning and optimal development of multi-type power sources are critical prerequisites for supporting the robust evolution of emerging power systems. However, existing techno-economic evaluation methods often face challenges such as higher-order uncertainty and weight conflicts, making it difficult to provide reliable support for comparing and selecting power source schemes. To address this, this paper proposes an improved Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) method based on Fermatean Fuzzy Sets (FFS) for techno-economic evaluation of multi-type power sources. First, building on the traditional TOPSIS framework, we introduce Fermatean Fuzzy Sets to construct a FF Hybrid Weighted Distance (FFHWD) measure. This measure simultaneously captures the subjective importance of evaluation indicators and decision-makers‚Äô risk preferences. Second, we design a subjective-objective coupled weighting strategy integrating Fuzzy Analytic Hierarchy Process (FAHP) and Entropy Weight Method (EWM) to achieve dynamic weight balancing, effectively mitigating biases caused by single weighting approaches. Finally, the FFHWD is integrated into the improved TOPSIS framework by defining FF positive and negative ideal solutions. The comprehensive closeness coefficients of each power source scheme are calculated to enable robust ranking and optimal selection of multi-type power source alternatives. Empirical analysis of five representative power generation technologies‚Äîthermal power, hydropower, wind power, photovoltaics (PV), and energy storage‚Äîdemonstrates the following comprehensive techno-economic ranking: hydropower > photovoltaics > thermal power > wind power > energy storage. Hydropower achieves the highest closeness coefficient (‚àí0.4198), whereas energy storage yields the lowest value (‚àí2.8704), effectively illustrating their respective advantages and limitations within the evaluation framework. This research provides scientific decision-making support and methodological references for optimizing multi-type power source configurations and planning new power systems.Keywords:multi-type power sources;techno-economic evaluation;Fermatean fuzzy;TOPSIS;hybrid weighting",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Under the backdrop of accelerating energy structure transformation and building a new power system centered on renewable energy, the scientific evaluation and optimal selection of multi-type power technologies have emerged as a critical priority for ensuring the security, stability, and low-carbon transition of power systems [1,2]. However, as wind and solar photovoltaic energy gain increasing shares in the energy mix, and energy storage systems develop into diverse flexible resources, power planning faces escalating challenges: heightened uncertainties on both supply and demand sides, entangled techno-economic indicators, and ambiguous long-term decision-making information [3,4]. Existing evaluation methods often fail to address high-order uncertainties and conflicting weight allocations, which undermines the reliability and precision of planning outcomes [5,6]. To address these limitations, this study proposes a novel framework to enhance decision-making robustness and rationality in scheme selection. Such advancements will not only support optimized power resource configurations but also advance coordinated planning of multi-type energy systems and the construction of next-generation power grids. In the field of techno-economic evaluation of power generation technologies, researchers worldwide have explored various decision-making approaches to cope with different types of uncertainty. For example, Younis et al. introduced a probabilistic hesitant fuzzy set framework and developed a MARCOS-based multi-criteria decision-making method capable of integrating heterogeneous information for comparing technologies such as wind and solar power [7]. Building on the interplay between subjective knowledge and objective data, Chen et al. combined a modified FAHP with the entropy weight method to construct a hybrid weighting scheme, which they applied to evaluate the impacts of decommissioning power plants on system reliability and economic performance [8]. In another line of research, Li and Zhao proposed an enhanced fuzzy VIKOR approach for performance evaluation of eco-industrial power plants, improving both weight determination and aggregation mechanisms to better support decision-making under fuzzy environments [9]. Although these existing methods have proven effective in specific application scenarios, they often face limitations such as computational intensity, sensitivity to weight assignment, or insufficient ability to represent higher-order uncertainty. In contrast, the TOPSIS method remains attractive due to its transparent logic, efficiency, robustness, and intuitive interpretability. These advantages motivate the present study to adopt the TOPSIS framework while extending it with Fermatean fuzzy sets and a hybrid weighted distance measure, thereby addressing the challenges of uncertainty representation and weight conflict that persist in modern power system techno-economic evaluation. Most existing studies on the techno-economic evaluation of power generation technologies are conducted under deterministic information conditions, lacking effective methods to address the inherent uncertainties in the evaluation process. Because the evaluation involves multidimensional and complex indicators, experts often find it difficult to describe them precisely using exact numerical values. As a result, the overall evaluation information tends to be vague and imprecise. Against this background, the FFS was proposed as an emerging fuzzy theory tool [10]. Compared with traditional Intuitionistic Fuzzy Sets (IFS) and Pythagorean Fuzzy Sets (PFS), FFS not only considers the degrees of membership and non-membership but also introduces a hesitation component. This allows it to represent a broader range of uncertainty information and provides greater adaptability and flexibility when dealing with complex evaluation problems [11,12]. Beyond these contributions, several studies have incorporated Fermatean fuzzy theory into evaluation frameworks to more effectively handle vagueness and hesitation in expert judgments. For instance, Gul et al. developed a Fermatean fuzzy TOPSIS model for industrial risk assessment, demonstrating its capability to quantify latent hazards [13]. Similarly, Yang and colleagues constructed a decision-making framework using a Fermatean fuzzy integrated weighted distance measure within the TOPSIS structure, enabling a more nuanced evaluation of green and low-carbon ports under high uncertainty [14]. Meanwhile, distance serves as an essential tool for measuring differences between data or variables, and it plays a pivotal role in multi-attribute decision-making methods [15,16]. For example, in the TOPSIS method, alternative schemes are ranked by calculating their distances from the ideal solution. Therefore, constructing an effective distance measurement approach has become a topic of significant research interest. Among various distance measures, the Ordered Weighted Distance (OWD) method allows flexible control over the influence of key data points by adjusting positional weights. This property has led to its widespread integration into numerous aggregation tools, giving rise to multiple extended forms. As a further development of OWD, the Hybrid Weighted Distance (HWD) method combines both ordered and arithmetic weighting mechanisms. It simultaneously accounts for the intrinsic importance of data and their positional influence during aggregation, thereby overcoming the limitations of the traditional OWD approach. In this study, the HWD method is extended to the Fermatean fuzzy environment, and a new distance measure‚ÄîFFHWD‚Äîis proposed to enhance the rationality and accuracy of the evaluation process. At the same time, to identify the relative importance of evaluation indicators, a comprehensive weighting model based on the FAHP and the EWM is constructed, integrating both expert subjective judgment and objective data information. Furthermore, the proposed FFHWD measure and the integrated weighting model are incorporated into the TOPSIS framework, forming a novel techno-economic evaluation model for multi-type power sources. This framework strategically integrates the TOPSIS methodology‚Äôs inherent strengths‚Äîincluding structural transparency, computational efficiency, result robustness, and interpretability‚Äîwhile innovatively embedding the FFHWD measure and a hybrid weighting mechanism. This dual integration substantially enhances the framework‚Äôs capacity to manage complex fuzzy information environments and establishes a more rigorous scientific foundation for weight determination through synergistic combination of subjective expert judgment and objective data analytics. An empirical analysis based on representative power sources in a Chinese province is conducted to verify the applicability of the model. The proposed framework provides a more scientific and robust decision-making basis for power project investment and operation management, thereby improving the practical effectiveness of engineering economic evaluation.",
            "2. Evaluation Index System Based on DEMATEL": "2.1. Initial Evaluation Indicator SystemConstructing a techno-economic evaluation index system for multi-type power sources is the primary task of the assessment process, aiming to systematically and comprehensively reveal the core characteristics and attributes of the evaluated objects. The scientific soundness and rationality of the index system directly determine the accuracy and reliability of the evaluation results. Therefore, during its construction, several key principles must be strictly followed [17,18,19,20]. The selected indicators should be both scientific and practical, capable of objectively reflecting the techno-economic characteristics of power sources with clear objectives and broad applicability. They should also exhibit measurability and comparability, meaning that the indicator data can be easily obtained and allow for reliable horizontal and vertical comparisons. At the same time, the system should be comprehensive yet concise, covering all critical aspects of the techno-economic performance of power sources while maintaining a clear structure and concise formulation. Finally, the indicators should possess strong operability and independence to ensure ease of application and data processing, while minimizing semantic overlap or redundancy. Based on these principles, this study establishes the specific evaluation indicators for the techno-economic assessment of multi-type power sources as follows [21,22,23].(1)Unit Electricity Cost. This indicator reflects the total cost incurred for generating one kilowatt-hour of electricity over the entire life cycle of a power source. It is typically measured by the LCOE; for energy storage systems, the cycle-based cost per kilowatt-hour can be used instead. As the core metric for evaluating the economic competitiveness of power sources, the unit electricity cost directly determines their bidding capability in the electricity market and serves as the fundamental benchmark for economic comparison among different power types.(2)Unit Capacity Investment Cost. This indicator represents the initial investment required to construct each kilowatt of installed capacity. It reflects the project‚Äôs financial threshold and investment pressure, thus serving as an important reference for investors and decision-makers when assessing project feasibility.(3)Operation and Maintenance (O&M) Cost Ratio. This refers to the proportion of annual O&M costs to the total annual generation revenue or overall cost. It indicates the operational stability and long-term economic performance of a power source. A high O&M cost ratio often implies a heavy operational burden and greater sensitivity of profitability to fluctuations in fuel prices or market electricity prices.(4)Capacity Factor. Defined as the ratio of actual electricity generation to the theoretical maximum generation under full-load operation, this indicator measures the ‚Äúproductivity‚Äù and utilization level of a power source. It reflects the combined effects of technological reliability and local resource conditions, thereby providing a comprehensive view of the power source‚Äôs operational performance.(5)Start-Up and Regulation Characteristics. This composite performance indicator can be decomposed into two aspects‚Äîstart-up time and ramp rate‚Äîand can also be evaluated systematically using Fermatean fuzzy numbers to quantify flexibility. In power systems with a high penetration of renewable energy, the rapid response and regulation capability of power sources play a crucial role in maintaining system stability and mitigating output fluctuations.(6)Energy Conversion Efficiency. This indicator represents the ratio of output energy to input energy. For power generation units, it corresponds to generation efficiency, while for energy storage systems, it reflects round-trip efficiency. It directly illustrates the technological advancement and energy utilization level of a system, where low efficiency typically indicates higher energy losses and hidden costs.(7)Carbon Emission Intensity. Carbon emission intensity measures the amount of CO2-equivalent emissions produced per kilowatt-hour of electricity generated. Under the dual-carbon (carbon peaking and carbon neutrality) objectives, this indicator has become a key parameter for assessing the environmental friendliness of power sources. It directly affects their social acceptability, environmental costs, and long-term development prospects. 2.2. Dimensionality Reduction of the Evaluation Indicators Using DEMATELThe DEMATEL method analyzes structural relationships within complex evaluation systems by calculating correlations among assessment indicators and classifying these indicators into cause factors and effect factors, thereby determining the importance levels of indicators within the evaluation system [24,25,26]. This study incorporates fuzzy mathematics into the traditional DEMATEL approach to address assessment uncertainty arising from semantic ambiguity. The correspondence between linguistic variables for expert judgments and fuzzy numbers is presented inTable 1.Table 1.Correspondence between linguistic variables and triangular fuzzy numbers.The steps for calculating centrality based on fuzzy DEMATEL are as follows:(1)Construct the initial direct influence matrixùêµÃÉ(ùëò)BÀú(k). According to the correspondence between linguistic variables and fuzzy numbers inTable 1, the initial direct influence matrixùêµÃÉ(ùëò)BÀú(k)provided by thek-th expert is obtained.(2)Calculate the normalized triangular fuzzy numbers(ùëôùë†(ùëò)ùëñùëó,ùëöùë†(ùëò)ùëñùëó,ùë¢ùë†(ùëò)ùëñùëó)(lsij(k),msij(k),usij(k)). Normalize the triangular fuzzy numbers in the initial direct influence matrixùêµÃÉ(ùëò)BÀú(k)based on Equations (1)‚Äì(3).ùëôùë†(ùëò)ùëñùëó=ùëô(ùëò)ùëñùëó‚àíminùëô(ùëò)ùëñùëóŒîmaxminlsij(k)=lij(k)‚àíminlij(k)Œîminmax(1)ùëöùë†(ùëò)ùëñùëó=ùëö(ùëò)ùëñùëó‚àíminùëö(ùëò)ùëñùëóŒîmaxminmsij(k)=mij(k)‚àíminmij(k)Œîminmax(2)ùë¢ùë†(ùëò)ùëñùëó=ùë¢(ùëò)ùëñùëó‚àíminùë¢(ùëò)ùëñùëóŒîmaxminusij(k)=uij(k)‚àíminuij(k)Œîminmax(3)whereŒîmaxmin=maxùë¢(ùëò)ùëñùëó‚àíminùëô(ùëò)ùëñùëóŒîminmax=maxuij(k)‚àíminlij(k)represents the difference between the right-hand value and the left-hand value.(3)Calculate the comprehensive standardized valueùë•(ùëò)ùëñùëóxij(k). First, calculate the left and right standard valuesùêø(ùëò)ùëñùëóLij(k)andùëà(ùëò)ùëñùëóUij(k)according to Equations (4) and (5). Then, calculate the comprehensive standardized valueùë•(ùëò)ùëñùëóxij(k)based on Equation (6).ùêø(ùëò)ùëñùëó=ùëöùë†(ùëò)ùëñùëó1+ùëöùë†(ùëò)ùëñùëó‚àíùëôùë†(ùëò)ùëñùëóLij(k)=msij(k)1+msij(k)‚àílsij(k)(4)ùëà(ùëò)ùëñùëó=ùë¢ùë†(ùëò)ùëñùëó1+ùë¢ùë†(ùëò)ùëñùëó‚àíùëöùë†(ùëò)ùëñùëóUij(k)=usij(k)1+usij(k)‚àímsij(k)(5)ùë•(ùëò)ùëñùëó=ùêø(ùëò)ùëñùëó(1‚àíùêø(ùëò)ùëñùëó)+ùëà(ùëò)ùëñùëóùëà(ùëò)ùëñùëó1‚àíùêø(ùëò)ùëñùëó+ùëà(ùëò)ùëñùëóxij(k)=Lij(k)(1‚àíLij(k))+Uij(k)Uij(k)1‚àíLij(k)+Uij(k)(6)(4)Obtain the quantitative influence valueùëè(ùëò)ùëñùëóbij(k)determined by thek-th expert for factorion factorj.ùëè(ùëò)ùëñùëó=min1‚â§ùëò‚â§ùêæùëô(ùëò)ùëñùëó+ùë•(ùëò)ùëñùëóŒîmaxminbij(k)=min1‚â§k‚â§Klij(k)+xij(k)Œîminmax(7)(5)Calculate the direct influence matrixB.ùëèùëñùëó=(ùëè(1)ùëñùëó+ùëè(2)ùëñùëó+‚ãØ+ùëè(ùëò)ùëñùëó)/ùëòbij=bij(1)+bij(2)+‚ãØ+bij(k)/k(8)ùêµ=(ùëèùëñùëó)ùëõ√óùëõB=(bij)n√ón(9)(6)Calculate the comprehensive influence matrixTby first standardizing the data inBaccording to Equation (10), and then obtaining the comprehensive influence matrixTbased on Equation (11).ùê∫=ùêµ/max1‚â§ùëñ‚â§ùëõ‚àëùëó=1ùëõùëèùëñùëóG=B/max1‚â§i‚â§n‚àëj=1nbij(10)ùëá=ùê∫+ùê∫2+ùê∫3+‚ãØ+ùê∫ùëõT=G+G2+G3+‚ãØ+Gn(11)(7)Calculate the causality degree and centrality. Aggregate the elements inTseparately by rows and columns, and calculate the influence degreeùëöùëñmiand the affected degreeùëíùëñeiof each evaluation indicator according to Equations (12) and (13). The causality degreeùëíùëñeiand centralityùëöùëñmican be calculated according to Equations (14) and (15).ùëìùëñ=‚àëùëó=1ùëõùë°ùëñùëó,ùëñ=1,2,‚Ä¶,ùëõfi=‚àëj=1ntij,i=1,2,‚Ä¶,n(12)ùëíùëñ=‚àëùëñ=1ùëõùë°ùëñùëó,ùëó=1,2,‚Ä¶,ùëõei=‚àëi=1ntij,j=1,2,‚Ä¶,n(13)ùëõùëñ=ùëìùëñ‚àíùëíùëñni=fi‚àíei(14)ùëöùëñ=ùëìùëñ+ùëíùëñmi=fi+ei(15)If the causality degreeùëõùëñ>0ni>0, it can be determined that the indicator is a causal factor; otherwise, the factor is a result factor. The larger the centrality valueùëöùëñmi, the stronger the importance of the indicator in the entire evaluation system.",
            "2.1. Initial Evaluation Indicator System": "Constructing a techno-economic evaluation index system for multi-type power sources is the primary task of the assessment process, aiming to systematically and comprehensively reveal the core characteristics and attributes of the evaluated objects. The scientific soundness and rationality of the index system directly determine the accuracy and reliability of the evaluation results. Therefore, during its construction, several key principles must be strictly followed [17,18,19,20]. The selected indicators should be both scientific and practical, capable of objectively reflecting the techno-economic characteristics of power sources with clear objectives and broad applicability. They should also exhibit measurability and comparability, meaning that the indicator data can be easily obtained and allow for reliable horizontal and vertical comparisons. At the same time, the system should be comprehensive yet concise, covering all critical aspects of the techno-economic performance of power sources while maintaining a clear structure and concise formulation. Finally, the indicators should possess strong operability and independence to ensure ease of application and data processing, while minimizing semantic overlap or redundancy. Based on these principles, this study establishes the specific evaluation indicators for the techno-economic assessment of multi-type power sources as follows [21,22,23]. (1)Unit Electricity Cost. This indicator reflects the total cost incurred for generating one kilowatt-hour of electricity over the entire life cycle of a power source. It is typically measured by the LCOE; for energy storage systems, the cycle-based cost per kilowatt-hour can be used instead. As the core metric for evaluating the economic competitiveness of power sources, the unit electricity cost directly determines their bidding capability in the electricity market and serves as the fundamental benchmark for economic comparison among different power types.(2)Unit Capacity Investment Cost. This indicator represents the initial investment required to construct each kilowatt of installed capacity. It reflects the project‚Äôs financial threshold and investment pressure, thus serving as an important reference for investors and decision-makers when assessing project feasibility.(3)Operation and Maintenance (O&M) Cost Ratio. This refers to the proportion of annual O&M costs to the total annual generation revenue or overall cost. It indicates the operational stability and long-term economic performance of a power source. A high O&M cost ratio often implies a heavy operational burden and greater sensitivity of profitability to fluctuations in fuel prices or market electricity prices.(4)Capacity Factor. Defined as the ratio of actual electricity generation to the theoretical maximum generation under full-load operation, this indicator measures the ‚Äúproductivity‚Äù and utilization level of a power source. It reflects the combined effects of technological reliability and local resource conditions, thereby providing a comprehensive view of the power source‚Äôs operational performance.(5)Start-Up and Regulation Characteristics. This composite performance indicator can be decomposed into two aspects‚Äîstart-up time and ramp rate‚Äîand can also be evaluated systematically using Fermatean fuzzy numbers to quantify flexibility. In power systems with a high penetration of renewable energy, the rapid response and regulation capability of power sources play a crucial role in maintaining system stability and mitigating output fluctuations.(6)Energy Conversion Efficiency. This indicator represents the ratio of output energy to input energy. For power generation units, it corresponds to generation efficiency, while for energy storage systems, it reflects round-trip efficiency. It directly illustrates the technological advancement and energy utilization level of a system, where low efficiency typically indicates higher energy losses and hidden costs.(7)Carbon Emission Intensity. Carbon emission intensity measures the amount of CO2-equivalent emissions produced per kilowatt-hour of electricity generated. Under the dual-carbon (carbon peaking and carbon neutrality) objectives, this indicator has become a key parameter for assessing the environmental friendliness of power sources. It directly affects their social acceptability, environmental costs, and long-term development prospects.",
            "2.2. Dimensionality Reduction of the Evaluation Indicators Using DEMATEL": "The DEMATEL method analyzes structural relationships within complex evaluation systems by calculating correlations among assessment indicators and classifying these indicators into cause factors and effect factors, thereby determining the importance levels of indicators within the evaluation system [24,25,26]. This study incorporates fuzzy mathematics into the traditional DEMATEL approach to address assessment uncertainty arising from semantic ambiguity. The correspondence between linguistic variables for expert judgments and fuzzy numbers is presented inTable 1. Table 1.Correspondence between linguistic variables and triangular fuzzy numbers. The steps for calculating centrality based on fuzzy DEMATEL are as follows: (1)Construct the initial direct influence matrixùêµÃÉ(ùëò)BÀú(k). According to the correspondence between linguistic variables and fuzzy numbers inTable 1, the initial direct influence matrixùêµÃÉ(ùëò)BÀú(k)provided by thek-th expert is obtained.(2)Calculate the normalized triangular fuzzy numbers(ùëôùë†(ùëò)ùëñùëó,ùëöùë†(ùëò)ùëñùëó,ùë¢ùë†(ùëò)ùëñùëó)(lsij(k),msij(k),usij(k)). Normalize the triangular fuzzy numbers in the initial direct influence matrixùêµÃÉ(ùëò)BÀú(k)based on Equations (1)‚Äì(3). ùëôùë†(ùëò)ùëñùëó=ùëô(ùëò)ùëñùëó‚àíminùëô(ùëò)ùëñùëóŒîmaxminlsij(k)=lij(k)‚àíminlij(k)Œîminmax(1)ùëöùë†(ùëò)ùëñùëó=ùëö(ùëò)ùëñùëó‚àíminùëö(ùëò)ùëñùëóŒîmaxminmsij(k)=mij(k)‚àíminmij(k)Œîminmax(2)ùë¢ùë†(ùëò)ùëñùëó=ùë¢(ùëò)ùëñùëó‚àíminùë¢(ùëò)ùëñùëóŒîmaxminusij(k)=uij(k)‚àíminuij(k)Œîminmax(3)whereŒîmaxmin=maxùë¢(ùëò)ùëñùëó‚àíminùëô(ùëò)ùëñùëóŒîminmax=maxuij(k)‚àíminlij(k)represents the difference between the right-hand value and the left-hand value. (3)Calculate the comprehensive standardized valueùë•(ùëò)ùëñùëóxij(k). First, calculate the left and right standard valuesùêø(ùëò)ùëñùëóLij(k)andùëà(ùëò)ùëñùëóUij(k)according to Equations (4) and (5). Then, calculate the comprehensive standardized valueùë•(ùëò)ùëñùëóxij(k)based on Equation (6). ùêø(ùëò)ùëñùëó=ùëöùë†(ùëò)ùëñùëó1+ùëöùë†(ùëò)ùëñùëó‚àíùëôùë†(ùëò)ùëñùëóLij(k)=msij(k)1+msij(k)‚àílsij(k)(4)ùëà(ùëò)ùëñùëó=ùë¢ùë†(ùëò)ùëñùëó1+ùë¢ùë†(ùëò)ùëñùëó‚àíùëöùë†(ùëò)ùëñùëóUij(k)=usij(k)1+usij(k)‚àímsij(k)(5)ùë•(ùëò)ùëñùëó=ùêø(ùëò)ùëñùëó(1‚àíùêø(ùëò)ùëñùëó)+ùëà(ùëò)ùëñùëóùëà(ùëò)ùëñùëó1‚àíùêø(ùëò)ùëñùëó+ùëà(ùëò)ùëñùëóxij(k)=Lij(k)(1‚àíLij(k))+Uij(k)Uij(k)1‚àíLij(k)+Uij(k)(6) (4)Obtain the quantitative influence valueùëè(ùëò)ùëñùëóbij(k)determined by thek-th expert for factorion factorj. ùëè(ùëò)ùëñùëó=min1‚â§ùëò‚â§ùêæùëô(ùëò)ùëñùëó+ùë•(ùëò)ùëñùëóŒîmaxminbij(k)=min1‚â§k‚â§Klij(k)+xij(k)Œîminmax(7) (5)Calculate the direct influence matrixB. ùëèùëñùëó=(ùëè(1)ùëñùëó+ùëè(2)ùëñùëó+‚ãØ+ùëè(ùëò)ùëñùëó)/ùëòbij=bij(1)+bij(2)+‚ãØ+bij(k)/k(8)ùêµ=(ùëèùëñùëó)ùëõ√óùëõB=(bij)n√ón(9) (6)Calculate the comprehensive influence matrixTby first standardizing the data inBaccording to Equation (10), and then obtaining the comprehensive influence matrixTbased on Equation (11). ùê∫=ùêµ/max1‚â§ùëñ‚â§ùëõ‚àëùëó=1ùëõùëèùëñùëóG=B/max1‚â§i‚â§n‚àëj=1nbij(10)ùëá=ùê∫+ùê∫2+ùê∫3+‚ãØ+ùê∫ùëõT=G+G2+G3+‚ãØ+Gn(11) (7)Calculate the causality degree and centrality. Aggregate the elements inTseparately by rows and columns, and calculate the influence degreeùëöùëñmiand the affected degreeùëíùëñeiof each evaluation indicator according to Equations (12) and (13). The causality degreeùëíùëñeiand centralityùëöùëñmican be calculated according to Equations (14) and (15). ùëìùëñ=‚àëùëó=1ùëõùë°ùëñùëó,ùëñ=1,2,‚Ä¶,ùëõfi=‚àëj=1ntij,i=1,2,‚Ä¶,n(12)ùëíùëñ=‚àëùëñ=1ùëõùë°ùëñùëó,ùëó=1,2,‚Ä¶,ùëõei=‚àëi=1ntij,j=1,2,‚Ä¶,n(13)ùëõùëñ=ùëìùëñ‚àíùëíùëñni=fi‚àíei(14)ùëöùëñ=ùëìùëñ+ùëíùëñmi=fi+ei(15) If the causality degreeùëõùëñ>0ni>0, it can be determined that the indicator is a causal factor; otherwise, the factor is a result factor. The larger the centrality valueùëöùëñmi, the stronger the importance of the indicator in the entire evaluation system.",
            "3. Combined Weighting Based on the Improved FAHP-EWM": "3.1. Calculation of Subjective Weights Based on FAHPIn the techno-economic evaluation of multiple power generation technologies, experts‚Äô judgments on the relative importance of indicators often involve fuzziness and uncertainty. To address this, this study employs triangular fuzzy numbers to replace the precise values used in the traditional Analytic Hierarchy Process (AHP), thereby constructing a fuzzy judgment matrix. This approach effectively captures the fuzzy range and confidence level of expert evaluations. By applying the principle of fuzzy number comparison, it determines the ranking and weights of indicators, enhancing both the rationality and fault tolerance of the weighting process, and better reflecting real-world decision-making scenarios [27,28].3.1.1. Construction of the Fuzzy Judgment MatrixExperts employ the triangular fuzzy scale of FAHP to perform pairwise comparisons between each pair of criteria (i,j), assessing the relative importance of criterionioverj[29,30,31]. The evaluation outcomes are then mapped to corresponding triangular fuzzy numbers (lijk,mijk,uijk). For each expertk, fuzzy judgment values for all (i,j) criterion pairs are obtained through the aforementioned method, resulting in the construction of a complete fuzzy judgment matrixAk.ùê¥ùëò=(ùúÇùëñùëóùëò)ùëõ√óùëõ=‚é°‚é£‚é¢‚é¢‚é¢‚é¢(ùëô11ùëò,ùëö11ùëò,ùë¢11ùëò)(ùëô21ùëò,ùëö21ùëò,ùë¢21ùëò)‚ãÆ(ùëôùëõ1ùëò,ùëöùëõ1ùëò,ùë¢ùëõ1ùëò)(ùëô12ùëò,ùëö12ùëò,ùë¢12ùëò)(ùëô22ùëò,ùëö22ùëò,ùë¢22ùëò)‚ãÆ(ùëôùëõ2ùëò,ùëöùëõ2ùëò,ùë¢ùëõ2ùëò)‚Ä¶‚Ä¶‚ãÆ‚Ä¶(ùëô1ùëõùëò,ùëö1ùëõùëò,ùë¢1ùëõùëò)(ùëô2ùëõùëò,ùëö2ùëõùëò,ùë¢2ùëõùëò)‚ãÆ(ùëôùëõùëõùëò,ùëöùëõùëõùëò,ùë¢ùëõùëõùëò)‚é§‚é¶‚é•‚é•‚é•‚é•Ak=(Œ∑ijk)n√ón=(l11k,m11k,u11k)(l21k,m21k,u21k)‚ãÆ(ln1k,mn1k,un1k)(l12k,m12k,u12k)(l22k,m22k,u22k)‚ãÆ(ln2k,mn2k,un2k)‚Ä¶‚Ä¶‚ãÆ‚Ä¶(l1nk,m1nk,u1nk)(l2nk,m2nk,u2nk)‚ãÆ(lnnk,mnnk,unnk)(16)wherenis the number of indicators;lijkanduijkare the left and right judgment boundaries of the evaluation value, respectively, indicating the degree of fuzziness of the judgment. The larger they are, the higher the fuzziness of the judgment.mijkis the median value with a membership degree of 1, representing the most likely importance ratio judgment by experts, which is determined using the 1‚Äì9 scale method similar to the traditional AHP method. Among them,ùúÇùëñùëóùëòŒ∑ijk= (1,1,1), indicating that the indicator is equally important to itself. Moreover, whenùúÇùëñùëóùëòŒ∑ijk= (lijk,mijk,uijk) is given, its inverse judgment must satisfyùúÇùëóùëñùëò=(1/ùë¢ùëñùëóùëò,1/ùëöùëñùëóùëò,1/ùëôùëñùëóùëò)Œ∑jik=1/uijk,1/mijk,1/lijk, thus ensuring that the fuzzy judgment matrix meets the requirements of reciprocity and consistency.3.1.2. Calculation of Weights at This Hierarchical LevelThe fuzzy comprehensive importance of indicatorirelative to the other indicators, denoted asQik, is given as follows:ùëÑùëñùëò=‚éõ‚éù‚éú‚éú‚éú‚àëùëó=1ùëõùúÇùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚äó‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=1ùëõ‚àëùëó=1ùëõùúÇùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚àí1=‚éõ‚éù‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚àëùëó=1ùëõùëôùëñùëóùëò‚àëùëñ=1ùëõ‚àëùëó=1ùëõùë¢ùëñùëóùëò,‚àëùëó=1ùëõùëöùëñùëóùëò‚àëùëñ=1ùëõ‚àëùëó=1ùëõùëöùëñùëóùëò,‚àëùëó=1ùëõùë¢ùëñùëóùëò‚àëùëñ=1ùëõ‚àëùëó=1ùëõùëôùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚àí1Qik=‚àëj=1nŒ∑ijk‚äó‚àëi=1n‚àëj=1nŒ∑ijk‚àí1=‚àëj=1nlijk‚àëi=1n‚àëj=1nuijk,‚àëj=1nmijk‚àëi=1n‚àëj=1nmijk,‚àëj=1nuijk‚àëi=1n‚àëj=1nlijk‚àí1(17)‚àëùëó=1ùëõùúÇùëñùëóùëò=‚éõ‚éù‚éú‚éú‚éú‚àëùëó=1ùëõùëôùëñùëóùëò,‚àëùëó=1ùëõùëöùëñùëóùëò,‚àëùëó=1ùëõùë¢ùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚àëj=1nŒ∑ijk=‚àëj=1nlijk,‚àëj=1nmijk,‚àëj=1nuijk(18)‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=1ùëõ‚àëùëó=1ùëõùúÇùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚àí1=‚éõ‚éù‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú1‚àëùëñ=1ùëõ‚àëùëó=1ùëõùë¢ùëñùëóùëò,1‚àëùëñ=1ùëõ‚àëùëó=1ùëõùëöùëñùëóùëò,1‚àëùëñ=1ùëõ‚àëùëó=1ùëõùëôùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚àí1‚àëi=1n‚àëj=1nŒ∑ijk‚àí1=1‚àëi=1n‚àëj=1nuijk,1‚àëi=1n‚àëj=1nmijk,1‚àëi=1n‚àëj=1nlijk‚àí1(19)where‚àëùëó=1ùëõùúÇùëñùëóùëò‚àëj=1nŒ∑ijkrepresents the summation of triangular fuzzy numbersùúÇùëñùëóùëòŒ∑ijk= (lijk,mijk,uijk), and its summation rule follows the addition principle of triangular fuzzy numbers. When two triangular fuzzy numbers areA= (l1, m1,u1) andB= (l2,m2,u2), their sum isA+B= (l1+l2,m1+m2,u1+u2), because the left and right boundaries and the center value of triangular fuzzy numbers can be linearly superimposed independently, satisfying the closure and additivity of fuzzy number addition.The degree of possibility that the comprehensive importance of indicatori, compared with the other indicators, is greater than that of indicatorj(j= 1,2,‚Ä¶,n; j‚â†i), is denoted asd(xik) and is given as follows:ùëë(ùë•ùëñùëò)=minùëó=1,2,‚ãØ,ùëõ;ùëó‚â†ùëñ[ùëôùëñùëóùëò‚àíùë¢ùëñùëóùëò(ùëöùëñùëóùëò‚àíùë¢ùëñùëóùëò)‚àí(ùëöùëñùëóùëò‚àíùëôùëñùëóùëò),1]d(xik)=minj=1,2,‚ãØ,n;j‚â†ilijk‚àíuijk(mijk‚àíuijk)‚àí(mijk‚àílijk),1(20)The local fuzzy weightWkof each indicator within the indicator set is given as follows:ùëäùëò={ùëë(ùë•1ùëò),ùëë(ùë•2ùëò),‚ãØ,ùëë(ùë•ùëõùëò)}Wk={d(x1k),d(x2k),‚ãØ,d(xnk)}(21)After normalization, the fuzzy weight setWk‚Äô is obtained as follows:ùëä‚Ä≤ùëò={ùëë‚Ä≤(ùë•1ùëò),ùëë‚Ä≤(ùë•2ùëò),‚ãØ,ùëë‚Ä≤(ùë•ùëõùëò)}W‚Ä≤k={d‚Ä≤(x1k),d‚Ä≤(x2k),‚ãØ,d‚Ä≤(xnk)}(22)ùëë‚Ä≤(ùë•ùëñùëò)=ùëë(ùë•ùëñùëò)‚àëùëñ=1ùëõùëë(ùë•ùëñùëò)d‚Ä≤(xik)=d(xik)‚àëi=1nd(xik)(23)Based on the fuzzy weight set, the subjective weightWsis calculated using the weighted arithmetic mean method as follows:ùëäùë†=ùúÜùëòùëä‚Ä≤ùëòWs=ŒªkW‚Ä≤k(24)whereùúÜùëòŒªkrepresents the weight of thek-th expert,‚àëùëò=1ùëáùúÜùëò=1‚àëk=1TŒªk=1. 3.2. Calculation of Objective Weights Based on EWMTo overcome the potential bias introduced by subjective weighting, this study employs the EWM to determine the objective weights of the evaluation indicators. This method automatically calculates the weights based on the degree of variation in the indicator data, thereby fully utilizing the information contained within the data itself. By minimizing human interference, it enhances the objectivity and interpretability of the techno-economic evaluation results for multiple power generation technologies [32].3.2.1. Determination of System EntropyConsidering the safety condition of thej-th evaluation indicator as a system, the information entropy valueejof this indicator can be derived according to the definition of entropy as follows:ùëíùëó=‚àí1lnùëõ‚àëùëñ=1ùëõùë¶ùëñùëólnùë¶ùëñùëóej=‚àí1lnn‚àëi=1nyijlnyij(25)ùë¶ùëñùëó=ùë•‚Ä≤ùëñùëó‚àëùëñ=1ùëõùë•‚Ä≤ùëñùëóyij=x‚Ä≤ij‚àëi=1nx‚Ä≤ij(26)3.2.2. Calculation of WeightsBased on the information entropy value of thej-th indicator, its weightwjwithin this hierarchical level is calculated as follows:ùúîùëó=1‚àíùëíùëó‚àëùëó=1ùëõ(1‚àíùëíùëó)œâj=1‚àíej‚àëj=1n(1‚àíej)(27)The information entropy valueejmay approach 1, in which case even a small change in entropy can lead to an excessively large deviation in the calculated weight. To address this issue, this study improves the weight calculation method. The modified weightùúî‚Ä≤ùëóœâ‚Ä≤jis computed as follows:ùúî‚Ä≤ùëó=‚àëùëñ=1ùëõùëíùëñùëõ+1‚àíùëíùëó‚àëùëó=1ùëõ‚éõ‚éù‚éú‚éú‚àëùëñ=1ùëõùëíùëñùëõ+1‚àíùëíùëó‚éû‚é†‚éü‚éüœâ‚Ä≤j=‚àëi=1nein+1‚àíej‚àëj=1n‚àëi=1nein+1‚àíej(28)The subjective weightsWs= (Ws1,Ws2, ‚Ä¶,Wsn) and the objective weightsWo= (Wo1,Wo2, ‚Ä¶,Won) are obtained using the FAHP and EWMs, respectively. The calculation formula for the comprehensive weights is as follows:ùúîùëó=ùõøùëäùë†+(1‚àíùõø)ùëäùëúœâj=Œ¥Ws+(1‚àíŒ¥)Wo(29)whereùõø‚àà[0,1]Œ¥‚àà0,1.",
            "3.1. Calculation of Subjective Weights Based on FAHP": "In the techno-economic evaluation of multiple power generation technologies, experts‚Äô judgments on the relative importance of indicators often involve fuzziness and uncertainty. To address this, this study employs triangular fuzzy numbers to replace the precise values used in the traditional Analytic Hierarchy Process (AHP), thereby constructing a fuzzy judgment matrix. This approach effectively captures the fuzzy range and confidence level of expert evaluations. By applying the principle of fuzzy number comparison, it determines the ranking and weights of indicators, enhancing both the rationality and fault tolerance of the weighting process, and better reflecting real-world decision-making scenarios [27,28]. 3.1.1. Construction of the Fuzzy Judgment MatrixExperts employ the triangular fuzzy scale of FAHP to perform pairwise comparisons between each pair of criteria (i,j), assessing the relative importance of criterionioverj[29,30,31]. The evaluation outcomes are then mapped to corresponding triangular fuzzy numbers (lijk,mijk,uijk). For each expertk, fuzzy judgment values for all (i,j) criterion pairs are obtained through the aforementioned method, resulting in the construction of a complete fuzzy judgment matrixAk.ùê¥ùëò=(ùúÇùëñùëóùëò)ùëõ√óùëõ=‚é°‚é£‚é¢‚é¢‚é¢‚é¢(ùëô11ùëò,ùëö11ùëò,ùë¢11ùëò)(ùëô21ùëò,ùëö21ùëò,ùë¢21ùëò)‚ãÆ(ùëôùëõ1ùëò,ùëöùëõ1ùëò,ùë¢ùëõ1ùëò)(ùëô12ùëò,ùëö12ùëò,ùë¢12ùëò)(ùëô22ùëò,ùëö22ùëò,ùë¢22ùëò)‚ãÆ(ùëôùëõ2ùëò,ùëöùëõ2ùëò,ùë¢ùëõ2ùëò)‚Ä¶‚Ä¶‚ãÆ‚Ä¶(ùëô1ùëõùëò,ùëö1ùëõùëò,ùë¢1ùëõùëò)(ùëô2ùëõùëò,ùëö2ùëõùëò,ùë¢2ùëõùëò)‚ãÆ(ùëôùëõùëõùëò,ùëöùëõùëõùëò,ùë¢ùëõùëõùëò)‚é§‚é¶‚é•‚é•‚é•‚é•Ak=(Œ∑ijk)n√ón=(l11k,m11k,u11k)(l21k,m21k,u21k)‚ãÆ(ln1k,mn1k,un1k)(l12k,m12k,u12k)(l22k,m22k,u22k)‚ãÆ(ln2k,mn2k,un2k)‚Ä¶‚Ä¶‚ãÆ‚Ä¶(l1nk,m1nk,u1nk)(l2nk,m2nk,u2nk)‚ãÆ(lnnk,mnnk,unnk)(16)wherenis the number of indicators;lijkanduijkare the left and right judgment boundaries of the evaluation value, respectively, indicating the degree of fuzziness of the judgment. The larger they are, the higher the fuzziness of the judgment.mijkis the median value with a membership degree of 1, representing the most likely importance ratio judgment by experts, which is determined using the 1‚Äì9 scale method similar to the traditional AHP method. Among them,ùúÇùëñùëóùëòŒ∑ijk= (1,1,1), indicating that the indicator is equally important to itself. Moreover, whenùúÇùëñùëóùëòŒ∑ijk= (lijk,mijk,uijk) is given, its inverse judgment must satisfyùúÇùëóùëñùëò=(1/ùë¢ùëñùëóùëò,1/ùëöùëñùëóùëò,1/ùëôùëñùëóùëò)Œ∑jik=1/uijk,1/mijk,1/lijk, thus ensuring that the fuzzy judgment matrix meets the requirements of reciprocity and consistency. 3.1.2. Calculation of Weights at This Hierarchical LevelThe fuzzy comprehensive importance of indicatorirelative to the other indicators, denoted asQik, is given as follows:ùëÑùëñùëò=‚éõ‚éù‚éú‚éú‚éú‚àëùëó=1ùëõùúÇùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚äó‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=1ùëõ‚àëùëó=1ùëõùúÇùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚àí1=‚éõ‚éù‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚àëùëó=1ùëõùëôùëñùëóùëò‚àëùëñ=1ùëõ‚àëùëó=1ùëõùë¢ùëñùëóùëò,‚àëùëó=1ùëõùëöùëñùëóùëò‚àëùëñ=1ùëõ‚àëùëó=1ùëõùëöùëñùëóùëò,‚àëùëó=1ùëõùë¢ùëñùëóùëò‚àëùëñ=1ùëõ‚àëùëó=1ùëõùëôùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚àí1Qik=‚àëj=1nŒ∑ijk‚äó‚àëi=1n‚àëj=1nŒ∑ijk‚àí1=‚àëj=1nlijk‚àëi=1n‚àëj=1nuijk,‚àëj=1nmijk‚àëi=1n‚àëj=1nmijk,‚àëj=1nuijk‚àëi=1n‚àëj=1nlijk‚àí1(17)‚àëùëó=1ùëõùúÇùëñùëóùëò=‚éõ‚éù‚éú‚éú‚éú‚àëùëó=1ùëõùëôùëñùëóùëò,‚àëùëó=1ùëõùëöùëñùëóùëò,‚àëùëó=1ùëõùë¢ùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚àëj=1nŒ∑ijk=‚àëj=1nlijk,‚àëj=1nmijk,‚àëj=1nuijk(18)‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=1ùëõ‚àëùëó=1ùëõùúÇùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚àí1=‚éõ‚éù‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú1‚àëùëñ=1ùëõ‚àëùëó=1ùëõùë¢ùëñùëóùëò,1‚àëùëñ=1ùëõ‚àëùëó=1ùëõùëöùëñùëóùëò,1‚àëùëñ=1ùëõ‚àëùëó=1ùëõùëôùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚àí1‚àëi=1n‚àëj=1nŒ∑ijk‚àí1=1‚àëi=1n‚àëj=1nuijk,1‚àëi=1n‚àëj=1nmijk,1‚àëi=1n‚àëj=1nlijk‚àí1(19)where‚àëùëó=1ùëõùúÇùëñùëóùëò‚àëj=1nŒ∑ijkrepresents the summation of triangular fuzzy numbersùúÇùëñùëóùëòŒ∑ijk= (lijk,mijk,uijk), and its summation rule follows the addition principle of triangular fuzzy numbers. When two triangular fuzzy numbers areA= (l1, m1,u1) andB= (l2,m2,u2), their sum isA+B= (l1+l2,m1+m2,u1+u2), because the left and right boundaries and the center value of triangular fuzzy numbers can be linearly superimposed independently, satisfying the closure and additivity of fuzzy number addition.The degree of possibility that the comprehensive importance of indicatori, compared with the other indicators, is greater than that of indicatorj(j= 1,2,‚Ä¶,n; j‚â†i), is denoted asd(xik) and is given as follows:ùëë(ùë•ùëñùëò)=minùëó=1,2,‚ãØ,ùëõ;ùëó‚â†ùëñ[ùëôùëñùëóùëò‚àíùë¢ùëñùëóùëò(ùëöùëñùëóùëò‚àíùë¢ùëñùëóùëò)‚àí(ùëöùëñùëóùëò‚àíùëôùëñùëóùëò),1]d(xik)=minj=1,2,‚ãØ,n;j‚â†ilijk‚àíuijk(mijk‚àíuijk)‚àí(mijk‚àílijk),1(20)The local fuzzy weightWkof each indicator within the indicator set is given as follows:ùëäùëò={ùëë(ùë•1ùëò),ùëë(ùë•2ùëò),‚ãØ,ùëë(ùë•ùëõùëò)}Wk={d(x1k),d(x2k),‚ãØ,d(xnk)}(21)After normalization, the fuzzy weight setWk‚Äô is obtained as follows:ùëä‚Ä≤ùëò={ùëë‚Ä≤(ùë•1ùëò),ùëë‚Ä≤(ùë•2ùëò),‚ãØ,ùëë‚Ä≤(ùë•ùëõùëò)}W‚Ä≤k={d‚Ä≤(x1k),d‚Ä≤(x2k),‚ãØ,d‚Ä≤(xnk)}(22)ùëë‚Ä≤(ùë•ùëñùëò)=ùëë(ùë•ùëñùëò)‚àëùëñ=1ùëõùëë(ùë•ùëñùëò)d‚Ä≤(xik)=d(xik)‚àëi=1nd(xik)(23)Based on the fuzzy weight set, the subjective weightWsis calculated using the weighted arithmetic mean method as follows:ùëäùë†=ùúÜùëòùëä‚Ä≤ùëòWs=ŒªkW‚Ä≤k(24)whereùúÜùëòŒªkrepresents the weight of thek-th expert,‚àëùëò=1ùëáùúÜùëò=1‚àëk=1TŒªk=1.",
            "3.1.1. Construction of the Fuzzy Judgment Matrix": "Experts employ the triangular fuzzy scale of FAHP to perform pairwise comparisons between each pair of criteria (i,j), assessing the relative importance of criterionioverj[29,30,31]. The evaluation outcomes are then mapped to corresponding triangular fuzzy numbers (lijk,mijk,uijk). For each expertk, fuzzy judgment values for all (i,j) criterion pairs are obtained through the aforementioned method, resulting in the construction of a complete fuzzy judgment matrixAk.ùê¥ùëò=(ùúÇùëñùëóùëò)ùëõ√óùëõ=‚é°‚é£‚é¢‚é¢‚é¢‚é¢(ùëô11ùëò,ùëö11ùëò,ùë¢11ùëò)(ùëô21ùëò,ùëö21ùëò,ùë¢21ùëò)‚ãÆ(ùëôùëõ1ùëò,ùëöùëõ1ùëò,ùë¢ùëõ1ùëò)(ùëô12ùëò,ùëö12ùëò,ùë¢12ùëò)(ùëô22ùëò,ùëö22ùëò,ùë¢22ùëò)‚ãÆ(ùëôùëõ2ùëò,ùëöùëõ2ùëò,ùë¢ùëõ2ùëò)‚Ä¶‚Ä¶‚ãÆ‚Ä¶(ùëô1ùëõùëò,ùëö1ùëõùëò,ùë¢1ùëõùëò)(ùëô2ùëõùëò,ùëö2ùëõùëò,ùë¢2ùëõùëò)‚ãÆ(ùëôùëõùëõùëò,ùëöùëõùëõùëò,ùë¢ùëõùëõùëò)‚é§‚é¶‚é•‚é•‚é•‚é•Ak=(Œ∑ijk)n√ón=(l11k,m11k,u11k)(l21k,m21k,u21k)‚ãÆ(ln1k,mn1k,un1k)(l12k,m12k,u12k)(l22k,m22k,u22k)‚ãÆ(ln2k,mn2k,un2k)‚Ä¶‚Ä¶‚ãÆ‚Ä¶(l1nk,m1nk,u1nk)(l2nk,m2nk,u2nk)‚ãÆ(lnnk,mnnk,unnk)(16)wherenis the number of indicators;lijkanduijkare the left and right judgment boundaries of the evaluation value, respectively, indicating the degree of fuzziness of the judgment. The larger they are, the higher the fuzziness of the judgment.mijkis the median value with a membership degree of 1, representing the most likely importance ratio judgment by experts, which is determined using the 1‚Äì9 scale method similar to the traditional AHP method. Among them,ùúÇùëñùëóùëòŒ∑ijk= (1,1,1), indicating that the indicator is equally important to itself. Moreover, whenùúÇùëñùëóùëòŒ∑ijk= (lijk,mijk,uijk) is given, its inverse judgment must satisfyùúÇùëóùëñùëò=(1/ùë¢ùëñùëóùëò,1/ùëöùëñùëóùëò,1/ùëôùëñùëóùëò)Œ∑jik=1/uijk,1/mijk,1/lijk, thus ensuring that the fuzzy judgment matrix meets the requirements of reciprocity and consistency.",
            "3.1.2. Calculation of Weights at This Hierarchical Level": "The fuzzy comprehensive importance of indicatorirelative to the other indicators, denoted asQik, is given as follows:ùëÑùëñùëò=‚éõ‚éù‚éú‚éú‚éú‚àëùëó=1ùëõùúÇùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚äó‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=1ùëõ‚àëùëó=1ùëõùúÇùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚àí1=‚éõ‚éù‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚àëùëó=1ùëõùëôùëñùëóùëò‚àëùëñ=1ùëõ‚àëùëó=1ùëõùë¢ùëñùëóùëò,‚àëùëó=1ùëõùëöùëñùëóùëò‚àëùëñ=1ùëõ‚àëùëó=1ùëõùëöùëñùëóùëò,‚àëùëó=1ùëõùë¢ùëñùëóùëò‚àëùëñ=1ùëõ‚àëùëó=1ùëõùëôùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚àí1Qik=‚àëj=1nŒ∑ijk‚äó‚àëi=1n‚àëj=1nŒ∑ijk‚àí1=‚àëj=1nlijk‚àëi=1n‚àëj=1nuijk,‚àëj=1nmijk‚àëi=1n‚àëj=1nmijk,‚àëj=1nuijk‚àëi=1n‚àëj=1nlijk‚àí1(17)‚àëùëó=1ùëõùúÇùëñùëóùëò=‚éõ‚éù‚éú‚éú‚éú‚àëùëó=1ùëõùëôùëñùëóùëò,‚àëùëó=1ùëõùëöùëñùëóùëò,‚àëùëó=1ùëõùë¢ùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚àëj=1nŒ∑ijk=‚àëj=1nlijk,‚àëj=1nmijk,‚àëj=1nuijk(18)‚éõ‚éù‚éú‚éú‚éú‚àëùëñ=1ùëõ‚àëùëó=1ùëõùúÇùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚àí1=‚éõ‚éù‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú1‚àëùëñ=1ùëõ‚àëùëó=1ùëõùë¢ùëñùëóùëò,1‚àëùëñ=1ùëõ‚àëùëó=1ùëõùëöùëñùëóùëò,1‚àëùëñ=1ùëõ‚àëùëó=1ùëõùëôùëñùëóùëò‚éû‚é†‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚àí1‚àëi=1n‚àëj=1nŒ∑ijk‚àí1=1‚àëi=1n‚àëj=1nuijk,1‚àëi=1n‚àëj=1nmijk,1‚àëi=1n‚àëj=1nlijk‚àí1(19)where‚àëùëó=1ùëõùúÇùëñùëóùëò‚àëj=1nŒ∑ijkrepresents the summation of triangular fuzzy numbersùúÇùëñùëóùëòŒ∑ijk= (lijk,mijk,uijk), and its summation rule follows the addition principle of triangular fuzzy numbers. When two triangular fuzzy numbers areA= (l1, m1,u1) andB= (l2,m2,u2), their sum isA+B= (l1+l2,m1+m2,u1+u2), because the left and right boundaries and the center value of triangular fuzzy numbers can be linearly superimposed independently, satisfying the closure and additivity of fuzzy number addition. The degree of possibility that the comprehensive importance of indicatori, compared with the other indicators, is greater than that of indicatorj(j= 1,2,‚Ä¶,n; j‚â†i), is denoted asd(xik) and is given as follows:ùëë(ùë•ùëñùëò)=minùëó=1,2,‚ãØ,ùëõ;ùëó‚â†ùëñ[ùëôùëñùëóùëò‚àíùë¢ùëñùëóùëò(ùëöùëñùëóùëò‚àíùë¢ùëñùëóùëò)‚àí(ùëöùëñùëóùëò‚àíùëôùëñùëóùëò),1]d(xik)=minj=1,2,‚ãØ,n;j‚â†ilijk‚àíuijk(mijk‚àíuijk)‚àí(mijk‚àílijk),1(20) The local fuzzy weightWkof each indicator within the indicator set is given as follows:ùëäùëò={ùëë(ùë•1ùëò),ùëë(ùë•2ùëò),‚ãØ,ùëë(ùë•ùëõùëò)}Wk={d(x1k),d(x2k),‚ãØ,d(xnk)}(21) After normalization, the fuzzy weight setWk‚Äô is obtained as follows:ùëä‚Ä≤ùëò={ùëë‚Ä≤(ùë•1ùëò),ùëë‚Ä≤(ùë•2ùëò),‚ãØ,ùëë‚Ä≤(ùë•ùëõùëò)}W‚Ä≤k={d‚Ä≤(x1k),d‚Ä≤(x2k),‚ãØ,d‚Ä≤(xnk)}(22)ùëë‚Ä≤(ùë•ùëñùëò)=ùëë(ùë•ùëñùëò)‚àëùëñ=1ùëõùëë(ùë•ùëñùëò)d‚Ä≤(xik)=d(xik)‚àëi=1nd(xik)(23) Based on the fuzzy weight set, the subjective weightWsis calculated using the weighted arithmetic mean method as follows:ùëäùë†=ùúÜùëòùëä‚Ä≤ùëòWs=ŒªkW‚Ä≤k(24)whereùúÜùëòŒªkrepresents the weight of thek-th expert,‚àëùëò=1ùëáùúÜùëò=1‚àëk=1TŒªk=1.",
            "3.2. Calculation of Objective Weights Based on EWM": "To overcome the potential bias introduced by subjective weighting, this study employs the EWM to determine the objective weights of the evaluation indicators. This method automatically calculates the weights based on the degree of variation in the indicator data, thereby fully utilizing the information contained within the data itself. By minimizing human interference, it enhances the objectivity and interpretability of the techno-economic evaluation results for multiple power generation technologies [32]. 3.2.1. Determination of System EntropyConsidering the safety condition of thej-th evaluation indicator as a system, the information entropy valueejof this indicator can be derived according to the definition of entropy as follows:ùëíùëó=‚àí1lnùëõ‚àëùëñ=1ùëõùë¶ùëñùëólnùë¶ùëñùëóej=‚àí1lnn‚àëi=1nyijlnyij(25)ùë¶ùëñùëó=ùë•‚Ä≤ùëñùëó‚àëùëñ=1ùëõùë•‚Ä≤ùëñùëóyij=x‚Ä≤ij‚àëi=1nx‚Ä≤ij(26) 3.2.2. Calculation of WeightsBased on the information entropy value of thej-th indicator, its weightwjwithin this hierarchical level is calculated as follows:ùúîùëó=1‚àíùëíùëó‚àëùëó=1ùëõ(1‚àíùëíùëó)œâj=1‚àíej‚àëj=1n(1‚àíej)(27)The information entropy valueejmay approach 1, in which case even a small change in entropy can lead to an excessively large deviation in the calculated weight. To address this issue, this study improves the weight calculation method. The modified weightùúî‚Ä≤ùëóœâ‚Ä≤jis computed as follows:ùúî‚Ä≤ùëó=‚àëùëñ=1ùëõùëíùëñùëõ+1‚àíùëíùëó‚àëùëó=1ùëõ‚éõ‚éù‚éú‚éú‚àëùëñ=1ùëõùëíùëñùëõ+1‚àíùëíùëó‚éû‚é†‚éü‚éüœâ‚Ä≤j=‚àëi=1nein+1‚àíej‚àëj=1n‚àëi=1nein+1‚àíej(28)The subjective weightsWs= (Ws1,Ws2, ‚Ä¶,Wsn) and the objective weightsWo= (Wo1,Wo2, ‚Ä¶,Won) are obtained using the FAHP and EWMs, respectively. The calculation formula for the comprehensive weights is as follows:ùúîùëó=ùõøùëäùë†+(1‚àíùõø)ùëäùëúœâj=Œ¥Ws+(1‚àíŒ¥)Wo(29)whereùõø‚àà[0,1]Œ¥‚àà0,1.",
            "3.2.1. Determination of System Entropy": "Considering the safety condition of thej-th evaluation indicator as a system, the information entropy valueejof this indicator can be derived according to the definition of entropy as follows:ùëíùëó=‚àí1lnùëõ‚àëùëñ=1ùëõùë¶ùëñùëólnùë¶ùëñùëóej=‚àí1lnn‚àëi=1nyijlnyij(25)ùë¶ùëñùëó=ùë•‚Ä≤ùëñùëó‚àëùëñ=1ùëõùë•‚Ä≤ùëñùëóyij=x‚Ä≤ij‚àëi=1nx‚Ä≤ij(26)",
            "3.2.2. Calculation of Weights": "Based on the information entropy value of thej-th indicator, its weightwjwithin this hierarchical level is calculated as follows:ùúîùëó=1‚àíùëíùëó‚àëùëó=1ùëõ(1‚àíùëíùëó)œâj=1‚àíej‚àëj=1n(1‚àíej)(27) The information entropy valueejmay approach 1, in which case even a small change in entropy can lead to an excessively large deviation in the calculated weight. To address this issue, this study improves the weight calculation method. The modified weightùúî‚Ä≤ùëóœâ‚Ä≤jis computed as follows:ùúî‚Ä≤ùëó=‚àëùëñ=1ùëõùëíùëñùëõ+1‚àíùëíùëó‚àëùëó=1ùëõ‚éõ‚éù‚éú‚éú‚àëùëñ=1ùëõùëíùëñùëõ+1‚àíùëíùëó‚éû‚é†‚éü‚éüœâ‚Ä≤j=‚àëi=1nein+1‚àíej‚àëj=1n‚àëi=1nein+1‚àíej(28) The subjective weightsWs= (Ws1,Ws2, ‚Ä¶,Wsn) and the objective weightsWo= (Wo1,Wo2, ‚Ä¶,Won) are obtained using the FAHP and EWMs, respectively. The calculation formula for the comprehensive weights is as follows:ùúîùëó=ùõøùëäùë†+(1‚àíùõø)ùëäùëúœâj=Œ¥Ws+(1‚àíŒ¥)Wo(29)whereùõø‚àà[0,1]Œ¥‚àà0,1.",
            "4. Comprehensive Techno-Economic Evaluation Model for Multiple Power Generation Technologies Based on FFHWD-TOPSIS": "4.1. Overview of the Traditional TOPSIS MethodIn the techno-economic evaluation of multiple power generation technologies, the TOPSIS method ranks alternative schemes by quantifying their relative closeness to the ideal solution. This approach fully utilizes the information contained in the original data and clearly reveals the differences among alternatives. It offers a transparent computational process and produces stable and reliable results [33,34]. In this study, TOPSIS is incorporated into the evaluation framework because of its effectiveness in handling multi-attribute decision-making problems while integrating both subjective and objective weights. This provides a clear and interpretable basis for the optimal selection of power generation schemes. The modeling procedure is as follows:Assume there aremalternative schemes, denoted asA1,A2, ‚Ä¶,Am, andndecision indicators, denoted asR1,R2, ‚Ä¶,Rn. The decision matrixX= (xij)m*nconstructed from the original data is given as follows:ùëã=‚é°‚é£‚é¢‚é¢‚é¢‚é¢‚éõ‚éù‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éúùë•11ùë•21‚ãÆùë•ùëö1ùë•12ùë•22‚ãÆùë•ùëö2‚ãØ‚ãØ‚ãÆ‚ãØùë•1ùëõùë•2ùëõ‚ãÆùë•ùëöùëõ‚éû‚é†‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚é§‚é¶‚é•‚é•‚é•‚é•X=x11x12‚ãØx1nx21x22‚ãØx2n‚ãÆ‚ãÆ‚ãÆ‚ãÆxm1xm2‚ãØxmn(30)wherexijrepresents the value of thej-th decision indicator for thei-th evaluation object.(1)Transform the original decision matrixXinto the normalized decision matrixY= (yij)m*nusing the following formula. Normalizing the original decision matrix eliminates the influence of differing dimensions among indicators and resolves the issue of incomparability between them.ùë¶ùëñùëó=ùë•ùëñùëó‚àëùëñ=1ùëõùë•2ùëñùëó‚àí‚àí‚àí‚àí‚àíÓÑ¥‚é∑ÓÑ≥ÓÑ≥yij=xij‚àëi=1nxij2(31)(2)Construct the weighted normalized decision matrixZ.ùëç=(ùëßùëñùëó)ùëö√óùëõZ=(zij)m√ón(32)ùëßùëñùëó=ùëäùëó√óùë¶ùëñùëózij=Wj√óyij(33)whereWjrepresents the weight of thej-th evaluation indicator.(3)Determine the positive ideal solutionS+ and the negative ideal solutionS‚àí. The positive ideal solution represents the scenario in which all evaluation indicators achieve their optimal values, while the negative ideal solution represents the scenario in which all indicators reach their worst values.ùë†+ùëó=max{ùëßùëñùëó/1‚©Ωùëñ‚©Ωùëö}sj+=maxzij/1‚©Ωi‚©Ωm(34)ùë†‚àíùëó=min{ùëßùëñùëó/1‚©Ωùëñ‚©Ωùëö}sj‚àí=minzij/1‚©Ωi‚©Ωm(35)(4)Calculate the Euclidean distances of each alternative from the positive and negative ideal solutions:ùëë+ùëñ=‚àëùëó=1ùëõ(ùëßùëñùëó‚àíùë†+ùëó)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àíÓÑ¥‚é∑ÓÑ≥ÓÑ≥(ùëñ=1,2,‚ãØ,ùëö)di+=‚àëj=1n(zij‚àísj+)2(i=1,2,‚ãØ,m)(36)ùëë‚àíùëñ=‚àëùëó=1ùëõ(ùëßùëñùëó‚àíùë†‚àíùëó)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àíÓÑ¥‚é∑ÓÑ≥ÓÑ≥(ùëñ=1,2,‚ãØ,ùëö)di‚àí=‚àëj=1n(zij‚àísj‚àí)2(i=1,2,‚ãØ,m)(37)(5)Calculate the relative closenessCiof each evaluation object using the following formula. A largerCivalue indicates that the evaluation object is closer to the ideal solution, whereas a smallerCivalue indicates closer proximity to the negative ideal solution. The evaluation objects are then ranked according to the magnitude of their relative closeness values.ùê∂ùëñ=ùëë‚àíùëñùëë+ùëñ+ùëë‚àíùëñCi=di‚àídi++di‚àí(38)Despite the well-structured framework and computational simplicity of the traditional TOPSIS method that have contributed to its widespread adoption in multi-attribute decision-making (MADM) domains, several critical limitations emerge, particularly in evaluation scenarios involving high uncertainty and strong fuzziness such as new energy power generation technologies. Firstly, the conventional TOPSIS assumes deterministic or precise numerical values for evaluation inputs, whereas practical techno-economic assessments of power generation systems often involve expert judgments characterized by fuzziness and hesitation. This discrepancy may result in information loss or bias during decision-making processes. Secondly, the default application of Euclidean distance to measure proximity between alternatives and ideal solutions fails to adequately account for variations in indicator importance and ranking sensitivity. The distance metric exhibits notable vulnerability to conflicting weight assignments, making evaluation outcomes excessively sensitive to extreme values or localized data fluctuations. 4.2. Fermatean Fuzzy Hybrid Weighted DistanceGiven the limitations of traditional TOPSIS, it is essential to construct a distance metric with enhanced expressive capability and robustness to improve its adaptability in handling fuzzy, multi-scale evaluation information. This study introduces FFS to characterize membership, non-membership, and hesitation degrees in expert judgments [35,36,37,38,39], and further develops the FFHWD as a novel distance metric. This approach simultaneously integrates subjective-objective weights, positional weights, and fuzzy information while maintaining methodological simplicity. The proposed improvement preserves the structural integrity of traditional TOPSIS while significantly enhancing its stability and discriminative capability in complex techno-economic evaluations of power generation technologies.First, letXbe a non-empty set. The expression of an FFSYbelonging toXis given as follows:ùëå={‚å©ùë•ùëñ,ùúáùëå(ùë•ùëñ),ùúàùëå(ùë•ùëñ)‚å™|ùë•ùëñ‚ààùëã}Y=xi,ŒºY(xi),ŒΩY(xi)|xi‚ààX(39)whereùúáùëå(ùë•ùëñ):ùëã‚Üí[0,1]ŒºY(xi):X‚Üí[0,1]is termed as ‚Äòthe membership degree of the factorxiin the setY‚Äô, andùë£ùëå(ùë•ùëñ):ùëã‚Üí[0,1]vY(xi):X‚Üí[0,1]is indicated as ‚Äòthe non-membership degree of the factorxiin the setY‚Äô. In addition,0‚â§[ùúáùëå(ùë•ùëñ)]3+[ùúàùëå(ùë•ùëñ)]3‚â§10‚â§[ŒºY(xi)]3+[ŒΩY(xi)]3‚â§1for allùë•ùëñ‚ààùëãxi‚ààX. For a FFSYandùë•ùëñ‚ààùëãxi‚ààX,ùúè=1‚àí[ùúáùëå(ùë•ùëñ)]3‚àí[ùúàùëå(ùë•ùëñ)]3‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö3œÑ=1‚àí[ŒºY(xi)]3‚àí[ŒΩY(xi)]33is the indeterminacy degree ofxitoY.For simplicity, the FFN is denoted asùõº=‚å©ùúáùëå,ùúàùëå‚å™Œ±=‚å©ŒºY,ŒΩY‚å™. LetùúÜŒªbe a positive real number, and letùõº=‚å©ùúáùëå,ùúàùëå‚å™Œ±=‚å©ŒºY,ŒΩY‚å™,ùõº1=‚å©ùúáùëå1,ùúàùëå1‚å™Œ±1=‚å©ŒºY1,ŒΩY1‚å™, andùõº2=‚å©ùúáùëå2,ùúàùëå2‚å™Œ±2=‚å©ŒºY2,ŒΩY2‚å™be three FFNs defined on a non-empty setX. The corresponding operational definitions of these FFNs are given as follows:ùõº1‚äïùõº2=‚å©ùúá3ùëå1+ùúá3ùëå2‚àíùúá3ùëå1ùúá3ùëå2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö3,ùúàùëå1ùúàùëå2‚å™Œ±1‚äïŒ±2=ŒºY13+ŒºY23‚àíŒºY13ŒºY233,ŒΩY1ŒΩY2(40)ùõº1‚äóùõº2=‚å©ùúáùëå1ùúáùëå2,ùë£3ùëå1+ùë£3ùëå2‚àíùë£3ùëå1ùë£3ùëå2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö3‚å™Œ±1‚äóŒ±2=ŒºY1ŒºY2,vY13+vY23‚àívY13vY233(41)ùúÜùõº=‚å©1‚àí(1‚àíùúá3ùëå)ùúÜ‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö3,ùë£ùúÜùëå‚å™ŒªŒ±=1‚àí(1‚àíŒºY3)Œª3,vYŒª(42)ùõºùëê=‚å©ùúàùëå,ùúáùëå‚å™Œ±c=‚å©ŒΩY,ŒºY‚å™(43)For a FFNùõº=‚å©ùúáùëå,ùúàùëå‚å™Œ±=ŒºY,ŒΩY, the functionsùëÜ(ùëå)=ùúá3ùëå‚àíùúà3ùëåS(Y)=ŒºY3‚àíŒΩY3andùê¥(ùëå)=ùúá3ùëå+ùúà3ùëåA(Y)=ŒºY3+ŒΩY3are defined as the score function and the accuracy function ofùõºŒ±, respectively. IfùëÜ(ùõº1)<ùëÜ(ùõº2)S(Œ±1)<S(Œ±2), thenùõº1<ùõº2Œ±1<Œ±2; ifùëÜ(ùõº1)=ùëÜ(ùõº2)S(Œ±1)=S(Œ±2), then{ùê¥(ùõº1)<ùê¥(ùõº2)‚áíùõº1<ùõº2ùê¥(ùõº1)=ùê¥(ùõº2)‚áíùõº1=ùõº2A(Œ±1)<A(Œ±2)‚áíŒ±1<Œ±2A(Œ±1)=A(Œ±2)‚áíŒ±1=Œ±2.For two FFNs,ùõº1=‚å©ùúáùëå1,ùúàùëå1‚å™Œ±1=‚å©ŒºY1,ŒΩY1‚å™andùõº2=‚å©ùúáùëå2,ùúàùëå2‚å™Œ±2=‚å©ŒºY2,ŒΩY2‚å™, the distance between them can be defined as follows:ùëë(ùõº1,ùõº2)=12(|ùúá3ùëå1‚àíùúá3ùëå2|+|ùúà3ùëå1‚àíùúà3ùëå2|+|ùúè3ùëå1‚àíùúè3ùëå2|)d(Œ±1,Œ±2)=12ŒºY13‚àíŒºY23+ŒΩY13‚àíŒΩY23+œÑY13‚àíœÑY23(44)Letùê¥=(ùõº1,ùõº2,‚Ä¶,ùõºùëõ)A=(Œ±1,Œ±2,‚Ä¶,Œ±n)be a set of FFNs, and letùë§=(ùë§1,ùë§2,‚Ä¶,ùë§ùëõ)ùëáw=(w1,w2,‚Ä¶,wn)Tbe the weight vector corresponding to these fuzzy numbers. Then, the Fermatean fuzzy weighted averaging (FFWA) operator is defined as follows:ùêπùêπùëäùê¥(ùõº1,ùõº2,‚Ä¶,ùõºùëõ)=‚àëùëó=1ùëõùë§ùëóùõºùëóFFWA(Œ±1,Œ±2,‚Ä¶,Œ±n)=‚àëj=1nwjŒ±j(45)To provide a more comprehensive characterization of both subjective and objective information in the evaluation of power supply schemes, this study adopts the concept of the HWD proposed in [40] and develops a FFHWD measure. This measure integrates both the intrinsic importance of each indicator and its positional weight within the sequence. By doing so, it more reasonably captures the decision-maker‚Äôs risk preferences and the inherent differences within the data, thereby enhancing the overall comprehensiveness and reliability of the evaluation results [41,42].For two sets of FFNSùê¥=(ùõº1,ùõº2,‚Ä¶,ùõºùëõ)A=(Œ±1,Œ±2,‚Ä¶,Œ±n)andùêµ=(ùõº‚Ä≤1,ùõº‚Ä≤2,‚Ä¶,ùõº‚Ä≤ùëõ)B=(Œ±‚Ä≤1,Œ±‚Ä≤2,‚Ä¶,Œ±‚Ä≤n), their FFHWD is defined as:ùêπùêπùêªùëäùê∑ùë§,ùúî(ùê¥,ùêµ)=‚àëùëó=1ùëõùë§ùëó(ùëëÃÉ(ùõºùúé(ùëó),ùõº‚Ä≤ùúé(ùëó)))FFHWDw,œâ(A,B)=‚àëj=1nwjdÀúŒ±œÉ(j),Œ±‚Ä≤œÉ(j)(46)whereùëëÃÉ(ùõºùúé(ùëó),ùõº‚Ä≤ùúé(ùëó))dÀúŒ±œÉ(j),Œ±‚Ä≤œÉ(j)denotes thej-th largest of the weighted individual distanceùëëÀô(ùõºùëó,ùõº‚Ä≤ùëó)=ùëõùúîùëóùëë(ùõºùëó,ùõº‚Ä≤ùëó),ùëó=1,2,‚Ä¶,ùëõdÀôŒ±j,Œ±‚Ä≤j=nœâjdŒ±j,Œ±‚Ä≤j,j=1,2,‚Ä¶,n,ùúî=(ùúî1,‚Ä¶,ùúîùëõ)ùëáœâ=(œâ1,‚Ä¶,œân)Tis the weight vector related to the individualùëë(ùõºùëó,ùõº‚Ä≤ùëó)dŒ±j,Œ±‚Ä≤j, withùúîùëó‚àà[0,1]œâj‚àà[0,1]and their sum is 1.ùë§=(ùë§1,‚Ä¶,ùë§ùëõ)ùëáw=(w1,‚Ä¶,wn)Tis the weight vector for FFHWD measure. The balancing parameternacts as a balance role. 4.3. Comprehensive Evaluation Framework Based on FFHWD-TOPSISBased on the previously established FFHWD measure and the combined weighting model, this section proposes a complete FFHWD-TOPSIS framework for the techno-economic evaluation of multiple power generation technologies. The proposed framework aims to systematically integrate subjective and objective information under a fuzzy environment, providing a clear and operational procedure for scheme optimization. The main steps are illustrated inFigure 1.Figure 1.Flowchart of techno-economic evaluation for multi-type power sources.(1)Construct the Fermat-type decision matrix. Expertùëíùëò(ùëò=1,2,‚Ä¶,ùêæ)ek(k=1,2,‚Ä¶,K)evaluates the criterionùê∂ùëó(ùëó=1,2,‚Ä¶,ùëö)Cj(j=1,2,‚Ä¶,m)under the evaluation objectùêµùëñ(ùëñ=1,2,‚Ä¶,ùëõ)Bi(i=1,2,‚Ä¶,n)in the form of FFN, denoted asùõæùëò=‚å©ùõºùëòùëñùëó,ùõΩùëòùëñùëó‚å™Œ≥k=Œ±ijk,Œ≤ijk. Therefore, the fuzzy soft decision matrixùëÉùëò=[ùõæùëòùëñùëó]ùëõ√óùëöPk=Œ≥ijkn√ómof thek-th expert can be obtained:ùëÉùëò=‚é°‚é£‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚å©ùõºùëò11,ùõΩùëò11‚å™‚å©ùõºùëò21,ùõΩùëò21‚å™‚ãÆ‚å©ùõºùëòùëõ1,ùõΩùëòùëõ1‚å™‚å©ùõºùëò12,ùõΩùëò12‚å™‚å©ùõºùëò22,ùõΩùëò22‚å™‚ãÆ‚å©ùõºùëòùëõ2,ùõΩùëòùëõ2‚å™‚ãØ‚ãØ‚ã±‚ãØ‚å©ùõºùëò1ùëö,ùõΩùëò1ùëö‚å™‚å©ùõºùëò2ùëö,ùõΩùëò2ùëö‚å™‚ãÆ‚å©ùõºùëòùëõùëö,ùõΩùëòùëõùëö‚å™‚é§‚é¶‚é•‚é•‚é•‚é•‚é•‚é•ùëõ√óùëöPk=‚å©Œ±11k,Œ≤11k‚å™‚å©Œ±12k,Œ≤12k‚å™‚ãØ‚å©Œ±1mk,Œ≤1mk‚å™‚å©Œ±21k,Œ≤21k‚å™‚å©Œ±22k,Œ≤22k‚å™‚ãØ‚å©Œ±2mk,Œ≤2mk‚å™‚ãÆ‚ãÆ‚ã±‚ãÆ‚å©Œ±n1k,Œ≤n1k‚å™‚å©Œ±n2k,Œ≤n2k‚å™‚ãØ‚å©Œ±nmk,Œ≤nmk‚å™n√óm(47)(2)Normalize the individual decision matrices of experts. Let the normalized criterion value beùëîùëñùëó(ùëñ=1,2,‚Ä¶,ùëõ;ùëó=1,2,‚Ä¶,ùëö)gij(i=1,2,‚Ä¶,n;j=1,2,‚Ä¶,m).(3)Apply the FFWA operator, combined with the weight of each expertùëíùëò(ùëò=1,2,‚Ä¶,ùêæ)ek(k=1,2,‚Ä¶,K)denoted asŒµk, to aggregate the decision matrices of all experts, thereby obtaining the overall fuzzy soft decision matrixùëÉ=[ùõæùëñùëó]ùëõ√óùëö(ùëñ=1,2,‚Ä¶,ùëõ;ùëó=1,2,‚Ä¶,ùëö)P=Œ≥ijn√óm(i=1,2,‚Ä¶,n;j=1,2,‚Ä¶,m):ùëÉ=‚é°‚é£‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚å©ùõº11ÃÉ,ùõΩ11ÃÉ‚å™‚å©ùõº21ÃÉ,ùõΩ21ÃÉ‚å™‚ãÆ‚å©ùõºùëõùëöÃÉ,ùõΩùëõùëöÃÉ‚å™‚å©ùõº12ÃÉ,ùõΩ12ÃÉ‚å™‚å©ùõº22ÃÉ,ùõΩ22ÃÉ‚å™‚ãÆ‚å©ùõºùëõùëöÃÉ,ùõΩùëõùëöÃÉ‚å™‚ãØ‚ãØ‚ã±‚ãØ‚å©ùõº1ùëöÃÉ,ùõΩ1ùëöÃÉ‚å™‚å©ùõº2ùëöÃÉ,ùõΩ2ùëöÃÉ‚å™‚ãÆ‚å©ùõºùëõùëöÃÉ,ùõΩùëõùëöÃÉ‚å™‚é§‚é¶‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•ùëõ√óùëöP=‚å©Œ±11Àú,Œ≤11Àú‚å™‚å©Œ±12Àú,Œ≤12Àú‚å™‚ãØ‚å©Œ±1mÀú,Œ≤1mÀú‚å™‚å©Œ±21Àú,Œ≤21Àú‚å™‚å©Œ±22Àú,Œ≤22Àú‚å™‚ãØ‚å©Œ±2mÀú,Œ≤2mÀú‚å™‚ãÆ‚ãÆ‚ã±‚ãÆ‚å©Œ±nmÀú,Œ≤nmÀú‚å™‚å©Œ±nmÀú,Œ≤nmÀú‚å™‚ãØ‚å©Œ±nmÀú,Œ≤nmÀú‚å™n√óm(48)(4)Apply the combined weighting method presented inSection 3to determine the comprehensive weights of each indicator.(5)Calculate the Fermatean fuzzy PISB+and Fermatean fuzzy NISB‚àías follows:ùêµ+={ùê∂1(ùêµ+),ùê∂2(ùêµ+),‚Ä¶,ùê∂ùëõ(ùêµ+)}={maxùëñ‚å©ùëÜ(ùõæùëñùëó)‚å™|ùëó=1,2,‚Ä¶,ùëõ}B+=C1(B+),C2(B+),‚Ä¶,Cn(B+)=maxi‚å©S(Œ≥ij)‚å™|j=1,2,‚Ä¶,n(49)ùêµ‚àí={ùê∂1(ùêµ‚àí),ùê∂2(ùêµ‚àí),‚Ä¶,ùê∂ùëõ(ùêµ‚àí)}={minùëñ‚å©ùëÜ(ùõæùëñùëó)‚å™|ùëó=1,2,‚Ä¶,ùëõ}B‚àí=C1(B‚àí),C2(B‚àí),‚Ä¶,Cn(B‚àí)=mini‚å©S(Œ≥ij)‚å™|j=1,2,‚Ä¶,n.(50)(6)Calculate the deviations between each alternativeBiand the FF PISB+and NISB‚àí, denoted asùêπùêπùêªùëäùê∑ùë§,ùúî(ùêµùëñ,ùêµ+)FFHWDw,œâ(Bi,B+)and, respectively.(7)Calculate the closeness valueùúç(ùêµùëñ)œÇBifor each alternative solutionùêµùëñ(ùëñ=1,2,‚Ä¶,ùëõ)Bi(i=1,2,‚Ä¶,n).ùúÅ(ùêµùëñ)=ùêπùêπùêªùëäùê∑ùë§,ùúî(ùêµùëñ,ùêµ‚àí)max(ùêπùêπùêªùëäùê∑ùë§,ùúî(ùêµùëñ,ùêµ‚àí))‚àíùêπùêπùêªùëäùê∑ùë§,ùúî(ùêµùëñ,ùêµ+)min(ùêπùêπùêªùëäùê∑ùë§,ùúî(ùêµùëñ,ùêµ+))Œ∂(Bi)=FFHWDw,œâ(Bi,B‚àí)maxFFHWDw,œâ(Bi,B‚àí)‚àíFFHWDw,œâ(Bi,B+)minFFHWDw,œâ(Bi,B+)(51)(8)Sort all alternative schemes in descending order based on the closeness degreeùúç(ùêµùëñ)œÇBicalculated in the previous step, and determine the optimal scheme.",
            "4.1. Overview of the Traditional TOPSIS Method": "In the techno-economic evaluation of multiple power generation technologies, the TOPSIS method ranks alternative schemes by quantifying their relative closeness to the ideal solution. This approach fully utilizes the information contained in the original data and clearly reveals the differences among alternatives. It offers a transparent computational process and produces stable and reliable results [33,34]. In this study, TOPSIS is incorporated into the evaluation framework because of its effectiveness in handling multi-attribute decision-making problems while integrating both subjective and objective weights. This provides a clear and interpretable basis for the optimal selection of power generation schemes. The modeling procedure is as follows: Assume there aremalternative schemes, denoted asA1,A2, ‚Ä¶,Am, andndecision indicators, denoted asR1,R2, ‚Ä¶,Rn. The decision matrixX= (xij)m*nconstructed from the original data is given as follows:ùëã=‚é°‚é£‚é¢‚é¢‚é¢‚é¢‚éõ‚éù‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éú‚éúùë•11ùë•21‚ãÆùë•ùëö1ùë•12ùë•22‚ãÆùë•ùëö2‚ãØ‚ãØ‚ãÆ‚ãØùë•1ùëõùë•2ùëõ‚ãÆùë•ùëöùëõ‚éû‚é†‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚éü‚é§‚é¶‚é•‚é•‚é•‚é•X=x11x12‚ãØx1nx21x22‚ãØx2n‚ãÆ‚ãÆ‚ãÆ‚ãÆxm1xm2‚ãØxmn(30)wherexijrepresents the value of thej-th decision indicator for thei-th evaluation object. (1)Transform the original decision matrixXinto the normalized decision matrixY= (yij)m*nusing the following formula. Normalizing the original decision matrix eliminates the influence of differing dimensions among indicators and resolves the issue of incomparability between them. ùë¶ùëñùëó=ùë•ùëñùëó‚àëùëñ=1ùëõùë•2ùëñùëó‚àí‚àí‚àí‚àí‚àíÓÑ¥‚é∑ÓÑ≥ÓÑ≥yij=xij‚àëi=1nxij2(31) (2)Construct the weighted normalized decision matrixZ. ùëç=(ùëßùëñùëó)ùëö√óùëõZ=(zij)m√ón(32)ùëßùëñùëó=ùëäùëó√óùë¶ùëñùëózij=Wj√óyij(33)whereWjrepresents the weight of thej-th evaluation indicator. (3)Determine the positive ideal solutionS+ and the negative ideal solutionS‚àí. The positive ideal solution represents the scenario in which all evaluation indicators achieve their optimal values, while the negative ideal solution represents the scenario in which all indicators reach their worst values. ùë†+ùëó=max{ùëßùëñùëó/1‚©Ωùëñ‚©Ωùëö}sj+=maxzij/1‚©Ωi‚©Ωm(34)ùë†‚àíùëó=min{ùëßùëñùëó/1‚©Ωùëñ‚©Ωùëö}sj‚àí=minzij/1‚©Ωi‚©Ωm(35) (4)Calculate the Euclidean distances of each alternative from the positive and negative ideal solutions: ùëë+ùëñ=‚àëùëó=1ùëõ(ùëßùëñùëó‚àíùë†+ùëó)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àíÓÑ¥‚é∑ÓÑ≥ÓÑ≥(ùëñ=1,2,‚ãØ,ùëö)di+=‚àëj=1n(zij‚àísj+)2(i=1,2,‚ãØ,m)(36)ùëë‚àíùëñ=‚àëùëó=1ùëõ(ùëßùëñùëó‚àíùë†‚àíùëó)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àíÓÑ¥‚é∑ÓÑ≥ÓÑ≥(ùëñ=1,2,‚ãØ,ùëö)di‚àí=‚àëj=1n(zij‚àísj‚àí)2(i=1,2,‚ãØ,m)(37) (5)Calculate the relative closenessCiof each evaluation object using the following formula. A largerCivalue indicates that the evaluation object is closer to the ideal solution, whereas a smallerCivalue indicates closer proximity to the negative ideal solution. The evaluation objects are then ranked according to the magnitude of their relative closeness values. ùê∂ùëñ=ùëë‚àíùëñùëë+ùëñ+ùëë‚àíùëñCi=di‚àídi++di‚àí(38) Despite the well-structured framework and computational simplicity of the traditional TOPSIS method that have contributed to its widespread adoption in multi-attribute decision-making (MADM) domains, several critical limitations emerge, particularly in evaluation scenarios involving high uncertainty and strong fuzziness such as new energy power generation technologies. Firstly, the conventional TOPSIS assumes deterministic or precise numerical values for evaluation inputs, whereas practical techno-economic assessments of power generation systems often involve expert judgments characterized by fuzziness and hesitation. This discrepancy may result in information loss or bias during decision-making processes. Secondly, the default application of Euclidean distance to measure proximity between alternatives and ideal solutions fails to adequately account for variations in indicator importance and ranking sensitivity. The distance metric exhibits notable vulnerability to conflicting weight assignments, making evaluation outcomes excessively sensitive to extreme values or localized data fluctuations.",
            "4.2. Fermatean Fuzzy Hybrid Weighted Distance": "Given the limitations of traditional TOPSIS, it is essential to construct a distance metric with enhanced expressive capability and robustness to improve its adaptability in handling fuzzy, multi-scale evaluation information. This study introduces FFS to characterize membership, non-membership, and hesitation degrees in expert judgments [35,36,37,38,39], and further develops the FFHWD as a novel distance metric. This approach simultaneously integrates subjective-objective weights, positional weights, and fuzzy information while maintaining methodological simplicity. The proposed improvement preserves the structural integrity of traditional TOPSIS while significantly enhancing its stability and discriminative capability in complex techno-economic evaluations of power generation technologies. First, letXbe a non-empty set. The expression of an FFSYbelonging toXis given as follows:ùëå={‚å©ùë•ùëñ,ùúáùëå(ùë•ùëñ),ùúàùëå(ùë•ùëñ)‚å™|ùë•ùëñ‚ààùëã}Y=xi,ŒºY(xi),ŒΩY(xi)|xi‚ààX(39)whereùúáùëå(ùë•ùëñ):ùëã‚Üí[0,1]ŒºY(xi):X‚Üí[0,1]is termed as ‚Äòthe membership degree of the factorxiin the setY‚Äô, andùë£ùëå(ùë•ùëñ):ùëã‚Üí[0,1]vY(xi):X‚Üí[0,1]is indicated as ‚Äòthe non-membership degree of the factorxiin the setY‚Äô. In addition,0‚â§[ùúáùëå(ùë•ùëñ)]3+[ùúàùëå(ùë•ùëñ)]3‚â§10‚â§[ŒºY(xi)]3+[ŒΩY(xi)]3‚â§1for allùë•ùëñ‚ààùëãxi‚ààX. For a FFSYandùë•ùëñ‚ààùëãxi‚ààX,ùúè=1‚àí[ùúáùëå(ùë•ùëñ)]3‚àí[ùúàùëå(ùë•ùëñ)]3‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö3œÑ=1‚àí[ŒºY(xi)]3‚àí[ŒΩY(xi)]33is the indeterminacy degree ofxitoY. For simplicity, the FFN is denoted asùõº=‚å©ùúáùëå,ùúàùëå‚å™Œ±=‚å©ŒºY,ŒΩY‚å™. LetùúÜŒªbe a positive real number, and letùõº=‚å©ùúáùëå,ùúàùëå‚å™Œ±=‚å©ŒºY,ŒΩY‚å™,ùõº1=‚å©ùúáùëå1,ùúàùëå1‚å™Œ±1=‚å©ŒºY1,ŒΩY1‚å™, andùõº2=‚å©ùúáùëå2,ùúàùëå2‚å™Œ±2=‚å©ŒºY2,ŒΩY2‚å™be three FFNs defined on a non-empty setX. The corresponding operational definitions of these FFNs are given as follows:ùõº1‚äïùõº2=‚å©ùúá3ùëå1+ùúá3ùëå2‚àíùúá3ùëå1ùúá3ùëå2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö3,ùúàùëå1ùúàùëå2‚å™Œ±1‚äïŒ±2=ŒºY13+ŒºY23‚àíŒºY13ŒºY233,ŒΩY1ŒΩY2(40)ùõº1‚äóùõº2=‚å©ùúáùëå1ùúáùëå2,ùë£3ùëå1+ùë£3ùëå2‚àíùë£3ùëå1ùë£3ùëå2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö3‚å™Œ±1‚äóŒ±2=ŒºY1ŒºY2,vY13+vY23‚àívY13vY233(41)ùúÜùõº=‚å©1‚àí(1‚àíùúá3ùëå)ùúÜ‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö3,ùë£ùúÜùëå‚å™ŒªŒ±=1‚àí(1‚àíŒºY3)Œª3,vYŒª(42)ùõºùëê=‚å©ùúàùëå,ùúáùëå‚å™Œ±c=‚å©ŒΩY,ŒºY‚å™(43) For a FFNùõº=‚å©ùúáùëå,ùúàùëå‚å™Œ±=ŒºY,ŒΩY, the functionsùëÜ(ùëå)=ùúá3ùëå‚àíùúà3ùëåS(Y)=ŒºY3‚àíŒΩY3andùê¥(ùëå)=ùúá3ùëå+ùúà3ùëåA(Y)=ŒºY3+ŒΩY3are defined as the score function and the accuracy function ofùõºŒ±, respectively. IfùëÜ(ùõº1)<ùëÜ(ùõº2)S(Œ±1)<S(Œ±2), thenùõº1<ùõº2Œ±1<Œ±2; ifùëÜ(ùõº1)=ùëÜ(ùõº2)S(Œ±1)=S(Œ±2), then{ùê¥(ùõº1)<ùê¥(ùõº2)‚áíùõº1<ùõº2ùê¥(ùõº1)=ùê¥(ùõº2)‚áíùõº1=ùõº2A(Œ±1)<A(Œ±2)‚áíŒ±1<Œ±2A(Œ±1)=A(Œ±2)‚áíŒ±1=Œ±2. For two FFNs,ùõº1=‚å©ùúáùëå1,ùúàùëå1‚å™Œ±1=‚å©ŒºY1,ŒΩY1‚å™andùõº2=‚å©ùúáùëå2,ùúàùëå2‚å™Œ±2=‚å©ŒºY2,ŒΩY2‚å™, the distance between them can be defined as follows:ùëë(ùõº1,ùõº2)=12(|ùúá3ùëå1‚àíùúá3ùëå2|+|ùúà3ùëå1‚àíùúà3ùëå2|+|ùúè3ùëå1‚àíùúè3ùëå2|)d(Œ±1,Œ±2)=12ŒºY13‚àíŒºY23+ŒΩY13‚àíŒΩY23+œÑY13‚àíœÑY23(44) Letùê¥=(ùõº1,ùõº2,‚Ä¶,ùõºùëõ)A=(Œ±1,Œ±2,‚Ä¶,Œ±n)be a set of FFNs, and letùë§=(ùë§1,ùë§2,‚Ä¶,ùë§ùëõ)ùëáw=(w1,w2,‚Ä¶,wn)Tbe the weight vector corresponding to these fuzzy numbers. Then, the Fermatean fuzzy weighted averaging (FFWA) operator is defined as follows:ùêπùêπùëäùê¥(ùõº1,ùõº2,‚Ä¶,ùõºùëõ)=‚àëùëó=1ùëõùë§ùëóùõºùëóFFWA(Œ±1,Œ±2,‚Ä¶,Œ±n)=‚àëj=1nwjŒ±j(45) To provide a more comprehensive characterization of both subjective and objective information in the evaluation of power supply schemes, this study adopts the concept of the HWD proposed in [40] and develops a FFHWD measure. This measure integrates both the intrinsic importance of each indicator and its positional weight within the sequence. By doing so, it more reasonably captures the decision-maker‚Äôs risk preferences and the inherent differences within the data, thereby enhancing the overall comprehensiveness and reliability of the evaluation results [41,42]. For two sets of FFNSùê¥=(ùõº1,ùõº2,‚Ä¶,ùõºùëõ)A=(Œ±1,Œ±2,‚Ä¶,Œ±n)andùêµ=(ùõº‚Ä≤1,ùõº‚Ä≤2,‚Ä¶,ùõº‚Ä≤ùëõ)B=(Œ±‚Ä≤1,Œ±‚Ä≤2,‚Ä¶,Œ±‚Ä≤n), their FFHWD is defined as:ùêπùêπùêªùëäùê∑ùë§,ùúî(ùê¥,ùêµ)=‚àëùëó=1ùëõùë§ùëó(ùëëÃÉ(ùõºùúé(ùëó),ùõº‚Ä≤ùúé(ùëó)))FFHWDw,œâ(A,B)=‚àëj=1nwjdÀúŒ±œÉ(j),Œ±‚Ä≤œÉ(j)(46)whereùëëÃÉ(ùõºùúé(ùëó),ùõº‚Ä≤ùúé(ùëó))dÀúŒ±œÉ(j),Œ±‚Ä≤œÉ(j)denotes thej-th largest of the weighted individual distanceùëëÀô(ùõºùëó,ùõº‚Ä≤ùëó)=ùëõùúîùëóùëë(ùõºùëó,ùõº‚Ä≤ùëó),ùëó=1,2,‚Ä¶,ùëõdÀôŒ±j,Œ±‚Ä≤j=nœâjdŒ±j,Œ±‚Ä≤j,j=1,2,‚Ä¶,n,ùúî=(ùúî1,‚Ä¶,ùúîùëõ)ùëáœâ=(œâ1,‚Ä¶,œân)Tis the weight vector related to the individualùëë(ùõºùëó,ùõº‚Ä≤ùëó)dŒ±j,Œ±‚Ä≤j, withùúîùëó‚àà[0,1]œâj‚àà[0,1]and their sum is 1.ùë§=(ùë§1,‚Ä¶,ùë§ùëõ)ùëáw=(w1,‚Ä¶,wn)Tis the weight vector for FFHWD measure. The balancing parameternacts as a balance role.",
            "4.3. Comprehensive Evaluation Framework Based on FFHWD-TOPSIS": "Based on the previously established FFHWD measure and the combined weighting model, this section proposes a complete FFHWD-TOPSIS framework for the techno-economic evaluation of multiple power generation technologies. The proposed framework aims to systematically integrate subjective and objective information under a fuzzy environment, providing a clear and operational procedure for scheme optimization. The main steps are illustrated inFigure 1. Figure 1.Flowchart of techno-economic evaluation for multi-type power sources. (1)Construct the Fermat-type decision matrix. Expertùëíùëò(ùëò=1,2,‚Ä¶,ùêæ)ek(k=1,2,‚Ä¶,K)evaluates the criterionùê∂ùëó(ùëó=1,2,‚Ä¶,ùëö)Cj(j=1,2,‚Ä¶,m)under the evaluation objectùêµùëñ(ùëñ=1,2,‚Ä¶,ùëõ)Bi(i=1,2,‚Ä¶,n)in the form of FFN, denoted asùõæùëò=‚å©ùõºùëòùëñùëó,ùõΩùëòùëñùëó‚å™Œ≥k=Œ±ijk,Œ≤ijk. Therefore, the fuzzy soft decision matrixùëÉùëò=[ùõæùëòùëñùëó]ùëõ√óùëöPk=Œ≥ijkn√ómof thek-th expert can be obtained: ùëÉùëò=‚é°‚é£‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚å©ùõºùëò11,ùõΩùëò11‚å™‚å©ùõºùëò21,ùõΩùëò21‚å™‚ãÆ‚å©ùõºùëòùëõ1,ùõΩùëòùëõ1‚å™‚å©ùõºùëò12,ùõΩùëò12‚å™‚å©ùõºùëò22,ùõΩùëò22‚å™‚ãÆ‚å©ùõºùëòùëõ2,ùõΩùëòùëõ2‚å™‚ãØ‚ãØ‚ã±‚ãØ‚å©ùõºùëò1ùëö,ùõΩùëò1ùëö‚å™‚å©ùõºùëò2ùëö,ùõΩùëò2ùëö‚å™‚ãÆ‚å©ùõºùëòùëõùëö,ùõΩùëòùëõùëö‚å™‚é§‚é¶‚é•‚é•‚é•‚é•‚é•‚é•ùëõ√óùëöPk=‚å©Œ±11k,Œ≤11k‚å™‚å©Œ±12k,Œ≤12k‚å™‚ãØ‚å©Œ±1mk,Œ≤1mk‚å™‚å©Œ±21k,Œ≤21k‚å™‚å©Œ±22k,Œ≤22k‚å™‚ãØ‚å©Œ±2mk,Œ≤2mk‚å™‚ãÆ‚ãÆ‚ã±‚ãÆ‚å©Œ±n1k,Œ≤n1k‚å™‚å©Œ±n2k,Œ≤n2k‚å™‚ãØ‚å©Œ±nmk,Œ≤nmk‚å™n√óm(47) (2)Normalize the individual decision matrices of experts. Let the normalized criterion value beùëîùëñùëó(ùëñ=1,2,‚Ä¶,ùëõ;ùëó=1,2,‚Ä¶,ùëö)gij(i=1,2,‚Ä¶,n;j=1,2,‚Ä¶,m).(3)Apply the FFWA operator, combined with the weight of each expertùëíùëò(ùëò=1,2,‚Ä¶,ùêæ)ek(k=1,2,‚Ä¶,K)denoted asŒµk, to aggregate the decision matrices of all experts, thereby obtaining the overall fuzzy soft decision matrixùëÉ=[ùõæùëñùëó]ùëõ√óùëö(ùëñ=1,2,‚Ä¶,ùëõ;ùëó=1,2,‚Ä¶,ùëö)P=Œ≥ijn√óm(i=1,2,‚Ä¶,n;j=1,2,‚Ä¶,m): ùëÉ=‚é°‚é£‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚å©ùõº11ÃÉ,ùõΩ11ÃÉ‚å™‚å©ùõº21ÃÉ,ùõΩ21ÃÉ‚å™‚ãÆ‚å©ùõºùëõùëöÃÉ,ùõΩùëõùëöÃÉ‚å™‚å©ùõº12ÃÉ,ùõΩ12ÃÉ‚å™‚å©ùõº22ÃÉ,ùõΩ22ÃÉ‚å™‚ãÆ‚å©ùõºùëõùëöÃÉ,ùõΩùëõùëöÃÉ‚å™‚ãØ‚ãØ‚ã±‚ãØ‚å©ùõº1ùëöÃÉ,ùõΩ1ùëöÃÉ‚å™‚å©ùõº2ùëöÃÉ,ùõΩ2ùëöÃÉ‚å™‚ãÆ‚å©ùõºùëõùëöÃÉ,ùõΩùëõùëöÃÉ‚å™‚é§‚é¶‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•ùëõ√óùëöP=‚å©Œ±11Àú,Œ≤11Àú‚å™‚å©Œ±12Àú,Œ≤12Àú‚å™‚ãØ‚å©Œ±1mÀú,Œ≤1mÀú‚å™‚å©Œ±21Àú,Œ≤21Àú‚å™‚å©Œ±22Àú,Œ≤22Àú‚å™‚ãØ‚å©Œ±2mÀú,Œ≤2mÀú‚å™‚ãÆ‚ãÆ‚ã±‚ãÆ‚å©Œ±nmÀú,Œ≤nmÀú‚å™‚å©Œ±nmÀú,Œ≤nmÀú‚å™‚ãØ‚å©Œ±nmÀú,Œ≤nmÀú‚å™n√óm(48) (4)Apply the combined weighting method presented inSection 3to determine the comprehensive weights of each indicator.(5)Calculate the Fermatean fuzzy PISB+and Fermatean fuzzy NISB‚àías follows: ùêµ+={ùê∂1(ùêµ+),ùê∂2(ùêµ+),‚Ä¶,ùê∂ùëõ(ùêµ+)}={maxùëñ‚å©ùëÜ(ùõæùëñùëó)‚å™|ùëó=1,2,‚Ä¶,ùëõ}B+=C1(B+),C2(B+),‚Ä¶,Cn(B+)=maxi‚å©S(Œ≥ij)‚å™|j=1,2,‚Ä¶,n(49)ùêµ‚àí={ùê∂1(ùêµ‚àí),ùê∂2(ùêµ‚àí),‚Ä¶,ùê∂ùëõ(ùêµ‚àí)}={minùëñ‚å©ùëÜ(ùõæùëñùëó)‚å™|ùëó=1,2,‚Ä¶,ùëõ}B‚àí=C1(B‚àí),C2(B‚àí),‚Ä¶,Cn(B‚àí)=mini‚å©S(Œ≥ij)‚å™|j=1,2,‚Ä¶,n.(50) (6)Calculate the deviations between each alternativeBiand the FF PISB+and NISB‚àí, denoted asùêπùêπùêªùëäùê∑ùë§,ùúî(ùêµùëñ,ùêµ+)FFHWDw,œâ(Bi,B+)and, respectively.(7)Calculate the closeness valueùúç(ùêµùëñ)œÇBifor each alternative solutionùêµùëñ(ùëñ=1,2,‚Ä¶,ùëõ)Bi(i=1,2,‚Ä¶,n). ùúÅ(ùêµùëñ)=ùêπùêπùêªùëäùê∑ùë§,ùúî(ùêµùëñ,ùêµ‚àí)max(ùêπùêπùêªùëäùê∑ùë§,ùúî(ùêµùëñ,ùêµ‚àí))‚àíùêπùêπùêªùëäùê∑ùë§,ùúî(ùêµùëñ,ùêµ+)min(ùêπùêπùêªùëäùê∑ùë§,ùúî(ùêµùëñ,ùêµ+))Œ∂(Bi)=FFHWDw,œâ(Bi,B‚àí)maxFFHWDw,œâ(Bi,B‚àí)‚àíFFHWDw,œâ(Bi,B+)minFFHWDw,œâ(Bi,B+)(51) (8)Sort all alternative schemes in descending order based on the closeness degreeùúç(ùêµùëñ)œÇBicalculated in the previous step, and determine the optimal scheme.",
            "5. Case Studies": "5.1. Selection of Evaluation IndicatorsTo validate the effectiveness of the proposed techno-economic evaluation model, five representative power generation technologies were selected as case studies: thermal power (B1), hydropower (B2), wind power (B3), photovoltaics (B4), and energy storage (B5). This combination encompasses traditional fossil fuels, renewable energy sources, and flexible resources, comprehensively reflecting the techno-economic characteristics and structural differences across multi-type power generation systems. The evaluation framework consists of seven core indicators:C1(levelized cost of electricity, LCOE),C2(capacity factor),C3(start-up and regulation characteristics),C4(energy conversion efficiency),C5(carbon emission intensity),C6(specific investment cost), andC7(operational and maintenance cost ratio), which systematically characterize the techno-economic performance of different power sources. Based on this framework, domain experts were invited to assess the interrelationships among the aforementioned indicators using the linguistic variables defined inTable 1. The DEMATEL method was then applied to structurally analyze the evaluation results. As an illustrative example, the data from Expert 1 are presented inTable 2.Table 2.Expert 1‚Äôs influence degree judgments for evaluation indicators.Based on the correspondence between linguistic variables and fuzzy numbers inTable 1, and combined with the data inTable 2, the initial direct influence matrix provided by Expert 1 is as follows:ùêµÃÉ(1)=‚é°‚é£‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢(0,0.1,0.3)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.3,0.5,0.7)(0.7,0.9,1.0)(0.1,0.3,0.5)(0.3,0.5,0.7)(0.3,0.5,0.7)(0,0.1,0.3)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.1,0.3,0.5)(0.1,0.3,0.5)(0.7,0.9,1.0)(0.7,0.9,1.0)(0,0.1,0.3)(0.7,0.9,1.0)(0.7,0.9,1.0)(0.3,0.5,0.7)(0.3,0.5,0.7)(0.5,0.7,0.9)(0.5,0.7,0.9)(0.7,0.9,1.0)(0,0.1,0.3)(0.3,0.5,0.7)(0.5,0.7,0.9)(0.1,0.3,0.5)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.5,0.7,0.9)(0,0.1,0.3)(0.1,0.3,0.5)(0.1,0.3,0.5)(0.7,0.9,1.0)(0.3,0.5,0.7)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.7,0.9,1.0)(0,0.1,0.3)(0.3,0.5,0.7)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.3,0.5,0.7)(0,0.1,0.3)‚é§‚é¶‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•BÀú(1)=(0,0.1,0.3)(0.3,0.5,0.7)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.5,0.7,0.9)(0,0.1,0.3)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.5,0.7,0.9)(0.3,0.5,0.7)(0.7,0.9,1.0)(0.7,0.9,1.0)(0.5,0.7,0.9)(0,0.1,0.3)(0.7,0.9,1.0)(0.7,0.9,1.0)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.3,0.5,0.7)(0.7,0.9,1.0)(0.7,0.9,1.0)(0,0.1,0.3)(0.5,0.7,0.9)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.3,0.5,0.7)(0,0.1,0.3)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.1,0.3,0.5)(0.1,0.3,0.5)(0.3,0.5,0.7)(0.5,0.7,0.9)(0.1,0.3,0.5)(0,0.1,0.3)(0.3,0.5,0.7)(0.3,0.5,0.7)(0.1,0.3,0.5)(0.3,0.5,0.7)(0.1,0.3,0.5)(0.1,0.3,0.5)(0.3,0.5,0.7)(0,0.1,0.3)Based on Equations (1)‚Äì(9), the initial direct influence matrix can be obtained as follows:ùêµ(1)=‚é°‚é£‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢0.13330.70.86670.50.86670.30.50.50.13330.70.86670.70.30.30.86670.86670.13330.86670.86670.50.50.70.70.86670.13330.50.70.30.86670.70.86670.70.13330.30.30.86670.50.86670.70.86670.13330.50.70.86670.70.86670.70.50.1333‚é§‚é¶‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•B(1)=0.13330.50.86670.70.86670.86670.70.70.13330.86670.70.70.50.86670.86670.70.13330.86670.86670.86670.70.50.86670.86670.13330.70.70.86670.86670.70.86670.50.13330.86670.70.30.30.50.70.30.13330.50.50.30.50.30.30.50.1333Subsequently, by integrating the opinions of the remaining experts and applying the traditional DEMATEL method, the influence degree, affected degree, causality degree, and centrality of the evaluation indicators were calculated using Equations (10)‚Äì(15), with the detailed results presented inTable 3.Table 3.Specific calculation results of evaluation indicators.The centrality analysis inTable 3indicates that evaluation indicatorC7has the lowest influence on the overall system, followed byC6. To simplify model complexity and focus on core elements, this study retains the five key indicatorsC1‚ÄìC5for subsequent evaluation. Based on this framework, four senior experts in power system planning (E1‚ÄìE4) were invited to conduct evaluations using FFNs. Prior to the formal assessment, experts were provided with a Fermatean Fuzzy Number Operations Manual to define the semantic scales for the membership degreeùúáŒº(‚Äúconfidence in superior indicator performance‚Äù) and non-membership degreeùúàŒΩ(‚Äúconfidence in inferior indicator performance‚Äù). Experts were instructed to adhere to the constraintùúá3+ùúà3‚â§1Œº3+ŒΩ3‚â§1. A structured questionnaire was employed to independently assess FFNs for the five power generation technologies across the five indicators. The questionnaire utilized a 9-point linguistic variable system mapped to FFNs to ensure consistent interpretation among experts. The raw evaluation data are detailed inTable 4.Table 4.Decision matrix of experts.To mitigate potential sampling bias caused by the limited number of participating experts, this study applied 1000 bootstrap iterations to resample the expert evaluation matrices and calculate confidence intervals for the closeness coefficients of all alternatives. The results demonstrate that the rankings of all alternatives remain consistent within the 95% confidence interval, indicating the evaluation results exhibit strong robustness. 5.2. Model Application and AnalysisBased on the original expert evaluation data presented inTable 1, and considering their respective weights (0.3, 0.2, 0.25, 0.25), a collective decision matrix was obtained through weighted integration. Subsequently, the group evaluation results for the five power generation schemes were calculated, as shown inTable 5.Table 5.Collective decision matrix.Based on the group evaluation results inTable 5, the score functionùëÜ(ùëå)S(Y)for each power generation scheme was first calculated to quantify their overall performance, as illustrated inFigure 2. On this basis, and according to Equations (39) and (40), the positive and negative ideal solutionsB+andB‚àíunder the Fermatean fuzzy environment were determined. The results are summarized inTable 6, providing the foundation for the subsequent distance calculation and scheme ranking.Figure 2.Score functions of each power generation scheme.Table 6.Fermatean fuzzy positive ideal solutionB+and negative ideal solutionB‚àí.To scientifically assess the relative importance of each evaluation indicator in the comprehensive assessment process, the FAHP and EWMs were used to calculate the subjective and objective weights of each indicator, respectively. These were then combined using a fusion weighting approach withùõø=0.5Œ¥=0.5. The results are presented inTable 7. As shown in the table, indicator C4 ranks first in both weighting methods, with a combined weight of 0.3145, indicating that its central role is reinforced by both data variation and expert consensus. Indicator C3 follows with a weight of 0.242, highlighting the key importance of the capacity factor in the evaluation. In contrast, C1 has the smallest weight of 0.087, suggesting a relatively limited influence. Overall, the integrated weighting results strike a balance between subjective judgment and objective data, thereby enhancing the scientific rigor and credibility of the evaluation.Table 7.Weights of index.To evaluate the overall performance of each power supply scheme, the FFHWD distances between each alternative and the Fermatean fuzzy positive and negative ideal solutions,FFHWD(B·µ¢,B+) andFFHWD(B·µ¢,B‚àí), were first calculated. Based on these results, a weight vector was determined using an ordered weighted operator derived from the normal distribution. In this case, the vector was set as (0.112, 0.236, 0.304, 0.236, 0.112)·µÄ. Subsequently, the relative closeness valuesùúç(ùêµùëñ)œÇ(Bi)were computed to measure how close each alternative is to the ideal solution. According to the closeness values presented inTable 8, the five power supply types can be ranked in terms of their techno-economic performance as follows: Hydropower (B2) exhibits the highest closeness value (‚Äì0.4198), indicating the best overall performance. Photovoltaic power (B4) and thermal power (B1) rank second and third, respectively. In contrast, wind power (B3) and energy storage (B5) show relatively low closeness values (‚Äì1.8562 and ‚Äì2.8704, respectively). This suggests that both technologies are less competitive in the current evaluation framework‚Äîparticularly energy storage, which performs poorly across several key indicators, leading to a lower overall ranking.Table 8.Integrated weighted distance between alternatives with PIS and NIS.Additionally, to systematically evaluate the impact of parameterùõøŒ¥, we conducted a sensitivity analysis.Table 9presents the ranking outcomes of various power generation types and their corresponding relative closeness values when parameterùõøŒ¥is assigned different numerical values.Table 9.Comparison of relative closeness and ranking orders under different parameter values.This sensitivity analysis demonstrates that the evaluation results exhibit high robustness when parameter Œ¥ varies within the range of 0.1 to 0.9: the ranking of the five power generation types remains consistentlyB2>B4>B1>B3>B5without any positional changes. As Œ¥ increases, the relative closeness values of all alternatives decrease overall, but their relative gaps remain stable, confirming the method‚Äôs low sensitivity to variations in the proportion of subjective and objective weighting. Notably, hydropower (B2) consistently ranks first, while energy storage (B5) shows a significant gap from the optimal solution (differing by over 2.0), highlighting its current technological and economic disadvantages. 5.3. Comparative Analysis for Model Validation5.3.1. Comparison of Different Distance Measurement MethodsTo verify the effectiveness and superiority of the proposed FFHWD method, a comparative analysis was conducted against two existing distance measures‚ÄîFWAD and FOWD. Specifically, in the third step of the evaluation framework, the FWAD and FOWD measures were, respectively, applied to calculate the distances between each alternative and the Fermatean fuzzy positive (B+) and negative (B‚àí) ideal solutions. Based on these calculations, two comparative evaluation frameworks were established, namely FWAD-TOPSIS and FOWD-TOPSIS. The corresponding relative closeness values of each evaluation object under the two frameworks were then obtained. The results are presented inTable 10andFigure 3.Figure 3.Relative closeness under different frameworks.Table 10.Closenessùúç(ùêµùëñ)œÇ(Bi)of the alternativeBi.As shown by the ranking results above, the evaluation order obtained by FFWAD-TOPSIS isB2>B1>B4>B3>B5, while that of FOWD-TOPSIS isB4>B2>B1>B3>B5. The two methods identifyB2andB4as the optimal alternatives, respectively, and exhibit noticeable differences in their overall rankings. The main reason lies in their weighting mechanisms. FFWAD focuses solely on the objective weights of different criteria and fails to incorporate the experts‚Äô subjective judgments. In contrast, FOWD captures the subjective preferences of decision-makers but overlooks the inherent importance differences among indicators. By comparison, the FFHWD method proposed in this study integrates the strengths of both bounded and arithmetic weighting schemes. It effectively balances subjective and objective information, thereby achieving more comprehensive data integration in the evaluation process and enhancing the rationality and stability of the final ranking results.5.3.2. Comparison of Different Evaluation MethodsTo thoroughly validate the validity and robustness of the proposed FFHWD-TOPSIS evaluation method, a systematic comparison is conducted against three widely adopted multi-criteria decision-making (MCDM) approaches in the field of energy system assessment: the VIKOR method based on the compromise solution principle, the MARCOS method integrating reference ideal solutions, and the classical hierarchical weighting AHP method.Table 11summarizes the ranking outcomes of alternatives derived from these methods.Table 11.Comparative results of different evaluation methods.The comparative results show that all four methods consistently rank hydropower (B2) as the top choice and energy storage (B5) as the least preferred, validating the consensus in evaluation outcomes. The primary discrepancy lies in the ranking of thermal power (B1) and photovoltaics (B4): VIKOR and AHP prioritize thermal power over photovoltaics, whereas MARCOS and the proposed FFHWD-TOPSIS method yield superior rankings for photovoltaics. This discrepancy arises from the proposed method‚Äôs use of Fermatean fuzzy sets to effectively capture higher-order uncertainties in evaluation information, combined with a hybrid weighted distance measure that simultaneously accounts for indicator importance and decision-makers‚Äô risk preferences. This dual mechanism enhances the scientific rigor of weight assignment and the rationality of distance calculations, thereby improving the robustness and interpretability of the ranking results. In contrast, traditional methods exhibit limitations in handling fuzziness and integrating weights, making them more prone to ranking biases.Although the FFHWD-TOPSIS framework and the MARCOS method generate identical ranking outcomes in this specific numerical case, this consistency does not indicate equivalent modeling capabilities or decision robustness between the two approaches. The alignment in final rankings primarily results from the strong dominance relationships among the five alternatives across multiple critical criteria. These inherently stable ranking patterns can be captured by diverse multi-criteria decision-making techniques. More importantly, the methodological strengths of the FFHWD-TOPSIS approach extend beyond consistent results in a single dataset, manifesting more significantly in the following aspects:(1)Enhanced higher-order uncertainty handling capability. While the MARCOS method relies on standardized deterministic numerical inputs, FFHWD-TOPSIS explicitly incorporates Fermatean fuzzy membership degrees, non-membership degrees, and hesitation margins, enabling a more expressive representation of fuzzy expert knowledge.(2)Integrated hybrid weighting mechanism combining subjective and objective elements. The FFHWD measure simultaneously considers both inherent criterion importance and positional risk preferences, overcoming the purely data-driven limitations of MARCOS and effectively mitigating weight conflict sensitivity issues.(3)Superior ranking stability under parameter and data perturbations. As demonstrated in the sensitivity analysis, our approach maintains ranking robustness across variations in mixed weight proportions, whereas the MARCOS method lacks this flexible robustness modeling capability.In summary, FFHWD-TOPSIS does not merely pursue numerical discrepancies with classical approaches on isolated datasets, but demonstrates superior generalization potential and expanded modeling capabilities in handling multi-source uncertainties, conflicting criteria integration, and disturbance resistance. The core objective of this comparative analysis is not to assert universal numerical superiority, but to validate that even under strong dominance structures, the proposed method can generate consistent and interpretable stable outcomes comparable to established MCDM techniques, while equipping evaluators with extended capabilities to address more complex fuzzy environments. This dual advantage constitutes its critical practical value in multi-type power generation assessment for next-generation power systems.",
            "5.1. Selection of Evaluation Indicators": "To validate the effectiveness of the proposed techno-economic evaluation model, five representative power generation technologies were selected as case studies: thermal power (B1), hydropower (B2), wind power (B3), photovoltaics (B4), and energy storage (B5). This combination encompasses traditional fossil fuels, renewable energy sources, and flexible resources, comprehensively reflecting the techno-economic characteristics and structural differences across multi-type power generation systems. The evaluation framework consists of seven core indicators:C1(levelized cost of electricity, LCOE),C2(capacity factor),C3(start-up and regulation characteristics),C4(energy conversion efficiency),C5(carbon emission intensity),C6(specific investment cost), andC7(operational and maintenance cost ratio), which systematically characterize the techno-economic performance of different power sources. Based on this framework, domain experts were invited to assess the interrelationships among the aforementioned indicators using the linguistic variables defined inTable 1. The DEMATEL method was then applied to structurally analyze the evaluation results. As an illustrative example, the data from Expert 1 are presented inTable 2. Table 2.Expert 1‚Äôs influence degree judgments for evaluation indicators. Based on the correspondence between linguistic variables and fuzzy numbers inTable 1, and combined with the data inTable 2, the initial direct influence matrix provided by Expert 1 is as follows:ùêµÃÉ(1)=‚é°‚é£‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢(0,0.1,0.3)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.3,0.5,0.7)(0.7,0.9,1.0)(0.1,0.3,0.5)(0.3,0.5,0.7)(0.3,0.5,0.7)(0,0.1,0.3)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.1,0.3,0.5)(0.1,0.3,0.5)(0.7,0.9,1.0)(0.7,0.9,1.0)(0,0.1,0.3)(0.7,0.9,1.0)(0.7,0.9,1.0)(0.3,0.5,0.7)(0.3,0.5,0.7)(0.5,0.7,0.9)(0.5,0.7,0.9)(0.7,0.9,1.0)(0,0.1,0.3)(0.3,0.5,0.7)(0.5,0.7,0.9)(0.1,0.3,0.5)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.5,0.7,0.9)(0,0.1,0.3)(0.1,0.3,0.5)(0.1,0.3,0.5)(0.7,0.9,1.0)(0.3,0.5,0.7)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.7,0.9,1.0)(0,0.1,0.3)(0.3,0.5,0.7)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.3,0.5,0.7)(0,0.1,0.3)‚é§‚é¶‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•BÀú(1)=(0,0.1,0.3)(0.3,0.5,0.7)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.5,0.7,0.9)(0,0.1,0.3)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.5,0.7,0.9)(0.3,0.5,0.7)(0.7,0.9,1.0)(0.7,0.9,1.0)(0.5,0.7,0.9)(0,0.1,0.3)(0.7,0.9,1.0)(0.7,0.9,1.0)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.3,0.5,0.7)(0.7,0.9,1.0)(0.7,0.9,1.0)(0,0.1,0.3)(0.5,0.7,0.9)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.7,0.9,1.0)(0.3,0.5,0.7)(0,0.1,0.3)(0.7,0.9,1.0)(0.5,0.7,0.9)(0.1,0.3,0.5)(0.1,0.3,0.5)(0.3,0.5,0.7)(0.5,0.7,0.9)(0.1,0.3,0.5)(0,0.1,0.3)(0.3,0.5,0.7)(0.3,0.5,0.7)(0.1,0.3,0.5)(0.3,0.5,0.7)(0.1,0.3,0.5)(0.1,0.3,0.5)(0.3,0.5,0.7)(0,0.1,0.3) Based on Equations (1)‚Äì(9), the initial direct influence matrix can be obtained as follows:ùêµ(1)=‚é°‚é£‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢0.13330.70.86670.50.86670.30.50.50.13330.70.86670.70.30.30.86670.86670.13330.86670.86670.50.50.70.70.86670.13330.50.70.30.86670.70.86670.70.13330.30.30.86670.50.86670.70.86670.13330.50.70.86670.70.86670.70.50.1333‚é§‚é¶‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•B(1)=0.13330.50.86670.70.86670.86670.70.70.13330.86670.70.70.50.86670.86670.70.13330.86670.86670.86670.70.50.86670.86670.13330.70.70.86670.86670.70.86670.50.13330.86670.70.30.30.50.70.30.13330.50.50.30.50.30.30.50.1333 Subsequently, by integrating the opinions of the remaining experts and applying the traditional DEMATEL method, the influence degree, affected degree, causality degree, and centrality of the evaluation indicators were calculated using Equations (10)‚Äì(15), with the detailed results presented inTable 3. Table 3.Specific calculation results of evaluation indicators. The centrality analysis inTable 3indicates that evaluation indicatorC7has the lowest influence on the overall system, followed byC6. To simplify model complexity and focus on core elements, this study retains the five key indicatorsC1‚ÄìC5for subsequent evaluation. Based on this framework, four senior experts in power system planning (E1‚ÄìE4) were invited to conduct evaluations using FFNs. Prior to the formal assessment, experts were provided with a Fermatean Fuzzy Number Operations Manual to define the semantic scales for the membership degreeùúáŒº(‚Äúconfidence in superior indicator performance‚Äù) and non-membership degreeùúàŒΩ(‚Äúconfidence in inferior indicator performance‚Äù). Experts were instructed to adhere to the constraintùúá3+ùúà3‚â§1Œº3+ŒΩ3‚â§1. A structured questionnaire was employed to independently assess FFNs for the five power generation technologies across the five indicators. The questionnaire utilized a 9-point linguistic variable system mapped to FFNs to ensure consistent interpretation among experts. The raw evaluation data are detailed inTable 4. Table 4.Decision matrix of experts. To mitigate potential sampling bias caused by the limited number of participating experts, this study applied 1000 bootstrap iterations to resample the expert evaluation matrices and calculate confidence intervals for the closeness coefficients of all alternatives. The results demonstrate that the rankings of all alternatives remain consistent within the 95% confidence interval, indicating the evaluation results exhibit strong robustness.",
            "5.2. Model Application and Analysis": "Based on the original expert evaluation data presented inTable 1, and considering their respective weights (0.3, 0.2, 0.25, 0.25), a collective decision matrix was obtained through weighted integration. Subsequently, the group evaluation results for the five power generation schemes were calculated, as shown inTable 5. Table 5.Collective decision matrix. Based on the group evaluation results inTable 5, the score functionùëÜ(ùëå)S(Y)for each power generation scheme was first calculated to quantify their overall performance, as illustrated inFigure 2. On this basis, and according to Equations (39) and (40), the positive and negative ideal solutionsB+andB‚àíunder the Fermatean fuzzy environment were determined. The results are summarized inTable 6, providing the foundation for the subsequent distance calculation and scheme ranking. Figure 2.Score functions of each power generation scheme. Table 6.Fermatean fuzzy positive ideal solutionB+and negative ideal solutionB‚àí. To scientifically assess the relative importance of each evaluation indicator in the comprehensive assessment process, the FAHP and EWMs were used to calculate the subjective and objective weights of each indicator, respectively. These were then combined using a fusion weighting approach withùõø=0.5Œ¥=0.5. The results are presented inTable 7. As shown in the table, indicator C4 ranks first in both weighting methods, with a combined weight of 0.3145, indicating that its central role is reinforced by both data variation and expert consensus. Indicator C3 follows with a weight of 0.242, highlighting the key importance of the capacity factor in the evaluation. In contrast, C1 has the smallest weight of 0.087, suggesting a relatively limited influence. Overall, the integrated weighting results strike a balance between subjective judgment and objective data, thereby enhancing the scientific rigor and credibility of the evaluation. Table 7.Weights of index. To evaluate the overall performance of each power supply scheme, the FFHWD distances between each alternative and the Fermatean fuzzy positive and negative ideal solutions,FFHWD(B·µ¢,B+) andFFHWD(B·µ¢,B‚àí), were first calculated. Based on these results, a weight vector was determined using an ordered weighted operator derived from the normal distribution. In this case, the vector was set as (0.112, 0.236, 0.304, 0.236, 0.112)·µÄ. Subsequently, the relative closeness valuesùúç(ùêµùëñ)œÇ(Bi)were computed to measure how close each alternative is to the ideal solution. According to the closeness values presented inTable 8, the five power supply types can be ranked in terms of their techno-economic performance as follows: Hydropower (B2) exhibits the highest closeness value (‚Äì0.4198), indicating the best overall performance. Photovoltaic power (B4) and thermal power (B1) rank second and third, respectively. In contrast, wind power (B3) and energy storage (B5) show relatively low closeness values (‚Äì1.8562 and ‚Äì2.8704, respectively). This suggests that both technologies are less competitive in the current evaluation framework‚Äîparticularly energy storage, which performs poorly across several key indicators, leading to a lower overall ranking. Table 8.Integrated weighted distance between alternatives with PIS and NIS. Additionally, to systematically evaluate the impact of parameterùõøŒ¥, we conducted a sensitivity analysis.Table 9presents the ranking outcomes of various power generation types and their corresponding relative closeness values when parameterùõøŒ¥is assigned different numerical values. Table 9.Comparison of relative closeness and ranking orders under different parameter values. This sensitivity analysis demonstrates that the evaluation results exhibit high robustness when parameter Œ¥ varies within the range of 0.1 to 0.9: the ranking of the five power generation types remains consistentlyB2>B4>B1>B3>B5without any positional changes. As Œ¥ increases, the relative closeness values of all alternatives decrease overall, but their relative gaps remain stable, confirming the method‚Äôs low sensitivity to variations in the proportion of subjective and objective weighting. Notably, hydropower (B2) consistently ranks first, while energy storage (B5) shows a significant gap from the optimal solution (differing by over 2.0), highlighting its current technological and economic disadvantages.",
            "5.3. Comparative Analysis for Model Validation": "5.3.1. Comparison of Different Distance Measurement MethodsTo verify the effectiveness and superiority of the proposed FFHWD method, a comparative analysis was conducted against two existing distance measures‚ÄîFWAD and FOWD. Specifically, in the third step of the evaluation framework, the FWAD and FOWD measures were, respectively, applied to calculate the distances between each alternative and the Fermatean fuzzy positive (B+) and negative (B‚àí) ideal solutions. Based on these calculations, two comparative evaluation frameworks were established, namely FWAD-TOPSIS and FOWD-TOPSIS. The corresponding relative closeness values of each evaluation object under the two frameworks were then obtained. The results are presented inTable 10andFigure 3.Figure 3.Relative closeness under different frameworks.Table 10.Closenessùúç(ùêµùëñ)œÇ(Bi)of the alternativeBi.As shown by the ranking results above, the evaluation order obtained by FFWAD-TOPSIS isB2>B1>B4>B3>B5, while that of FOWD-TOPSIS isB4>B2>B1>B3>B5. The two methods identifyB2andB4as the optimal alternatives, respectively, and exhibit noticeable differences in their overall rankings. The main reason lies in their weighting mechanisms. FFWAD focuses solely on the objective weights of different criteria and fails to incorporate the experts‚Äô subjective judgments. In contrast, FOWD captures the subjective preferences of decision-makers but overlooks the inherent importance differences among indicators. By comparison, the FFHWD method proposed in this study integrates the strengths of both bounded and arithmetic weighting schemes. It effectively balances subjective and objective information, thereby achieving more comprehensive data integration in the evaluation process and enhancing the rationality and stability of the final ranking results. 5.3.2. Comparison of Different Evaluation MethodsTo thoroughly validate the validity and robustness of the proposed FFHWD-TOPSIS evaluation method, a systematic comparison is conducted against three widely adopted multi-criteria decision-making (MCDM) approaches in the field of energy system assessment: the VIKOR method based on the compromise solution principle, the MARCOS method integrating reference ideal solutions, and the classical hierarchical weighting AHP method.Table 11summarizes the ranking outcomes of alternatives derived from these methods.Table 11.Comparative results of different evaluation methods.The comparative results show that all four methods consistently rank hydropower (B2) as the top choice and energy storage (B5) as the least preferred, validating the consensus in evaluation outcomes. The primary discrepancy lies in the ranking of thermal power (B1) and photovoltaics (B4): VIKOR and AHP prioritize thermal power over photovoltaics, whereas MARCOS and the proposed FFHWD-TOPSIS method yield superior rankings for photovoltaics. This discrepancy arises from the proposed method‚Äôs use of Fermatean fuzzy sets to effectively capture higher-order uncertainties in evaluation information, combined with a hybrid weighted distance measure that simultaneously accounts for indicator importance and decision-makers‚Äô risk preferences. This dual mechanism enhances the scientific rigor of weight assignment and the rationality of distance calculations, thereby improving the robustness and interpretability of the ranking results. In contrast, traditional methods exhibit limitations in handling fuzziness and integrating weights, making them more prone to ranking biases.Although the FFHWD-TOPSIS framework and the MARCOS method generate identical ranking outcomes in this specific numerical case, this consistency does not indicate equivalent modeling capabilities or decision robustness between the two approaches. The alignment in final rankings primarily results from the strong dominance relationships among the five alternatives across multiple critical criteria. These inherently stable ranking patterns can be captured by diverse multi-criteria decision-making techniques. More importantly, the methodological strengths of the FFHWD-TOPSIS approach extend beyond consistent results in a single dataset, manifesting more significantly in the following aspects:(1)Enhanced higher-order uncertainty handling capability. While the MARCOS method relies on standardized deterministic numerical inputs, FFHWD-TOPSIS explicitly incorporates Fermatean fuzzy membership degrees, non-membership degrees, and hesitation margins, enabling a more expressive representation of fuzzy expert knowledge.(2)Integrated hybrid weighting mechanism combining subjective and objective elements. The FFHWD measure simultaneously considers both inherent criterion importance and positional risk preferences, overcoming the purely data-driven limitations of MARCOS and effectively mitigating weight conflict sensitivity issues.(3)Superior ranking stability under parameter and data perturbations. As demonstrated in the sensitivity analysis, our approach maintains ranking robustness across variations in mixed weight proportions, whereas the MARCOS method lacks this flexible robustness modeling capability.In summary, FFHWD-TOPSIS does not merely pursue numerical discrepancies with classical approaches on isolated datasets, but demonstrates superior generalization potential and expanded modeling capabilities in handling multi-source uncertainties, conflicting criteria integration, and disturbance resistance. The core objective of this comparative analysis is not to assert universal numerical superiority, but to validate that even under strong dominance structures, the proposed method can generate consistent and interpretable stable outcomes comparable to established MCDM techniques, while equipping evaluators with extended capabilities to address more complex fuzzy environments. This dual advantage constitutes its critical practical value in multi-type power generation assessment for next-generation power systems.",
            "5.3.1. Comparison of Different Distance Measurement Methods": "To verify the effectiveness and superiority of the proposed FFHWD method, a comparative analysis was conducted against two existing distance measures‚ÄîFWAD and FOWD. Specifically, in the third step of the evaluation framework, the FWAD and FOWD measures were, respectively, applied to calculate the distances between each alternative and the Fermatean fuzzy positive (B+) and negative (B‚àí) ideal solutions. Based on these calculations, two comparative evaluation frameworks were established, namely FWAD-TOPSIS and FOWD-TOPSIS. The corresponding relative closeness values of each evaluation object under the two frameworks were then obtained. The results are presented inTable 10andFigure 3. Figure 3.Relative closeness under different frameworks. Table 10.Closenessùúç(ùêµùëñ)œÇ(Bi)of the alternativeBi. As shown by the ranking results above, the evaluation order obtained by FFWAD-TOPSIS isB2>B1>B4>B3>B5, while that of FOWD-TOPSIS isB4>B2>B1>B3>B5. The two methods identifyB2andB4as the optimal alternatives, respectively, and exhibit noticeable differences in their overall rankings. The main reason lies in their weighting mechanisms. FFWAD focuses solely on the objective weights of different criteria and fails to incorporate the experts‚Äô subjective judgments. In contrast, FOWD captures the subjective preferences of decision-makers but overlooks the inherent importance differences among indicators. By comparison, the FFHWD method proposed in this study integrates the strengths of both bounded and arithmetic weighting schemes. It effectively balances subjective and objective information, thereby achieving more comprehensive data integration in the evaluation process and enhancing the rationality and stability of the final ranking results.",
            "5.3.2. Comparison of Different Evaluation Methods": "To thoroughly validate the validity and robustness of the proposed FFHWD-TOPSIS evaluation method, a systematic comparison is conducted against three widely adopted multi-criteria decision-making (MCDM) approaches in the field of energy system assessment: the VIKOR method based on the compromise solution principle, the MARCOS method integrating reference ideal solutions, and the classical hierarchical weighting AHP method.Table 11summarizes the ranking outcomes of alternatives derived from these methods. Table 11.Comparative results of different evaluation methods. The comparative results show that all four methods consistently rank hydropower (B2) as the top choice and energy storage (B5) as the least preferred, validating the consensus in evaluation outcomes. The primary discrepancy lies in the ranking of thermal power (B1) and photovoltaics (B4): VIKOR and AHP prioritize thermal power over photovoltaics, whereas MARCOS and the proposed FFHWD-TOPSIS method yield superior rankings for photovoltaics. This discrepancy arises from the proposed method‚Äôs use of Fermatean fuzzy sets to effectively capture higher-order uncertainties in evaluation information, combined with a hybrid weighted distance measure that simultaneously accounts for indicator importance and decision-makers‚Äô risk preferences. This dual mechanism enhances the scientific rigor of weight assignment and the rationality of distance calculations, thereby improving the robustness and interpretability of the ranking results. In contrast, traditional methods exhibit limitations in handling fuzziness and integrating weights, making them more prone to ranking biases. Although the FFHWD-TOPSIS framework and the MARCOS method generate identical ranking outcomes in this specific numerical case, this consistency does not indicate equivalent modeling capabilities or decision robustness between the two approaches. The alignment in final rankings primarily results from the strong dominance relationships among the five alternatives across multiple critical criteria. These inherently stable ranking patterns can be captured by diverse multi-criteria decision-making techniques. More importantly, the methodological strengths of the FFHWD-TOPSIS approach extend beyond consistent results in a single dataset, manifesting more significantly in the following aspects: (1)Enhanced higher-order uncertainty handling capability. While the MARCOS method relies on standardized deterministic numerical inputs, FFHWD-TOPSIS explicitly incorporates Fermatean fuzzy membership degrees, non-membership degrees, and hesitation margins, enabling a more expressive representation of fuzzy expert knowledge.(2)Integrated hybrid weighting mechanism combining subjective and objective elements. The FFHWD measure simultaneously considers both inherent criterion importance and positional risk preferences, overcoming the purely data-driven limitations of MARCOS and effectively mitigating weight conflict sensitivity issues.(3)Superior ranking stability under parameter and data perturbations. As demonstrated in the sensitivity analysis, our approach maintains ranking robustness across variations in mixed weight proportions, whereas the MARCOS method lacks this flexible robustness modeling capability. In summary, FFHWD-TOPSIS does not merely pursue numerical discrepancies with classical approaches on isolated datasets, but demonstrates superior generalization potential and expanded modeling capabilities in handling multi-source uncertainties, conflicting criteria integration, and disturbance resistance. The core objective of this comparative analysis is not to assert universal numerical superiority, but to validate that even under strong dominance structures, the proposed method can generate consistent and interpretable stable outcomes comparable to established MCDM techniques, while equipping evaluators with extended capabilities to address more complex fuzzy environments. This dual advantage constitutes its critical practical value in multi-type power generation assessment for next-generation power systems.",
            "6. Conclusions": "This study introduces a novel TOPSIS-based framework (FFHWD-TOPSIS) that integrates Fermatean fuzzy sets with a hybrid weighted distance measure and a combined FAHP‚ÄìEWM weighting strategy, addressing higher-order uncertainty and weight conflicts in techno-economic evaluation. The FFHWD measure captures both subjective importance and decision-maker risk preferences and is integrated into an improved TOPSIS model via Fermatean fuzzy positive and negative ideal solutions. (1)Performance ranking: The FFHWD-TOPSIS model was applied to five representative power sources (thermal, hydropower, wind, photovoltaic, and energy storage) using expert evaluations. The computed closeness coefficients rank hydropower (B2) highest in overall performance (best techno-economic score), with photovoltaic (B4) and thermal power (B1) in second and third place, respectively. In contrast, wind power (B3) and energy storage (B5) have much lower closeness values and thus appear less competitive under the current evaluation framework.(2)Comparative analysis: Compared to existing Fermatean-fuzzy TOPSIS variants using FWAD or FOWD distance measures, the proposed FFHWD-TOPSIS yields more balanced and stable rankings. By effectively blending objective and subjective weighting information, the method enhances the rationality and consistency of the final ranking. These findings demonstrate that the FFHWD-TOPSIS framework provides reliable, nuanced decision support for optimizing multi-type power source configurations under uncertainty."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2079-9292/14/23/4770",
        "scraped_at": "2025-12-05 23:55:08"
    },
    {
        "title": "Sensor-Based Cyber Risk Management in Railway Infrastructure Under the NIS2 Directive",
        "authors": "byRafa≈Ç Wachnik,Katarzyna ChruzikandBoles≈Çaw Pochopie≈Ñ",
        "journal": "Sensors2025,25(23), 7384; https://doi.org/10.3390/s25237384 (registering¬†DOI) - 4 Dec 2025",
        "abstract": "This study introduces a sensor-centric cybersecurity framework for railway infrastructure that extends Failure Mode and Effects Analysis (FMEA) from traditional reliability evaluation into the domain of cyber-induced failures affecting data integrity, availability and authenticity. The contribution lies in bridging regulatory obligations of the NIS2 Directive with field-layer monitoring by enabling risk indicators to evolve dynamically rather than remain static documentation artefacts. The approach is demonstrated using a scenario-based dataset collected from approximately 250 trackside, rolling-stock, environmental and power-monitoring sensors deployed over a 25 km operational segment, with representative anomalies generated through controlled spoofing, replay and injection conditions. Risk was evaluated using RPN scores derived from Severity‚ÄìOccurrence‚ÄìDetectability scales, while anomaly-detection performance was observed through detection-latency variation, changes in RPN distribution, and qualitative responsiveness of timestamp-based alerts. Instead of presenting a fixed benchmark, the results show how evidence from real sensor streams can recalibrate O and D factors in near-real-time and reduce undetected exposure windows, enabling measurable compliance documentation aligned with NIS2 Article 21. The findings confirm that coupling FMEA with streaming telemetry creates a verifiable risk-evaluation loop and supports a transition toward continuous, evidence-driven cybersecurity governance in railway systems.Keywords:railway cybersecurity;NIS2 Directive;sensor networks;IoT;FMEA;anomaly detection;machine learning;risk management;critical infrastructure",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "The ongoing digitalization of railway transport has resulted in a rapid expansion of sensor-based systems, which continuously monitor infrastructure condition, train movement, energy consumption, and environmental parameters. These sensors, interconnected through Internet of Things (IoT) networks and supervisory control systems (SCADA), form the foundation of modern railway operations. They enable predictive maintenance, traffic optimization, and real-time safety monitoring. However, the same interconnectedness also introduces new vulnerabilities. A compromised sensor node, falsified measurement, or disrupted data stream can propagate through operational systems, affecting traffic control, rolling stock safety, and signaling integrity [1,2,3,4]. In this context, cybersecurity and data integrity have become essential prerequisites for safe and resilient railway operation. The growing reliance on distributed sensor networks, edge devices, and cloud-based analytics transforms traditional railway infrastructure into a cyber‚Äìphysical system, where the distinction between digital and physical safety is blurred. Consequently, ensuring cybersecurity at the sensor and network level is no longer a purely technical matter but a critical requirement for transport continuity and public safety [5,6,7]. Within the European Union, the NIS2 Directive (Directive (EU) 2022/2555) defines new obligations for operators of essential services, including railway transport [5,6,7,8,9,10]. The Directive mandates a high common level of cybersecurity by emphasizing risk management, monitoring, and the continuous evaluation of control effectiveness. For the railway sector, these obligations apply not only to corporate IT systems but also to operational technology (OT) and embedded sensor networks responsible for monitoring tracks, vehicles, and signaling equipment. While the NIS2 Directive outlines ten categories of cybersecurity measures, two are particularly relevant for sensor-based railway systems: Criterion 1: Risk analysis and information system security policy, andCriterion 6: Policies and procedures for assessing the effectiveness of risk management measures. These two provisions jointly form a continuous improvement loop. Criterion 1 defines how risks, including those originating from sensor networks, are identified, prioritized, and mitigated. Criterion 6 ensures that these risk controls are periodically evaluated and adjusted to address evolving cyber threats. In practice, fulfilling these obligations requires methods capable of linking technical vulnerabilities of sensors and IoT systems with organizational risk governance and compliance monitoring. However, most existing research and industrial practice in railway cybersecurity focus on incident response, supply chain security, or high-level governance models, with less attention given to the sensor-level risk mechanisms that underpin system reliability [11,12,13]. As a result, there is a need for structured analytical tools that can transform abstract compliance requirements into quantifiable risk indicators directly associated with sensor network performance and integrity. Throughout this paper, the following terminology is used consistently: Information Technology (IT), Operational Technology (OT), Supervisory Control and Data Acquisition (SCADA), Internet of Things (IoT), axle counter, point machine, Global System for Mobile Communications‚ÄìRailway (GSM-R), Long-Term Evolution for Machines (LTE-M) and LoRa Wide Area Network (LoRaWAN). Research Objectives and ScopeThis study was designed to address the above research gap by developing and testing a sensor-centric risk assessment framework aligned with NIS2 requirements. The proposed framework extends the Failure Mode and Effects Analysis (FMEA) method‚Äîwidely recognized in safety and reliability engineering‚Äîto the domain of cyber risk management in sensor-based railway infrastructure.The objectives of this study were as follows:To analyze the role of sensor and IoT systems in shaping the cybersecurity posture of railway transport infrastructure.To adapt the FMEA methodology for identifying and prioritizing cyber failure modes affecting data integrity, availability, and authenticity within sensor networks.To map the results of sensor-level risk analysis to NIS2 criteria‚Äîparticularly to the requirements concerning risk analysis (Criterion 1) and effectiveness assessment (Criterion 6).To demonstrate, through a case study, how FMEA-based indicators can support both compliance verification and real-time cybersecurity management in railway IoT environments.By combining regulatory analysis with a sensor-focused risk evaluation, the research sought to bridge the gap between policy-driven compliance and data-driven resilience. The central hypothesis was that FMEA, when applied to distributed sensor systems, can operationalize NIS2 provisions by providing measurable, auditable, and technically grounded indicators of cybersecurity performance.The remainder of the paper is structured as follows.Section 2presents the regulatory and technological context relevant to NIS2 and railway sensor systems.Section 3introduces the process-oriented governance model used to align risk assessment with directive requirements.Section 4describes the adapted FMEA methodology.Section 5outlines the scenario-based case study, andSection 6discusses the analytical workflow for anomaly detection.Section 7provides a process-level integration of the results, andSection 8concludes the paper with implications for NIS2 governance.",
            "Research Objectives and Scope": "This study was designed to address the above research gap by developing and testing a sensor-centric risk assessment framework aligned with NIS2 requirements. The proposed framework extends the Failure Mode and Effects Analysis (FMEA) method‚Äîwidely recognized in safety and reliability engineering‚Äîto the domain of cyber risk management in sensor-based railway infrastructure. The objectives of this study were as follows: To analyze the role of sensor and IoT systems in shaping the cybersecurity posture of railway transport infrastructure.To adapt the FMEA methodology for identifying and prioritizing cyber failure modes affecting data integrity, availability, and authenticity within sensor networks.To map the results of sensor-level risk analysis to NIS2 criteria‚Äîparticularly to the requirements concerning risk analysis (Criterion 1) and effectiveness assessment (Criterion 6).To demonstrate, through a case study, how FMEA-based indicators can support both compliance verification and real-time cybersecurity management in railway IoT environments. By combining regulatory analysis with a sensor-focused risk evaluation, the research sought to bridge the gap between policy-driven compliance and data-driven resilience. The central hypothesis was that FMEA, when applied to distributed sensor systems, can operationalize NIS2 provisions by providing measurable, auditable, and technically grounded indicators of cybersecurity performance. The remainder of the paper is structured as follows.Section 2presents the regulatory and technological context relevant to NIS2 and railway sensor systems.Section 3introduces the process-oriented governance model used to align risk assessment with directive requirements.Section 4describes the adapted FMEA methodology.Section 5outlines the scenario-based case study, andSection 6discusses the analytical workflow for anomaly detection.Section 7provides a process-level integration of the results, andSection 8concludes the paper with implications for NIS2 governance.",
            "2. Regulatory and Technological Context": "The NIS2 Directive [8] establishes a unified legal framework to strengthen cybersecurity across critical infrastructure sectors in the European Union, including transport, energy, and digital services [8,9,10]. Railway operators and infrastructure managers are classified asessential entities, which subjects them to the highest level of obligations in terms of risk management, incident reporting, and system resilience. Article 21 of NIS2 sets out ten categories of cybersecurity measures, emphasizing governance, risk assessment, business continuity, supply chain security, and testing of control effectiveness. Among these, risk analysis (Criterion 1) and evaluation of the effectiveness of measures (Criterion 6) form the operational backbone of a mature cybersecurity system. They require that all technological assets‚Äîranging from corporate IT systems to embedded sensors‚Äîbe analyzed for vulnerabilities, with risks quantified and periodically reviewed based on measurable performance indicators. Instead of describing the transformation of railway cybersecurity as a paradigm shift, the paper now quantifies its drivers: increasing sensor connectivity, anomaly detection-detection latency by >60% in tested scenarios, and measurable change in RPN-class distribution (42% Criterion 1/36% Criterion 6). These metrics demonstrate structural change rather than conceptual evolution. Cybersecurity is no longer limited to the protection of digital data but extends to the integrity and reliability of sensor-generated information, which directly influences train control, infrastructure safety, and service continuity. Therefore, implementing NIS2 within railway operations demands a multi-layered approach encompassing governance, technology, and sensor-level resilience. In modern railway systems, sensors are ubiquitous components responsible for monitoring track conditions, axle loads, wheel vibrations, temperature, energy distribution, and signaling parameters. These devices are typically integrated into IoT ecosystems that communicate through wired or wireless networks, edge gateways, and cloud-based management platforms. While this architecture enhances operational awareness, it simultaneously increases the attack surface. Cyber threats affecting railway sensors and IoT systems can be grouped into three main categories [1,5,14]: Data integrity attacks, such as spoofing, injection, or tampering with measurement values. These can mislead control systems or maintenance algorithms, leading to unsafe operational decisions.Availability disruptions, including denial-of-service (DoS) attacks or network congestion, which can delay sensor communication or disable critical monitoring functions.Confidentiality breaches, where unauthorized access to sensor data may reveal sensitive operational parameters or be used for targeted attacks on rolling stock and signaling subsystems. The NIS2 Directive implicitly encompasses these risks through its emphasis on technical and organizational measures for ensuring system security. However, translating regulatory requirements into technical implementations for sensor systems requires a structured risk assessment methodology. Tools such as the Failure Mode and Effects Analysis (FMEA) can provide a bridge between compliance mandates and technical evaluation by quantifying the likelihood, severity, and detectability of cyber-related sensor failures. Moreover, under NIS2, operators must demonstrate evidence-based compliance‚Äîfor example, by documenting risk assessments, performance indicators (KPIs/KRIs), and audit trails. In the context of sensor networks, this means that the cybersecurity of each sensing element, its communication interface, and its data-handling processes must be verifiable and traceable. Challenges of Integrating IT, OT, and Sensor Data in RailwaysThe cybersecurity landscape of the railway sector is defined by the coexistence of information technology (IT), operational technology (OT), and sensor networks‚Äîeach governed by different architectures, standards, and life cycles [6,7,15,16,17].IT systems (e.g., enterprise networks, scheduling systems) follow rapid update cycles and are usually well covered by cybersecurity standards such as ISO/IEC 27001 [18].OT systems (e.g., signaling, interlocking, traction power) are characterized by long operational lifetimes and high safety-criticality, with strict certification processes.Sensor networks and IoT devices bridge these two worlds, introducing connectivity between digital management systems and physical infrastructure, often via wireless links or edge computing gateways.This hybrid architecture introduces multiple integration challenges:Heterogeneity of communication protocols‚Äîsensors may use protocols such as M-Bus, Modbus, or MQTT, which vary in encryption capabilities and authentication mechanisms.Legacy interoperability‚Äîmany railway OT systems were not designed with cybersecurity in mind, creating compatibility issues when connecting with modern IoT devices.Limited computational resources at the edge‚Äîlow-power sensors often cannot support advanced encryption or intrusion detection mechanisms.Data synchronization and validation‚Äîensuring that data transmitted from thousands of sensors remains accurate, timestamped, and tamper-proof requires robust validation mechanisms.Security monitoring and anomaly detection‚Äîtraditional SIEM tools are insufficient for real-time analysis of sensor data streams; instead, AI-based or ML-based anomaly detection is increasingly necessary.These challenges directly relate to the NIS2 obligation of continuous improvement. To comply effectively, railway operators must establish monitoring systems capable of real-time sensor data analysis, early threat detection, and automatic correlation between sensor anomalies and cybersecurity incidents.Technological convergence between IT, OT, and IoT therefore requires a sensor-centric cybersecurity architecture‚Äîone that combines device-level protection, encrypted communication, and centralized monitoring of integrity and performance. In this architecture, FMEA-based risk assessment serves as a foundation for identifying critical failure modes in sensor systems and for aligning mitigation priorities with the regulatory requirements of NIS2.",
            "Challenges of Integrating IT, OT, and Sensor Data in Railways": "The cybersecurity landscape of the railway sector is defined by the coexistence of information technology (IT), operational technology (OT), and sensor networks‚Äîeach governed by different architectures, standards, and life cycles [6,7,15,16,17]. IT systems (e.g., enterprise networks, scheduling systems) follow rapid update cycles and are usually well covered by cybersecurity standards such as ISO/IEC 27001 [18].OT systems (e.g., signaling, interlocking, traction power) are characterized by long operational lifetimes and high safety-criticality, with strict certification processes.Sensor networks and IoT devices bridge these two worlds, introducing connectivity between digital management systems and physical infrastructure, often via wireless links or edge computing gateways. This hybrid architecture introduces multiple integration challenges: Heterogeneity of communication protocols‚Äîsensors may use protocols such as M-Bus, Modbus, or MQTT, which vary in encryption capabilities and authentication mechanisms.Legacy interoperability‚Äîmany railway OT systems were not designed with cybersecurity in mind, creating compatibility issues when connecting with modern IoT devices.Limited computational resources at the edge‚Äîlow-power sensors often cannot support advanced encryption or intrusion detection mechanisms.Data synchronization and validation‚Äîensuring that data transmitted from thousands of sensors remains accurate, timestamped, and tamper-proof requires robust validation mechanisms.Security monitoring and anomaly detection‚Äîtraditional SIEM tools are insufficient for real-time analysis of sensor data streams; instead, AI-based or ML-based anomaly detection is increasingly necessary. These challenges directly relate to the NIS2 obligation of continuous improvement. To comply effectively, railway operators must establish monitoring systems capable of real-time sensor data analysis, early threat detection, and automatic correlation between sensor anomalies and cybersecurity incidents. Technological convergence between IT, OT, and IoT therefore requires a sensor-centric cybersecurity architecture‚Äîone that combines device-level protection, encrypted communication, and centralized monitoring of integrity and performance. In this architecture, FMEA-based risk assessment serves as a foundation for identifying critical failure modes in sensor systems and for aligning mitigation priorities with the regulatory requirements of NIS2.",
            "3. Architecture of Railway Sensor System": "3.1. Types of Sensors and Their FunctionsModern railway operations rely on an extensive ecosystem of smart sensors that continuously collect, transmit, and process information crucial for safety, efficiency, and predictive maintenance. These sensors can be categorized into four functional domains, each with distinct cybersecurity implications:Infrastructure and Track Sensors‚ÄîThese devices monitor rail geometry, axle loads, temperature, vibration, and structural integrity of tracks and bridges. Examples include accelerometers, strain gauges, and fiber-optic sensors embedded in the track bed. They often operate in harsh environmental conditions and transmit data via wired or wireless networks to local control units.‚óãCyber relevance: data spoofing or unauthorized calibration could lead to false alarms or undetected degradation of track stability.Rolling Stock Sensors‚ÄîInstalled on locomotives and wagons, these sensors capture wheel speed, traction motor performance, brake pressure, and onboard diagnostics. Data are typically transmitted to fleet management systems through mobile networks or train-to-ground communication.‚óãCyber relevance: attacks on onboard sensors or communication channels can result in manipulated diagnostics or disrupted predictive maintenance cycles.Signaling and Control Sensors‚ÄîThese include axle counters, point machines, and interlocking sensors that support train detection and signal aspect management. Their communication with control centers occurs through real-time, safety-critical networks.‚óãCyber relevance: spoofed sensor signals can compromise train separation logic, leading to operational hazards.Environmental and Energy Sensors‚ÄîThese systems track temperature, humidity, wind, and energy distribution within stations and traction substations.‚óãCyber relevance: false readings or denial-of-service attacks could affect energy balancing or trigger unnecessary shutdowns.Each of these sensor categories contributes to a multi-layered data environment, where information from thousands of distributed nodes is aggregated, analyzed, and used for operational decision-making. The diversity of data types and transmission media introduces complexity that must be systematically addressed through both risk assessment and cybersecurity governance in compliance with NIS2. 3.2. Data Flow and Communication LayersThe architecture of railway sensor systems can be conceptualized as a four-tier model comprising edge, communication, control, and cloud layers (Figure 1). Each layer performs distinct functions and faces unique cybersecurity challenges.Figure 1.Layered architecture of a sensor-based cybersecurity system for railway infrastructure.Sensor and Edge Layer‚ÄîThis layer includes the physical sensors and local processing units that collect and pre-process raw data. Edge devices are responsible for filtering, normalization, and short-term buffering of data. They typically operate under limited computational power and memory constraints.‚óãCybersecurity challenges: lack of encryption, weak firmware protection, and physical accessibility to edge devices increase the risk of tampering and unauthorized firmware updates.Communication Layer‚ÄîThis layer links sensors and controllers through a combination of wired (Ethernet, RS485, M-Bus) and wireless technologies (Wi-Fi, GSM-R, LTE-M, 5G, LoRaWAN). Communication protocols vary in their level of security‚Äîwhile modern standards provide encryption and authentication, legacy systems often rely on unencrypted channels.‚óãCybersecurity challenges: vulnerability to man-in-the-middle attacks, signal jamming, and unauthorized interception of telemetry data.Control and Supervisory Layer‚ÄîAt this level, SCADA systems and interlocking controllers aggregate sensor data and generate operational commands. Real-time processing at this layer enables train routing, traffic control, and fault detection.‚óãCybersecurity challenges: manipulation of control signals, injection of false data, and lateral movement of attackers between IT and OT networks.Cloud and Analytics Layer‚ÄîData from control centers are transmitted to cloud-based analytics platforms for storage, machine learning (ML), and long-term performance optimization. Predictive maintenance models use this aggregated data to identify anomalies and optimize operations.‚óãCybersecurity challenges: ensuring integrity and confidentiality of large data sets, access management for multi-tenant platforms, and compliance with GDPR and NIS2 reporting obligations.To maintain the process-oriented scope of the study, the edge and cloud analytical components are represented as conceptual modules rather than full architecture diagrams. The edge module performs lightweight preprocessing (windowed statistics, 1‚Äì2-layer autoencoder compression, threshold-based deviation scores), while the cloud module aggregates multi-sensor data using clustering and correlation analysis. This conceptual representation highlights the analytical flow‚Äîpreprocessing ‚Üí anomaly scoring ‚Üí FMEA parameter update‚Äîwithout specifying vendor-specific or implementation-specific model architectures.This layered structure ensures operational efficiency but also propagates cascading vulnerabilities‚Äîa compromise at the edge or communication layer can propagate upward, affecting decision-making at supervisory or cloud levels.The proposed NIS2‚ÄìFMEA mapping is intentionally process-oriented. Rather than providing a technical audit trail, the model establishes a traceable linkage between regulatory requirements (Article 21), risk categories (impact‚Äìlikelihood‚Äìdetectability), and sensor-derived evidence streams. The chain of evidence is therefore conceptual but verifiable at the procedural level: each FMEA dimension corresponds to a documented NIS2 obligation (e.g., monitoring, anomaly detection, incident handling), and each is supported by measurable artefacts, such as sensor logs, anomaly alerts, or detection timestamps, which can be referenced during compliance assessments.Consequently, defense-in-depth strategies must be implemented, encompassing encryption, segmentation, anomaly detection, and continuous verification of sensor data integrity. 3.3. Cyber Threat Landscape in Railway Sensor NetworksThe interconnection of IT, OT, and IoT components in railway systems introduces a broad and evolving cyber threat landscape, which can be categorized according to the primary target and intended impact:Sensor Data Manipulation Attacks (Integrity)‚ÄîAdversaries may alter measurement values, replay valid data packets, or inject false readings. Such attacks can mislead algorithms responsible for predictive maintenance or condition monitoring. In railway contexts, falsified axle counter data or incorrect vibration readings could trigger erroneous control actions.Denial-of-Service and Jamming (Availability)‚ÄîAttackers may overload communication channels or jam radio frequencies, preventing timely transmission of sensor data. In critical systems, even short interruptions can lead to operational delays or degraded safety margins.Unauthorized Access and Malware Infiltration (Confidentiality and Control)‚ÄîWeak authentication or outdated firmware can allow unauthorized access to sensor gateways. Once compromised, adversaries can use these devices as entry points into OT networks.Supply Chain Exploitation‚ÄîAs sensors are sourced from multiple vendors, compromised firmware or counterfeit components may introduce hidden backdoors. This risk is particularly relevant under NIS2, which emphasizes the need for supply chain risk management.Data Correlation and Privacy Risks‚ÄîAggregated sensor data can inadvertently reveal sensitive operational patterns. Protecting data privacy and limiting unauthorized correlation across systems are emerging regulatory concerns.These threats underscore the necessity of implementing continuous monitoring and sensor-level anomaly detection mechanisms. Real-time analytics can identify abnormal patterns in sensor behavior, signal inconsistencies, or timing anomalies that indicate potential compromise.Under the NIS2 framework, such monitoring directly supports Criterion 6‚Äîassessment of effectiveness, by enabling operators to demonstrate the functionality and reliability of cybersecurity controls. Furthermore, risk identification through structured approaches like FMEA provides a quantitative basis for prioritizing mitigation measures and integrating them into an organization‚Äôs cybersecurity policy (Criterion 1).",
            "3.1. Types of Sensors and Their Functions": "Modern railway operations rely on an extensive ecosystem of smart sensors that continuously collect, transmit, and process information crucial for safety, efficiency, and predictive maintenance. These sensors can be categorized into four functional domains, each with distinct cybersecurity implications: Infrastructure and Track Sensors‚ÄîThese devices monitor rail geometry, axle loads, temperature, vibration, and structural integrity of tracks and bridges. Examples include accelerometers, strain gauges, and fiber-optic sensors embedded in the track bed. They often operate in harsh environmental conditions and transmit data via wired or wireless networks to local control units.‚óãCyber relevance: data spoofing or unauthorized calibration could lead to false alarms or undetected degradation of track stability.Rolling Stock Sensors‚ÄîInstalled on locomotives and wagons, these sensors capture wheel speed, traction motor performance, brake pressure, and onboard diagnostics. Data are typically transmitted to fleet management systems through mobile networks or train-to-ground communication.‚óãCyber relevance: attacks on onboard sensors or communication channels can result in manipulated diagnostics or disrupted predictive maintenance cycles.Signaling and Control Sensors‚ÄîThese include axle counters, point machines, and interlocking sensors that support train detection and signal aspect management. Their communication with control centers occurs through real-time, safety-critical networks.‚óãCyber relevance: spoofed sensor signals can compromise train separation logic, leading to operational hazards.Environmental and Energy Sensors‚ÄîThese systems track temperature, humidity, wind, and energy distribution within stations and traction substations.‚óãCyber relevance: false readings or denial-of-service attacks could affect energy balancing or trigger unnecessary shutdowns. Each of these sensor categories contributes to a multi-layered data environment, where information from thousands of distributed nodes is aggregated, analyzed, and used for operational decision-making. The diversity of data types and transmission media introduces complexity that must be systematically addressed through both risk assessment and cybersecurity governance in compliance with NIS2.",
            "3.2. Data Flow and Communication Layers": "The architecture of railway sensor systems can be conceptualized as a four-tier model comprising edge, communication, control, and cloud layers (Figure 1). Each layer performs distinct functions and faces unique cybersecurity challenges. Figure 1.Layered architecture of a sensor-based cybersecurity system for railway infrastructure. Sensor and Edge Layer‚ÄîThis layer includes the physical sensors and local processing units that collect and pre-process raw data. Edge devices are responsible for filtering, normalization, and short-term buffering of data. They typically operate under limited computational power and memory constraints.‚óãCybersecurity challenges: lack of encryption, weak firmware protection, and physical accessibility to edge devices increase the risk of tampering and unauthorized firmware updates.Communication Layer‚ÄîThis layer links sensors and controllers through a combination of wired (Ethernet, RS485, M-Bus) and wireless technologies (Wi-Fi, GSM-R, LTE-M, 5G, LoRaWAN). Communication protocols vary in their level of security‚Äîwhile modern standards provide encryption and authentication, legacy systems often rely on unencrypted channels.‚óãCybersecurity challenges: vulnerability to man-in-the-middle attacks, signal jamming, and unauthorized interception of telemetry data.Control and Supervisory Layer‚ÄîAt this level, SCADA systems and interlocking controllers aggregate sensor data and generate operational commands. Real-time processing at this layer enables train routing, traffic control, and fault detection.‚óãCybersecurity challenges: manipulation of control signals, injection of false data, and lateral movement of attackers between IT and OT networks.Cloud and Analytics Layer‚ÄîData from control centers are transmitted to cloud-based analytics platforms for storage, machine learning (ML), and long-term performance optimization. Predictive maintenance models use this aggregated data to identify anomalies and optimize operations.‚óãCybersecurity challenges: ensuring integrity and confidentiality of large data sets, access management for multi-tenant platforms, and compliance with GDPR and NIS2 reporting obligations. To maintain the process-oriented scope of the study, the edge and cloud analytical components are represented as conceptual modules rather than full architecture diagrams. The edge module performs lightweight preprocessing (windowed statistics, 1‚Äì2-layer autoencoder compression, threshold-based deviation scores), while the cloud module aggregates multi-sensor data using clustering and correlation analysis. This conceptual representation highlights the analytical flow‚Äîpreprocessing ‚Üí anomaly scoring ‚Üí FMEA parameter update‚Äîwithout specifying vendor-specific or implementation-specific model architectures. This layered structure ensures operational efficiency but also propagates cascading vulnerabilities‚Äîa compromise at the edge or communication layer can propagate upward, affecting decision-making at supervisory or cloud levels. The proposed NIS2‚ÄìFMEA mapping is intentionally process-oriented. Rather than providing a technical audit trail, the model establishes a traceable linkage between regulatory requirements (Article 21), risk categories (impact‚Äìlikelihood‚Äìdetectability), and sensor-derived evidence streams. The chain of evidence is therefore conceptual but verifiable at the procedural level: each FMEA dimension corresponds to a documented NIS2 obligation (e.g., monitoring, anomaly detection, incident handling), and each is supported by measurable artefacts, such as sensor logs, anomaly alerts, or detection timestamps, which can be referenced during compliance assessments. Consequently, defense-in-depth strategies must be implemented, encompassing encryption, segmentation, anomaly detection, and continuous verification of sensor data integrity.",
            "3.3. Cyber Threat Landscape in Railway Sensor Networks": "The interconnection of IT, OT, and IoT components in railway systems introduces a broad and evolving cyber threat landscape, which can be categorized according to the primary target and intended impact: Sensor Data Manipulation Attacks (Integrity)‚ÄîAdversaries may alter measurement values, replay valid data packets, or inject false readings. Such attacks can mislead algorithms responsible for predictive maintenance or condition monitoring. In railway contexts, falsified axle counter data or incorrect vibration readings could trigger erroneous control actions.Denial-of-Service and Jamming (Availability)‚ÄîAttackers may overload communication channels or jam radio frequencies, preventing timely transmission of sensor data. In critical systems, even short interruptions can lead to operational delays or degraded safety margins.Unauthorized Access and Malware Infiltration (Confidentiality and Control)‚ÄîWeak authentication or outdated firmware can allow unauthorized access to sensor gateways. Once compromised, adversaries can use these devices as entry points into OT networks.Supply Chain Exploitation‚ÄîAs sensors are sourced from multiple vendors, compromised firmware or counterfeit components may introduce hidden backdoors. This risk is particularly relevant under NIS2, which emphasizes the need for supply chain risk management.Data Correlation and Privacy Risks‚ÄîAggregated sensor data can inadvertently reveal sensitive operational patterns. Protecting data privacy and limiting unauthorized correlation across systems are emerging regulatory concerns. These threats underscore the necessity of implementing continuous monitoring and sensor-level anomaly detection mechanisms. Real-time analytics can identify abnormal patterns in sensor behavior, signal inconsistencies, or timing anomalies that indicate potential compromise. Under the NIS2 framework, such monitoring directly supports Criterion 6‚Äîassessment of effectiveness, by enabling operators to demonstrate the functionality and reliability of cybersecurity controls. Furthermore, risk identification through structured approaches like FMEA provides a quantitative basis for prioritizing mitigation measures and integrating them into an organization‚Äôs cybersecurity policy (Criterion 1).",
            "4. Methodology: FMEA-Based Risk Assessment for Sensor Networks": "4.1. Adapting FMEA to Sensor Data Integrity and AvailabilityThe Failure Mode and Effects Analysis (FMEA) is a structured, quantitative method used to identify, assess, and prioritize potential failure modes in complex systems. In the context of railway sensor networks, the method was adapted to evaluate not only hardware or software failures but also cyber-induced disruptions that affect the integrity, availability, or authenticity of sensor data [15,18,19].Unlike traditional FMEA applications, which focus on mechanical or process reliability, the sensor-centric FMEA introduced in this study integrates cybersecurity risk factors into the evaluation matrix. Each failure mode corresponds to a potential cyber event or system condition capable of distorting, interrupting, or degrading sensor-based functions. Examples include sensor spoofing, data injection, unauthorized firmware updates, or communication link jamming.The central advantage of FMEA lies in its ability to translate qualitative cybersecurity observations into quantifiable indicators‚Äîthe Risk Priority Number (RPN)‚Äîthat support compliance with NIS2 Article 21. By measuring the severity (S), occurrence (O), and detectability (D) of cyber-related sensor failures, the method produces a prioritized list of vulnerabilities that can be directly mapped to regulatory criteria. 4.2. Parameters and Metrics (Severity, Occurrence, Detection)The adapted FMEA procedure consists of four main stages:Identification of sensor-level failure modesAssessment of risk parameters (S, O, D)Computation of RPN valuesMapping of risks to NIS2 obligationsEach parameter was defined with respect to sensor network cybersecurity as follows:Severity (S): Assesses the operational and safety impact of a compromised sensor or data stream. High values indicate scenarios where false or missing data could lead to service disruption, degraded safety margins, or violation of NIS2 obligations (e.g., failure to ensure availability and integrity of systems).‚óãExample: tampering with track vibration sensor data leading to missed detection of rail defects (S = 9‚Äì10).Occurrence (O): Reflects the estimated frequency or probability of the threat materializing, based on historical incident data, vulnerability reports, and expert assessment.‚óãExample: moderate probability of network packet injection in unencrypted communication channels (O = 6‚Äì8).Detection (D): Represents the likelihood that the anomaly will be detected before it causes operational impact. Low detectability (high D values) typically indicates insufficient monitoring, absence of anomaly detection algorithms, or limited visibility in IoT subsystems.‚óãExample: low probability of detecting spoofed sensor data without cross-validation mechanisms (D = 8‚Äì10).The Risk Priority Number (RPN) is calculated as:RPN=ùëÜ√óùëÇ√óùê∑RPN=S√óO√óDwhere each parameter is rated on a 1‚Äì10 scale. Higher RPN values indicate higher risk and therefore higher priority for mitigation.The mapping of cybersecurity indicators to FMEA scales is performed as follows:(i) Mapping to Occurrence (O):CVSS Exploitability 0.1‚Äì1.9 ‚Üí O = 2‚Äì3 (low likelihood)CVSS Exploitability 2.0‚Äì5.9 ‚Üí O = 4‚Äì7 (moderate)CVSS Exploitability 6.0‚Äì10.0 ‚Üí O = 8‚Äì10 (high)This mapping was validated through expert elicitation using historical incidents documented by ENISA [7,14].(ii) Mapping to Detectability (D):Systems with redundant sensing, timestamp cross-checking, and cryptographic signatures ‚Üí D = 1‚Äì3Systems with partial logging or incomplete timestamp consistency ‚Üí D = 4‚Äì7Systems lacking integrity checks, relying on single-path telemetry ‚Üí D = 8‚Äì10These ranges reflect the monitoring maturity assessments commonly used in NIS2 conformity audits.(iii) Mapping to Severity (S):CVSS Impact ‚â§ 3.9 ‚Üí S = 3‚Äì44.0‚Äì6.9 ‚Üí S = 5‚Äì7‚â•7.0 ‚Üí S = 8‚Äì10Severity is tied to operational safety impact and safety margin reduction defined in EN 50126/50129 [20,21] risk categories.Based on the resulting RPN scores, risks are classified into three categories:Rpn RangeRisk LevelInterpretation1‚Äì120AcceptableManaged through routine monitoring121‚Äì150TolerableRequires improvement actions151‚Äì1000UnacceptableImmediate corrective action requiredThis classification supports structured decision-making by linking technical risk assessment with regulatory compliance documentation required by NIS2 Article 21 (2).The thresholds used to classify RPN values into ‚Äúacceptable,‚Äù ‚Äútolerable,‚Äù and ‚Äúunacceptable‚Äù categories were calibrated through three rounds of expert elicitation with specialists in signaling, IoT security, and OT risk assurance. The ranges (‚â§120; 121‚Äì150; ‚â•151) reflect the harmonization of traditional engineering FMEA interpretations [15] with cybersecurity impact levels inspired by CVSS scoring tiers and NIS2 Article 21 (2) treatment requirements. The calibration was intentionally conservative: any scenario with O or D ‚â• 8 automatically pushes the RPN into the ‚Äúunacceptable‚Äù tier, even with S in the medium range, reflecting the high regulatory importance of detectability under NIS2.To ensure auditability and methodological transparency, the scoring scales used in the adapted FMEA model were calibrated using a structured expert-based mapping to established cybersecurity taxonomies, including CVSS v3.1 severity metrics and CWE weakness categories. Severity (S) was aligned with the CVSS ‚ÄúImpact Sub score,‚Äù whereas Occurrence (O) incorporated the CVSS ‚ÄúExploitability Sub score‚Äù and historical CWE frequency patterns. Detectability (D) was calibrated through expert assessment of monitoring capabilities, log completeness, timestamp consistency, redundancy levels, and OT/IoT diagnostic mechanisms. Each of these elements is routinely used in compliance assessments performed by national competent authorities under NIS2, allowing the scoring model to be auditable and reproducible at the procedural level. 4.3. Mapping FMEA Results to NIS2 Article 21 RequirementsThe calibration steps, mapping tables, and justification of thresholds were documented in a structured expert assessment protocol and are therefore auditable according to NIS2 governance requirements. Each FMEA parameter (S, O, D) is linked to explicit evidence categories: CVSS metrics, CWE categories, sensor monitoring artefacts, log completeness, and redundancy configurations. This creates a clear evidence chain for conformity assessments performed by national authorities, ensuring that the risk scoring process is transparent and repeatable even if the underlying datasets are not publicly released.To operationalize the link between sensor-level risks and legal compliance, the FMEA outputs were mapped to the relevant NIS2 criteria. The analysis focused particularly on two categories:Criterion 1‚ÄîRisk Analysis and Security PolicyCorresponds to risks arising from inadequate identification, documentation, or governance of vulnerabilities in sensor networks. Failure modes such asabsence of asset inventory,undefined responsibilities for IoT security, orlack of firmware update policyfall into this group.‚óãFMEA linkage: high S and O scores, combined with moderate detectability, indicate systemic policy deficiencies that hinder early identification of threats.‚óãNIS2 compliance implication: organizations must implement structured risk analysis procedures that encompass all IoT and OT assets.Criterion 6‚ÄîEffectiveness Assessment of Risk Management MeasuresRelates to the evaluation of controls applied to mitigate sensor-related threats. Failure modes such as missing performance indicators, ineffective anomaly detection algorithms, or absence of post-incident review reflect weaknesses in the feedback loop required by NIS2.‚óãFMEA linkage: high D values (poor detectability) suggest insufficient monitoring and lack of continuous evaluation mechanisms.‚óãNIS2 compliance implication: the organization must establish metrics (KPIs/KRIs) and periodic reviews to demonstrate that implemented controls remain effective.The results obtained through the FMEA procedure allowed for the quantitative identification of cyber-induced failure modes at the sensor level. These results were subsequently prioritized using the calculated Risk Priority Number (RPN) to distinguish the most critical vulnerabilities affecting data integrity, availability, and authenticity.Table 1summarizes representative examples of the identified failure modes, their corresponding RPN values, and associated NIS2 criteria. The inclusion of this table supports transparency of the analytical process and ensures traceability between technical observations, quantified cybersecurity risks, and proposed mitigation strategies.Table 1.Risk Priority Numbers (RPN) calculated for representative cyber failure modes in railway sensor networks. 4.4. Methodological Integration with Real-Time Sensor MonitoringTo ensure scalability and automation, the proposed FMEA model can be integrated with real-time monitoring systems that process sensor data streams from IoT and OT devices. Continuous analysis of anomalies, deviations, or missing data packets provides feedback to update the FMEA parameters dynamically.For instance:a sudden increase in communication errors may elevate the Occurrence (O) parameter;delayed detection of an anomaly increases Detection (D);and a recurring fault type raises the overall RPN priority.This integration creates a closed-loop system where sensor data not only support operational decisions but also inform cyber risk management and regulatory compliance tracking. Such linkage between quantitative FMEA indicators and live monitoring aligns directly withSensors‚Äô research focus on data-driven risk assessment and sensor-enabled decision support.",
            "4.1. Adapting FMEA to Sensor Data Integrity and Availability": "The Failure Mode and Effects Analysis (FMEA) is a structured, quantitative method used to identify, assess, and prioritize potential failure modes in complex systems. In the context of railway sensor networks, the method was adapted to evaluate not only hardware or software failures but also cyber-induced disruptions that affect the integrity, availability, or authenticity of sensor data [15,18,19]. Unlike traditional FMEA applications, which focus on mechanical or process reliability, the sensor-centric FMEA introduced in this study integrates cybersecurity risk factors into the evaluation matrix. Each failure mode corresponds to a potential cyber event or system condition capable of distorting, interrupting, or degrading sensor-based functions. Examples include sensor spoofing, data injection, unauthorized firmware updates, or communication link jamming. The central advantage of FMEA lies in its ability to translate qualitative cybersecurity observations into quantifiable indicators‚Äîthe Risk Priority Number (RPN)‚Äîthat support compliance with NIS2 Article 21. By measuring the severity (S), occurrence (O), and detectability (D) of cyber-related sensor failures, the method produces a prioritized list of vulnerabilities that can be directly mapped to regulatory criteria.",
            "4.2. Parameters and Metrics (Severity, Occurrence, Detection)": "The adapted FMEA procedure consists of four main stages: Identification of sensor-level failure modesAssessment of risk parameters (S, O, D)Computation of RPN valuesMapping of risks to NIS2 obligations Each parameter was defined with respect to sensor network cybersecurity as follows: Severity (S): Assesses the operational and safety impact of a compromised sensor or data stream. High values indicate scenarios where false or missing data could lead to service disruption, degraded safety margins, or violation of NIS2 obligations (e.g., failure to ensure availability and integrity of systems).‚óãExample: tampering with track vibration sensor data leading to missed detection of rail defects (S = 9‚Äì10).Occurrence (O): Reflects the estimated frequency or probability of the threat materializing, based on historical incident data, vulnerability reports, and expert assessment.‚óãExample: moderate probability of network packet injection in unencrypted communication channels (O = 6‚Äì8).Detection (D): Represents the likelihood that the anomaly will be detected before it causes operational impact. Low detectability (high D values) typically indicates insufficient monitoring, absence of anomaly detection algorithms, or limited visibility in IoT subsystems.‚óãExample: low probability of detecting spoofed sensor data without cross-validation mechanisms (D = 8‚Äì10). The Risk Priority Number (RPN) is calculated as:RPN=ùëÜ√óùëÇ√óùê∑RPN=S√óO√óDwhere each parameter is rated on a 1‚Äì10 scale. Higher RPN values indicate higher risk and therefore higher priority for mitigation. The mapping of cybersecurity indicators to FMEA scales is performed as follows: (i) Mapping to Occurrence (O): CVSS Exploitability 0.1‚Äì1.9 ‚Üí O = 2‚Äì3 (low likelihood)CVSS Exploitability 2.0‚Äì5.9 ‚Üí O = 4‚Äì7 (moderate)CVSS Exploitability 6.0‚Äì10.0 ‚Üí O = 8‚Äì10 (high) This mapping was validated through expert elicitation using historical incidents documented by ENISA [7,14]. (ii) Mapping to Detectability (D): Systems with redundant sensing, timestamp cross-checking, and cryptographic signatures ‚Üí D = 1‚Äì3Systems with partial logging or incomplete timestamp consistency ‚Üí D = 4‚Äì7Systems lacking integrity checks, relying on single-path telemetry ‚Üí D = 8‚Äì10 These ranges reflect the monitoring maturity assessments commonly used in NIS2 conformity audits. (iii) Mapping to Severity (S): CVSS Impact ‚â§ 3.9 ‚Üí S = 3‚Äì44.0‚Äì6.9 ‚Üí S = 5‚Äì7‚â•7.0 ‚Üí S = 8‚Äì10 Severity is tied to operational safety impact and safety margin reduction defined in EN 50126/50129 [20,21] risk categories. Based on the resulting RPN scores, risks are classified into three categories:Rpn RangeRisk LevelInterpretation1‚Äì120AcceptableManaged through routine monitoring121‚Äì150TolerableRequires improvement actions151‚Äì1000UnacceptableImmediate corrective action required This classification supports structured decision-making by linking technical risk assessment with regulatory compliance documentation required by NIS2 Article 21 (2). The thresholds used to classify RPN values into ‚Äúacceptable,‚Äù ‚Äútolerable,‚Äù and ‚Äúunacceptable‚Äù categories were calibrated through three rounds of expert elicitation with specialists in signaling, IoT security, and OT risk assurance. The ranges (‚â§120; 121‚Äì150; ‚â•151) reflect the harmonization of traditional engineering FMEA interpretations [15] with cybersecurity impact levels inspired by CVSS scoring tiers and NIS2 Article 21 (2) treatment requirements. The calibration was intentionally conservative: any scenario with O or D ‚â• 8 automatically pushes the RPN into the ‚Äúunacceptable‚Äù tier, even with S in the medium range, reflecting the high regulatory importance of detectability under NIS2. To ensure auditability and methodological transparency, the scoring scales used in the adapted FMEA model were calibrated using a structured expert-based mapping to established cybersecurity taxonomies, including CVSS v3.1 severity metrics and CWE weakness categories. Severity (S) was aligned with the CVSS ‚ÄúImpact Sub score,‚Äù whereas Occurrence (O) incorporated the CVSS ‚ÄúExploitability Sub score‚Äù and historical CWE frequency patterns. Detectability (D) was calibrated through expert assessment of monitoring capabilities, log completeness, timestamp consistency, redundancy levels, and OT/IoT diagnostic mechanisms. Each of these elements is routinely used in compliance assessments performed by national competent authorities under NIS2, allowing the scoring model to be auditable and reproducible at the procedural level.",
            "4.3. Mapping FMEA Results to NIS2 Article 21 Requirements": "The calibration steps, mapping tables, and justification of thresholds were documented in a structured expert assessment protocol and are therefore auditable according to NIS2 governance requirements. Each FMEA parameter (S, O, D) is linked to explicit evidence categories: CVSS metrics, CWE categories, sensor monitoring artefacts, log completeness, and redundancy configurations. This creates a clear evidence chain for conformity assessments performed by national authorities, ensuring that the risk scoring process is transparent and repeatable even if the underlying datasets are not publicly released. To operationalize the link between sensor-level risks and legal compliance, the FMEA outputs were mapped to the relevant NIS2 criteria. The analysis focused particularly on two categories: Criterion 1‚ÄîRisk Analysis and Security PolicyCorresponds to risks arising from inadequate identification, documentation, or governance of vulnerabilities in sensor networks. Failure modes such asabsence of asset inventory,undefined responsibilities for IoT security, orlack of firmware update policyfall into this group.‚óãFMEA linkage: high S and O scores, combined with moderate detectability, indicate systemic policy deficiencies that hinder early identification of threats.‚óãNIS2 compliance implication: organizations must implement structured risk analysis procedures that encompass all IoT and OT assets.Criterion 6‚ÄîEffectiveness Assessment of Risk Management MeasuresRelates to the evaluation of controls applied to mitigate sensor-related threats. Failure modes such as missing performance indicators, ineffective anomaly detection algorithms, or absence of post-incident review reflect weaknesses in the feedback loop required by NIS2.‚óãFMEA linkage: high D values (poor detectability) suggest insufficient monitoring and lack of continuous evaluation mechanisms.‚óãNIS2 compliance implication: the organization must establish metrics (KPIs/KRIs) and periodic reviews to demonstrate that implemented controls remain effective. The results obtained through the FMEA procedure allowed for the quantitative identification of cyber-induced failure modes at the sensor level. These results were subsequently prioritized using the calculated Risk Priority Number (RPN) to distinguish the most critical vulnerabilities affecting data integrity, availability, and authenticity. Table 1summarizes representative examples of the identified failure modes, their corresponding RPN values, and associated NIS2 criteria. The inclusion of this table supports transparency of the analytical process and ensures traceability between technical observations, quantified cybersecurity risks, and proposed mitigation strategies. Table 1.Risk Priority Numbers (RPN) calculated for representative cyber failure modes in railway sensor networks.",
            "4.4. Methodological Integration with Real-Time Sensor Monitoring": "To ensure scalability and automation, the proposed FMEA model can be integrated with real-time monitoring systems that process sensor data streams from IoT and OT devices. Continuous analysis of anomalies, deviations, or missing data packets provides feedback to update the FMEA parameters dynamically. For instance: a sudden increase in communication errors may elevate the Occurrence (O) parameter;delayed detection of an anomaly increases Detection (D);and a recurring fault type raises the overall RPN priority. This integration creates a closed-loop system where sensor data not only support operational decisions but also inform cyber risk management and regulatory compliance tracking. Such linkage between quantitative FMEA indicators and live monitoring aligns directly withSensors‚Äô research focus on data-driven risk assessment and sensor-enabled decision support.",
            "5. Case Study: Cyber Risk Analysis of Railway IoT Sensor Systems": "5.1. Description of the Sensor Network and Data InterfacesThe case study was conducted on a representative railway IoT architecture comprising approximately 250 sensor nodes deployed along a 25 km double-track mainline and associated depot facilities. The network included trackside, rolling-stock and environmental measurement devices, reflecting sampling and communication characteristics typically observed in operational railway systems. Track vibration sensors operated at 100‚Äì200 Hz, axle-load sensors at 50‚Äì100 Hz, temperature probes at 1‚Äì2 Hz, while signaling telemetry (e.g., axle counters and point machine status) was event-driven with timestamp resolution in the 10‚Äì50 ms range. Labels for ‚Äúnormal‚Äù and ‚Äúanomalous‚Äù conditions were generated through expert interpretation of recorded operating data and supplemented with controlled spoofing, replay and data-injection scenarios. This design aligned with the purpose of the study‚Äînot to develop a large-scale benchmark dataset, but to demonstrate how live sensor evidence can drive process-oriented FMEA updates under NIS2 through real-time variation in risk indicators rather than offline statistical annotation [1,2,14].The sensors measured parameters such as:Track geometry and vibration (accelerometers, strain gauges),Axle load and wheel condition (strain and acoustic sensors),Environmental factors (temperature, humidity, wind),Energy and power supply stability (current and voltage sensors), andControl and signaling data (axle counters, point machine status).All data were collected via LoRaWAN and LTE-M gateways, aggregated at local control units, and transmitted to a cloud-based monitoring platform for predictive maintenance analytics. This hybrid architecture combined edge computing (for initial filtering and anomaly pre-detection) with centralized data analytics, forming a complete IoT‚ÄìOT data loop.From a cybersecurity perspective, the network represented a typical environment described in NIS2 Article 21: it included both essential OT systems (signaling, energy control) and connected IoT devices with varying levels of security maturity. This diversity created a realistic scenario for evaluating cyber-induced sensor failures using the adapted FMEA methodology.It is important to clarify that the reported quantitative indicators (‚Äúdetection accuracy improved by 45%, detection latency reduced by 60%, and mean RPN reduced by 34%‚Äù) do not originate from a single, fully reproducible machine-learning experiment based on a closed dataset. Instead, they result from an expert-based evaluation approach (expert elicitation) and comparative assessment of several operational scenarios conducted in a demonstration environment. The evaluation combines three sources of evidence: (i) recorded sensor streams from selected railway infrastructure devices, (ii) simulated spoofing and replay events executed in a controlled test setting, and (iii) structured assessments provided by cybersecurity experts responsible for IoT and signaling systems. This approach is process-driven‚Äîconsistent with the NIS2 Directive‚Äôs governance requirements‚Äîand is not intended to represent a complete, statistically validated ML experiment.Accordingly, the reported percentages should be interpreted as aggregated scenario-based performance indicators derived from iterative expert assessments, rather than as outcomes of a reproducible benchmark with formal training/validation/test splits. Their purpose is to illustrate how automated anomaly analytics influence the O and D parameters in the FMEA matrix, and therefore the dynamic evolution of RPN values within an NIS2-aligned risk-management cycle. 5.2. Identification and Classification of Cyber ThreatsCyberattack scenarios‚Äîincluding spoofing, replay, data injection, and communication jamming‚Äîwere implemented using lightweight scripts designed to approximate typical anomalies rather than to emulate full adversarial toolchains. Replay attacks were generated by re-transmitting previously captured sequences with offsets ranging between 100 ms and 5 s, while injection scenarios involved amplitude perturbations of 5‚Äì20% for vibration data and timestamp manipulation of 20‚Äì80 ms for signaling messages. These exercises were intended to create representative disturbance patterns to support expert evaluation of FMEA parameters, rather than to constitute formal penetration-testing campaigns or adversarial ML benchmarks.Based on expert workshops, incident reports, and analysis of (14) and ERA data, 30 cyber failure modes were identified. They were grouped into four categories corresponding to major threat vectors in sensor networks:Data Integrity Threats‚Äîmodification, replay, or spoofing of sensor readings;Availability Threats‚Äîcommunication disruptions, DoS or energy starvation;Configuration and Firmware Threats‚Äîunauthorized calibration or malicious firmware updates;Monitoring and Evaluation Gaps‚Äîabsence of anomaly detection, incomplete logging, or ineffective response processes.Each failure mode was scored using the parameters defined inSection 4(Severity, Occurrence, Detection).Table 2presents a representative subset of ten key threats ranked by their calculated RPN values.Table 2.Representative subset of cyber failure modes and corresponding RPN values for railway sensor systems.Lower, yet still significant risks included API vulnerabilities and environmental sensor tampering, primarily due to moderate detectability and lack of redundancy in local data validation.The analysis used a combination of real and synthetically generated data, including vibration, temperature, axle-load, and selected signaling telemetry parameters. The sample sizes were intentionally limited‚Äîthe study was not designed as a full-scale big-data experiment‚Äîand individual scenario windows ranged from approximately 30 min to 2 h. The objective was not to construct a reproducible ML benchmark but to evaluate how different anomaly patterns affect the FMEA scoring process and its alignment with NIS2 requirements. 5.3. Quantitative Risk Evaluation and Pattern AnalysisTo illustrate the end-to-end linkage between NIS2 controls, FMEA scoring, and evidence generation, a simplified scenario-based example is provided. In an axle-counter replay scenario, a previously recorded sequence of occupancy messages is re-injected with a temporal offset. The edge-layer model flags an anomaly due to timestamp inconsistency (KPI: anomaly detection latency; KRI: frequency of timestamp deviations). This triggers a response action in accordance with operational procedures (control: event handling and escalation). Based on the observed reduction in detectability (D) and occurrence (O), the RPN score is recalculated and reclassified from ‚Äútolerable‚Äù to ‚Äúunacceptable.‚Äù The resulting artefacts‚Äîdetection logs, timestamps, anomaly scores, and updated RPN classification‚Äîconstitute the audit-relevant evidence supporting NIS2 Article 21 requirements. This example is process-oriented and does not imply a full technical implementation.The latency reduction from approximately 10 s to 2 s refers to a scenario-based evaluation in which redundant sensor channels (primary + secondary timestamp stream) were used to validate the consistency of event timing. The mechanism relies on parallel timestamp comparison rather than on a specific network topology or buffering architecture. The ‚Äúbefore‚Äù and ‚Äúafter‚Äù latency values reflect expert-observed differences in anomaly-notification delay when redundancy checks are enabled, rather than the outcome of a fully instrumented network-performance benchmark.The results of the FMEA were aggregated to visualize the distribution of risk priority numbers (RPN) across the main NIS2 criteria.Figure 2illustrates this distribution, showing that approximately 42% of identified risks were related to Criterion 1 (risk analysis and information system security), 36% to Criterion 6 (control effectiveness assessment), and the remaining 22% to shared or cross-domain issues.Figure 2.Distribution of identified cyber risks across NIS2 criteria based on FMEA results.This visualization provides a clear overview of how the proposed approach supports regulatory alignment by linking technical failure data with specific NIS2 compliance indicators.This confirms that organizational and monitoring deficiencies‚Äîrather than purely technical failures‚Äîrepresent the dominant contributors to high-risk scores.The mean RPN value across all analyzed modes was 326, with the top quartile exceeding 430, a level classified as unacceptable under the adopted risk scale.The FMEA results also revealed a strong correlation between detectability (D) and overall risk, indicating that poor visibility into sensor data integrity substantially amplifies cybersecurity exposure. In practice, this suggests that investments in anomaly detection and real-time analytics may yield higher resilience benefits than isolated hardware upgrades. 5.4. Discussion of Critical Risks and NIS2 MappingThe case study findings confirm that compliance with NIS2 Article 21 in the railway sector cannot be achieved without addressing cybersecurity at the sensor and data layer. The FMEA-based analysis revealed two key dependency loops relevant to regulatory implementation:Policy‚ÄìTechnology Alignment (Criterion 1)‚óãHigh RPN values for spoofing and firmware tampering indicate insufficient governance over sensor lifecycle management and patch control.‚óãNIS2 compliance therefore requires explicit inclusion of IoT/OT asset inventories, firmware assurance processes, and configuration management policies within risk analysis frameworks.Monitoring‚ÄìEvaluation Loop (Criterion 6)‚óãFailures linked to ineffective anomaly detection and missing audit trails demonstrate the need for continuous monitoring systems that integrate sensor data analytics into cybersecurity dashboards.‚óãUnder NIS2, railway operators must evidence that such monitoring systems support regular testing and review of control effectiveness.Based on the prioritization of failure modes obtained from the FMEA, targeted mitigation strategies were proposed to address the most critical vulnerabilities identified within the sensor network.In practical terms, the evidence links can be summarized as follows:NIS2 requirement: monitoring & detection ‚Üí FMEA (D): supported by log completeness, sensor availability, and timestamp consistency.NIS2 requirement: vulnerability & risk management ‚Üí FMEA (O): informed by anomaly frequency, historical incident data, and attack simulations.NIS2 requirement: operational continuity ‚Üí FMEA (S): assessed through the severity of degradation in sensor-driven safety functions.These categories offer a structured, verifiable reasoning path without implying a technical certification model.Table 3presents these key mitigation actions, linking each recommended measure to the corresponding failure mode and NIS2 criterion. This approach ensures that risk reduction activities are not only technically effective but also verifiable within the regulatory framework, facilitating evidence-based cybersecurity governance in line with the NIS2 Directive.Table 3.Key mitigation measures for the most critical sensor-level cyber failure modes identified in the FMEA. 5.5. Lessons LearnedThe case study demonstrates that sensor-level cybersecurity plays a pivotal role in ensuring the resilience of railway operations. The main findings can be summarized as follows:High-risk failure modes are primarily associated with compromised data integrity and insufficient monitoring mechanisms.Detectability (D) is the most influential factor in RPN escalation, emphasizing the importance of sensor data analytics and anomaly detection tools.The integration of FMEA with live monitoring enables continuous reassessment of risks, directly supporting the NIS2 requirement for periodic evaluation of control effectiveness.Finally, the approach provides a transparent, evidence-based method for demonstrating compliance to national competent authorities and supervisory bodies.By operationalizing cybersecurity governance at the level of sensors and IoT devices, railway operators can move from a compliance-driven posture to a data-driven resilience model, where technical and organizational measures are unified within a measurable risk management framework.",
            "5.1. Description of the Sensor Network and Data Interfaces": "The case study was conducted on a representative railway IoT architecture comprising approximately 250 sensor nodes deployed along a 25 km double-track mainline and associated depot facilities. The network included trackside, rolling-stock and environmental measurement devices, reflecting sampling and communication characteristics typically observed in operational railway systems. Track vibration sensors operated at 100‚Äì200 Hz, axle-load sensors at 50‚Äì100 Hz, temperature probes at 1‚Äì2 Hz, while signaling telemetry (e.g., axle counters and point machine status) was event-driven with timestamp resolution in the 10‚Äì50 ms range. Labels for ‚Äúnormal‚Äù and ‚Äúanomalous‚Äù conditions were generated through expert interpretation of recorded operating data and supplemented with controlled spoofing, replay and data-injection scenarios. This design aligned with the purpose of the study‚Äînot to develop a large-scale benchmark dataset, but to demonstrate how live sensor evidence can drive process-oriented FMEA updates under NIS2 through real-time variation in risk indicators rather than offline statistical annotation [1,2,14]. The sensors measured parameters such as: Track geometry and vibration (accelerometers, strain gauges),Axle load and wheel condition (strain and acoustic sensors),Environmental factors (temperature, humidity, wind),Energy and power supply stability (current and voltage sensors), andControl and signaling data (axle counters, point machine status). All data were collected via LoRaWAN and LTE-M gateways, aggregated at local control units, and transmitted to a cloud-based monitoring platform for predictive maintenance analytics. This hybrid architecture combined edge computing (for initial filtering and anomaly pre-detection) with centralized data analytics, forming a complete IoT‚ÄìOT data loop. From a cybersecurity perspective, the network represented a typical environment described in NIS2 Article 21: it included both essential OT systems (signaling, energy control) and connected IoT devices with varying levels of security maturity. This diversity created a realistic scenario for evaluating cyber-induced sensor failures using the adapted FMEA methodology. It is important to clarify that the reported quantitative indicators (‚Äúdetection accuracy improved by 45%, detection latency reduced by 60%, and mean RPN reduced by 34%‚Äù) do not originate from a single, fully reproducible machine-learning experiment based on a closed dataset. Instead, they result from an expert-based evaluation approach (expert elicitation) and comparative assessment of several operational scenarios conducted in a demonstration environment. The evaluation combines three sources of evidence: (i) recorded sensor streams from selected railway infrastructure devices, (ii) simulated spoofing and replay events executed in a controlled test setting, and (iii) structured assessments provided by cybersecurity experts responsible for IoT and signaling systems. This approach is process-driven‚Äîconsistent with the NIS2 Directive‚Äôs governance requirements‚Äîand is not intended to represent a complete, statistically validated ML experiment. Accordingly, the reported percentages should be interpreted as aggregated scenario-based performance indicators derived from iterative expert assessments, rather than as outcomes of a reproducible benchmark with formal training/validation/test splits. Their purpose is to illustrate how automated anomaly analytics influence the O and D parameters in the FMEA matrix, and therefore the dynamic evolution of RPN values within an NIS2-aligned risk-management cycle.",
            "5.2. Identification and Classification of Cyber Threats": "Cyberattack scenarios‚Äîincluding spoofing, replay, data injection, and communication jamming‚Äîwere implemented using lightweight scripts designed to approximate typical anomalies rather than to emulate full adversarial toolchains. Replay attacks were generated by re-transmitting previously captured sequences with offsets ranging between 100 ms and 5 s, while injection scenarios involved amplitude perturbations of 5‚Äì20% for vibration data and timestamp manipulation of 20‚Äì80 ms for signaling messages. These exercises were intended to create representative disturbance patterns to support expert evaluation of FMEA parameters, rather than to constitute formal penetration-testing campaigns or adversarial ML benchmarks. Based on expert workshops, incident reports, and analysis of (14) and ERA data, 30 cyber failure modes were identified. They were grouped into four categories corresponding to major threat vectors in sensor networks: Data Integrity Threats‚Äîmodification, replay, or spoofing of sensor readings;Availability Threats‚Äîcommunication disruptions, DoS or energy starvation;Configuration and Firmware Threats‚Äîunauthorized calibration or malicious firmware updates;Monitoring and Evaluation Gaps‚Äîabsence of anomaly detection, incomplete logging, or ineffective response processes. Each failure mode was scored using the parameters defined inSection 4(Severity, Occurrence, Detection).Table 2presents a representative subset of ten key threats ranked by their calculated RPN values. Table 2.Representative subset of cyber failure modes and corresponding RPN values for railway sensor systems. Lower, yet still significant risks included API vulnerabilities and environmental sensor tampering, primarily due to moderate detectability and lack of redundancy in local data validation. The analysis used a combination of real and synthetically generated data, including vibration, temperature, axle-load, and selected signaling telemetry parameters. The sample sizes were intentionally limited‚Äîthe study was not designed as a full-scale big-data experiment‚Äîand individual scenario windows ranged from approximately 30 min to 2 h. The objective was not to construct a reproducible ML benchmark but to evaluate how different anomaly patterns affect the FMEA scoring process and its alignment with NIS2 requirements.",
            "5.3. Quantitative Risk Evaluation and Pattern Analysis": "To illustrate the end-to-end linkage between NIS2 controls, FMEA scoring, and evidence generation, a simplified scenario-based example is provided. In an axle-counter replay scenario, a previously recorded sequence of occupancy messages is re-injected with a temporal offset. The edge-layer model flags an anomaly due to timestamp inconsistency (KPI: anomaly detection latency; KRI: frequency of timestamp deviations). This triggers a response action in accordance with operational procedures (control: event handling and escalation). Based on the observed reduction in detectability (D) and occurrence (O), the RPN score is recalculated and reclassified from ‚Äútolerable‚Äù to ‚Äúunacceptable.‚Äù The resulting artefacts‚Äîdetection logs, timestamps, anomaly scores, and updated RPN classification‚Äîconstitute the audit-relevant evidence supporting NIS2 Article 21 requirements. This example is process-oriented and does not imply a full technical implementation. The latency reduction from approximately 10 s to 2 s refers to a scenario-based evaluation in which redundant sensor channels (primary + secondary timestamp stream) were used to validate the consistency of event timing. The mechanism relies on parallel timestamp comparison rather than on a specific network topology or buffering architecture. The ‚Äúbefore‚Äù and ‚Äúafter‚Äù latency values reflect expert-observed differences in anomaly-notification delay when redundancy checks are enabled, rather than the outcome of a fully instrumented network-performance benchmark. The results of the FMEA were aggregated to visualize the distribution of risk priority numbers (RPN) across the main NIS2 criteria. Figure 2illustrates this distribution, showing that approximately 42% of identified risks were related to Criterion 1 (risk analysis and information system security), 36% to Criterion 6 (control effectiveness assessment), and the remaining 22% to shared or cross-domain issues. Figure 2.Distribution of identified cyber risks across NIS2 criteria based on FMEA results. This visualization provides a clear overview of how the proposed approach supports regulatory alignment by linking technical failure data with specific NIS2 compliance indicators. This confirms that organizational and monitoring deficiencies‚Äîrather than purely technical failures‚Äîrepresent the dominant contributors to high-risk scores. The mean RPN value across all analyzed modes was 326, with the top quartile exceeding 430, a level classified as unacceptable under the adopted risk scale. The FMEA results also revealed a strong correlation between detectability (D) and overall risk, indicating that poor visibility into sensor data integrity substantially amplifies cybersecurity exposure. In practice, this suggests that investments in anomaly detection and real-time analytics may yield higher resilience benefits than isolated hardware upgrades.",
            "5.4. Discussion of Critical Risks and NIS2 Mapping": "The case study findings confirm that compliance with NIS2 Article 21 in the railway sector cannot be achieved without addressing cybersecurity at the sensor and data layer. The FMEA-based analysis revealed two key dependency loops relevant to regulatory implementation: Policy‚ÄìTechnology Alignment (Criterion 1)‚óãHigh RPN values for spoofing and firmware tampering indicate insufficient governance over sensor lifecycle management and patch control.‚óãNIS2 compliance therefore requires explicit inclusion of IoT/OT asset inventories, firmware assurance processes, and configuration management policies within risk analysis frameworks.Monitoring‚ÄìEvaluation Loop (Criterion 6)‚óãFailures linked to ineffective anomaly detection and missing audit trails demonstrate the need for continuous monitoring systems that integrate sensor data analytics into cybersecurity dashboards.‚óãUnder NIS2, railway operators must evidence that such monitoring systems support regular testing and review of control effectiveness. Based on the prioritization of failure modes obtained from the FMEA, targeted mitigation strategies were proposed to address the most critical vulnerabilities identified within the sensor network. In practical terms, the evidence links can be summarized as follows: NIS2 requirement: monitoring & detection ‚Üí FMEA (D): supported by log completeness, sensor availability, and timestamp consistency.NIS2 requirement: vulnerability & risk management ‚Üí FMEA (O): informed by anomaly frequency, historical incident data, and attack simulations.NIS2 requirement: operational continuity ‚Üí FMEA (S): assessed through the severity of degradation in sensor-driven safety functions. These categories offer a structured, verifiable reasoning path without implying a technical certification model. Table 3presents these key mitigation actions, linking each recommended measure to the corresponding failure mode and NIS2 criterion. This approach ensures that risk reduction activities are not only technically effective but also verifiable within the regulatory framework, facilitating evidence-based cybersecurity governance in line with the NIS2 Directive. Table 3.Key mitigation measures for the most critical sensor-level cyber failure modes identified in the FMEA.",
            "5.5. Lessons Learned": "The case study demonstrates that sensor-level cybersecurity plays a pivotal role in ensuring the resilience of railway operations. The main findings can be summarized as follows: High-risk failure modes are primarily associated with compromised data integrity and insufficient monitoring mechanisms.Detectability (D) is the most influential factor in RPN escalation, emphasizing the importance of sensor data analytics and anomaly detection tools.The integration of FMEA with live monitoring enables continuous reassessment of risks, directly supporting the NIS2 requirement for periodic evaluation of control effectiveness.Finally, the approach provides a transparent, evidence-based method for demonstrating compliance to national competent authorities and supervisory bodies. By operationalizing cybersecurity governance at the level of sensors and IoT devices, railway operators can move from a compliance-driven posture to a data-driven resilience model, where technical and organizational measures are unified within a measurable risk management framework.",
            "6. Sensor-Based Monitoring and Anomaly Detection": "6.1. Integrating Real-Time Sensor Data for Cyber Risk DetectionThe detection performance values presented inSection 6should be understood as synthesized results from a set of controlled test scenarios combined with expert cybersecurity assessments, rather than statistically validated outcomes of a fully reproducible ML pipeline. Their primary function is to demonstrate how sensor-driven anomaly analytics influence the O and D dimensions of the FMEA process, rather than to establish comparative ML performance claims.Modern railway infrastructure generates vast volumes of heterogeneous data through distributed sensor networks [2,4,13]. These data streams‚Äîoriginating from trackside accelerometers, temperature probes, axle counters, and power monitoring units‚Äîform the operational backbone of intelligent transportation systems. The same data that serve maintenance and safety purposes can also be harnessed to enhance cybersecurity monitoring and threat detection.The demonstration scenarios employed a set of abstract model classes representing analytical approaches commonly used in OT/IoT monitoring systems. At the edge layer, lightweight statistical and autoencoder-style models (1‚Äì2 hidden layers) were used to support low-latency anomaly detection under constrained computational resources. At the cloud layer, more expressive analytical models‚Äîsuch as 3‚Äì5-layer MLP networks or clustering algorithms (e.g., DBSCAN)‚Äîwere used to illustrate cross-sensor correlation. These models are representative abstractions whose purpose is to illustrate analytical workflow and its influence on FMEA parameters, rather than to optimize machine-learning performance.In the context of NIS2 compliance, continuous monitoring is essential to demonstrate that risk management measures remain effective (Criterion 6). By integrating sensor data into cybersecurity dashboards, operators can detect deviations that signal potential cyber incidents. For example:Abnormal latency in data transmission may indicate network congestion or a denial-of-service attack.Inconsistent measurement patterns across redundant sensors may suggest data spoofing or unauthorized calibration.Loss of synchronization in timestamps may point to compromised gateways or tampered communication modules.The incorporation of these data into Security Information and Event Management (SIEM) or Operational Technology (OT) Security Operation Center (SOC) platforms enables cross-domain correlation between physical and digital indicators. Thus, railway operators can shift from static compliance verification to dynamic cyber risk detection grounded in real-time evidence.To achieve this integration, a standardized data acquisition and normalization layer was introduced in the studied system. Sensor telemetry is converted into unified message structures containing metadata such as device ID, time offset, and cryptographic signature. This allows cybersecurity tools to ingest sensor data in a format compatible with anomaly detection algorithms and regulatory audit logs.The demonstration environment also incorporated typical resource constraints observed in railway IoT/edge deployments. Edge devices operated with CPUs in the 200‚Äì600 MHz range, 128‚Äì512 MB RAM, and energy budgets compatible with battery-powered or intermittently powered nodes. These constraints informed the choice of lightweight anomaly-detection logic at the edge layer. In contrast, the cloud layer assumed unrestricted compute resources suitable for archival analytics and cross-domain correlation. These resource characterizations serve to contextualize the scenario exercises rather than to prescribe a specific hardware platform.Feature extraction combined time-domain and frequency-domain attributes, including RMS amplitude, standard deviation, energy measures, FFT-based low- and mid-frequency components, and temporal instability indicators. For event-driven signaling data (e.g., axle counter messages), semantic features such as inter-event timing, sequence consistency, and cross-channel correlations were used. Feature selection followed an expert-based approach consistent with predictive maintenance practices in railway infrastructure.Feature selection followed a domain-expert approach using standard time- and frequency-domain indicators: RMS amplitude, variance, FFT low-band components, signal energy, and temporal consistency metrics. For signaling telemetry, semantic features such as inter-event timing and channel correlation were used. This level of abstraction is sufficient for explaining the FMEA-to-NIS2 linkage without specifying implementation-level feature engineering pipelines.Redundancy checks consisted of comparing sequential timestamp streams from two independent sensing paths, allowing anomalies caused by replay offsets to be detected earlier without requiring changes to the underlying communication network. 6.2. Use of Machine Learning and Edge AnalyticsGiven the expert-driven and scenario-oriented character of the study, the edge and cloud analytical components represent abstracted model classes rather than specific, fully parameterized architectures. At the edge layer, lightweight statistical models (e.g., moving-window z-score filters and simple autoencoder-style compression models using 1‚Äì2 hidden layers) were used to illustrate latency-sensitive anomaly detection under constrained resources. At the cloud layer, more expressive models (e.g., 3‚Äì5-layer feed-forward networks or clustering methods such as DBSCAN) were used to demonstrate cross-sensor correlation. Hyperparameters were selected through expert tuning aimed at illustrating behavioral differences rather than optimizing ML performance. This abstraction avoids tying the approach to a specific vendor or platform and maintains the paper‚Äôs focus on NIS2 process integration.The analytical workflow used three window sizes: short (200‚Äì500 ms), medium (1‚Äì2 s), and long (5‚Äì10 s), depending on the characteristics of the signal. Sampling rates were those described inSection 5.1(100‚Äì200 Hz for vibration, 1‚Äì2 Hz for temperature, 50‚Äì100 Hz for axle-load sensors). These values were selected to represent typical operational conditions and to support the evaluation of how different temporal horizons affect O and D scoring in FMEA, rather than to tune ML performance.Window lengths were chosen to reflect operational categories of sensor behaviors (short: 200‚Äì500 ms; medium: 1‚Äì2 s; long: 5‚Äì10 s). Thresholds for anomaly alerts were adapted using simple percentile-based or median absolute deviation heuristics, aligned with expert consensus rather than formal optimization. This approach supports the governance-oriented focus of the study, where the objective is to demonstrate how anomaly indications influence FMEA scoring rather than optimize model sensitivity.While traditional rule-based monitoring can detect predefined anomalies, the complexity of sensor environments in railways requires machine learning (ML) and edge analytics for adaptive detection of emerging threats.ML algorithms, trained on historical operational data, can identify subtle deviations that may precede failures or cyber intrusions. Several analytical strategies were considered within the case study:Supervised Learning for Signature AnomaliesClassification algorithms such as Random Forest or Support Vector Machines were trained on labeled datasets comprising normal and compromised sensor states. These models achieved detection accuracies above 90% for spoofed and replayed signals in simulated tests.Unsupervised Learning for Novel ThreatsAutoencoders and clustering algorithms (e.g., DBSCAN) were deployed to discover outliers in multi-sensor data streams without requiring prior labeling. This approach was effective in detecting unusual traffic bursts and irregular vibration sequences indicative of jamming or data injection.Edge-Level Analytics for Low Latency DetectionTo minimize response times, lightweight models were implemented directly on edge gateways. These models perform local inference, flagging abnormal sensor behavior before forwarding alerts to the central platform. Such decentralization enhances system resilience by reducing dependence on cloud connectivity.Federated Learning for Secure CollaborationGiven the distributed nature of railway networks, federated learning architectures were proposed to allow multiple subsystems (e.g., depots, control centers) to collaboratively train ML models without exchanging raw sensor data, thereby preserving confidentiality.In alignment with Sensors‚Äô research scope, these approaches highlight the dual role of sensing infrastructure: it serves not only as a data source for operational control, but also as a sensor array for cybersecurity itself.By continuously adapting detection thresholds through learning algorithms, the system can dynamically adjust FMEA parameters (Occurrence and Detection), enabling data-driven updates of the risk profile. 6.3. Example Applications: Track Condition Monitoring, Power Systems, and Signaling SensorsThe edge layer operated under a latency budget of <200‚Äì300 ms for mission-critical anomaly detection. Cloud-level processing allowed latencies of approximately 1‚Äì2 s for correlation and trend analysis. Task allocation between edge and cloud was influenced by resource constraints (200‚Äì600 MHz CPU, 128‚Äì512 MB RAM) and by NIS2 requirements for prompt incident detection and auditable evidence availability. These parameters are typical for OT/IoT deployments and represent scenario-based assumptions rather than hardware-specific performance benchmarks.Three operational domains were analyzed to illustrate how sensor-based monitoring contributes to cybersecurity and regulatory compliance:Track Condition Monitoring Systems (TCMS)Sensors along the rail line measure vibration and strain to detect geometry changes or mechanical defects. In this study, anomalies in vibration spectra were also correlated with possible data manipulation attempts. A sudden spectral shift unaccompanied by physical cause triggered an alert for potential data injection.‚óãCyber implication: The event was mapped to NIS2 Criterion 6, confirming the effectiveness of continuous anomaly evaluation.Traction Power and Energy Distribution SystemsIoT sensors measure voltage, current, and transformer temperatures. Deviations from baseline patterns can indicate not only equipment overloads but also malicious firmware tampering affecting measurement calibration.‚óãPreventive measure: Secure firmware validation through checksum monitoring was deployed, enabling automated risk reassessment (reduction in RPN from 405 to 270).Signaling and Interlocking SensorsAxle counters and point machines transmit occupancy data to control systems. Cross-validation among redundant sensors allowed early detection of spoofed inputs.‚óãOutcome: The integration of redundant verification reduced detection latency from 10 s to 2 s, significantly improving system resilience and audit traceability under NIS2 requirements.These examples confirm that sensor data analytics directly enhance cybersecurity performance indicators, bridging the gap between technical operations and governance-level reporting. By providing continuous evidence of control functionality, such monitoring enables railway operators to demonstrate Criterion 6 compliance through measurable, sensor-derived KPIs. 6.4. Implementation Framework and Evaluation MetricsTo standardize the use of sensor data for cybersecurity assessment, an implementation framework was defined, comprising four functional layers:Data Acquisition Layer‚Äîgathers raw sensor measurements, metadata, and health indicators.Processing and Analytics Layer‚Äîapplies ML models, statistical filters, and anomaly scoring.Decision Support Layer‚Äîtranslates anomalies into actionable alerts, updating FMEA parameters and RPN values.Reporting and Compliance Layer‚Äîgenerates NIS2-aligned metrics, such as detection rate, mean time to detect (MTTD), and mean time to respond (MTTR).Key performance indicators used for evaluating system effectiveness include:Detection Accuracy (DA) = (True Positives/Total Events) √ó 100%;False Alarm Rate (FAR) = (False Positives/Total Alerts) √ó 100%;Detection Latency (DL)‚Äîaverage time between event onset and alert generation;Risk Reduction Index (RRI)‚Äîratio of baseline RPN to post-mitigation RPN.In pilot testing, integration of sensor analytics resulted in a 34% reduction in mean RPN values, a 45% improvement in detection accuracy, and a 60% decrease in detection latency compared with baseline manual assessments. These improvements demonstrate the tangible benefits of embedding sensor analytics into the cybersecurity management cycle mandated by NIS2. 6.5. Alignment with NIS2 and Broader ImplicationsThe integration of real-time sensor analytics into risk management not only supports technical resilience but also provides documented evidence for supervisory authorities.Under Article 21 of NIS2, essential entities must establish procedures for testing, auditing, and evaluating the effectiveness of controls. The approach presented in this study fulfills these obligations by:providing continuous quantitative feedback (through updated RPNs),ensuring traceability of detection and response actions, andenabling automated compliance reporting based on sensor data logs.Furthermore, the methodology aligns with emerging EU initiatives on digital twins and predictive maintenance, where sensor-driven cyber‚Äìphysical models support proactive decision-making. In this sense, railway infrastructure becomes a self-monitoring ecosystem, capable of identifying both mechanical and cyber anomalies in real time.",
            "6.1. Integrating Real-Time Sensor Data for Cyber Risk Detection": "The detection performance values presented inSection 6should be understood as synthesized results from a set of controlled test scenarios combined with expert cybersecurity assessments, rather than statistically validated outcomes of a fully reproducible ML pipeline. Their primary function is to demonstrate how sensor-driven anomaly analytics influence the O and D dimensions of the FMEA process, rather than to establish comparative ML performance claims. Modern railway infrastructure generates vast volumes of heterogeneous data through distributed sensor networks [2,4,13]. These data streams‚Äîoriginating from trackside accelerometers, temperature probes, axle counters, and power monitoring units‚Äîform the operational backbone of intelligent transportation systems. The same data that serve maintenance and safety purposes can also be harnessed to enhance cybersecurity monitoring and threat detection. The demonstration scenarios employed a set of abstract model classes representing analytical approaches commonly used in OT/IoT monitoring systems. At the edge layer, lightweight statistical and autoencoder-style models (1‚Äì2 hidden layers) were used to support low-latency anomaly detection under constrained computational resources. At the cloud layer, more expressive analytical models‚Äîsuch as 3‚Äì5-layer MLP networks or clustering algorithms (e.g., DBSCAN)‚Äîwere used to illustrate cross-sensor correlation. These models are representative abstractions whose purpose is to illustrate analytical workflow and its influence on FMEA parameters, rather than to optimize machine-learning performance. In the context of NIS2 compliance, continuous monitoring is essential to demonstrate that risk management measures remain effective (Criterion 6). By integrating sensor data into cybersecurity dashboards, operators can detect deviations that signal potential cyber incidents. For example: Abnormal latency in data transmission may indicate network congestion or a denial-of-service attack.Inconsistent measurement patterns across redundant sensors may suggest data spoofing or unauthorized calibration.Loss of synchronization in timestamps may point to compromised gateways or tampered communication modules. The incorporation of these data into Security Information and Event Management (SIEM) or Operational Technology (OT) Security Operation Center (SOC) platforms enables cross-domain correlation between physical and digital indicators. Thus, railway operators can shift from static compliance verification to dynamic cyber risk detection grounded in real-time evidence. To achieve this integration, a standardized data acquisition and normalization layer was introduced in the studied system. Sensor telemetry is converted into unified message structures containing metadata such as device ID, time offset, and cryptographic signature. This allows cybersecurity tools to ingest sensor data in a format compatible with anomaly detection algorithms and regulatory audit logs. The demonstration environment also incorporated typical resource constraints observed in railway IoT/edge deployments. Edge devices operated with CPUs in the 200‚Äì600 MHz range, 128‚Äì512 MB RAM, and energy budgets compatible with battery-powered or intermittently powered nodes. These constraints informed the choice of lightweight anomaly-detection logic at the edge layer. In contrast, the cloud layer assumed unrestricted compute resources suitable for archival analytics and cross-domain correlation. These resource characterizations serve to contextualize the scenario exercises rather than to prescribe a specific hardware platform. Feature extraction combined time-domain and frequency-domain attributes, including RMS amplitude, standard deviation, energy measures, FFT-based low- and mid-frequency components, and temporal instability indicators. For event-driven signaling data (e.g., axle counter messages), semantic features such as inter-event timing, sequence consistency, and cross-channel correlations were used. Feature selection followed an expert-based approach consistent with predictive maintenance practices in railway infrastructure. Feature selection followed a domain-expert approach using standard time- and frequency-domain indicators: RMS amplitude, variance, FFT low-band components, signal energy, and temporal consistency metrics. For signaling telemetry, semantic features such as inter-event timing and channel correlation were used. This level of abstraction is sufficient for explaining the FMEA-to-NIS2 linkage without specifying implementation-level feature engineering pipelines. Redundancy checks consisted of comparing sequential timestamp streams from two independent sensing paths, allowing anomalies caused by replay offsets to be detected earlier without requiring changes to the underlying communication network.",
            "6.2. Use of Machine Learning and Edge Analytics": "Given the expert-driven and scenario-oriented character of the study, the edge and cloud analytical components represent abstracted model classes rather than specific, fully parameterized architectures. At the edge layer, lightweight statistical models (e.g., moving-window z-score filters and simple autoencoder-style compression models using 1‚Äì2 hidden layers) were used to illustrate latency-sensitive anomaly detection under constrained resources. At the cloud layer, more expressive models (e.g., 3‚Äì5-layer feed-forward networks or clustering methods such as DBSCAN) were used to demonstrate cross-sensor correlation. Hyperparameters were selected through expert tuning aimed at illustrating behavioral differences rather than optimizing ML performance. This abstraction avoids tying the approach to a specific vendor or platform and maintains the paper‚Äôs focus on NIS2 process integration. The analytical workflow used three window sizes: short (200‚Äì500 ms), medium (1‚Äì2 s), and long (5‚Äì10 s), depending on the characteristics of the signal. Sampling rates were those described inSection 5.1(100‚Äì200 Hz for vibration, 1‚Äì2 Hz for temperature, 50‚Äì100 Hz for axle-load sensors). These values were selected to represent typical operational conditions and to support the evaluation of how different temporal horizons affect O and D scoring in FMEA, rather than to tune ML performance. Window lengths were chosen to reflect operational categories of sensor behaviors (short: 200‚Äì500 ms; medium: 1‚Äì2 s; long: 5‚Äì10 s). Thresholds for anomaly alerts were adapted using simple percentile-based or median absolute deviation heuristics, aligned with expert consensus rather than formal optimization. This approach supports the governance-oriented focus of the study, where the objective is to demonstrate how anomaly indications influence FMEA scoring rather than optimize model sensitivity. While traditional rule-based monitoring can detect predefined anomalies, the complexity of sensor environments in railways requires machine learning (ML) and edge analytics for adaptive detection of emerging threats. ML algorithms, trained on historical operational data, can identify subtle deviations that may precede failures or cyber intrusions. Several analytical strategies were considered within the case study: Supervised Learning for Signature AnomaliesClassification algorithms such as Random Forest or Support Vector Machines were trained on labeled datasets comprising normal and compromised sensor states. These models achieved detection accuracies above 90% for spoofed and replayed signals in simulated tests.Unsupervised Learning for Novel ThreatsAutoencoders and clustering algorithms (e.g., DBSCAN) were deployed to discover outliers in multi-sensor data streams without requiring prior labeling. This approach was effective in detecting unusual traffic bursts and irregular vibration sequences indicative of jamming or data injection.Edge-Level Analytics for Low Latency DetectionTo minimize response times, lightweight models were implemented directly on edge gateways. These models perform local inference, flagging abnormal sensor behavior before forwarding alerts to the central platform. Such decentralization enhances system resilience by reducing dependence on cloud connectivity.Federated Learning for Secure CollaborationGiven the distributed nature of railway networks, federated learning architectures were proposed to allow multiple subsystems (e.g., depots, control centers) to collaboratively train ML models without exchanging raw sensor data, thereby preserving confidentiality. In alignment with Sensors‚Äô research scope, these approaches highlight the dual role of sensing infrastructure: it serves not only as a data source for operational control, but also as a sensor array for cybersecurity itself. By continuously adapting detection thresholds through learning algorithms, the system can dynamically adjust FMEA parameters (Occurrence and Detection), enabling data-driven updates of the risk profile.",
            "6.3. Example Applications: Track Condition Monitoring, Power Systems, and Signaling Sensors": "The edge layer operated under a latency budget of <200‚Äì300 ms for mission-critical anomaly detection. Cloud-level processing allowed latencies of approximately 1‚Äì2 s for correlation and trend analysis. Task allocation between edge and cloud was influenced by resource constraints (200‚Äì600 MHz CPU, 128‚Äì512 MB RAM) and by NIS2 requirements for prompt incident detection and auditable evidence availability. These parameters are typical for OT/IoT deployments and represent scenario-based assumptions rather than hardware-specific performance benchmarks. Three operational domains were analyzed to illustrate how sensor-based monitoring contributes to cybersecurity and regulatory compliance: Track Condition Monitoring Systems (TCMS)Sensors along the rail line measure vibration and strain to detect geometry changes or mechanical defects. In this study, anomalies in vibration spectra were also correlated with possible data manipulation attempts. A sudden spectral shift unaccompanied by physical cause triggered an alert for potential data injection.‚óãCyber implication: The event was mapped to NIS2 Criterion 6, confirming the effectiveness of continuous anomaly evaluation.Traction Power and Energy Distribution SystemsIoT sensors measure voltage, current, and transformer temperatures. Deviations from baseline patterns can indicate not only equipment overloads but also malicious firmware tampering affecting measurement calibration.‚óãPreventive measure: Secure firmware validation through checksum monitoring was deployed, enabling automated risk reassessment (reduction in RPN from 405 to 270).Signaling and Interlocking SensorsAxle counters and point machines transmit occupancy data to control systems. Cross-validation among redundant sensors allowed early detection of spoofed inputs.‚óãOutcome: The integration of redundant verification reduced detection latency from 10 s to 2 s, significantly improving system resilience and audit traceability under NIS2 requirements. These examples confirm that sensor data analytics directly enhance cybersecurity performance indicators, bridging the gap between technical operations and governance-level reporting. By providing continuous evidence of control functionality, such monitoring enables railway operators to demonstrate Criterion 6 compliance through measurable, sensor-derived KPIs.",
            "6.4. Implementation Framework and Evaluation Metrics": "To standardize the use of sensor data for cybersecurity assessment, an implementation framework was defined, comprising four functional layers: Data Acquisition Layer‚Äîgathers raw sensor measurements, metadata, and health indicators.Processing and Analytics Layer‚Äîapplies ML models, statistical filters, and anomaly scoring.Decision Support Layer‚Äîtranslates anomalies into actionable alerts, updating FMEA parameters and RPN values.Reporting and Compliance Layer‚Äîgenerates NIS2-aligned metrics, such as detection rate, mean time to detect (MTTD), and mean time to respond (MTTR). Key performance indicators used for evaluating system effectiveness include: Detection Accuracy (DA) = (True Positives/Total Events) √ó 100%;False Alarm Rate (FAR) = (False Positives/Total Alerts) √ó 100%;Detection Latency (DL)‚Äîaverage time between event onset and alert generation;Risk Reduction Index (RRI)‚Äîratio of baseline RPN to post-mitigation RPN. In pilot testing, integration of sensor analytics resulted in a 34% reduction in mean RPN values, a 45% improvement in detection accuracy, and a 60% decrease in detection latency compared with baseline manual assessments. These improvements demonstrate the tangible benefits of embedding sensor analytics into the cybersecurity management cycle mandated by NIS2.",
            "6.5. Alignment with NIS2 and Broader Implications": "The integration of real-time sensor analytics into risk management not only supports technical resilience but also provides documented evidence for supervisory authorities. Under Article 21 of NIS2, essential entities must establish procedures for testing, auditing, and evaluating the effectiveness of controls. The approach presented in this study fulfills these obligations by: providing continuous quantitative feedback (through updated RPNs),ensuring traceability of detection and response actions, andenabling automated compliance reporting based on sensor data logs. Furthermore, the methodology aligns with emerging EU initiatives on digital twins and predictive maintenance, where sensor-driven cyber‚Äìphysical models support proactive decision-making. In this sense, railway infrastructure becomes a self-monitoring ecosystem, capable of identifying both mechanical and cyber anomalies in real time.",
            "7. Discussion": "The findings of this study highlight that sensor networks are not only data sources but also critical cybersecurity assets whose reliability directly determines the resilience of railway operations [1,2,7]. By adapting FMEA to the sensor and IoT context, the analysis revealed that most high-priority risks originated from data integrity breaches and limited detectability of anomalies, rather than from hardware malfunctions alone. This insight confirms the growing convergence between safety engineering and cybersecurity, as both domains rely on the accuracy and availability of measurement data. In practice, the vulnerability of a vibration or axle counter sensor to spoofing or firmware manipulation can have consequences equivalent to a physical fault in safety-critical systems. Therefore, risk analysis in modern railway infrastructure must account for cyber-induced sensor failures as a core category of operational hazards. Detailed architecture diagrams were intentionally omitted, as the precise structure and hyperparameters of the models are not the scientific contribution of this work. The models serve as analytical mechanisms supporting the process-driven FMEA-NIS2 integration, whereas the paper‚Äôs objective is not to present a formalized ML benchmark. Future work will expand the implementation aspects with full architectures and evaluation protocols. Detailed technical diagrams of the edge and cloud models were not included, as the purpose of this work is to demonstrate a process-level integration of sensor analytics with NIS2 governance and FMEA scoring. Architecture-level optimization, threshold learning, and model-specific diagnostics fall outside the scope of the present study and are planned as part of future technical work. The integration of quantitative FMEA indicators with real-time monitoring enabled a continuous risk management loop. This loop satisfies the dual intent of NIS2 Article 21: (1) establishing structured risk analysis procedures (Criterion 1) and (2) continuously evaluating the effectiveness of implemented measures (Criterion 6). A qualitative sensitivity analysis was performed to assess how changes in O and D influence the resulting RPN classifications. The analysis demonstrated that a ¬±1 change in D leads to an average ¬±12‚Äì18% change in total RPN, confirming that detectability is the dominant factor in cyber-induced sensor failures. This aligns with industry observations and ENISA guidance, which indicate that insufficient monitoring and incomplete logs are major amplifiers of systemic cyber risk. Although not a formal statistical sensitivity study, the analysis is fully auditable and follows a structured expert-evaluation process. By updating Risk Priority Numbers (RPNs) based on live sensor data, the system dynamically quantified how technical and organizational controls influence residual risk, thereby transforming regulatory obligations into measurable, evidence-based indicators. A key outcome of the research was the identification of a bidirectional dependency between cybersecurity governance and sensor data integrity. On one hand, well-defined policies‚Äîcovering configuration management, authentication, and monitoring‚Äîenable early detection of anomalies in sensor networks.On the other hand, the quality and completeness of sensor data directly affect the ability to verify policy effectiveness and compliance with NIS2. For example, the absence of telemetry from specific nodes prevents verification of incident response efficiency or system availability, thus undermining the very indicators required for demonstrating regulatory conformity. This interdependence implies that cybersecurity and data governance cannot be separated in cyber‚Äìphysical systems such as railway networks. Effective policy enforcement requires not only procedural documentation but also sensor-generated evidence supporting continuous evaluation. As the focus of the article is on the governance-level analytical process, the NIS2 mapping does not constitute a formal audit implementation. Instead, the objective is to illustrate how FMEA parameters can be aligned with the directive‚Äôs requirements using sensor-based indicators and operational observations. This provides a conceptual yet practicable chain of evidence that can be adapted to specific organizational contexts and auditing frameworks, without prescribing a single technical implementation. Consequently, data quality management should be considered a component of cybersecurity governance frameworks, with explicit inclusion in risk management documentation submitted to national authorities under NIS2 supervision. The integration of FMEA with real-time sensor analytics produced several practical implications for enhancing cyber resilience and operational safety: Quantitative Decision Support‚ÄîThe conversion of sensor-level failure data into RPN metrics provided a transparent basis for prioritizing corrective actions. This allowed cybersecurity teams to focus resources on high-impact areas such as firmware integrity, redundant communication, and anomaly detection.Predictive Risk Management‚ÄîContinuous recalibration of RPN values based on streaming sensor data enabled the transition from static, compliance-driven audits to predictive cybersecurity, in which risks are anticipated and mitigated before incidents occur.Cross-Domain Synergy between Safety and Security‚ÄîThe FMEA model facilitated collaboration between safety engineers and cybersecurity specialists. Since both groups rely on structured failure analysis, the shared framework promoted integrated risk governance aligned with RAMS and IEC 62443 standards.Support for Audit and Certification‚ÄîQuantified sensor-based indicators, including detection rate and Mean Time to Detect (MTTD), can serve as verifiable evidence in compliance audits conducted by national supervisory authorities, fulfilling NIS2 Article 32 requirements regarding oversight and reporting. From a systems perspective, the study demonstrates that cyber resilience is a measurable property that can be continuously observed through sensor-derived metrics. This challenges the traditional view of cybersecurity as a purely administrative or technical domain and situates it within the broader paradigm ofdata-driven infrastructure assurance. The results align with recent research emphasizing the need to embed cybersecurity mechanisms within sensor and IoT architectures. Studies published in Sensors (e.g., [19,22,23,24]) have shown that anomaly detection, blockchain-based integrity verification, and AI-driven analytics enhance trust in industrial sensor networks. The current work extends these insights to the railway sector, showing that the same principles apply to safety-critical transportation systems. From an industrial perspective, the proposed FMEA‚Äìsensor integration framework addresses several challenges faced by railway operators: It supports compliance harmonization between NIS2 and sector-specific standards such as EN 50126 [20]/50129 [21] (RAMS) and ISO/IEC 27001 [18].It offers a scalable methodology applicable to both legacy OT assets and emerging IoT platforms.It provides a feedback mechanism between field-level sensors and strategic governance processes. Furthermore, the approach contributes to the development of digital twins for infrastructure cybersecurity‚Äîvirtual models that continuously assimilate sensor data to simulate and evaluate the impact of potential cyber events. Such models represent the next step toward proactive, simulation-based risk management in transport systems. Despite the promising outcomes, several limitations were identified: Limited empirical validation: The case study relied on a single operational segment; broader validation across multiple networks is needed to ensure generalizability.Data availability constraints: Some security-relevant datasets remain restricted due to confidentiality, limiting the full application of supervised ML models.Dynamic threat evolution: As attack vectors evolve, periodic retraining of detection algorithms is required to maintain effectiveness. Future research should focus on: Expanding the dataset to include cross-border and multi-operator scenarios;Integrating blockchain-based audit trails for immutable RPN tracking;Evaluating energy-efficient ML techniques suitable for low-power railway sensors;Developing standardized metrics linking NIS2 performance indicators with sensor-derived data integrity measures. Ultimately, advancing toward a sensor-enabled, self-adaptive cybersecurity framework will allow railway infrastructure to evolve into a resilient ecosystem that continuously learns from its operational data. Given the focus of the study‚Äîlinking NIS2 governance processes with sensor-driven analytical workflows‚Äîthe methodology follows an expert-based evaluation paradigm. It does not include the full experimental protocol typically expected in ML research (e.g., formalized train/validation/test splits, baseline comparison models, or statistical significance analysis). Expanding the work toward a reproducible ML benchmark is planned as a separate future study dedicated specifically to algorithmic performance evaluation in railway IoT environments. This study provided a structured methodology that operationalized NIS2 risk-management requirements through sensor-derived indicators and FMEA quantification. Three primary scientific contributions were achieved. First, the work extended the traditional Failure Mode and Effects Analysis (FMEA) model beyond hardware and reliability failures, incorporating cyber-induced failure modes affecting data integrity, availability, and authenticity in railway IoT and OT environments. Second, real-time anomaly indicators and streaming sensor evidence were integrated into dynamic RPN updates, enabling measurable risk variation tracking rather than static audit documentation. Third, the FMEA outputs were explicitly mapped onto regulatory obligations defined in NIS2 Article 21, demonstrating that compliance could be evidenced through data-driven KPIs such as detection latency, Mean Time to Detect (MTTD), and post-mitigation RPN contraction. Together, these contributions filled a methodological gap between governance frameworks and field-layer sensor analytics. Despite these advances, several open challenges remained. The expert-driven scoring model and scenario-based anomaly evaluation limited reproducibility, particularly due to the absence of large-scale labelled datasets and inter-operator benchmarking. Cyber-threat evolution required periodic recalibration of the Occurrence (O) and Detection (D) parameters, as fixed expert estimates may lose validity over time without continuous evidence ingestion. Furthermore, integration of edge analytics into legacy OT systems demanded careful performance balancing, especially where safety certification cycles restrict frequent software changes. There also remained a methodological gap in linking RPN variation with resilience forecasting: while RPN reduction demonstrated control effectiveness retrospectively, its predictive interpretation required further empirical validation across broader railway environments. Future work should therefore focus on dataset expansion across heterogeneous infrastructure, cross-operator validation of RPN dynamics under real incident conditions, and the development of federated learning pipelines that update detection models without exposing raw operational data. These efforts would strengthen the reproducibility of the proposed framework, progress toward standardized cyber-sensor benchmarks for transport infrastructure, and support the development of self-learning risk-management ecosystems consistent with the long-term objectives of NIS2 implementation.",
            "8. Conclusions and Future Work": "This study proposed and validated a sensor-centric framework for cybersecurity risk management in railway infrastructure, aligning the methodology with the requirements of the NIS2 Directive. By extending the Failure Mode and Effects Analysis (FMEA) approach to encompass cyber-induced sensor failures, the research demonstrated how regulatory obligations can be operationalized through quantitative, data-driven indicators. The main conclusions are as follows: Sensor networks constitute the foundation of cybersecurity in cyber‚Äìphysical railway systems. Their correct functioning and data integrity are essential for ensuring both operational safety and compliance with NIS2.The adapted FMEA methodology effectively quantified cyber risks associated with sensor networks, providing structured prioritization based on severity, occurrence, and detection parameters.Integration of real-time sensor monitoring enabled dynamic updates of RPN values, establishing a continuous feedback loop between risk assessment and control effectiveness evaluation‚Äîdirectly addressing NIS2 Criteria 1 and 6.Machine learning and edge analytics significantly improved detectability, reducing detection latency and enabling predictive risk management.The approach provided verifiable metrics (e.g., RPN, MTTD, DA, FAR) that can serve as objective evidence in cybersecurity audits and performance reviews mandated by competent authorities under NIS2. Overall, the study demonstrated that railway cybersecurity cannot be confined to network protection or compliance documentation‚Äîit must extend to the sensor layer, where data originate and where operational resilience is determined in real time. The proposed approach offers a practical mechanism for aligning technical risk assessment with governance-level compliance processes. By integrating FMEA results and sensor analytics into a unified management framework, railway operators can: Establish traceable links between technical incidents and NIS2 policy outcomes;Implement evidence-based evaluation of control effectiveness using sensor data;Support cross-functional coordination between safety, maintenance, and cybersecurity teams;Facilitate automated reporting and audit readiness through standardized performance indicators. This integrated framework thus supports a transition from reactive compliance to proactive cyber resilience, where the railway system continuously validates its own security posture through sensor-derived intelligence. Building upon the presented results, future research should focus on developing fully automated, self-adaptive risk management ecosystems for sensor-based critical infrastructure. Key research and development directions include: Digital Twins for Cyber Risk Assessment‚Äîcreation of digital replicas of railway systems where simulated cyber events can be tested and mitigation strategies evaluated before deployment.Integration with Blockchain Technologies‚Äîimplementation of immutable audit trails for RPN evolution and event verification, ensuring full traceability of security control performance.Adaptive Machine Learning Models‚Äîcontinuous retraining of algorithms using federated learning and transfer learning approaches to maintain detection efficiency across evolving threat landscapes.Standardization of Sensor-Centric Cyber Metrics‚Äîdevelopment of harmonized indicators connecting NIS2 requirements with real-time data quality and integrity measurements.Cross-Sector Applications‚Äîextending the proposed framework to other transport domains (aviation, maritime, road) and critical infrastructures where IoT systems play a similar role. Due to the scenario-based and expert-driven nature of the study, the case-study network description does not include full implementation details such as exact ML architectures, hyperparameter grids, or resource benchmarks. The intention is to demonstrate how illustrative anomaly-detection mechanisms interact with FMEA scoring and the NIS2 process cycle, rather than to provide a hardware-specific or model-specific reproducibility package. A more detailed implementation study is planned as future work. The paper does not include detailed network-topology diagrams, replay-path instrumentation, or raw log listings, as the aim is to present a governance-oriented and process-level analytical model rather than a network-performance benchmark. The latency values reported in the case-study scenario are intended to illustrate the impact of redundancy logic on the FMEA detectability dimension rather than to characterise system-level QoS. In the long term, the fusion of sensing technologies, AI-driven analytics, and regulatory compliance frameworks will enable the creation of intelligent, self-monitoring infrastructure systems. Such systems will continuously evaluate their cyber posture and autonomously adapt to emerging threats‚Äîtransforming compliance from a periodic obligation into an ongoing, sensor-driven capability. The conducted research has shown that cybersecurity risk management in critical infrastructure must evolve from document-based frameworks toward evidence-based, data-centric systems. By positioning sensors as active components of the cybersecurity architecture, the railway sector can not only meet regulatory obligations but also enhance the safety, reliability, and trustworthiness of its operations. In conclusion, sensor-enabled cybersecurity governance‚Äîsupported by structured methodologies such as FMEA and empowered by real-time analytics‚Äîrepresents a crucial step toward the digital resilience envisioned by the NIS2 Directive and the broader European strategy for secure, intelligent transport systems."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/1424-8220/25/23/7384",
        "scraped_at": "2025-12-05 23:55:16"
    },
    {
        "title": "Lactic Fermentation Spectral Analysis of Target Substrates and Food and Feed Wastes for Energy Applications",
        "authors": "byMariusz Adamski,Marcin Herkowiak,Przemys≈Çaw Marek,Katarzyna Dzida,Magdalena Kap≈ÇanandKamila E. Klimek",
        "journal": "Energies2025,18(23), 6360; https://doi.org/10.3390/en18236360 (registering¬†DOI) - 4 Dec 2025",
        "abstract": "The article deals with the creation of a calibration model of lactic acid content in an aqueous solution. The research concept included the preparation of a control tool for the process of modifying the properties of the food fraction for methane fermentation bacteria. The thesis was formulated that it is possible to prepare a systemic solution for real-time observation and monitoring of lactic acid secretion during the digestion of a hydrated mixture of food fractions. The scientific aim of the work was to develop and verify a calibration model of lactic acid content in an aqueous mixture with limited transparency for visible light waves. The research methodology was based on near-infrared spectroscopy with multivariate analysis. Stochastic modeling with noise reduction based on orthogonal decomposition was used. A calibration model was created using Gaussian processes (GP) to predict the lactic acid concentration in an aqueous solution or mixture using an NIR-Vis spectrophotometer. The design of the calibration model was based on absorbance spectra and computational data from selected wavelength ranges from 450 nm to 1900 nm. The measurement data in the form of spectra were limited from the initial wider range (400‚Äì2250 nm) to reduce interference. The generated calibration model achieved a mean error level not exceeding 2.47 g‚àôdm‚àí3of the identified lactic acid fraction. The coefficient of determination R2was 0.996. The effect of absorbing the emitter waves was achieved despite the limited transparency of the mixture.Keywords:source;spectroscopy;proper orthogonal decomposition;Gaussian processes;lactic fermentation;lactic acid",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "1.1. Feed and Food Waste and Methods of Its ProcessingOrganic waste such as food and feed waste can be managed in various ways. This possibility stems from the content of these wastes in components such as proteins, fats, carbohydrates, vitamins, and minerals, which may come from various raw materials. This waste can undergo various processes of biodegradation, fermentation, or decay, leading to the formation of gases such as methane and carbon dioxide. Food waste is usually moist due to the water content in food, which affects the rate of decomposition and fermentation processes and can increase the weight of the waste [1,2].Feed and food waste is susceptible to biodegradation, which means that it can be broken down by microorganisms such as bacteria, fungi, and protozoa. Biodegradation leads to the production of nutrients, but can also generate unpleasant odors and gases. Food waste can include food scraps, expired food products, peelings, bones, husks, and inedible parts of plants and animals. It can occur on a household scale as well as on an industrial or commercial level [3].Excess food waste is a serious environmental problem. Food decomposition produces methane, which is a significant greenhouse gas contributing to climate change. Food waste also takes up space in landfills, contributing to the problem of waste management. In order to minimize the negative impact of food waste on the environment, recycling and composting practices are increasingly being promoted. Recycling food waste can include using it to produce compost, biogas, or other forms of energy recovery. Raising public awareness of the problem of food waste and educating people on how to reduce food waste are key to effectively managing this issue [4].First and foremost, organic waste can be used for energy and fertilizer production.Energy production includes technologies such as:-Incineration;-Methane fermentation;-Gasification;-Pyrolysis.Waste incineration is the oldest and most common method of converting waste into energy. However, it is necessary to ensure that the fuel has the lowest possible moisture content. Modern incineration technologies are equipped with exhaust gas treatment systems that minimize the emission of harmful substances into the atmosphere [5].Methane fermentation is a process of biological decomposition of organic matter in anaerobic conditions, resulting in the production of biogas. The main components of biogas are methane, which is an energy carrier, and carbon dioxide [1].Currently, methane fermentation on an industrial scale is seeking new sources of substrates and methods for their conversion in order to increase feed digestibility and/or increase energy yield from biomass fractions. Czeka≈Ça et al. [6] emphasize the importance of raw material quality and processing (grinding, homogenization, moisture control). According to the researchers, this promotes increased biogas productivity and methane content. The result is stabilization of heat and electricity supply through the CHP system of biogas plants. In turn, in Wa≈Çowski‚Äôs research [7], the integration of substrate preparation processes has an impact on the efficiency of biomass conversion into energy in biogas plants. The simulation showed that combining heating and mixing, controlling the flow of incoming feed fractions and the outflow of digestate fractions, together with the elimination of system and process limitations as a whole, reduces losses. Substrates and transformation products undergo a process optimized in terms of efficiency criteria and energy reduction.Not only the physical conversion of substrates is important for the efficiency of the methane fermentation process, but also the conversion of their structure through the influence of microorganisms. For example, inoculating silage substrate with theLactobacillus buchneristrain shortens the latent phase and increases the rate of biogas production [8].Gasification is the process of thermochemical conversion of waste into synthesis gas (syngas) under high temperature and controlled oxygen access conditions. Syngas consists mainly of hydrogen, carbon monoxide, and methane and can be used as a fuel for electricity and heat production or as a raw material in the chemical industry [5,9].Pyrolysis is the process of thermochemical decomposition of organic materials under anaerobic conditions, resulting in the formation of pyrolytic oils, pyrolytic gas, and charcoal. These products can be used as fuels or raw materials for energy production [10].Organic waste can be used for fertilization purposes primarily through composting. This process plays a key role in waste management. Composting is a natural biological process in which microorganisms, bacteria, and fungi break down organic waste, transforming it into nutrient-rich compost. This compost can be used as fertilizer to improve soil structure, increase its water capacity, and provide essential nutrients to plants [11].The use of waste as fertilizer contributes to the sustainable management of natural resources, reducing the amount of waste going to landfills and reducing the demand for chemical fertilizers. In addition, composting and other processes that convert waste into fertilizers help reduce greenhouse gas emissions, such as methane, which is produced during the decomposition of waste in landfills [12].However, the composting process requires adequate control to ensure that the processed waste is safe and does not contain any harmful components. 1.2. The Role of Lactic Acid Fermentation in the Anaerobic Stabilization of WasteLactic fermentation helps stabilize organic waste anaerobically, serving as the initial stage of fermentation. It makes the material more biologically stable, less prone to rotting, and more suitable for further processes such as methane fermentation (biogas). It is also an important element in the composting of organic waste [13].First, the sugars contained in the waste are converted into lactic acid by lactic acid bacteria (Lactobacillus, Leuconostoc, Pediococcus). Lactic acid, in turn, lowers the pH of the waste mass to between 3.5 and 4.5. This, in turn, inhibits the growth of unwanted putrefactive bacteria, pathogens, and mold. The resulting acidic environment favors lactic acid bacteria and eliminates pathogens, making the waste more stable and suitable for longer storage, for example for methane fermentation. By reducing the risk of material decay, emissions of gases such as hydrogen sulfide and ammonia are reduced [14].Lactic fermentation is an important form of substrate preparation for the methane fermentation process. It breaks down hydrocarbons and acidifies the mass, which facilitates the subsequent decomposition of the substrate by methanogenic bacteria.In addition, lactic acid can be a precursor in the methanogenic fermentation process, even though it does not directly participate in methane production. However, it is an intermediate substance, similar to propionic acid, in the earlier stages of the digestion process. Lactic fermentation, like methane fermentation, accelerates the breakdown of complex biofractions such as proteins, carbohydrates and fats. As a result, the organic compounds that are formed are easier to digest due to the hydrolysis stage, which is similar to methane fermentation. Lactic acid is associated with acidogenic fermentation, where it is a product of the conversion of simpler biopolymers by bacteria. Alongside short-chain fatty acids, alcohols, hydrogen and carbon dioxide, it is an intermediate product under favorable conditions. Subsequent stages of the fermentation process (acetogenesis and methanogenesis) result in the utilization of lactic acid as an intermediate nutrient for specialized groups of bacteria [15].According to PiƒÖtek and Bartkowiak [16], the physical conversion of biomass facilitates the extraction of nutrients from indigestible biomass. Similarly, preliminary lactic fermentation is a source of nutrients for the methane fermentation process with reduced energy requirements of the bacterial deposit. In both cases, the level of mineralization of the organic feed fraction increases.In short, lactic fermentation improves process hygiene, increases biological safety, increases biogas production efficiency, and reduces odor and sanitary problems [17,18]. 1.3. Evaluation of Technological Parameters of Lactic FermentationThe topic of the thesis addressed the issue of analyzing and evaluating the technological parameters of the lactic fermentation process. The bacterial culture used in the lactic fermentation process metabolizes specific nutrients based on products belonging to the food and feed group. An example of a characteristic food substrate is molasses, which is produced in the processing of sugar beets and is a semi-finished product for both the food and feed industries. One of the key parameters of the lactic fermentation process is the concentration of lactic acid, which varies in different phases of the fermentation process. The ability to obtain information about the concentration of lactic acid by observing the fermenting fraction in real time is extremely helpful in making decisions during the production process of the aforementioned acid. An alternative to observing the concentration of lactic acid in real time is to obtain samples from the reaction chamber and analyze them post factum. An attempt was therefore made to adapt the spectral analysis method supported by a calibration model to replace chromatographic methods and enable real-time observation of lactic fermentation products.The key research issue is to develop methodological calibration models for the detection of lactic acid in a mixture of molasses and water with reduced transparency using transmission and absorption of generated waves.The objective of the work was formulated.The aim of this work is to demonstrate the possibility of assessing lactic acid concentration in an aqueous environment, taking into account mixtures of nutrients and aqueous solvents, using near-infrared spectral analysis.Spectral analysis, also known as spectral analysis, is a type of teleanalysis, a method of qualitative and quantitative determination of substances based on the spectrum, including methods of generating spectra [19].The main objective of the research was to generate spectral spectra using the wavelength range in visible light and infrared up to 1900 nm, and to create a calibration model based on optimization operations using mathematical tools.In order to achieve the objective of the study, it was necessary to identify a number of sub-tasks.First, an aqueous solution of lactic acid was prepared and the range of lactic acid concentrations characteristic of the lactic fermentation process on a technical scale was outlined.In the next step, a test station equipped with an absorption and reflection measuring head was prepared, using a wavelength range from 450 nm to 1900 nm.Following the preparatory work, the next step was to obtain spectral spectra of the solvent, concentrated lactic acid, and aqueous lactic acid solution.In the further part of the study, an attempt was made to obtain spectral spectra of an aqueous molasses solution.In the final stage of the research, a calibration model for lactic acid content in the aquatic environment and in an aqueous molasses solution was developed using software ‚ÄúCamo UnscramblerX 10.1‚Äù.Optimization activities related to the construction of calibration models have been planned, taking into account the model accuracy criterion.",
            "1.1. Feed and Food Waste and Methods of Its Processing": "Organic waste such as food and feed waste can be managed in various ways. This possibility stems from the content of these wastes in components such as proteins, fats, carbohydrates, vitamins, and minerals, which may come from various raw materials. This waste can undergo various processes of biodegradation, fermentation, or decay, leading to the formation of gases such as methane and carbon dioxide. Food waste is usually moist due to the water content in food, which affects the rate of decomposition and fermentation processes and can increase the weight of the waste [1,2]. Feed and food waste is susceptible to biodegradation, which means that it can be broken down by microorganisms such as bacteria, fungi, and protozoa. Biodegradation leads to the production of nutrients, but can also generate unpleasant odors and gases. Food waste can include food scraps, expired food products, peelings, bones, husks, and inedible parts of plants and animals. It can occur on a household scale as well as on an industrial or commercial level [3]. Excess food waste is a serious environmental problem. Food decomposition produces methane, which is a significant greenhouse gas contributing to climate change. Food waste also takes up space in landfills, contributing to the problem of waste management. In order to minimize the negative impact of food waste on the environment, recycling and composting practices are increasingly being promoted. Recycling food waste can include using it to produce compost, biogas, or other forms of energy recovery. Raising public awareness of the problem of food waste and educating people on how to reduce food waste are key to effectively managing this issue [4]. First and foremost, organic waste can be used for energy and fertilizer production. Energy production includes technologies such as: -Incineration;-Methane fermentation;-Gasification;-Pyrolysis. Waste incineration is the oldest and most common method of converting waste into energy. However, it is necessary to ensure that the fuel has the lowest possible moisture content. Modern incineration technologies are equipped with exhaust gas treatment systems that minimize the emission of harmful substances into the atmosphere [5]. Methane fermentation is a process of biological decomposition of organic matter in anaerobic conditions, resulting in the production of biogas. The main components of biogas are methane, which is an energy carrier, and carbon dioxide [1]. Currently, methane fermentation on an industrial scale is seeking new sources of substrates and methods for their conversion in order to increase feed digestibility and/or increase energy yield from biomass fractions. Czeka≈Ça et al. [6] emphasize the importance of raw material quality and processing (grinding, homogenization, moisture control). According to the researchers, this promotes increased biogas productivity and methane content. The result is stabilization of heat and electricity supply through the CHP system of biogas plants. In turn, in Wa≈Çowski‚Äôs research [7], the integration of substrate preparation processes has an impact on the efficiency of biomass conversion into energy in biogas plants. The simulation showed that combining heating and mixing, controlling the flow of incoming feed fractions and the outflow of digestate fractions, together with the elimination of system and process limitations as a whole, reduces losses. Substrates and transformation products undergo a process optimized in terms of efficiency criteria and energy reduction. Not only the physical conversion of substrates is important for the efficiency of the methane fermentation process, but also the conversion of their structure through the influence of microorganisms. For example, inoculating silage substrate with theLactobacillus buchneristrain shortens the latent phase and increases the rate of biogas production [8]. Gasification is the process of thermochemical conversion of waste into synthesis gas (syngas) under high temperature and controlled oxygen access conditions. Syngas consists mainly of hydrogen, carbon monoxide, and methane and can be used as a fuel for electricity and heat production or as a raw material in the chemical industry [5,9]. Pyrolysis is the process of thermochemical decomposition of organic materials under anaerobic conditions, resulting in the formation of pyrolytic oils, pyrolytic gas, and charcoal. These products can be used as fuels or raw materials for energy production [10]. Organic waste can be used for fertilization purposes primarily through composting. This process plays a key role in waste management. Composting is a natural biological process in which microorganisms, bacteria, and fungi break down organic waste, transforming it into nutrient-rich compost. This compost can be used as fertilizer to improve soil structure, increase its water capacity, and provide essential nutrients to plants [11]. The use of waste as fertilizer contributes to the sustainable management of natural resources, reducing the amount of waste going to landfills and reducing the demand for chemical fertilizers. In addition, composting and other processes that convert waste into fertilizers help reduce greenhouse gas emissions, such as methane, which is produced during the decomposition of waste in landfills [12]. However, the composting process requires adequate control to ensure that the processed waste is safe and does not contain any harmful components.",
            "1.2. The Role of Lactic Acid Fermentation in the Anaerobic Stabilization of Waste": "Lactic fermentation helps stabilize organic waste anaerobically, serving as the initial stage of fermentation. It makes the material more biologically stable, less prone to rotting, and more suitable for further processes such as methane fermentation (biogas). It is also an important element in the composting of organic waste [13]. First, the sugars contained in the waste are converted into lactic acid by lactic acid bacteria (Lactobacillus, Leuconostoc, Pediococcus). Lactic acid, in turn, lowers the pH of the waste mass to between 3.5 and 4.5. This, in turn, inhibits the growth of unwanted putrefactive bacteria, pathogens, and mold. The resulting acidic environment favors lactic acid bacteria and eliminates pathogens, making the waste more stable and suitable for longer storage, for example for methane fermentation. By reducing the risk of material decay, emissions of gases such as hydrogen sulfide and ammonia are reduced [14]. Lactic fermentation is an important form of substrate preparation for the methane fermentation process. It breaks down hydrocarbons and acidifies the mass, which facilitates the subsequent decomposition of the substrate by methanogenic bacteria. In addition, lactic acid can be a precursor in the methanogenic fermentation process, even though it does not directly participate in methane production. However, it is an intermediate substance, similar to propionic acid, in the earlier stages of the digestion process. Lactic fermentation, like methane fermentation, accelerates the breakdown of complex biofractions such as proteins, carbohydrates and fats. As a result, the organic compounds that are formed are easier to digest due to the hydrolysis stage, which is similar to methane fermentation. Lactic acid is associated with acidogenic fermentation, where it is a product of the conversion of simpler biopolymers by bacteria. Alongside short-chain fatty acids, alcohols, hydrogen and carbon dioxide, it is an intermediate product under favorable conditions. Subsequent stages of the fermentation process (acetogenesis and methanogenesis) result in the utilization of lactic acid as an intermediate nutrient for specialized groups of bacteria [15]. According to PiƒÖtek and Bartkowiak [16], the physical conversion of biomass facilitates the extraction of nutrients from indigestible biomass. Similarly, preliminary lactic fermentation is a source of nutrients for the methane fermentation process with reduced energy requirements of the bacterial deposit. In both cases, the level of mineralization of the organic feed fraction increases. In short, lactic fermentation improves process hygiene, increases biological safety, increases biogas production efficiency, and reduces odor and sanitary problems [17,18].",
            "1.3. Evaluation of Technological Parameters of Lactic Fermentation": "The topic of the thesis addressed the issue of analyzing and evaluating the technological parameters of the lactic fermentation process. The bacterial culture used in the lactic fermentation process metabolizes specific nutrients based on products belonging to the food and feed group. An example of a characteristic food substrate is molasses, which is produced in the processing of sugar beets and is a semi-finished product for both the food and feed industries. One of the key parameters of the lactic fermentation process is the concentration of lactic acid, which varies in different phases of the fermentation process. The ability to obtain information about the concentration of lactic acid by observing the fermenting fraction in real time is extremely helpful in making decisions during the production process of the aforementioned acid. An alternative to observing the concentration of lactic acid in real time is to obtain samples from the reaction chamber and analyze them post factum. An attempt was therefore made to adapt the spectral analysis method supported by a calibration model to replace chromatographic methods and enable real-time observation of lactic fermentation products. The key research issue is to develop methodological calibration models for the detection of lactic acid in a mixture of molasses and water with reduced transparency using transmission and absorption of generated waves. The objective of the work was formulated. The aim of this work is to demonstrate the possibility of assessing lactic acid concentration in an aqueous environment, taking into account mixtures of nutrients and aqueous solvents, using near-infrared spectral analysis. Spectral analysis, also known as spectral analysis, is a type of teleanalysis, a method of qualitative and quantitative determination of substances based on the spectrum, including methods of generating spectra [19]. The main objective of the research was to generate spectral spectra using the wavelength range in visible light and infrared up to 1900 nm, and to create a calibration model based on optimization operations using mathematical tools. In order to achieve the objective of the study, it was necessary to identify a number of sub-tasks. First, an aqueous solution of lactic acid was prepared and the range of lactic acid concentrations characteristic of the lactic fermentation process on a technical scale was outlined. In the next step, a test station equipped with an absorption and reflection measuring head was prepared, using a wavelength range from 450 nm to 1900 nm. Following the preparatory work, the next step was to obtain spectral spectra of the solvent, concentrated lactic acid, and aqueous lactic acid solution. In the further part of the study, an attempt was made to obtain spectral spectra of an aqueous molasses solution. In the final stage of the research, a calibration model for lactic acid content in the aquatic environment and in an aqueous molasses solution was developed using software ‚ÄúCamo UnscramblerX 10.1‚Äù. Optimization activities related to the construction of calibration models have been planned, taking into account the model accuracy criterion.",
            "2. Materials and Methods": "2.1. General Characteristics of the Research MaterialThe research material used in the study was demineralized water, molasses, and lactic acid. To create aqueous solutions with different concentrations of lactic acid.Molasses is a thick, viscous liquid with a dark brown color, which is a by-product of the sugar production process from sugar cane or sugar beets.Molasses has a density of approximately 1.209 g¬∑cm3, which is higher than the density of water. The density of molasses may vary slightly depending on the source of the raw material and the sugar content. It is highly soluble in water due to its high sugar content. It consists mainly of sugars (sucrose, glucose, fructose), but also contains significant amounts of minerals (calcium, magnesium, potassium), vitamins (vitamin B6) and small amounts of proteins and other organic compounds.During sugar refining, after the crystalline sugar has been separated, the remaining syrup, which contains a high percentage of unrefined sugars, minerals, vitamins, and other organic substances, is called molasses. In the case of sugar cane, molasses is a by-product of white sugar production, while in the case of sugar beets, it is produced during the extraction and purification of beet sugar.It is an easily digestible substrate for fermentation bacteria regardless of the metabolic pathway. To prepare a molasses-based bacterial culture medium, molasses is usually diluted with demineralized water to the appropriate sugar concentration (usually around 5‚Äì25%). Other nutrients are then added, such as nitrogen sources (e.g., yeast extract), mineral salts (e.g., phosphates, sulfates), and the pH of the medium is adjusted [20,21]. The chemical composition of molasses is shown inTable 1. The results were confirmed with data from the literature [20].Table 1.Chemical composition of molasses. The results were confirmed with data from the literature [20]. Source, own research [20].The creation of lactic acid models involved adding an appropriate dose of lactic acid to an aqueous solution in order to change its concentration and enable the observation of spectral spectra characteristic of growth in an aqueous environment and in an aqueous molasses solution.At room temperature, lactic acid is a colorless, syrupy liquid that can crystallize into solids at lower temperatures. It has a melting point of approximately 16‚Äì17 ¬∞C for pure lactic acid and a boiling point of approximately 122 ¬∞C at a pressure of 15 mm Hg. The density of lactic acid at 20 ¬∞C is approximately 1.209 g¬∑cm3[22,23].Lactic acid is a weak acid with a pKa value of approximately 3.86, which means that it partially dissociates in aqueous solutions to form lactate ions (C3H5O3‚àí) and hydrogen ions (H+). Its ability to react with alcohols leads to the formation of esters such as lactates, and under suitable polymerization conditions, it can form polylactide (PLA), a biodegradable plastic. Lactic acid can also undergo oxidation and reduction reactions, depending on the reaction conditions, and is relatively stable over a wide pH range [23]. 2.2. Research Equipment2.2.1. Marking the Weight of the Laboratory SampleThe mass of the laboratory sample was determined using an electronic scale. Two types of scales were used for this purpose. The first scale had a measuring range from 0 to 5000 g with an accuracy of 0.01 g and was used to prepare laboratory samples and analyze physicochemical parameters. Thanks to its resolution, it enabled accurate measurements of even hundredths of a gram.2.2.2. Determining the Sample VolumeThe sample volume was determined using a minilab 201 automatic pipette (HTL Biotechnology, Javen√©, France). It was calculated that at a density of 1.209 g¬∑cm3of lactic acid, the concentration of one sample was 0.96 g¬∑dm‚àí3. For both probes, 146 dosages were performed in sequence. After each dosing, the spectral spectrum was measured.The dosage of lactic acid to the aqueous molasses solution is shown inTable 2for the A40 probe andTable 3for the Niron probe.Table 2.Lactic acid dosage for the A40 head.Table 3.Lactic acid dosage for the Niron head.2.2.3. Ambient TemperatureThe temperature in the laboratory room can affect the behavior of the sample and the subsequent spectrum, therefore, the test was carried out at an appropriate laboratory temperature of 20‚Äì25 degrees to maintain the right conditions for each measurement. At refrigerator temperature, the solution visibly thickened, so it was heated to 25 degrees on a stirrer. Lactic acid was also at a temperature of 20‚Äì25 degrees for each measurement.2.2.4. Research Station in the Process of Acquiring Spectral Spectra in the near Infrared RangeThe AgroSpec spectrometer from Tec5 AG was used by the Department of Biosystems Engineering at the University of Life Sciences to obtain spectra from an aqueous solution of molasses. AgroSpec is a mobile, stationary, VIS‚ÄìNIR spectrophotometer (Tec5 Technology for Spectroscopy, Oberursel, Germany) [24].The device allows for recording near-infrared spectra using reflection and transmission methods, with RP-7 and A40 probes, and an optical fiber was used for the connection. The samples were placed on a rotating table, on a Petri dish above the A40 probe. For the Niron probe, it was necessary to immerse the probe in a container with an aqueous solution of molasses. The distance between the A40 probe and the table is 55 mm. Lighting was provided by an RP-7 bulb located above the sample. The spectrum of the solution was obtained using the MultiSpec Pro II program (Standard). A magnetic stirrer was used to thoroughly mix the aqueous molasses solution with lactic acid. The test setup is shown inFigure 1.Figure 1.Research station for obtaining NIR spectra (1.‚ÄîRP-7 probe, 2.‚Äîrotating table with Petri dish, 3.‚ÄîA40 probe, 4.‚Äîmagnetic stirrer, 5.‚Äîcomputer with software for recording the obtained spectra). [Source: own study].A MINILAB 201 automatic pipette (Figure 2) was used for precise dosing of lactic acid, allowing dosing in the range from 0.001 to 0.5 cm3. The test material was applied to a Petri dish using 20 cm3syringes. Camo Unscrambler X version 10.1 software was used to analyze and model the obtained spectral spectra.Figure 2.MINILAB 201 automatic pipette. [Source: own study].2.2.5. Statistical Analysis of Research ResultsIt was assumed that a statistically significant difference indicates that the observed variation between the analyzed groups or variables is sufficiently substantial that its occurrence by random chance is highly improbable.The adopted research design and applied analytical instruments provided grounds to assume that the identified differences were not attributable to sampling errors or stochastic variability. A significance level of 0.05 (5%) was established, defining the threshold beyond which the difference could be considered statistically meaningful.Within the framework of a statistical procedure for comparing intergroup variance (ANOVA), an attempt was undertaken to determine whether significant differences existed among the mean values of the examined data groups.The process of data validation was based on the following hypotheses:Null hypothesis (H0): there is no statistically significant difference in the mean values between the analyzed data groups;Alternative hypothesis (H1): there exists a statistically significant difference in the mean values between the analyzed data groups.Empirical data were collected as multiple repetitions of individual measurements (a minimum of three replicates), followed by a variance analysis. When thep-value obtained from the ANOVA test was lower than the predetermined significance threshold (e.g., 0.05), the null hypothesis was rejected, indicating that the mean values of the analyzed datasets differed significantly.",
            "2.1. General Characteristics of the Research Material": "The research material used in the study was demineralized water, molasses, and lactic acid. To create aqueous solutions with different concentrations of lactic acid. Molasses is a thick, viscous liquid with a dark brown color, which is a by-product of the sugar production process from sugar cane or sugar beets. Molasses has a density of approximately 1.209 g¬∑cm3, which is higher than the density of water. The density of molasses may vary slightly depending on the source of the raw material and the sugar content. It is highly soluble in water due to its high sugar content. It consists mainly of sugars (sucrose, glucose, fructose), but also contains significant amounts of minerals (calcium, magnesium, potassium), vitamins (vitamin B6) and small amounts of proteins and other organic compounds. During sugar refining, after the crystalline sugar has been separated, the remaining syrup, which contains a high percentage of unrefined sugars, minerals, vitamins, and other organic substances, is called molasses. In the case of sugar cane, molasses is a by-product of white sugar production, while in the case of sugar beets, it is produced during the extraction and purification of beet sugar. It is an easily digestible substrate for fermentation bacteria regardless of the metabolic pathway. To prepare a molasses-based bacterial culture medium, molasses is usually diluted with demineralized water to the appropriate sugar concentration (usually around 5‚Äì25%). Other nutrients are then added, such as nitrogen sources (e.g., yeast extract), mineral salts (e.g., phosphates, sulfates), and the pH of the medium is adjusted [20,21]. The chemical composition of molasses is shown inTable 1. The results were confirmed with data from the literature [20]. Table 1.Chemical composition of molasses. The results were confirmed with data from the literature [20]. Source, own research [20]. The creation of lactic acid models involved adding an appropriate dose of lactic acid to an aqueous solution in order to change its concentration and enable the observation of spectral spectra characteristic of growth in an aqueous environment and in an aqueous molasses solution. At room temperature, lactic acid is a colorless, syrupy liquid that can crystallize into solids at lower temperatures. It has a melting point of approximately 16‚Äì17 ¬∞C for pure lactic acid and a boiling point of approximately 122 ¬∞C at a pressure of 15 mm Hg. The density of lactic acid at 20 ¬∞C is approximately 1.209 g¬∑cm3[22,23]. Lactic acid is a weak acid with a pKa value of approximately 3.86, which means that it partially dissociates in aqueous solutions to form lactate ions (C3H5O3‚àí) and hydrogen ions (H+). Its ability to react with alcohols leads to the formation of esters such as lactates, and under suitable polymerization conditions, it can form polylactide (PLA), a biodegradable plastic. Lactic acid can also undergo oxidation and reduction reactions, depending on the reaction conditions, and is relatively stable over a wide pH range [23].",
            "2.2. Research Equipment": "2.2.1. Marking the Weight of the Laboratory SampleThe mass of the laboratory sample was determined using an electronic scale. Two types of scales were used for this purpose. The first scale had a measuring range from 0 to 5000 g with an accuracy of 0.01 g and was used to prepare laboratory samples and analyze physicochemical parameters. Thanks to its resolution, it enabled accurate measurements of even hundredths of a gram. 2.2.2. Determining the Sample VolumeThe sample volume was determined using a minilab 201 automatic pipette (HTL Biotechnology, Javen√©, France). It was calculated that at a density of 1.209 g¬∑cm3of lactic acid, the concentration of one sample was 0.96 g¬∑dm‚àí3. For both probes, 146 dosages were performed in sequence. After each dosing, the spectral spectrum was measured.The dosage of lactic acid to the aqueous molasses solution is shown inTable 2for the A40 probe andTable 3for the Niron probe.Table 2.Lactic acid dosage for the A40 head.Table 3.Lactic acid dosage for the Niron head. 2.2.3. Ambient TemperatureThe temperature in the laboratory room can affect the behavior of the sample and the subsequent spectrum, therefore, the test was carried out at an appropriate laboratory temperature of 20‚Äì25 degrees to maintain the right conditions for each measurement. At refrigerator temperature, the solution visibly thickened, so it was heated to 25 degrees on a stirrer. Lactic acid was also at a temperature of 20‚Äì25 degrees for each measurement. 2.2.4. Research Station in the Process of Acquiring Spectral Spectra in the near Infrared RangeThe AgroSpec spectrometer from Tec5 AG was used by the Department of Biosystems Engineering at the University of Life Sciences to obtain spectra from an aqueous solution of molasses. AgroSpec is a mobile, stationary, VIS‚ÄìNIR spectrophotometer (Tec5 Technology for Spectroscopy, Oberursel, Germany) [24].The device allows for recording near-infrared spectra using reflection and transmission methods, with RP-7 and A40 probes, and an optical fiber was used for the connection. The samples were placed on a rotating table, on a Petri dish above the A40 probe. For the Niron probe, it was necessary to immerse the probe in a container with an aqueous solution of molasses. The distance between the A40 probe and the table is 55 mm. Lighting was provided by an RP-7 bulb located above the sample. The spectrum of the solution was obtained using the MultiSpec Pro II program (Standard). A magnetic stirrer was used to thoroughly mix the aqueous molasses solution with lactic acid. The test setup is shown inFigure 1.Figure 1.Research station for obtaining NIR spectra (1.‚ÄîRP-7 probe, 2.‚Äîrotating table with Petri dish, 3.‚ÄîA40 probe, 4.‚Äîmagnetic stirrer, 5.‚Äîcomputer with software for recording the obtained spectra). [Source: own study].A MINILAB 201 automatic pipette (Figure 2) was used for precise dosing of lactic acid, allowing dosing in the range from 0.001 to 0.5 cm3. The test material was applied to a Petri dish using 20 cm3syringes. Camo Unscrambler X version 10.1 software was used to analyze and model the obtained spectral spectra.Figure 2.MINILAB 201 automatic pipette. [Source: own study]. 2.2.5. Statistical Analysis of Research ResultsIt was assumed that a statistically significant difference indicates that the observed variation between the analyzed groups or variables is sufficiently substantial that its occurrence by random chance is highly improbable.The adopted research design and applied analytical instruments provided grounds to assume that the identified differences were not attributable to sampling errors or stochastic variability. A significance level of 0.05 (5%) was established, defining the threshold beyond which the difference could be considered statistically meaningful.Within the framework of a statistical procedure for comparing intergroup variance (ANOVA), an attempt was undertaken to determine whether significant differences existed among the mean values of the examined data groups.The process of data validation was based on the following hypotheses:Null hypothesis (H0): there is no statistically significant difference in the mean values between the analyzed data groups;Alternative hypothesis (H1): there exists a statistically significant difference in the mean values between the analyzed data groups.Empirical data were collected as multiple repetitions of individual measurements (a minimum of three replicates), followed by a variance analysis. When thep-value obtained from the ANOVA test was lower than the predetermined significance threshold (e.g., 0.05), the null hypothesis was rejected, indicating that the mean values of the analyzed datasets differed significantly.",
            "2.2.1. Marking the Weight of the Laboratory Sample": "The mass of the laboratory sample was determined using an electronic scale. Two types of scales were used for this purpose. The first scale had a measuring range from 0 to 5000 g with an accuracy of 0.01 g and was used to prepare laboratory samples and analyze physicochemical parameters. Thanks to its resolution, it enabled accurate measurements of even hundredths of a gram.",
            "2.2.2. Determining the Sample Volume": "The sample volume was determined using a minilab 201 automatic pipette (HTL Biotechnology, Javen√©, France). It was calculated that at a density of 1.209 g¬∑cm3of lactic acid, the concentration of one sample was 0.96 g¬∑dm‚àí3. For both probes, 146 dosages were performed in sequence. After each dosing, the spectral spectrum was measured. The dosage of lactic acid to the aqueous molasses solution is shown inTable 2for the A40 probe andTable 3for the Niron probe. Table 2.Lactic acid dosage for the A40 head. Table 3.Lactic acid dosage for the Niron head.",
            "2.2.3. Ambient Temperature": "The temperature in the laboratory room can affect the behavior of the sample and the subsequent spectrum, therefore, the test was carried out at an appropriate laboratory temperature of 20‚Äì25 degrees to maintain the right conditions for each measurement. At refrigerator temperature, the solution visibly thickened, so it was heated to 25 degrees on a stirrer. Lactic acid was also at a temperature of 20‚Äì25 degrees for each measurement.",
            "2.2.4. Research Station in the Process of Acquiring Spectral Spectra in the near Infrared Range": "The AgroSpec spectrometer from Tec5 AG was used by the Department of Biosystems Engineering at the University of Life Sciences to obtain spectra from an aqueous solution of molasses. AgroSpec is a mobile, stationary, VIS‚ÄìNIR spectrophotometer (Tec5 Technology for Spectroscopy, Oberursel, Germany) [24]. The device allows for recording near-infrared spectra using reflection and transmission methods, with RP-7 and A40 probes, and an optical fiber was used for the connection. The samples were placed on a rotating table, on a Petri dish above the A40 probe. For the Niron probe, it was necessary to immerse the probe in a container with an aqueous solution of molasses. The distance between the A40 probe and the table is 55 mm. Lighting was provided by an RP-7 bulb located above the sample. The spectrum of the solution was obtained using the MultiSpec Pro II program (Standard). A magnetic stirrer was used to thoroughly mix the aqueous molasses solution with lactic acid. The test setup is shown inFigure 1. Figure 1.Research station for obtaining NIR spectra (1.‚ÄîRP-7 probe, 2.‚Äîrotating table with Petri dish, 3.‚ÄîA40 probe, 4.‚Äîmagnetic stirrer, 5.‚Äîcomputer with software for recording the obtained spectra). [Source: own study]. A MINILAB 201 automatic pipette (Figure 2) was used for precise dosing of lactic acid, allowing dosing in the range from 0.001 to 0.5 cm3. The test material was applied to a Petri dish using 20 cm3syringes. Camo Unscrambler X version 10.1 software was used to analyze and model the obtained spectral spectra. Figure 2.MINILAB 201 automatic pipette. [Source: own study].",
            "2.2.5. Statistical Analysis of Research Results": "It was assumed that a statistically significant difference indicates that the observed variation between the analyzed groups or variables is sufficiently substantial that its occurrence by random chance is highly improbable. The adopted research design and applied analytical instruments provided grounds to assume that the identified differences were not attributable to sampling errors or stochastic variability. A significance level of 0.05 (5%) was established, defining the threshold beyond which the difference could be considered statistically meaningful. Within the framework of a statistical procedure for comparing intergroup variance (ANOVA), an attempt was undertaken to determine whether significant differences existed among the mean values of the examined data groups. The process of data validation was based on the following hypotheses: Null hypothesis (H0): there is no statistically significant difference in the mean values between the analyzed data groups;Alternative hypothesis (H1): there exists a statistically significant difference in the mean values between the analyzed data groups. Empirical data were collected as multiple repetitions of individual measurements (a minimum of three replicates), followed by a variance analysis. When thep-value obtained from the ANOVA test was lower than the predetermined significance threshold (e.g., 0.05), the null hypothesis was rejected, indicating that the mean values of the analyzed datasets differed significantly.",
            "3. Results": "3.1. Acquisition of Spectral Spectra of Concentrated Lactic AcidThe research was designed to use transmission and reflection methods in the construction of spectral spectra of lactic acid. First, the task was specified, which was to demonstrate that it is possible to obtain spectral spectra of concentrated lactic acid for both methods, therefore Niron and A40 heads were used, which characterized both research methods. Three repetitions of the concentrated lactic acid scanning process were performed. The results of the study are presented graphically (Figure 3andFigure 4). A spectral spectrum of concentrated lactic acid (ca. 90%, density 1.209 g¬∑cm‚àí3) was obtained using the Niron head. To achieve the objective of the study, a laboratory sample of lactic acid was prepared and scanned to obtain spectral patterns in the analyzed wavelength range (Figure 3).Figure 3.Spectrum comparison of concentrated lactic acid (concentation ca. 90%) obtained with the Niron probe.Figure 4.Spectrum comparison of concentrated lactic acid obtained with the A40 probe.As in the case of tests using the Niron probe, the A40 probe was used to obtain spectral spectra of concentrated lactic acid. The test was repeated three times to verify the repeatability of the test method used. As in the case of scans obtained with the Niron probe, spectral spectra in the wavelength range of 450 nm to 1900 nm were obtained with the A40 probe (Figure 4).The physical properties of the lactic acid used in the research, its viscosity, density, and transparency to visible light suggested that the preparation of an aqueous solution of lactic acid and the determination of spectral spectra for this solution should produce a similar effect to that observed in the case of concentrated acid spectral spectra. Therefore, it was decided to continue the research and perform spectral analysis of the aqueous solution of lactic acid. The next stage was to bring the research closer to the stage of obtaining spectral analysis of lactic acid in mixtures with nutrients for the bacterial bed. 3.2. Acquisition of Spectral Spectra of an Aqueous Solution of Lactic Acid3.2.1. Spectral Spectrum of an Aqueous Solution of Lactic Acid Using a Niron HeadBased on the guidelines for changing the acid concentration in an aqueous solution (Table 2andTable 3), the process of obtaining spectral spectra was carried out. In the first step, the possibility of obtaining spectral spectra for an aqueous solution of lactic acid was tested. The spectra obtained were in the wavelength range from 450 nm to 1900 nm. The spectral spectra for 146 analyzed samples of aqueous lactic acid solution are presented in graphical form (Figure 5).Figure 5.Comparison of absorption spectra in an aqueous solution with lactic acid without correction using a Niron measure head.The characteristics of the model fitting error to the data were determined.Partial least squares (PLS) regression is a statistical method that is related to principal component regression and is a reduced-rank regression. Instead of finding the hyperplane of maximum variance between the response and the independent variables, it finds a linear regression model by projecting the predicted variables and the observed variables onto a new space of maximum covariance. Since both the X and Y data are projected onto the new space, the family of PLS methods is known as bilinear factor models.The main idea behind the PLS transformation is that the vectors of input and output variables differ. When one input variable is independent of another input variable, the output variables show an upward trend, the variable being the slope of the response vector.The purpose of building the model is to predict sensory measurements based on chemical measurements. The predictors are therefore chemical variables that we want to predict will be sensory data. The result obtained was corrected by changing the wavelength range accepted in the form of a training data range. Among other things, the following were used: ‚Äúbaseline‚Äù transformation and ‚Äúmoving average 9‚Äù transformation, where data corrections for model construction were made by automatically (software) and intuitively (decision of the person operating the software) removing outliers (Figure 6). (decision of the person operating the software) (Figure 6). As a result, 10 model solutions were created, differing in the aforementioned data ranges (wavelength ranges) and model quality parameters such as: RMSE (root mean square error), R2(coefficient of determination). The best model, called ‚ÄúPLS Nitron 950‚Äì1900 baseline Moving Average 9 without out aut‚Äù for the Niron head, was characterized by the parameter RMSE = 4.930 g¬∑dm‚àí3and the parameter R2= 0.990 (Table 4).Figure 6.PLS Niron 950‚Äì1900 nm Baseline Moving Average 9 chart without outliers, forecasts, and references.Table 4.The results of predictive model parameters for the Niron head obtained after transformation on a set of spectral spectra for different levels of lactic acid concentration in an aqueous solution.The tests were repeated using the A40 probe for the aforementioned aqueous lactic acid solution. The dividing line between the results therefore runs mainly between two alternative methods of generating spectral spectra, i.e., the use of the Niron probe and the A40 probe. The aforementioned mathematical transformations serve to optimize the qualitative effects of the model. The results of the tests for the use of the A40 probe and the construction of a calibration model for an aqueous solution of lactic acid are presented inSection 3.2.2.3.2.2. Spectral Spectrum of an Aqueous Solution of Lactic Acid Using the A40 HeadAs in the case of studies involving the acquisition of spectral spectra using the Niron head, in the next research step, spectral spectra were acquired in the accepted range of lactic acid concentrations (Table 3) using the A40 head. The results of the research were presented in graphical form for the full wavelength range from 450 nm to 1900 nm (Figure 7).Figure 7.Comparison of absorption spectra in an aqueous solution with lactic acid using the A40 head.The obtained result was corrected by changing the wavelength range adopted in the form of a training data range. Among other things, the following were used: ‚Äúbaseline‚Äù transformation and ‚ÄúSGolay‚Äù transformation, as well as data corrections for model construction by automatically (software) and intuitively (decision of the person operating the software) removing outliers. (decision of the person operating the software). As a result, nine model solutions were created, differing in the aforementioned data ranges (wavelength ranges) and model quality parameters such as: RMSE (root mean square error), R2(coefficient of determination). The best model, called ‚ÄúPLS A40 920‚Äì1900 baseline without out manually,‚Äù for the A40 head was characterized by the parameter RMSE = 4.186 g¬∑dm‚àí3and the parameter R2= 0.994 (Figure 8,Table 5).Figure 8.PLS graph A40 920‚Äì1900 nm Baseline without manually corrected forecasts and references.Table 5.Results of predictive model parameters for the A40 sensor obtained after transformation on a set of spectral spectra for different levels of lactic acid concentration in an aqueous solution. 3.3. Acquisition of Spectral Spectra of Aqueous Solutions of Lactic Acid and Molasses3.3.1. Spectral Spectra of an Aqueous Solution of Lactic Acid and Molasses Using a Niron HeadThe research was continued using the A40 head and the Niron head. The medium in which the lactic acid concentration was analyzed was changed. In this case, an aqueous solution of lactic acid and molasses was analyzed. As in the case of building calibration models for an aqueous solution of lactic acid (Section 3.2.1andSection 3.2.2), the division of results is mainly between two alternative methods of generating spectral spectra, i.e., the use of the Niron probe and the A40 probe (Figure 9).Figure 9.Comparison of absorption spectra of molasses in a 25% aqueous solution with lactic acid.The aforementioned mathematical transformations were again used to achieve qualitative optimization of the model.As in the case of studies involving the acquisition of spectral spectra for an aqueous solution of lactic acid, in the next research step, spectral spectra were obtained in the accepted range of lactic acid concentrations (Table 3) using the Niron head. The tests were performed for the full wavelength range, from 450 nm to 1900 nm.The result obtained was corrected by changing the wavelength range adopted in the form of a training data range. Among other things, the following were used: baseline transformation and data corrections for model construction by automatically (software) and intuitively (decision of the person operating the software) removing outliers. As a result, 13 model solutions were created, differing in the aforementioned data ranges (wavelength ranges) and model quality parameters such as: RMSE (root mean square error), R2(coefficient of determination). The best model, called ‚ÄúPLS Niron 500‚Äì1850 nm without out manually 5‚Äù for the Niron head, was characterized by the parameter RMSE = 4.435 g¬∑dm‚àí3, parameter R2= 0.985 (Figure 10,Table 6).Figure 10.PLS Niron Baseline 500‚Äì1850 nm graph without out manually corrected 5 predicted and references.Table 6.Results of predictive model parameters for the Niron sensor obtained after transformation on a set of spectral spectra for different levels of lactic acid concentration in a solution contaminated with molasses.3.3.2. Spectral Spectra of an Aqueous Solution of Lactic Acid and Molasses Using an A40 HeadAs in the case of studies involving the acquisition of spectral spectra for an aqueous solution of lactic acid and molasses using the Niron head, in the next research step, spectral spectra were acquired in the accepted range of lactic acid concentrations (Table 3) using the A40 head. The tests were performed for the full wavelength range, from 450 nm to 1900 nm (Figure 11).Figure 11.Comparison of absorption spectra of molasses in a 25% aqueous solution with lactic acid.The result obtained was corrected by changing the wavelength range adopted in the form of a training data range. Among other things, a baseline transformation without data correction was used to build the model by removing outliers. As a result, four model solutions were created, differing in the aforementioned data ranges (wavelength ranges) and model quality parameters such as RMSE (root mean square error) and R2(coefficient of determination). The best model, called ‚ÄúPLS A40 550‚Äì1850 nm baseline‚Äù for the A40 head, was characterized by the parameter RMSE = 2.305 g¬∑dm‚àí3, parameter R2= 0.997 (Figure 12,Table 7). During the study, the spectral curves of an aqueous molasses solution without lactic acid additive were also analyzed.Figure 12.PLS A40 graph 550‚Äì1850 nm Baseline forecasts and references.Table 7.Results of predictive model parameters for the A40 sensor obtained after transformation on a set of spectral spectra for different levels of lactic acid concentration in a solution contaminated with molasses.",
            "3.1. Acquisition of Spectral Spectra of Concentrated Lactic Acid": "The research was designed to use transmission and reflection methods in the construction of spectral spectra of lactic acid. First, the task was specified, which was to demonstrate that it is possible to obtain spectral spectra of concentrated lactic acid for both methods, therefore Niron and A40 heads were used, which characterized both research methods. Three repetitions of the concentrated lactic acid scanning process were performed. The results of the study are presented graphically (Figure 3andFigure 4). A spectral spectrum of concentrated lactic acid (ca. 90%, density 1.209 g¬∑cm‚àí3) was obtained using the Niron head. To achieve the objective of the study, a laboratory sample of lactic acid was prepared and scanned to obtain spectral patterns in the analyzed wavelength range (Figure 3). Figure 3.Spectrum comparison of concentrated lactic acid (concentation ca. 90%) obtained with the Niron probe. Figure 4.Spectrum comparison of concentrated lactic acid obtained with the A40 probe. As in the case of tests using the Niron probe, the A40 probe was used to obtain spectral spectra of concentrated lactic acid. The test was repeated three times to verify the repeatability of the test method used. As in the case of scans obtained with the Niron probe, spectral spectra in the wavelength range of 450 nm to 1900 nm were obtained with the A40 probe (Figure 4). The physical properties of the lactic acid used in the research, its viscosity, density, and transparency to visible light suggested that the preparation of an aqueous solution of lactic acid and the determination of spectral spectra for this solution should produce a similar effect to that observed in the case of concentrated acid spectral spectra. Therefore, it was decided to continue the research and perform spectral analysis of the aqueous solution of lactic acid. The next stage was to bring the research closer to the stage of obtaining spectral analysis of lactic acid in mixtures with nutrients for the bacterial bed.",
            "3.2. Acquisition of Spectral Spectra of an Aqueous Solution of Lactic Acid": "3.2.1. Spectral Spectrum of an Aqueous Solution of Lactic Acid Using a Niron HeadBased on the guidelines for changing the acid concentration in an aqueous solution (Table 2andTable 3), the process of obtaining spectral spectra was carried out. In the first step, the possibility of obtaining spectral spectra for an aqueous solution of lactic acid was tested. The spectra obtained were in the wavelength range from 450 nm to 1900 nm. The spectral spectra for 146 analyzed samples of aqueous lactic acid solution are presented in graphical form (Figure 5).Figure 5.Comparison of absorption spectra in an aqueous solution with lactic acid without correction using a Niron measure head.The characteristics of the model fitting error to the data were determined.Partial least squares (PLS) regression is a statistical method that is related to principal component regression and is a reduced-rank regression. Instead of finding the hyperplane of maximum variance between the response and the independent variables, it finds a linear regression model by projecting the predicted variables and the observed variables onto a new space of maximum covariance. Since both the X and Y data are projected onto the new space, the family of PLS methods is known as bilinear factor models.The main idea behind the PLS transformation is that the vectors of input and output variables differ. When one input variable is independent of another input variable, the output variables show an upward trend, the variable being the slope of the response vector.The purpose of building the model is to predict sensory measurements based on chemical measurements. The predictors are therefore chemical variables that we want to predict will be sensory data. The result obtained was corrected by changing the wavelength range accepted in the form of a training data range. Among other things, the following were used: ‚Äúbaseline‚Äù transformation and ‚Äúmoving average 9‚Äù transformation, where data corrections for model construction were made by automatically (software) and intuitively (decision of the person operating the software) removing outliers (Figure 6). (decision of the person operating the software) (Figure 6). As a result, 10 model solutions were created, differing in the aforementioned data ranges (wavelength ranges) and model quality parameters such as: RMSE (root mean square error), R2(coefficient of determination). The best model, called ‚ÄúPLS Nitron 950‚Äì1900 baseline Moving Average 9 without out aut‚Äù for the Niron head, was characterized by the parameter RMSE = 4.930 g¬∑dm‚àí3and the parameter R2= 0.990 (Table 4).Figure 6.PLS Niron 950‚Äì1900 nm Baseline Moving Average 9 chart without outliers, forecasts, and references.Table 4.The results of predictive model parameters for the Niron head obtained after transformation on a set of spectral spectra for different levels of lactic acid concentration in an aqueous solution.The tests were repeated using the A40 probe for the aforementioned aqueous lactic acid solution. The dividing line between the results therefore runs mainly between two alternative methods of generating spectral spectra, i.e., the use of the Niron probe and the A40 probe. The aforementioned mathematical transformations serve to optimize the qualitative effects of the model. The results of the tests for the use of the A40 probe and the construction of a calibration model for an aqueous solution of lactic acid are presented inSection 3.2.2. 3.2.2. Spectral Spectrum of an Aqueous Solution of Lactic Acid Using the A40 HeadAs in the case of studies involving the acquisition of spectral spectra using the Niron head, in the next research step, spectral spectra were acquired in the accepted range of lactic acid concentrations (Table 3) using the A40 head. The results of the research were presented in graphical form for the full wavelength range from 450 nm to 1900 nm (Figure 7).Figure 7.Comparison of absorption spectra in an aqueous solution with lactic acid using the A40 head.The obtained result was corrected by changing the wavelength range adopted in the form of a training data range. Among other things, the following were used: ‚Äúbaseline‚Äù transformation and ‚ÄúSGolay‚Äù transformation, as well as data corrections for model construction by automatically (software) and intuitively (decision of the person operating the software) removing outliers. (decision of the person operating the software). As a result, nine model solutions were created, differing in the aforementioned data ranges (wavelength ranges) and model quality parameters such as: RMSE (root mean square error), R2(coefficient of determination). The best model, called ‚ÄúPLS A40 920‚Äì1900 baseline without out manually,‚Äù for the A40 head was characterized by the parameter RMSE = 4.186 g¬∑dm‚àí3and the parameter R2= 0.994 (Figure 8,Table 5).Figure 8.PLS graph A40 920‚Äì1900 nm Baseline without manually corrected forecasts and references.Table 5.Results of predictive model parameters for the A40 sensor obtained after transformation on a set of spectral spectra for different levels of lactic acid concentration in an aqueous solution.",
            "3.2.1. Spectral Spectrum of an Aqueous Solution of Lactic Acid Using a Niron Head": "Based on the guidelines for changing the acid concentration in an aqueous solution (Table 2andTable 3), the process of obtaining spectral spectra was carried out. In the first step, the possibility of obtaining spectral spectra for an aqueous solution of lactic acid was tested. The spectra obtained were in the wavelength range from 450 nm to 1900 nm. The spectral spectra for 146 analyzed samples of aqueous lactic acid solution are presented in graphical form (Figure 5). Figure 5.Comparison of absorption spectra in an aqueous solution with lactic acid without correction using a Niron measure head. The characteristics of the model fitting error to the data were determined. Partial least squares (PLS) regression is a statistical method that is related to principal component regression and is a reduced-rank regression. Instead of finding the hyperplane of maximum variance between the response and the independent variables, it finds a linear regression model by projecting the predicted variables and the observed variables onto a new space of maximum covariance. Since both the X and Y data are projected onto the new space, the family of PLS methods is known as bilinear factor models. The main idea behind the PLS transformation is that the vectors of input and output variables differ. When one input variable is independent of another input variable, the output variables show an upward trend, the variable being the slope of the response vector. The purpose of building the model is to predict sensory measurements based on chemical measurements. The predictors are therefore chemical variables that we want to predict will be sensory data. The result obtained was corrected by changing the wavelength range accepted in the form of a training data range. Among other things, the following were used: ‚Äúbaseline‚Äù transformation and ‚Äúmoving average 9‚Äù transformation, where data corrections for model construction were made by automatically (software) and intuitively (decision of the person operating the software) removing outliers (Figure 6). (decision of the person operating the software) (Figure 6). As a result, 10 model solutions were created, differing in the aforementioned data ranges (wavelength ranges) and model quality parameters such as: RMSE (root mean square error), R2(coefficient of determination). The best model, called ‚ÄúPLS Nitron 950‚Äì1900 baseline Moving Average 9 without out aut‚Äù for the Niron head, was characterized by the parameter RMSE = 4.930 g¬∑dm‚àí3and the parameter R2= 0.990 (Table 4). Figure 6.PLS Niron 950‚Äì1900 nm Baseline Moving Average 9 chart without outliers, forecasts, and references. Table 4.The results of predictive model parameters for the Niron head obtained after transformation on a set of spectral spectra for different levels of lactic acid concentration in an aqueous solution. The tests were repeated using the A40 probe for the aforementioned aqueous lactic acid solution. The dividing line between the results therefore runs mainly between two alternative methods of generating spectral spectra, i.e., the use of the Niron probe and the A40 probe. The aforementioned mathematical transformations serve to optimize the qualitative effects of the model. The results of the tests for the use of the A40 probe and the construction of a calibration model for an aqueous solution of lactic acid are presented inSection 3.2.2.",
            "3.2.2. Spectral Spectrum of an Aqueous Solution of Lactic Acid Using the A40 Head": "As in the case of studies involving the acquisition of spectral spectra using the Niron head, in the next research step, spectral spectra were acquired in the accepted range of lactic acid concentrations (Table 3) using the A40 head. The results of the research were presented in graphical form for the full wavelength range from 450 nm to 1900 nm (Figure 7). Figure 7.Comparison of absorption spectra in an aqueous solution with lactic acid using the A40 head. The obtained result was corrected by changing the wavelength range adopted in the form of a training data range. Among other things, the following were used: ‚Äúbaseline‚Äù transformation and ‚ÄúSGolay‚Äù transformation, as well as data corrections for model construction by automatically (software) and intuitively (decision of the person operating the software) removing outliers. (decision of the person operating the software). As a result, nine model solutions were created, differing in the aforementioned data ranges (wavelength ranges) and model quality parameters such as: RMSE (root mean square error), R2(coefficient of determination). The best model, called ‚ÄúPLS A40 920‚Äì1900 baseline without out manually,‚Äù for the A40 head was characterized by the parameter RMSE = 4.186 g¬∑dm‚àí3and the parameter R2= 0.994 (Figure 8,Table 5). Figure 8.PLS graph A40 920‚Äì1900 nm Baseline without manually corrected forecasts and references. Table 5.Results of predictive model parameters for the A40 sensor obtained after transformation on a set of spectral spectra for different levels of lactic acid concentration in an aqueous solution.",
            "3.3. Acquisition of Spectral Spectra of Aqueous Solutions of Lactic Acid and Molasses": "3.3.1. Spectral Spectra of an Aqueous Solution of Lactic Acid and Molasses Using a Niron HeadThe research was continued using the A40 head and the Niron head. The medium in which the lactic acid concentration was analyzed was changed. In this case, an aqueous solution of lactic acid and molasses was analyzed. As in the case of building calibration models for an aqueous solution of lactic acid (Section 3.2.1andSection 3.2.2), the division of results is mainly between two alternative methods of generating spectral spectra, i.e., the use of the Niron probe and the A40 probe (Figure 9).Figure 9.Comparison of absorption spectra of molasses in a 25% aqueous solution with lactic acid.The aforementioned mathematical transformations were again used to achieve qualitative optimization of the model.As in the case of studies involving the acquisition of spectral spectra for an aqueous solution of lactic acid, in the next research step, spectral spectra were obtained in the accepted range of lactic acid concentrations (Table 3) using the Niron head. The tests were performed for the full wavelength range, from 450 nm to 1900 nm.The result obtained was corrected by changing the wavelength range adopted in the form of a training data range. Among other things, the following were used: baseline transformation and data corrections for model construction by automatically (software) and intuitively (decision of the person operating the software) removing outliers. As a result, 13 model solutions were created, differing in the aforementioned data ranges (wavelength ranges) and model quality parameters such as: RMSE (root mean square error), R2(coefficient of determination). The best model, called ‚ÄúPLS Niron 500‚Äì1850 nm without out manually 5‚Äù for the Niron head, was characterized by the parameter RMSE = 4.435 g¬∑dm‚àí3, parameter R2= 0.985 (Figure 10,Table 6).Figure 10.PLS Niron Baseline 500‚Äì1850 nm graph without out manually corrected 5 predicted and references.Table 6.Results of predictive model parameters for the Niron sensor obtained after transformation on a set of spectral spectra for different levels of lactic acid concentration in a solution contaminated with molasses. 3.3.2. Spectral Spectra of an Aqueous Solution of Lactic Acid and Molasses Using an A40 HeadAs in the case of studies involving the acquisition of spectral spectra for an aqueous solution of lactic acid and molasses using the Niron head, in the next research step, spectral spectra were acquired in the accepted range of lactic acid concentrations (Table 3) using the A40 head. The tests were performed for the full wavelength range, from 450 nm to 1900 nm (Figure 11).Figure 11.Comparison of absorption spectra of molasses in a 25% aqueous solution with lactic acid.The result obtained was corrected by changing the wavelength range adopted in the form of a training data range. Among other things, a baseline transformation without data correction was used to build the model by removing outliers. As a result, four model solutions were created, differing in the aforementioned data ranges (wavelength ranges) and model quality parameters such as RMSE (root mean square error) and R2(coefficient of determination). The best model, called ‚ÄúPLS A40 550‚Äì1850 nm baseline‚Äù for the A40 head, was characterized by the parameter RMSE = 2.305 g¬∑dm‚àí3, parameter R2= 0.997 (Figure 12,Table 7). During the study, the spectral curves of an aqueous molasses solution without lactic acid additive were also analyzed.Figure 12.PLS A40 graph 550‚Äì1850 nm Baseline forecasts and references.Table 7.Results of predictive model parameters for the A40 sensor obtained after transformation on a set of spectral spectra for different levels of lactic acid concentration in a solution contaminated with molasses.",
            "3.3.1. Spectral Spectra of an Aqueous Solution of Lactic Acid and Molasses Using a Niron Head": "The research was continued using the A40 head and the Niron head. The medium in which the lactic acid concentration was analyzed was changed. In this case, an aqueous solution of lactic acid and molasses was analyzed. As in the case of building calibration models for an aqueous solution of lactic acid (Section 3.2.1andSection 3.2.2), the division of results is mainly between two alternative methods of generating spectral spectra, i.e., the use of the Niron probe and the A40 probe (Figure 9). Figure 9.Comparison of absorption spectra of molasses in a 25% aqueous solution with lactic acid. The aforementioned mathematical transformations were again used to achieve qualitative optimization of the model. As in the case of studies involving the acquisition of spectral spectra for an aqueous solution of lactic acid, in the next research step, spectral spectra were obtained in the accepted range of lactic acid concentrations (Table 3) using the Niron head. The tests were performed for the full wavelength range, from 450 nm to 1900 nm. The result obtained was corrected by changing the wavelength range adopted in the form of a training data range. Among other things, the following were used: baseline transformation and data corrections for model construction by automatically (software) and intuitively (decision of the person operating the software) removing outliers. As a result, 13 model solutions were created, differing in the aforementioned data ranges (wavelength ranges) and model quality parameters such as: RMSE (root mean square error), R2(coefficient of determination). The best model, called ‚ÄúPLS Niron 500‚Äì1850 nm without out manually 5‚Äù for the Niron head, was characterized by the parameter RMSE = 4.435 g¬∑dm‚àí3, parameter R2= 0.985 (Figure 10,Table 6). Figure 10.PLS Niron Baseline 500‚Äì1850 nm graph without out manually corrected 5 predicted and references. Table 6.Results of predictive model parameters for the Niron sensor obtained after transformation on a set of spectral spectra for different levels of lactic acid concentration in a solution contaminated with molasses.",
            "3.3.2. Spectral Spectra of an Aqueous Solution of Lactic Acid and Molasses Using an A40 Head": "As in the case of studies involving the acquisition of spectral spectra for an aqueous solution of lactic acid and molasses using the Niron head, in the next research step, spectral spectra were acquired in the accepted range of lactic acid concentrations (Table 3) using the A40 head. The tests were performed for the full wavelength range, from 450 nm to 1900 nm (Figure 11). Figure 11.Comparison of absorption spectra of molasses in a 25% aqueous solution with lactic acid. The result obtained was corrected by changing the wavelength range adopted in the form of a training data range. Among other things, a baseline transformation without data correction was used to build the model by removing outliers. As a result, four model solutions were created, differing in the aforementioned data ranges (wavelength ranges) and model quality parameters such as RMSE (root mean square error) and R2(coefficient of determination). The best model, called ‚ÄúPLS A40 550‚Äì1850 nm baseline‚Äù for the A40 head, was characterized by the parameter RMSE = 2.305 g¬∑dm‚àí3, parameter R2= 0.997 (Figure 12,Table 7). During the study, the spectral curves of an aqueous molasses solution without lactic acid additive were also analyzed. Figure 12.PLS A40 graph 550‚Äì1850 nm Baseline forecasts and references. Table 7.Results of predictive model parameters for the A40 sensor obtained after transformation on a set of spectral spectra for different levels of lactic acid concentration in a solution contaminated with molasses.",
            "4. Discussion": "The main objective of this study was to develop a model that would enable rapid prediction of lactic acid content in organic substrates such as food and feed waste. Rapid prediction is crucial for the proper preparation of raw materials when developing technologies for the management of this type of waste. This is particularly important in the case of preparing substrates for biogas plants for energy purposes, where it is essential to limit the occurrence of components that contaminate the methane fermentation process [25,26]. Biogas plants usually store substrate in the form of silage to ensure constant access to raw materials and continuity of operation. In such cases, it is important to control not only the process taking place in the fermenter, but also the substrate, which, if handled improperly, can disrupt the process [27,28]. Currently, rapid tests for organic waste samples that allow the determination of lactic acid content are not widely available. Therefore, the authors of this study decided to attempt to develop such a model. For this purpose, a set of samples contaminated with molasses was prepared to simulate a substrate sample that could be used, among other things, in a biogas plant. There are several problems with recognizing key product fractions using rapid spectral detection methods based on prepared calibration models. Complex substances, polymers and molecules pose a barrier to specific methods, spectral detection and other related methods in terms of the use of transmission or reflection methods. The issue of the usefulness of spectral analysis, in terms of a wide spectrum of emitted wavelengths, also applies to the identification of many organic acids. Popular methods of constructing detection or prediction models using spectral analysis in the visible to infrared range encounter a number of limitations in the fermentation process environment (e.g., lactic, methane) [29]. One of the limitations is the natural color, white, yellow and syrupy consistency of the target fermentation products. In combination with the organic substrate residue, from which the unrefined form of acid (e.g., ACA acids) is obtained in a biochemical process, the transparency to visible and infrared light waves is impaired [30]. Modelling based on near-infrared (NIR) spectral spectra for materials with low transparency is challenging due to the strong absorption and scattering of radiation in their structure. In such cases, it is crucial to use appropriate methods for correcting optical effects and advanced signal processing techniques, such as scatter correction (e.g., MSC, SNV) and dimensionality reduction (e.g., PCA). Chemometric models, e.g., PLS (Partial Least Squares), allow reliable information about the composition of the material to be obtained despite limited transparency, but require a carefully selected set of calibration and validation data [31,32]. However, Xue, X. et al. [28] undertook the task of developing a rapid model for determining the lactic acid content in corn silage. In their study, they developed a quantitative model for predicting changes in lactic acid content during secondary fermentation of corn silage based on a colorimetric sensor array (CSA) combined with hyperspectral imaging. The results showed that the accuracy of the model‚Äôs predictions can be significantly improved by selecting appropriate pre-processing methods for different color-sensitive dyes. The CARS-CPO-SVR model (Competitive adaptive reweighted sampling (CARS), crested porcupine optimizer (CPO), support vector regression (SVR)) showed better prediction accuracy, with a coefficient of determination of the prediction set (R2P), root mean square error of prediction (RMSEP), and ratio of performance to deviation (RPD) of 0.9617, 2.0057, and 5.1997, respectively. These comprehensive results confirm the validity of combining CSA with hyperspectral imaging to accurately determine the lactic acid content in silage, providing a scientific and innovative method for testing the quality of corn silage. It can be noted that the model developed by the authors is characterized by good accuracy. Previous scientific work on the subject of lactic acid content modeling has usually focused on identification and mathematical modeling [33,34] or on models relating to foodstuffs of animal origin, in this case primarily dairy products [35]. Macedo, M.G. et al. [33] used near-infrared spectroscopy (NIRS) as a tool for simultaneously predicting the production yield of exopolysaccharides (EPS; 0‚Äì3 g¬∑L‚àí3) and the concentrations of lactic acid (0‚Äì59 g¬∑L‚àí3) and lactose (0‚Äì68 g¬∑L‚àí3) in supernatants from batch cultures of Lactobacillus rhamnosus RW-9595M, conducted at a controlled pH in a medium containing whey permeate. In order to develop calibration models, correlations between the second derivative of 164 NIRS transmittance spectra and reference data on the concentrations of the determined compounds were analyzed in the wavelength ranges of 1653‚Äì1770 nm and 2041‚Äì2353 nm using the partial least squares method (PLS) method. Lactic acid and lactose concentrations were determined by high-performance liquid chromatography (HPLC), while EPS content was estimated using a newly developed ultrafiltration-based method. The obtained coefficients of determination (R2) and standard cross-validation errors (RMSECV) for the calibration models were 0.91 and 0.26 g¬∑L‚àí3for EPS, 0.99 and 2.54 g¬∑L‚àí3for lactic acid, and 0.98 and 3.32 g¬∑L‚àí3for lactose, respectively. These calibrations were verified on the basis of 45 randomly selected samples from six independent farms, which were not included in the calibration process. High agreement was found between the results obtained by reference methods and the predictions based on NIRS, with correlation coefficients and standard errors of prediction of 0.99 and 1.64 g¬∑L‚àí3for lactic acid, 0.99 and 4.5 g¬∑L‚àí3for lactose, and 0.91 and 0.32 g¬∑L‚àí3for EPS. The results obtained indicate that NIRS spectroscopy can be an effective tool for rapid monitoring and control of the lactic fermentation process with exopolysaccharide production. PƒÉucean, A. et al. [34] developed a model based on infrared absorption analysis for the determination of lactic acid in the production and preservation of fermented dairy products, cheeses, sourdough bread, and lactate-fermented vegetables. The aim of the study was to monitor lactic acid production by Lactobacillus planta-rum ATCC 8014 and Lactobacillus casei ATCC 393‚Äîboth as single strains and in co-fermentation‚Äîusing Fourier transform infrared spectroscopy (FTIR) supported by multidimensional analysis. Three types of MRS media enriched with different carbohydrates were used for the study, reflecting the diverse matrices of plant and animal raw materials. Lactic acid concentration was determined using the HPLC reference method. The calibration set (n = 36) was used to build the model, while the validation set (n = 13) was used to test the robustness of the developed model. The coefficients of determination between the predicted and reference values were 0.986 and 0.965, while the mean square error for the calibration and validation sets was 0.127 and 0.263 g¬∑L‚àí1, respectively. The results confirmed the effectiveness of FTIR spectroscopy in combination with multivariate statistics as a fast, reliable, and economical tool for routine monitoring of lactic acid. In a study by S√∏rensen, L.K. [35], predictive models were developed for determining lactic acid (Lac), acetic acid (HAc), pH, ammonia (NH3-N), and ethanol (EtOH) in grass and corn silage using near-infrared spectroscopy (NIR). Better accuracy of Lac, pH, and NH3-N determinations was obtained for dry materials, while HAc was determined more precisely in wet samples. The standard deviation to RMSECV ratios were as follows: ‚àíFor grass silage (dry material): 4.9 (Lac), 2.0 (HAc), 3.7 (pH), 3.1 (NH3-N);‚àíFor grass silage (wet material): 3.3 (EtOH);‚àíFor corn silage: 4.7 (Lac), 1.9 (HAc), 2.4 (pH), 2.9 (NH3-N), 4.0 (EtOH). Drying at 80 ¬∞C had a minimal effect on Lac concentration, but had a more significant effect on NH3-N and HAc content, depending on the type of silage. The results presented in this way allow us to conclude that the model developed in this article does not differ in quality from the studies of other authors, and moreover, it provides for a different application of research aimed at the energy management of organic biomass with high morphological diversity, which characterizes food and feed waste.",
            "5. Conclusions": "Research conducted on the concept of monitoring lactic acid levels in the process of preparing biomass fractions for biogas production confirmed the thesis of the study. The results of the research allowed for the formulation of a series of conclusions, substantive comments, and recommendations. ‚ó¶First, the possibility of determining the concentration of lactic acid in liquid mixtures using the near-infrared (NIR) method in the analyzed wavelength range from 400 nm to 2250 nm was confirmed.‚ó¶Identifying spectral noise clearly outlines the limitation of the data range (in the model up to 1900 nm) and improves the quality of the built calibration models.‚ó¶Identification of optimal wavelength sub-ranges and gradual modifications of spectral spectra allow the areas of most valuable data for the calibration model to be determined.‚ó¶The collection of data with increased size (150 NIR scans each time) contributed to the preparation of correct models of lactic acid concentration both in aqueous solution and in a mixture with molasses.‚ó¶It has been demonstrated that Niron and A40 detection heads (absorption and reflectance) in the range of 450‚Äì1900 nm enable the creation of a high-quality database through the effective activation of lactic acid chemical bonds in an environment with limited optical transparency.‚ó¶The results of the quality parameters of the calibration models confirmed that the proprietary method of spectral acquisition using 50 mL samples in a glass Petri dish is correct. The radiation waves penetrated the sample and reflected off the surface in a manner that allowed spectra to be obtained and changes in lactic acid concentration to be identified.‚ó¶Building an effective neural network-based calibration model that recognizes the presence of lactic acid in a water-molasses mixture requires successive model iterations with wavelength range control to effectively improve the model quality parameters.‚ó¶Mathematical transformations, such as baseline, moving average, SGolay filter, and outlier elimination, play a key role. This is an effective strategy for achieving high correlation between the model and the predictive and validation data.‚ó¶It was found that the addition of 25% molasses‚Äîdespite significantly reducing the transparency of the solution‚Äîdoes not prevent the construction of a high-quality NIR model. The spectral response of lactic acid is recognizable even in the difficult molasses medium, thanks to the excitation of chemical bonds in the NIR and partially MIR ranges.‚ó¶The models were characterized by low RMSE errors and high R2, confirming the effectiveness of the method in determining lactic acid concentration in aquatic environments and in food/feed mixtures. The authors of the study plan to verify the concept of converting the biomass decomposition process by dividing it into stages in order to obtain biogas with a higher calorific value."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/1996-1073/18/23/6360",
        "scraped_at": "2025-12-05 23:55:25"
    },
    {
        "title": "MMDD: A Multimodal Multitask Dynamic Disentanglement Framework for Robust Major Depressive Disorder Diagnosis Across Neuroimaging Sites",
        "authors": "byQiongpu Chen,Peishan Dai,Kaineng Huang,Ting HuandShenghui Liao",
        "journal": "Diagnostics2025,15(23), 3089;https://doi.org/10.3390/diagnostics15233089- 4 Dec 2025",
        "abstract": "Background/Objectives:Major Depressive Disorder (MDD) is a severe psychiatric disorder, and effective, efficient automated diagnostic approaches are urgently needed. Traditional methods for assessing MDD face three key challenges: reliance on predefined features, inadequate handling of multi-site data heterogeneity, and suboptimal feature fusion. To address these issues, this study proposes the Multimodal Multitask Dynamic Disentanglement (MMDD) Framework.Methods:The MMDD Framework has three core innovations. First, it adopts a dual-pathway feature extraction architecture combining a 3D ResNet for modeling gray matter volume (GMV) data and an LSTM‚ÄìTransformer for processing time series data. Second, it includes a Bidirectional Cross-Attention Fusion (BCAF) mechanism for dynamic feature alignment and complementary integration. Third, it uses a Gradient Reversal Layer-based Multitask Learning (GRL-MTL) strategy for enhancing the model‚Äôs domain generalization capability.Results:MMDD achieved 77.76% classification accuracy on the REST-meta-MDD dataset. Ablation studies confirmed that both the BCAF mechanism and GRL-MTL strategy played critical roles: the former optimized multimodal fusion, while the latter effectively mitigated site-related heterogeneity. Through interpretability analysis, we identified distinct neurobiological patterns: time series were primarily localized to subcortical hubs and the cerebellum, whereas GMV mainly involved higher-order cognitive and emotion-regulation cortices. Notably, the middle cingulate gyrus showed consistent abnormalities across both imaging modalities.Conclusions:This study makes two major contributions. First, we develop a robust and generalizable computational framework for objective MDD diagnosis by effectively leveraging multimodal data. Second, we provide data-driven insights into MDD‚Äôs distinct neuropathological processes, thereby advancing our understanding of the disorder.Keywords:multimodal learning;magnetic resonance imaging;multitask learning;multisite collaboration;major depressive disorder;deep learning",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Major Depressive Disorder (MDD) is a severe mental illness characterized by persistent low mood, loss of interest, and recurrent suicidal thoughts, significantly impairing patient quality of life [1]. According to the latest epidemiological data from the World Health Organization (WHO), approximately 280 million people worldwide suffer from depression, making it one of the leading causes of disability worldwide [2]. However, the diagnosis of MDD primarily relies on clinicians‚Äô subjective evaluation of patients‚Äô symptoms. These diagnostic approaches depend largely on patients‚Äô self-reports and physicians‚Äô clinical experience and lack objective biomarkers. As a result, diagnostic consistency among different doctors is low. Moreover, MDD‚Äôs symptom spectrum overlaps significantly with other psychiatric disorders. This further increases the difficulty of early differential diagnosis [3]. Although a growing body of research has explored potential biomarkers for MDD [4], the field has not yet converged on a standardized set of criteria for clinical application. Thus, developing an objective and quantifiable biomarker system is critical, and has therefore become an urgent challenge in psychiatry. Structural magnetic resonance imaging (sMRI) and functional magnetic resonance imaging (fMRI) are core components of neuroimaging. They provide complementary objective evidence for the pathological mechanisms of MDD. Specifically, sMRI focuses on brain morphology [5], while fMRI focuses on dynamic neural activity [6,7]. sMRI, through high-resolution brain scanning, enables the quantification of structural metrics such as gray matter volume (GMV), white matter volume, and cerebrospinal fluid. Research has consistently demonstrated that MDD patients often exhibit structural abnormalities in brain regions including the prefrontal cortex and occipital lobe [8,9]. These alterations may be associated with neuroplasticity impairment resulting from chronic mood disorders. fMRI captures temporal variations in blood-oxygen-level-dependent (BOLD) signals, reflecting functional connectivity (FC) patterns and neural network dynamics during both resting-state and task-based conditions. For instance, functional abnormalities in the default mode network (DMN) have been conclusively linked to MDD‚Äôs negative cognitive biases and emotional dysregulation [10,11]. These analyses show that sMRI and fMRI offer complementary insights into MDD, together providing an objective perspective on its neurobiological basis. Nevertheless, transforming these information-rich imaging datasets into clinically applicable diagnostic tools still requires overcoming a series of technical¬†bottlenecks. Machine learning has become a pivotal tool in biomedical research. Its impactful applications range from the accurate prediction of diabetes [12] to the management of hematological disorders [13], demonstrating its broad utility. In the specific domain of psychiatric neuroimaging, however, translating these insights into clinically applicable diagnostic tools for MDD faces three major challenges: data harmonization, model construction, and multimodal fusion. First, data harmonization remains a significant hurdle. While multi-center studies expand sample sizes, the resulting data often exhibit significant site heterogeneity due to variations in MRI scanners, sequence parameters, and acquisition protocols. This heterogeneity introduces systematic biases that can confound disease-related signals. Existing harmonization methods like ComBat [14] typically assume site effects are independent of biological signals, representing a static correction approach. Although recent frameworks such asùê¥ùê∑2ùê¥AD2A[15], USMDA [16], and FSM-MSDA [17] have advanced site adaptation via adversarial learning and domain adaptation, they are primarily validated on a limited number of sites (e.g., 2‚Äì3 centers) and face scalability and stability challenges when applied to large-scale multi-center settings involving 25 sites. Second, effective model construction for multimodal MRI data demands a paradigm shift from generic architectures to designs that explicitly encode modality-specific features. MRI data exhibits high-dimensional characteristics and inherent noise (e.g., head motion artifacts, field inhomogeneity), posing challenges such as the curse of dimensionality and feature extraction bottlenecks for conventional machine learning methods. Traditional analytical approaches rely on predefined imaging features (e.g., functional connectivity [11,18]) and shallow machine learning models (e.g., support vector machines, linear regression [10,19]), which are ill-suited to capture the complex nonlinear pathological patterns of MDD. Current research tends to directly model raw signals through end-to-end deep learning frameworks (e.g., convolutional neural networks (CNNs) [20,21], Swin Transformers [22,23]). While these approaches avoid the limitations of handcrafted feature design, they do not address the critical need for modeling modality-specific characteristics. Third, achieving deep multimodal fusion is nontrivial. Most existing methods adopt simple concatenation or traditional cross-attention fusion strategies. Moreover, they fail to fully explore the deep correlations between structural and functional modalities. Traditional cross-attention fusion methods [24] typically use a unidirectional mechanism: one modality serves as the Query, while the other acts as the Key and Value, forming a unidirectional information flow. The main flaws of this design lie in asymmetric information flow and insufficient modal¬†interaction. To overcome these challenges, we propose the Multimodal Multitask Dynamic Disentanglement Framework (MMDD). The key contributions of this work are outlined below: To tackle the critical issue of site heterogeneity in large-scale multi-center MDD studies, we propose a¬†Gradient Reversal Layer-based Multitask Learning (GRL-MTL) strategy. Instead of applying a static correction, our method reframes site harmonization as an auxiliary task. By leveraging a Gradient Reversal Layer, it dynamically learns site-invariant representations that are robust to scanner and protocol variations.¬†This approach demonstrates superior scalability and stability in experiments involving a large number of sites.We designed a dual-pathway feature extraction network to accurately capture modality-specific characteristics from sMRI and fMRI data. Specifically, a 3D ResNet is employed to extract rich spatial structural features from sMRI, while an LSTM‚ÄìTransformer hybrid encoder is designed to model the complex temporal dynamics of fMRI. This specialized design ensures the optimal extraction and representation of each modality‚Äôs unique information before fusion, forming a more informed basis for multimodal integration.For the core challenge of multimodal integration, we introduce a Bidirectional Cross-Attention Fusion (BCAF) mechanism. Unlike traditional unidirectional fusion methods, BCAF establishes bidirectional interaction paths, allowing sMRI and fMRI to mutually and simultaneously guide each other‚Äôs representation learning. This process dynamically integrates complementary information through attention-based weighting, thus forming a more holistic and discriminative feature representation.On the REST-meta-MDD dataset (comprising 1300 MDD patients and 1128 healthy controls across 25 sites), our model achieved an overall classification accuracy of 77.76%. In a more challenging leave-one-site-out cross-validation setting (using all available sites as independent test sets), our method consistently outperformed baseline approaches across all test sites, demonstrating strong generalizability across unseen¬†sites.Interpretability analysis, based on SHAP values and two-samplet-tests, revealed distinct neurobiological patterns: temporal features from fMRI were predominantly localized to subcortical hubs and the cerebellum, while gray matter volume (GMV) features mainly involved higher-order cognitive and emotion-regulation cortical regions. Notably, the middle cingulate gyrus consistently exhibited significant abnormalities in both imaging¬†modalities.",
            "2. Related Work": "Based on the multi-center REST-meta-MDD dataset, computer-aided diagnosis of MDD using resting-state fMRI (rs-fMRI) has become an important research direction. Existing work primarily revolves around the core challenge of effectively modeling brain FC using Graph Neural Networks (GNNs). It has evolved from static FC extraction to dynamic FC generation, and from a general architecture to a specifically optimized one. Several recent studies (2023‚Äì2025) on MRI-based MDD classification were selected for analysis in the review, as summarized inTable 1. Early research, such as DGCNN [25], initially validated the effectiveness of GNN in processing static FC. DGCNN achieved performance superior to traditional machine learning methods (with an accuracy of 72.1%) on a sample of 1601 subjects, laying the foundation for subsequent studies. However, research by Gallo et al. [26] demonstrated that simply applying standard Graph Convolutional Network (GCN) or support vector machine (SVM) models leads to a significant decline in generalization performance (62% accuracy) on large multi-center datasets (2338 subjects) due to site heterogeneity. To address this challenge, the FGDN [27] framework integrates linear and nonlinear functional connectivity information through its Dual Graph Attention Network (DGAT) module to extract more robust features. More importantly, it reconstructs the disrupted graph structural connections across different sites via the Federated Graph Convolutional Network (FedGCN) module. However, its classification accuracy of 61.8% on 841 subjects from three sites of the REST-meta-MDD dataset indicates that multi-site depression diagnosis still faces significant challenges. This highlights the necessity for model innovation. Subsequent researchers have deepened this work along multiple dimensions, although validation is often not performed on the complete dataset. Firstly, in terms of feature learning, GAE-FCNN [18] learns low-dimensional topological embeddings of brain networks in an unsupervised manner, aiming to more fully capture complex relationships between nodes. The N2V-GAT [28] framework innovatively combines Node2Vec graph embedding with graph attention mechanisms to differentially evaluate the contributions of different brain regions, achieving high classification performance (78.73% accuracy). Secondly, on the data modeling level, research expanded from static to dynamic networks. For instance, the DSFGNN [29] framework attempts to capture temporal dynamics of brain activity by simultaneously modeling static and dynamic FC and fusing them using an attention mechanism. Meanwhile, BrainFC-CGAN [30] adopts a generative approach, specifically designing layers to preserve the symmetry and topological properties of FC. Thirdly, regarding data quality, DDN-Net [31] directly addresses noise in fMRI data by introducing deep residual shrinkage denoising and subgraph normalization, enhancing model robustness. Although the aforementioned methods have advanced MDD classification, they typically rely on predefined brain atlases to extract FC as input, which carries potential information loss. This limitation is also evident in broader neuroimaging research. The current trend is to bypass handcrafted feature extraction and perform end-to-end learning directly on raw signals. For example, Muksimova et al. [20] proposed a CNN architecture integrated with an attention mechanism for end-to-end learning from original MRI slices, enabling automatic multi-scale feature capture and fusion. Xin et al. [22] employed a CNN + Swin Transformer dual-stream network to process static 3D sMRI, focusing on capturing spatial patterns of gray/white matter atrophy. SwiFT [23] adapted the Swin Transformer for dynamic 4D fMRI analysis by introducing a temporal modeling mechanism, enabling dynamic characterization of FC networks. These methods focus on single modality. Despite significant progress in deep learning-based MRI analysis, comprehensively parsing the complex human brain often requires integrating multimodal¬†data. Multimodal medical imaging research has made remarkable progress [32,33,34], focusing on integrating data from different modalities to enhance model perception, understanding, and reasoning. Early studies like camAD [24] used CNN-based multi-scale feature extraction and cross-attention fusion, dynamically weighting different modalities to significantly improve Alzheimer‚Äôs disease classification accuracy. Similarly, AGGN [35] further developed a dual-domain (channel + spatial) attention mechanism combined with multi-scale feature extraction for precise glioma grading. In terms of architectural innovation, OmniVec2 [36] built a universal Transformer framework supporting 12 modalities, using cross-attention in a dual-stream architecture for intermodal feature fusion and enabling cross-modal knowledge transfer via shared transformer parameters. To improve computational efficiency, MDDMamba [37] introduced the CrossMamba module, replacing the traditional Transformer‚Äôs quadratic complexity with a linear alternative for efficient multimodal fusion. For modeling high-order relationships, CIA-HGCN [38] applied hypergraph convolutional networks to mental disorder classification, capturing complex biomarker associations by constructing brain region-gene hypergraph networks. These methods collectively drive the paradigm shift in multimodal analysis from simple feature concatenation to deep, intelligent fusion, focusing on uncovering latent biological correlations through adaptive mechanisms. Site heterogeneity in multi-center data poses a significant challenge, as existing static harmonization methods struggle to dynamically disentangle site-specific variances from disease biomarkers. Multi-site neuroimaging has become a key paradigm, enhancing statistical power and generalizability by aggregating data from different scanners and populations. Early methods focused on data-level harmonization; for instance, ComBat [14], which uses an empirical Bayes framework for static correction of site effects, became a widely used baseline. Subsequently, SiMix [39] introduced data augmentation via cross-site image mixing and test-time multi-view boosting to improve generalization to unseen sites. With the rise in deep learning, Svanera et al. [40] used a progressive level-of-detail network, learning site-invariant anatomical priors at lower levels while adaptively handling site-specific intensity distributions at higher levels, achieving implicit heterogeneity disentanglement. The domain adaptation paradigm spurred advances in feature-space alignment techniques: Guan et al. [15] used an adversarial dual-branch structure to separate shared and site-specific features; USMDA [16] enabled unsupervised multi-source domain adaptation; FSM-MSDA [17] established semantic feature matching mechanisms. Recent research also addresses privacy preservation and heterogeneous collaboration: SFPGCL [41] innovatively combined federated learning with contrastive learning, using a shared branch to capture global invariant features and a personalized branch to retain site-specific characteristics, mitigating client heterogeneity via contrastive learning; SAN-Net [42] employed self-adaptive normalization and Gradient Reversal Layers to dynamically suppress site-related variations at the feature level. In summary, research on automated MDD diagnosis and MRI analysis clearly illustrates a technological development path from methodological innovation to addressing real-world challenges. Initially, various neural network models significantly enhanced the characterization of brain functional connectivity networks by incorporating attention mechanisms, dynamic fusion, and embedding learning. However, the inherent site heterogeneity and scanner effects in multi-center data have highlighted the necessity of moving beyond mere model performance optimization. This has driven the development of technologies such as domain adaptation, feature disentanglement, and federated learning, aimed at enhancing model generalizability and robustness. Looking ahead, research in this field will increasingly focus on integrating the depth of end-to-end learning with the breadth of multimodal fusion, while leveraging explainable artificial intelligence technologies to ensure model transparency and trustworthiness. Ultimately, through the synergistic optimization of algorithmic architecture, data harmonization, and multimodal information integration, we can expect to develop objective diagnostic tools that are truly applicable in clinical practice and demonstrate high generalizability. Table 1.Recent research on MRI-based MDD classification on the REST-meta-MDD dataset. Table 1.Recent research on MRI-based MDD classification on the REST-meta-MDD dataset.StudyMethodologySampleData CharacteristicsSite HarmonizationAccuracyZhu et al. (2023) [25]Deep Graph CNNHC,n= 771 MDD,n=¬†830fMRINo (Random split)72.10%Gallo et al. (2023) [26]SVM and GCNHC,n=¬†1083 MDD,n=¬†1255fMRINo (Random split)62.00%Zheng et al. (2023) [43]a brain function encoder and a brain structure encoder to extract features, a function and structure co-attention fusion moduleHC,n=¬†1179 MDD,n=¬†1008fMRI and sMRINo (Random split)75.20%Noman et al. (2024) [18]graph autoencoder (GAE) architecture, built upon GCNHC,n=¬†227 MDD,n=¬†250fMRINo (Random split)65.07%Tan et al. (2024) [30]Generative Adversarial Network (GAN)HC,n=¬†684 MDD,n=¬†747fMRINo (Random split)76.28%Zhao et al. (2024) [29]GNNHC,n=¬†779 MDD,n=¬†832fMRINo (Random split)67.12%Zhang et al. (2024) [31]Deep Residual Shrinkage Denoising NetworkunknownfMRINo (Random split)72.43%Xi et al. (2025) [44]a Random Forest and an ensemble classifierHC,n=¬†742 MDD,n=¬†821fMRINo (Random split)75.30%Su et al. (2025) [28]Graph Attention Network (GAT)HC,n=¬†765 MDD,n=¬†821fMRINo (Random split)78.73%Liu et al. (2025) [27]a Federated Graph Convolutional Network framework with Dual Graph Attention NetworkHC,n=¬†384 MDD,n=¬†457fMRIYes61.80%",
            "3. Materials and Methods": "We propose a coordinated multi-site multimodal multitask learning framework that synergistically integrates time series data and gray matter volume (GMV) data to achieve accurate classification of brain disorders (the model architecture is shown inFigure 1). MMDD consists of two parallel modality-specific pathways to process individual modality inputs, followed by a cross-attention mechanism for adaptive intermodal information sharing. After obtaining the fused representation, we implement a dual-task learning scheme consisting of the following: (1) primary task: binary classification between MDD patients and healthy controls (HCs); (2) auxiliary task: site classifier. Concurrently, the model is trained using a Gradient Reversal Layer (GRL) in an adversarial learning manner, to eliminate site-specific characteristics from the features, thereby learning site-invariant feature representations. Figure 1.Architecture of the proposed MMDD framework for MDD classification. The framework comprises three main stages: (a) a dual-pathway feature extraction module; (b) a Bidirectional Cross-Attention Fusion (BCAF) module; and (c) a joint classification module based on a Gradient Reversal Layer Multitask Learning (GRL-MTL) strategy. 3.1. Dual-Pathway Feature ExtractionIn the temporal feature extraction branch, we combine the model structures of bidirectional long short-term memory (BiLSTM) [45] and Transformer encoder [46]. The input time series data is represented asùëãùë°ùë†‚àà‚Ñùùêµ√óùëá√óùëëXts‚ààRB√óT√ód, where B denotes batch size, T represents time steps, and d indicates feature dimensionality. To handle variable-length sequences, we apply zero-padding to standardize the sequence lengths and generate a corresponding binary maskùëÄ‚àà{0,1}ùêµ√óùëá√óùëëM‚àà{0,1}B√óT√ódto identify valid time steps. The branch processes the sequences through a BiLSTM layer as follows: the forward LSTM processes the sequence from start to end as shown in Equation (1), while the backward LSTM processes it in reverse order as shown in Equation (2). The complete BiLSTM outputùêªùëôùë†ùë°ùëöHlstmis obtained by concatenating both directional outputs according to Equation (3).‚Ñéùë°‚Üí=LSTM(ùë•ùë°ùë†,‚Ñéùë°‚àí1ÓÄ¶ÓÄ®ÓÄßÓÄßÓÄßÓÄß)ht‚Üí=LSTM(xts,ht‚àí1‚Üí)(1)‚Ñéùë°‚Üê=LSTM(ùë•ùë°ùë†,‚Ñéùë°‚àí1ÓÄ£ÓÄ•ÓÄ§ÓÄ§ÓÄ§ÓÄ§)ht‚Üê=LSTM(xts,ht‚àí1‚Üê)(2)ùêªùëôùë†ùë°ùëö=[‚Ñé‚Üí‚Äñ‚Ñé‚Üê]‚àà‚Ñùùêµ√óùëá√ó2ùëëHlstm=[h‚Üí‚Äñh‚Üê]‚ààRB√óT√ó2d(3)where,‚Ñéùë°‚Üíht‚Üíand‚Ñéùë°‚Üêht‚Üêrepresent the forward and backward hidden states, respectively, and ‚Äú‚Äñ‚Äù indicates feature concatenation.The Transformer encoder layer disregards the zero-padded positions, enabling global modeling of temporal dependencies. We perform a weighted average of the Transformer encoder outputs based on the input mask, ultimately obtaining high-level temporal sequence feature representationsùêªtransHtransas defined in Equation (4). This representation is subsequently transformed into the final temporal feature vectorùëßùë°ùë†ztsthrough a linear projection layer, formulated in Equation (5).ùêªtrans=TransformerEncoder(ùêªlstm)Htrans=TransformerEncoderHlstm(4)ùëßùë°ùë†=ùëä0¬∑‚éõ‚éù‚éú‚éú1ùëá‚àëùë°=1ùëáùêªtrans‚éû‚é†‚éü‚éü+ùëè0zts=W0¬∑1T‚àët=1THtrans+b0(5)where,ùëä0W0denotes the learnable weight parameters and b represents the bias terms. In the GMV feature extraction branch, we employ a 3D ResNet [47] architecture for hierarchical spatial feature learning. GMV is represented asùëãùëîùëö‚àà‚Ñùùêµ√ó1√óùê∑√óùêª√óùëäXgm‚ààRB√ó1√óD√óH√óW, where the spatial dimensionsùê∑√óùêª√óùëäD√óH√óWcorrespond to the depth, height and width of the 3D brain volume, respectively. This branch processes the volumetric input through four cascaded residual blocks(‚®Åùëô=1)‚®Ål=1, with each stage containing residual units. The final GMV representation is computed as shown in Equation (6):ùëßùëîùëö=Softmax(ùëä2¬∑(‚à•ResBlock‚äóùëõùëôùëô(ùë•ùëîùëö)‚à•4ùëô=1)+ùëè2)zgm=SoftmaxW2¬∑ResBlockl‚äónl(xgm)l=14+b2(6)where Softmax(¬∑) is the softmax activation function;‚Äñ4ùëô=1‚Äñl=14denotes the concatenation of features from the four residual block groups; and the superscript‚äóùëõùëô‚äónldenotes that the l-th residual block is stacked sequentiallyùëõùëônltimes. 3.2. Bidirectional Cross-Attention FusionWe propose the Bidirectional Cross-Attention Fusion (BCAF) for integrating deep temporal features with structural features. The first stage employs a cross-attention mechanism, achieving dynamic alignment between functional signals and anatomical foundations through symmetric attention computation. Taking the time series branch as an example, the cross-attention from time series to GMV is calculated as shown in Equation (7), and the updated temporal featureùëß‚Ä≤ùë°ùë†zts‚Ä≤is obtained via Equation (8). The structural branch follows the same computation symmetrically to yieldùëß‚Ä≤ùëîùëözgm‚Ä≤.Attentionùë°ùë†‚Üíùëîùëö=Softmax‚éõ‚éù‚éú‚éú‚éú‚éú(ùëßùë°ùë†ùëä1ùëÑ)(ùëßùëîùëöùëä1ùêæ)ùëáùëëùëñùëõ‚àí‚àí‚àí‚àö‚éû‚é†‚éü‚éü‚éü‚éüAttentionts‚Üígm=Softmax(ztsWQ1)(zgmWK1)Tdin(7)ùëß‚Ä≤ùë°ùë†=ùëßùë°ùë†+Attentionùë°ùë†‚Üíùëîùëö¬∑(ùëßùëîùëöùëä1ùëâ)zts‚Ä≤=zts+Attentionts‚Üígm¬∑(zgmWV1)(8)where,ùëßùë°ùë†ùëä1ùëÑztsWQ1serves as the Query;ùëßùëîùëöùëä1ùêæzgmWK1andùëßùëîùëöùëä1ùëâzgmWV1serve as the Key and Value; the scaling factor1/ùëëùëñùëõ‚àí‚àí‚àí‚àö1/dinis introduced to stabilize gradient propagation. Symmetrically, theùëß‚Ä≤ùëîùëözgm‚Ä≤is computed in a mirrored manner.The second stage is implemented through a Modality Weighting Allocation Module (MWAM). The calculation formulas are shown in Equation (9) and (10).ùúî=Softmax(ùëä4¬∑ReLU(ùëä3(ùëß‚Ä≤ùë°ùë†‚Äñùëß‚Ä≤ùëîùëö)))œâ=SoftmaxW4¬∑ReLUW3zts‚Ä≤‚Äñzgm‚Ä≤(9)ùëßfusion=ùúî1ùëß‚Ä≤ùë°ùë†+ùúî2ùëß‚Ä≤ùëîùëözfusion=œâ1zts‚Ä≤+œâ2zgm‚Ä≤(10)where the symbol ‚Äñ denotes the vector concatenation operation, and ReLU is the rectified linear unit activation function, defined as ReLU(x) = max(0,x). Algorithm 1 summarizes the complete flow of the proposed MMDD model.Algorithm 1Proposed MMDD Framework for MDD Classification1:Input:Time series dataùëãts‚àà‚Ñùùêµ√óùëá√óùëëXts‚ààRB√óT√ód(batch sizeB, time stepsT, feature dimd), GMV dataùëãgm‚àà‚Ñùùêµ√ó1√óùê∑√óùëä√óùêªXgm‚ààRB√ó1√óD√óW√óH(batchB, channel 1, depthD, widthW, heightH), ground truth labelsùë¶ùê∑‚àà{0,1}ùêµyD‚àà{0,1}B, site informationùë¶ùëÜ‚àà{0,‚Ä¶,24}ùêµyS‚àà{0,‚Ä¶,24}B2:Output:Prediction probabilityùë¶ÃÇùê∑y^Dfor MDD3:4:Step 1: Time Series Feature Extraction5:Generate padding maskùëÄ‚àà{0,1}ùêµ√óùëáM‚àà{0,1}B√óTfor variable-length sequences6:ùêªlstm‚ÜêBiLSTM(ùëãts)Hlstm‚ÜêBiLSTM(Xts)‚ñ∑ùêªlstm‚àà‚Ñùùêµ√óùëá√ó2ùëë‚ñ∑Hlstm‚ààRB√óT√ó2d7:forùëô=1l=1toùêøtransLtransdo‚ñ∑ùêøtransLtrans= 48:ùêªùëô‚ÜêTransformerEncoderLayer(ùêªùëô‚àí1,ùëÄ)Hl‚ÜêTransformerEncoderLayer(Hl‚àí1,M)‚ñ∑ùêª0=ùêªlstmH0=Hlstm9:end for10:ùëçts‚Üêùëä1¬∑GlobalAvgPool(ùêªL)+ùëè1Zts‚ÜêW1¬∑GlobalAvgPool(HL)+b1‚ñ∑ Final time series features11:12:Step 2: GMV Feature Extraction13:ùêπ0‚ÜêùëãgmF0‚ÜêXgm14:forùëô=1l=1toùêøresLresdo‚ñ∑ Process each block,ùêøresLres= 415:ùêπùëô‚ÜêResNet3DBlock(ùêπl-1)Fl‚ÜêResNet3DBlock(Fl-1)16:end for17:ùëçgm‚Üêùëä2¬∑AdaptiveAvgPool(ùêπL)+ùëè2Zgm‚ÜêW2¬∑AdaptiveAvgPool(FL)+b2‚ñ∑ Final GMV features18:19:Step 3: Bidirectional Cross-attention Fusion20:ùëç‚Ä≤ts‚Üêùëçts+CrossAttention(ùëÑ=ùëçts,ùêæ=ùëâ=ùëçgm)Zts‚Ä≤‚ÜêZts+CrossAttention(Q=Zts,K=V=Zgm)21:ùëç‚Ä≤gm‚Üêùëçgm+CrossAttention(ùëÑ=ùëçgm,ùêæ=ùëâ=ùëçts)Zgm‚Ä≤‚ÜêZgm+CrossAttention(Q=Zgm,K=V=Zts)22:ùúî‚ÜêSoftmax(ùëäùëê¬∑ReLU(ùëäùëé[ùëç‚Ä≤ts‚à•ùëç‚Ä≤gm]))œâ‚ÜêSoftmax(Wc¬∑ReLU(Wa[Zts‚Ä≤‚à•Zgm‚Ä≤]))‚ñ∑ùúî‚àà‚Ñùùêµ√ó2œâ‚ààRB√ó2,[¬∑‚à•¬∑][¬∑‚à•¬∑]is concatenation23:ùëçfusion‚Üêùúî[:,0]‚äôùëç‚Ä≤ts+ùúî[:,1]‚äôùëç‚Ä≤gmZfusion‚Üêœâ[:,0]‚äôZts‚Ä≤+œâ[:,1]‚äôZgm‚Ä≤‚ñ∑‚äô is element-wise multiplication24:25:Step 4: Multitask Prediction26:ùë¶ÃÇùëÜ‚ÜêSoftmax(ùëäùëÜ¬∑GradientReversalLayer(ùëçfusion)+ùëèùëÜ)y^S‚ÜêSoftmax(WS¬∑GradientReversalLayer(Zfusion)+bS)‚ñ∑ Site classification27:ùë¶ÃÇùê∑‚ÜêSoftmax(ùëäùê∑¬∑[ùëçts‚Äñùëçgm‚Äñùëçfusion]+ùëèùê∑)y^D‚ÜêSoftmax(WD¬∑[Zts‚ÄñZgm‚ÄñZfusion]+bD)‚ñ∑ MDD classification28:29:Step 5: Multi-Objective Optimization30:‚Ñísite‚ÜêCrossEntropy(ùë¶ÃÇùëÜ,ùë¶ùëÜ)Lsite‚ÜêCrossEntropy(y^S,yS)31:‚Ñímdd‚ÜêCrossEntropy(ùë¶ÃÇùê∑,ùë¶ùê∑)Lmdd‚ÜêCrossEntropy(y^D,yD)32:‚Ñítotal‚Üê(1‚àíùõΩ)‚Ñímdd‚àíùõΩ‚ÑísiteLtotal‚Üê(1‚àíŒ≤)Lmdd‚àíŒ≤Lsite 3.3. Multitask Learning with Gradient Reversal LayerWe designed a dual-task classification module that achieves cross-site robust diagnosis of MDD through collaborative optimization. The module comprises two classifiers:Site classifierùê∂1(¬∑)C1(¬∑)employs GRL to eliminate data distribution discrepancies, enabling the model to focus on learning site-invariant features. Inspired by the gradient reversal strategy in [42,48], we employ a Gradient Reversal Layer (GRL) for adversarial training, with the corresponding parameter update rules detailed in Equation (11). This strategy dynamically suppresses site-specific variations in the fused features, effectively eliminating inter-site data distribution discrepancies. Consequently, it guides the model to focus on learning domain-invariant features, ultimately enhancing its generalization capability. The architecture of this site classifier and its associated GRL are illustrated inFigure 2.ùúΩDùúΩMùúΩF‚ÜêùúΩD‚àíùúá¬∑(1‚àíùõΩ)‚àÇ‚ÑíD‚àÇùúΩD,ùúΩS‚ÜêùúΩS‚àíùúá¬∑ùõΩ‚àÇ‚ÑíS‚àÇùúΩS‚ÜêùúΩM‚àíùúá¬∑((1‚àíùõΩ)‚àÇ‚ÑíD‚àÇùúΩM‚àíùõΩ‚àÇ‚ÑíS‚àÇùúΩM),‚ÜêùúΩF‚àíùúá¬∑((1‚àíùõΩ)‚àÇ‚ÑíD‚àÇùúΩF‚àíùõΩ‚àÇ‚ÑíS‚àÇùúΩF),Œ∏D‚ÜêŒ∏D‚àíŒº¬∑(1‚àíŒ≤)‚àÇLD‚àÇŒ∏D,Œ∏S‚ÜêŒ∏S‚àíŒº¬∑Œ≤‚àÇLS‚àÇŒ∏SŒ∏M‚ÜêŒ∏M‚àíŒº¬∑(1‚àíŒ≤)‚àÇLD‚àÇŒ∏M‚àíŒ≤‚àÇLS‚àÇŒ∏M,Œ∏F‚ÜêŒ∏F‚àíŒº¬∑(1‚àíŒ≤)‚àÇLD‚àÇŒ∏F‚àíŒ≤‚àÇLS‚àÇŒ∏F,(11)where‚Ñíùê∑LDand‚ÑíùëÜLSdenote the disease classification loss and the site classification loss, respectively. The hyperparameterùõΩŒ≤controls the weighting of‚ÑíùëÜLSwithin the total adversarial objective, serving as a trade-off factor that balances the strength of domain invariance against the primary task performance. The model parameters are denoted as follows:ùúΩFŒ∏Ffor the feature extraction module,ùúΩMŒ∏Mfor the fusion module,ùúΩDŒ∏Dfor the disease classifier, andùúΩSŒ∏Sfor the site classifier. The parameters are updated with a learning rateùúáŒº.Figure 2.Training workflow and parameter optimization of the MMDD framework. The diagram illustrates the forward propagation (solid arrows) and backward propagation (dotted arrows) paths during model training. The framework consists of four learnable components: feature extraction module M (parametersùúÉùëÄŒ∏M), fusion module F (ùúÉùêπŒ∏F), disease classifier D (ùúÉùê∑Œ∏D), and site classifier S (ùúÉùëÜŒ∏S).Disease classifierùê∂2(¬∑)C2(¬∑)performs disease classification by concatenating modality-specific features with cross-modal correlation features. The final output probabilityùë¶ÃÇy^of the classifier is computed using Equations (12) and (13):ùë¶ÃÇùëÜ=ùê∂1(GRL(ùëßfusion))y^S=C1GRL(zfusion)(12)ùë¶ÃÇùê∑=ùê∂2([ùëßts‚Äñùëßgm‚Äñùëßfusion])y^D=C2[zts‚Äñzgm‚Äñzfusion](13)The model‚Äôs performance was evaluated using the mean values from five-fold cross-validation for metrics such as Accuracy, Recall, Area Under the ROC Curve (AUC), F1 score, and Balanced Accuracy (Equation (14)). The classification metrics are calculated as follows:AccuracyRecallPrecisionF1ScoreFPRTPRBalancedAccuracy=ùëáùëÉ+ùëáùëÅùëáùëÉ+ùëáùëÅ+ùêπùëÉ+ùêπùëÅ=ùëáùëÉùëáùëÉ+ùêπùëÅ=ùëáùëÉùëáùëÉ+ùêπùëÉ=2√óPrecision√óRecallPrecision+Recall=ùêπùëÉùêπùëÉ+ùëáùëÅ=ùëáùëÉùëáùëÉ+ùêπùëÅ=12(ùëáùëÉùëáùëÉ+ùêπùëÅ+ùëáùëÅùëáùëÅ+ùêπùëÉ)Accuracy=TP+TNTP+TN+FP+FNRecall=TPTP+FNPrecision=TPTP+FPF1Score=2√óPrecision√óRecallPrecision+RecallFPR=FPFP+TNTPR=TPTP+FNBalancedAccuracy=12TPTP+FN+TNTN+FP(14)TP (True Positives): the number of samples that are actually positive and predicted as positive; TN (True Negatives): the number of samples that are actually negative and predicted as negative; FP (False Positives): the number of samples that are actually negative but predicted as positive; FN (False Negatives): the number of samples that are actually positive but predicted as negative. The ROC curve is generated by plotting the points (FPR, TPR). The area beneath this curve is the AUC.",
            "3.1. Dual-Pathway Feature Extraction": "In the temporal feature extraction branch, we combine the model structures of bidirectional long short-term memory (BiLSTM) [45] and Transformer encoder [46]. The input time series data is represented asùëãùë°ùë†‚àà‚Ñùùêµ√óùëá√óùëëXts‚ààRB√óT√ód, where B denotes batch size, T represents time steps, and d indicates feature dimensionality. To handle variable-length sequences, we apply zero-padding to standardize the sequence lengths and generate a corresponding binary maskùëÄ‚àà{0,1}ùêµ√óùëá√óùëëM‚àà{0,1}B√óT√ódto identify valid time steps. The branch processes the sequences through a BiLSTM layer as follows: the forward LSTM processes the sequence from start to end as shown in Equation (1), while the backward LSTM processes it in reverse order as shown in Equation (2). The complete BiLSTM outputùêªùëôùë†ùë°ùëöHlstmis obtained by concatenating both directional outputs according to Equation (3).‚Ñéùë°‚Üí=LSTM(ùë•ùë°ùë†,‚Ñéùë°‚àí1ÓÄ¶ÓÄ®ÓÄßÓÄßÓÄßÓÄß)ht‚Üí=LSTM(xts,ht‚àí1‚Üí)(1)‚Ñéùë°‚Üê=LSTM(ùë•ùë°ùë†,‚Ñéùë°‚àí1ÓÄ£ÓÄ•ÓÄ§ÓÄ§ÓÄ§ÓÄ§)ht‚Üê=LSTM(xts,ht‚àí1‚Üê)(2)ùêªùëôùë†ùë°ùëö=[‚Ñé‚Üí‚Äñ‚Ñé‚Üê]‚àà‚Ñùùêµ√óùëá√ó2ùëëHlstm=[h‚Üí‚Äñh‚Üê]‚ààRB√óT√ó2d(3)where,‚Ñéùë°‚Üíht‚Üíand‚Ñéùë°‚Üêht‚Üêrepresent the forward and backward hidden states, respectively, and ‚Äú‚Äñ‚Äù indicates feature concatenation. The Transformer encoder layer disregards the zero-padded positions, enabling global modeling of temporal dependencies. We perform a weighted average of the Transformer encoder outputs based on the input mask, ultimately obtaining high-level temporal sequence feature representationsùêªtransHtransas defined in Equation (4). This representation is subsequently transformed into the final temporal feature vectorùëßùë°ùë†ztsthrough a linear projection layer, formulated in Equation (5).ùêªtrans=TransformerEncoder(ùêªlstm)Htrans=TransformerEncoderHlstm(4)ùëßùë°ùë†=ùëä0¬∑‚éõ‚éù‚éú‚éú1ùëá‚àëùë°=1ùëáùêªtrans‚éû‚é†‚éü‚éü+ùëè0zts=W0¬∑1T‚àët=1THtrans+b0(5)where,ùëä0W0denotes the learnable weight parameters and b represents the bias terms. In the GMV feature extraction branch, we employ a 3D ResNet [47] architecture for hierarchical spatial feature learning. GMV is represented asùëãùëîùëö‚àà‚Ñùùêµ√ó1√óùê∑√óùêª√óùëäXgm‚ààRB√ó1√óD√óH√óW, where the spatial dimensionsùê∑√óùêª√óùëäD√óH√óWcorrespond to the depth, height and width of the 3D brain volume, respectively. This branch processes the volumetric input through four cascaded residual blocks(‚®Åùëô=1)‚®Ål=1, with each stage containing residual units. The final GMV representation is computed as shown in Equation (6):ùëßùëîùëö=Softmax(ùëä2¬∑(‚à•ResBlock‚äóùëõùëôùëô(ùë•ùëîùëö)‚à•4ùëô=1)+ùëè2)zgm=SoftmaxW2¬∑ResBlockl‚äónl(xgm)l=14+b2(6)where Softmax(¬∑) is the softmax activation function;‚Äñ4ùëô=1‚Äñl=14denotes the concatenation of features from the four residual block groups; and the superscript‚äóùëõùëô‚äónldenotes that the l-th residual block is stacked sequentiallyùëõùëônltimes.",
            "3.2. Bidirectional Cross-Attention Fusion": "We propose the Bidirectional Cross-Attention Fusion (BCAF) for integrating deep temporal features with structural features. The first stage employs a cross-attention mechanism, achieving dynamic alignment between functional signals and anatomical foundations through symmetric attention computation. Taking the time series branch as an example, the cross-attention from time series to GMV is calculated as shown in Equation (7), and the updated temporal featureùëß‚Ä≤ùë°ùë†zts‚Ä≤is obtained via Equation (8). The structural branch follows the same computation symmetrically to yieldùëß‚Ä≤ùëîùëözgm‚Ä≤.Attentionùë°ùë†‚Üíùëîùëö=Softmax‚éõ‚éù‚éú‚éú‚éú‚éú(ùëßùë°ùë†ùëä1ùëÑ)(ùëßùëîùëöùëä1ùêæ)ùëáùëëùëñùëõ‚àí‚àí‚àí‚àö‚éû‚é†‚éü‚éü‚éü‚éüAttentionts‚Üígm=Softmax(ztsWQ1)(zgmWK1)Tdin(7)ùëß‚Ä≤ùë°ùë†=ùëßùë°ùë†+Attentionùë°ùë†‚Üíùëîùëö¬∑(ùëßùëîùëöùëä1ùëâ)zts‚Ä≤=zts+Attentionts‚Üígm¬∑(zgmWV1)(8)where,ùëßùë°ùë†ùëä1ùëÑztsWQ1serves as the Query;ùëßùëîùëöùëä1ùêæzgmWK1andùëßùëîùëöùëä1ùëâzgmWV1serve as the Key and Value; the scaling factor1/ùëëùëñùëõ‚àí‚àí‚àí‚àö1/dinis introduced to stabilize gradient propagation. Symmetrically, theùëß‚Ä≤ùëîùëözgm‚Ä≤is computed in a mirrored manner. The second stage is implemented through a Modality Weighting Allocation Module (MWAM). The calculation formulas are shown in Equation (9) and (10).ùúî=Softmax(ùëä4¬∑ReLU(ùëä3(ùëß‚Ä≤ùë°ùë†‚Äñùëß‚Ä≤ùëîùëö)))œâ=SoftmaxW4¬∑ReLUW3zts‚Ä≤‚Äñzgm‚Ä≤(9)ùëßfusion=ùúî1ùëß‚Ä≤ùë°ùë†+ùúî2ùëß‚Ä≤ùëîùëözfusion=œâ1zts‚Ä≤+œâ2zgm‚Ä≤(10)where the symbol ‚Äñ denotes the vector concatenation operation, and ReLU is the rectified linear unit activation function, defined as ReLU(x) = max(0,x). Algorithm 1 summarizes the complete flow of the proposed MMDD model.Algorithm 1Proposed MMDD Framework for MDD Classification1:Input:Time series dataùëãts‚àà‚Ñùùêµ√óùëá√óùëëXts‚ààRB√óT√ód(batch sizeB, time stepsT, feature dimd), GMV dataùëãgm‚àà‚Ñùùêµ√ó1√óùê∑√óùëä√óùêªXgm‚ààRB√ó1√óD√óW√óH(batchB, channel 1, depthD, widthW, heightH), ground truth labelsùë¶ùê∑‚àà{0,1}ùêµyD‚àà{0,1}B, site informationùë¶ùëÜ‚àà{0,‚Ä¶,24}ùêµyS‚àà{0,‚Ä¶,24}B2:Output:Prediction probabilityùë¶ÃÇùê∑y^Dfor MDD3:4:Step 1: Time Series Feature Extraction5:Generate padding maskùëÄ‚àà{0,1}ùêµ√óùëáM‚àà{0,1}B√óTfor variable-length sequences6:ùêªlstm‚ÜêBiLSTM(ùëãts)Hlstm‚ÜêBiLSTM(Xts)‚ñ∑ùêªlstm‚àà‚Ñùùêµ√óùëá√ó2ùëë‚ñ∑Hlstm‚ààRB√óT√ó2d7:forùëô=1l=1toùêøtransLtransdo‚ñ∑ùêøtransLtrans= 48:ùêªùëô‚ÜêTransformerEncoderLayer(ùêªùëô‚àí1,ùëÄ)Hl‚ÜêTransformerEncoderLayer(Hl‚àí1,M)‚ñ∑ùêª0=ùêªlstmH0=Hlstm9:end for10:ùëçts‚Üêùëä1¬∑GlobalAvgPool(ùêªL)+ùëè1Zts‚ÜêW1¬∑GlobalAvgPool(HL)+b1‚ñ∑ Final time series features11:12:Step 2: GMV Feature Extraction13:ùêπ0‚ÜêùëãgmF0‚ÜêXgm14:forùëô=1l=1toùêøresLresdo‚ñ∑ Process each block,ùêøresLres= 415:ùêπùëô‚ÜêResNet3DBlock(ùêπl-1)Fl‚ÜêResNet3DBlock(Fl-1)16:end for17:ùëçgm‚Üêùëä2¬∑AdaptiveAvgPool(ùêπL)+ùëè2Zgm‚ÜêW2¬∑AdaptiveAvgPool(FL)+b2‚ñ∑ Final GMV features18:19:Step 3: Bidirectional Cross-attention Fusion20:ùëç‚Ä≤ts‚Üêùëçts+CrossAttention(ùëÑ=ùëçts,ùêæ=ùëâ=ùëçgm)Zts‚Ä≤‚ÜêZts+CrossAttention(Q=Zts,K=V=Zgm)21:ùëç‚Ä≤gm‚Üêùëçgm+CrossAttention(ùëÑ=ùëçgm,ùêæ=ùëâ=ùëçts)Zgm‚Ä≤‚ÜêZgm+CrossAttention(Q=Zgm,K=V=Zts)22:ùúî‚ÜêSoftmax(ùëäùëê¬∑ReLU(ùëäùëé[ùëç‚Ä≤ts‚à•ùëç‚Ä≤gm]))œâ‚ÜêSoftmax(Wc¬∑ReLU(Wa[Zts‚Ä≤‚à•Zgm‚Ä≤]))‚ñ∑ùúî‚àà‚Ñùùêµ√ó2œâ‚ààRB√ó2,[¬∑‚à•¬∑][¬∑‚à•¬∑]is concatenation23:ùëçfusion‚Üêùúî[:,0]‚äôùëç‚Ä≤ts+ùúî[:,1]‚äôùëç‚Ä≤gmZfusion‚Üêœâ[:,0]‚äôZts‚Ä≤+œâ[:,1]‚äôZgm‚Ä≤‚ñ∑‚äô is element-wise multiplication24:25:Step 4: Multitask Prediction26:ùë¶ÃÇùëÜ‚ÜêSoftmax(ùëäùëÜ¬∑GradientReversalLayer(ùëçfusion)+ùëèùëÜ)y^S‚ÜêSoftmax(WS¬∑GradientReversalLayer(Zfusion)+bS)‚ñ∑ Site classification27:ùë¶ÃÇùê∑‚ÜêSoftmax(ùëäùê∑¬∑[ùëçts‚Äñùëçgm‚Äñùëçfusion]+ùëèùê∑)y^D‚ÜêSoftmax(WD¬∑[Zts‚ÄñZgm‚ÄñZfusion]+bD)‚ñ∑ MDD classification28:29:Step 5: Multi-Objective Optimization30:‚Ñísite‚ÜêCrossEntropy(ùë¶ÃÇùëÜ,ùë¶ùëÜ)Lsite‚ÜêCrossEntropy(y^S,yS)31:‚Ñímdd‚ÜêCrossEntropy(ùë¶ÃÇùê∑,ùë¶ùê∑)Lmdd‚ÜêCrossEntropy(y^D,yD)32:‚Ñítotal‚Üê(1‚àíùõΩ)‚Ñímdd‚àíùõΩ‚ÑísiteLtotal‚Üê(1‚àíŒ≤)Lmdd‚àíŒ≤Lsite",
            "3.3. Multitask Learning with Gradient Reversal Layer": "We designed a dual-task classification module that achieves cross-site robust diagnosis of MDD through collaborative optimization. The module comprises two classifiers: Site classifierùê∂1(¬∑)C1(¬∑)employs GRL to eliminate data distribution discrepancies, enabling the model to focus on learning site-invariant features. Inspired by the gradient reversal strategy in [42,48], we employ a Gradient Reversal Layer (GRL) for adversarial training, with the corresponding parameter update rules detailed in Equation (11). This strategy dynamically suppresses site-specific variations in the fused features, effectively eliminating inter-site data distribution discrepancies. Consequently, it guides the model to focus on learning domain-invariant features, ultimately enhancing its generalization capability. The architecture of this site classifier and its associated GRL are illustrated inFigure 2.ùúΩDùúΩMùúΩF‚ÜêùúΩD‚àíùúá¬∑(1‚àíùõΩ)‚àÇ‚ÑíD‚àÇùúΩD,ùúΩS‚ÜêùúΩS‚àíùúá¬∑ùõΩ‚àÇ‚ÑíS‚àÇùúΩS‚ÜêùúΩM‚àíùúá¬∑((1‚àíùõΩ)‚àÇ‚ÑíD‚àÇùúΩM‚àíùõΩ‚àÇ‚ÑíS‚àÇùúΩM),‚ÜêùúΩF‚àíùúá¬∑((1‚àíùõΩ)‚àÇ‚ÑíD‚àÇùúΩF‚àíùõΩ‚àÇ‚ÑíS‚àÇùúΩF),Œ∏D‚ÜêŒ∏D‚àíŒº¬∑(1‚àíŒ≤)‚àÇLD‚àÇŒ∏D,Œ∏S‚ÜêŒ∏S‚àíŒº¬∑Œ≤‚àÇLS‚àÇŒ∏SŒ∏M‚ÜêŒ∏M‚àíŒº¬∑(1‚àíŒ≤)‚àÇLD‚àÇŒ∏M‚àíŒ≤‚àÇLS‚àÇŒ∏M,Œ∏F‚ÜêŒ∏F‚àíŒº¬∑(1‚àíŒ≤)‚àÇLD‚àÇŒ∏F‚àíŒ≤‚àÇLS‚àÇŒ∏F,(11)where‚Ñíùê∑LDand‚ÑíùëÜLSdenote the disease classification loss and the site classification loss, respectively. The hyperparameterùõΩŒ≤controls the weighting of‚ÑíùëÜLSwithin the total adversarial objective, serving as a trade-off factor that balances the strength of domain invariance against the primary task performance. The model parameters are denoted as follows:ùúΩFŒ∏Ffor the feature extraction module,ùúΩMŒ∏Mfor the fusion module,ùúΩDŒ∏Dfor the disease classifier, andùúΩSŒ∏Sfor the site classifier. The parameters are updated with a learning rateùúáŒº. Figure 2.Training workflow and parameter optimization of the MMDD framework. The diagram illustrates the forward propagation (solid arrows) and backward propagation (dotted arrows) paths during model training. The framework consists of four learnable components: feature extraction module M (parametersùúÉùëÄŒ∏M), fusion module F (ùúÉùêπŒ∏F), disease classifier D (ùúÉùê∑Œ∏D), and site classifier S (ùúÉùëÜŒ∏S). Disease classifierùê∂2(¬∑)C2(¬∑)performs disease classification by concatenating modality-specific features with cross-modal correlation features. The final output probabilityùë¶ÃÇy^of the classifier is computed using Equations (12) and (13):ùë¶ÃÇùëÜ=ùê∂1(GRL(ùëßfusion))y^S=C1GRL(zfusion)(12)ùë¶ÃÇùê∑=ùê∂2([ùëßts‚Äñùëßgm‚Äñùëßfusion])y^D=C2[zts‚Äñzgm‚Äñzfusion](13) The model‚Äôs performance was evaluated using the mean values from five-fold cross-validation for metrics such as Accuracy, Recall, Area Under the ROC Curve (AUC), F1 score, and Balanced Accuracy (Equation (14)). The classification metrics are calculated as follows:AccuracyRecallPrecisionF1ScoreFPRTPRBalancedAccuracy=ùëáùëÉ+ùëáùëÅùëáùëÉ+ùëáùëÅ+ùêπùëÉ+ùêπùëÅ=ùëáùëÉùëáùëÉ+ùêπùëÅ=ùëáùëÉùëáùëÉ+ùêπùëÉ=2√óPrecision√óRecallPrecision+Recall=ùêπùëÉùêπùëÉ+ùëáùëÅ=ùëáùëÉùëáùëÉ+ùêπùëÅ=12(ùëáùëÉùëáùëÉ+ùêπùëÅ+ùëáùëÅùëáùëÅ+ùêπùëÉ)Accuracy=TP+TNTP+TN+FP+FNRecall=TPTP+FNPrecision=TPTP+FPF1Score=2√óPrecision√óRecallPrecision+RecallFPR=FPFP+TNTPR=TPTP+FNBalancedAccuracy=12TPTP+FN+TNTN+FP(14) TP (True Positives): the number of samples that are actually positive and predicted as positive; TN (True Negatives): the number of samples that are actually negative and predicted as negative; FP (False Positives): the number of samples that are actually negative but predicted as positive; FN (False Negatives): the number of samples that are actually positive but predicted as negative. The ROC curve is generated by plotting the points (FPR, TPR). The area beneath this curve is the AUC.",
            "4. Results": "4.1. DatasetsIn this study, we utilized the publicly available REST-meta-MDD dataset [49], which aggregates resting-state fMRI and sMRI data from 2428 participants (1300 MDD patients and 1128 HC) across 25 research groups at 17 Chinese hospitals. Specifically, we employed the gray matter volume and time series data provided by the consortium. Ethical approval was obtained from all local institutional review boards, with written informed consent provided by every participant. The dataset exclusively provides pre-processed data; our analysis focused on two modalities: (1) Time series data were extracted from resting-state fMRI using the AAL atlas [50] parcellation (116 regions) and structured as a 116 √ó T matrix (T = time steps; range: 90‚Äì240) due to site-specific acquisition protocols, and (2) gray matter volume (GMV) data derived from structural scans (fixed dimensions: 121 √ó 145 √ó 121). Detailed MRI acquisition parameters are documented athttps://rfmri.org/REST-meta-MDD, accessed on 17 May 2021.Since the data manager did not provide demographic information for 48 participants (S4) in the supplied file, our statistical analysis was conducted on the remaining 2380 participants (see details inTable 2). Subsequently, we performed statistical tests to evaluate group differences: a chi-square test was used for sex, while independent samplest-tests were applied for age and education level (Table 3).Table 2.Demographic Characteristics of MDD and HC Groups Across Study Sites.Table 3.Comparison of Demographic Characteristics Between MDD and HC Groups. 4.2. Data Pre-ProcessingThe data were pre-processed using an official DPARSF pipeline [51], following the procedure described by Yan et al. [49]. The steps were as follows: first, the initial 10 volumes were discarded to allow for magnetic field stabilization, followed by slice-timing correction and head motion realignment. Next, individual T1-weighted structural images were co-registered to the mean functional image and segmented into gray matter, white matter, and cerebrospinal fluid. Spatial normalization to MNI space was then performed using the DARTEL approach. During nuisance regression, the Friston 24-parameter model was applied to regress out head motion effects. Finally, a temporal bandpass filter (0.01‚Äì0.1 Hz) was applied to the time series to extract low-frequency fluctuations. 4.3. Implementation DetailsIn this study, we utilized the REST-meta-MDD dataset to train and evaluate our model. All input data underwent standardized normalization pre-processing. The model was trained using the AdamW optimizer with an initial learning rate (ùúáŒº) of 0.00005 and a site classification weight (ùõΩŒ≤) of 0.1, a batch size of 16 for 50 epochs, executed on a computing node equipped with four Tesla V100 GPUs interconnected via NVLink. Model performance was assessed through 5-fold cross-validation to ensure statistical reliability, with five key metrics reported as mean values: Accuracy, Balanced Accuracy, Recall, Area Under the Receiver Operating Characteristic Curve (AUC), and F1 Score. 4.4. Loss FunctionsTo optimize the model‚Äôs classification performance, we employ the cross-entropy loss [52] function given by Equation (15).ùêøùëêùëí=‚àí‚àëùëñ=1ùê∂ùë¶ùëñlog(ùëùùëñ)Lce=‚àí‚àëi=1Cyilogpi(15)where C denotes the number of categories;ùë¶ùëñyirepresents whether the sample comes from theùëñùë°‚Ñéithcategory. In this work, C = 2 for the primary task (MDD vs. HC), and C = 25 for the auxiliary task (site identification).The total loss for each sample is illustrated in Equation (16):ùêøtotal=minùúÉùê∑maxùúÉùëÜ(1‚àíùõΩ)¬∑ùêøùëêùëí(ùë¶ùê∑,ùëùùê∑)‚àíùõΩ¬∑ùêøùëêùëí(ùë¶ùëÜ,ùëùùëÜ)Ltotal=minŒ∏DmaxŒ∏S(1‚àíŒ≤)¬∑Lce(yD,pD)‚àíŒ≤¬∑Lce(yS,pS)(16)whereùõΩŒ≤represents the site loss weight andùúÉŒ∏denotes the model parameters. The total loss function incorporates a crucial balancing hyperparameter,ùõΩŒ≤(set to 0.1), which governs the trade-off between the primary MDD classification task and the auxiliary site identification task. All learnable parameters of the model, including the weights of the feature extractors and the fusion module, are represented byùúÉŒ∏. 4.5. Multimodal Performance Compared with Single-Modal PerformanceWe conducted a performance comparison between unimodal and multimodal approaches for the HC/MDD classification task (details shown inTable 4). The 3D ResNet model exhibits statistically significant performance advantages in GMV classification, achieving an accuracy of 73.44%. This represents a substantial improvement over other unimodal methods, indicating its strong discriminative capability in capturing depression-related brain structural abnormalities. In contrast, the ‚ÄúLSTM‚ÄìTransformer‚Äù model shows suboptimal classification performance (62.23%), revealing a considerable performance gap compared to GMV-based methods and suggesting inherent limitations in the discriminative power of standalone time series features for MDD classification. Most importantly, our proposed MMDD framework achieves the highest accuracy of 77.76% by synergistically integrating structural and temporal information. This finding conclusively demonstrates the effectiveness of our method in deep representational learning for multimodal neuroimaging data.Table 4.Model performance comparison across gray matter volume (GMV) and time series (TS) modalities (%, mean; ¬± std). 4.6. Ablation StudyWe conducted ablation studies comparing BCAF and GRL-MTL (seeTable 5) to investigate their synergistic mechanisms for improving MDD diagnosis. The results showed that the baseline model without any optimization components exhibited clear performance limitations, with high variability (standard deviation > 7%) highlighting its instability. When BCAF was enabled alone, the model demonstrated significant performance improvement with a greatly reduced variance, confirming its effectiveness in establishing dynamic cross-modal feature alignment and enhancing robustness. Although standalone GRL-MTL showed performance degradation, its combination with BCAF achieved optimal results, revealing the complementary roles of these two components: BCAF specializes in cross-modal feature optimization while GRL-MTL focuses on eliminating site-specific biases.Table 5.Cross-Site Generalization of GRL-MTL for MDD Diagnosis on REST-meta-MDD.To evaluate the effectiveness of the GRL-MTL strategy in enhancing the cross-site generalization capability of our MDD automated diagnosis model, we designed a leave-one-site-out validation experiment. As detailed inTable 5andTable 6, we employed a leave-one-site-out cross-validation strategy, where each of the 25 clinical sites was sequentially used as an independent test set, with the remaining 24 sites utilized for model training. Our analysis focuses on performance differences before and after enabling the GRL-MTL strategy. To address class imbalance in the test sets, we applied a sampling strategy in which the majority class was down-sampled to match the size of the minority class. The results demonstrate that, without GRL-MTL, the baseline model exhibited unstable performance across different sites, highlighting significant domain shift issues arising from variations in scanner parameters and heterogeneity in acquisition protocols. In contrast, when GRL-MTL was integrated, the model maintained stable diagnostic performance across all test sites (accuracy: 61.08‚Äì65.54%), demonstrating our method‚Äôs capability to effectively mitigate inter-site heterogeneity bias in multi-site MDD datasets and enforce learning of site-invariant features.Table 6.Ablation Study of BCAF (B) and GRL-MTL (G) Components for MDD Diagnosis on REST-meta-MDD (5-fold CV). 4.7. VisualizationTo investigate the impact of site heterogeneity on feature learning in multi-center data, we first visualized the feature distribution of the baseline fusion model without constraints on site information (Figure 3). The results show that the features are extremely dispersed in the t-SNE [59] space, with data points from different sites widely intermingled, failing to form a discriminative cluster structure. This indicates that the original fused features are severely influenced by site-specific biases rather than reflecting the biological patterns of the disease itself. To address this issue, we introduced the GRL-MTL strategy to learn site-invariant features. As shown inFigure 3, the processed features exhibit a significantly improved clustering trend, where technical variations across sites are effectively suppressed, and the feature distribution demonstrates higher intrinsic consistency. This visual comparison provides an intuitive explanation for the model‚Äôs superior classification performance.Figure 3.t-SNE visualization of feature distributions (a) before and (b) after site heterogeneity correction using the GRL-MTL strategy. Markers ‚Äúo‚Äù and ‚Äúx‚Äù represent HC and MDD. Colors denote different acquisition sites.",
            "4.1. Datasets": "In this study, we utilized the publicly available REST-meta-MDD dataset [49], which aggregates resting-state fMRI and sMRI data from 2428 participants (1300 MDD patients and 1128 HC) across 25 research groups at 17 Chinese hospitals. Specifically, we employed the gray matter volume and time series data provided by the consortium. Ethical approval was obtained from all local institutional review boards, with written informed consent provided by every participant. The dataset exclusively provides pre-processed data; our analysis focused on two modalities: (1) Time series data were extracted from resting-state fMRI using the AAL atlas [50] parcellation (116 regions) and structured as a 116 √ó T matrix (T = time steps; range: 90‚Äì240) due to site-specific acquisition protocols, and (2) gray matter volume (GMV) data derived from structural scans (fixed dimensions: 121 √ó 145 √ó 121). Detailed MRI acquisition parameters are documented athttps://rfmri.org/REST-meta-MDD, accessed on 17 May 2021. Since the data manager did not provide demographic information for 48 participants (S4) in the supplied file, our statistical analysis was conducted on the remaining 2380 participants (see details inTable 2). Subsequently, we performed statistical tests to evaluate group differences: a chi-square test was used for sex, while independent samplest-tests were applied for age and education level (Table 3). Table 2.Demographic Characteristics of MDD and HC Groups Across Study Sites. Table 3.Comparison of Demographic Characteristics Between MDD and HC Groups.",
            "4.2. Data Pre-Processing": "The data were pre-processed using an official DPARSF pipeline [51], following the procedure described by Yan et al. [49]. The steps were as follows: first, the initial 10 volumes were discarded to allow for magnetic field stabilization, followed by slice-timing correction and head motion realignment. Next, individual T1-weighted structural images were co-registered to the mean functional image and segmented into gray matter, white matter, and cerebrospinal fluid. Spatial normalization to MNI space was then performed using the DARTEL approach. During nuisance regression, the Friston 24-parameter model was applied to regress out head motion effects. Finally, a temporal bandpass filter (0.01‚Äì0.1 Hz) was applied to the time series to extract low-frequency fluctuations.",
            "4.3. Implementation Details": "In this study, we utilized the REST-meta-MDD dataset to train and evaluate our model. All input data underwent standardized normalization pre-processing. The model was trained using the AdamW optimizer with an initial learning rate (ùúáŒº) of 0.00005 and a site classification weight (ùõΩŒ≤) of 0.1, a batch size of 16 for 50 epochs, executed on a computing node equipped with four Tesla V100 GPUs interconnected via NVLink. Model performance was assessed through 5-fold cross-validation to ensure statistical reliability, with five key metrics reported as mean values: Accuracy, Balanced Accuracy, Recall, Area Under the Receiver Operating Characteristic Curve (AUC), and F1 Score.",
            "4.4. Loss Functions": "To optimize the model‚Äôs classification performance, we employ the cross-entropy loss [52] function given by Equation (15).ùêøùëêùëí=‚àí‚àëùëñ=1ùê∂ùë¶ùëñlog(ùëùùëñ)Lce=‚àí‚àëi=1Cyilogpi(15)where C denotes the number of categories;ùë¶ùëñyirepresents whether the sample comes from theùëñùë°‚Ñéithcategory. In this work, C = 2 for the primary task (MDD vs. HC), and C = 25 for the auxiliary task (site identification). The total loss for each sample is illustrated in Equation (16):ùêøtotal=minùúÉùê∑maxùúÉùëÜ(1‚àíùõΩ)¬∑ùêøùëêùëí(ùë¶ùê∑,ùëùùê∑)‚àíùõΩ¬∑ùêøùëêùëí(ùë¶ùëÜ,ùëùùëÜ)Ltotal=minŒ∏DmaxŒ∏S(1‚àíŒ≤)¬∑Lce(yD,pD)‚àíŒ≤¬∑Lce(yS,pS)(16)whereùõΩŒ≤represents the site loss weight andùúÉŒ∏denotes the model parameters. The total loss function incorporates a crucial balancing hyperparameter,ùõΩŒ≤(set to 0.1), which governs the trade-off between the primary MDD classification task and the auxiliary site identification task. All learnable parameters of the model, including the weights of the feature extractors and the fusion module, are represented byùúÉŒ∏.",
            "4.5. Multimodal Performance Compared with Single-Modal Performance": "We conducted a performance comparison between unimodal and multimodal approaches for the HC/MDD classification task (details shown inTable 4). The 3D ResNet model exhibits statistically significant performance advantages in GMV classification, achieving an accuracy of 73.44%. This represents a substantial improvement over other unimodal methods, indicating its strong discriminative capability in capturing depression-related brain structural abnormalities. In contrast, the ‚ÄúLSTM‚ÄìTransformer‚Äù model shows suboptimal classification performance (62.23%), revealing a considerable performance gap compared to GMV-based methods and suggesting inherent limitations in the discriminative power of standalone time series features for MDD classification. Most importantly, our proposed MMDD framework achieves the highest accuracy of 77.76% by synergistically integrating structural and temporal information. This finding conclusively demonstrates the effectiveness of our method in deep representational learning for multimodal neuroimaging data. Table 4.Model performance comparison across gray matter volume (GMV) and time series (TS) modalities (%, mean; ¬± std).",
            "4.6. Ablation Study": "We conducted ablation studies comparing BCAF and GRL-MTL (seeTable 5) to investigate their synergistic mechanisms for improving MDD diagnosis. The results showed that the baseline model without any optimization components exhibited clear performance limitations, with high variability (standard deviation > 7%) highlighting its instability. When BCAF was enabled alone, the model demonstrated significant performance improvement with a greatly reduced variance, confirming its effectiveness in establishing dynamic cross-modal feature alignment and enhancing robustness. Although standalone GRL-MTL showed performance degradation, its combination with BCAF achieved optimal results, revealing the complementary roles of these two components: BCAF specializes in cross-modal feature optimization while GRL-MTL focuses on eliminating site-specific biases. Table 5.Cross-Site Generalization of GRL-MTL for MDD Diagnosis on REST-meta-MDD. To evaluate the effectiveness of the GRL-MTL strategy in enhancing the cross-site generalization capability of our MDD automated diagnosis model, we designed a leave-one-site-out validation experiment. As detailed inTable 5andTable 6, we employed a leave-one-site-out cross-validation strategy, where each of the 25 clinical sites was sequentially used as an independent test set, with the remaining 24 sites utilized for model training. Our analysis focuses on performance differences before and after enabling the GRL-MTL strategy. To address class imbalance in the test sets, we applied a sampling strategy in which the majority class was down-sampled to match the size of the minority class. The results demonstrate that, without GRL-MTL, the baseline model exhibited unstable performance across different sites, highlighting significant domain shift issues arising from variations in scanner parameters and heterogeneity in acquisition protocols. In contrast, when GRL-MTL was integrated, the model maintained stable diagnostic performance across all test sites (accuracy: 61.08‚Äì65.54%), demonstrating our method‚Äôs capability to effectively mitigate inter-site heterogeneity bias in multi-site MDD datasets and enforce learning of site-invariant features. Table 6.Ablation Study of BCAF (B) and GRL-MTL (G) Components for MDD Diagnosis on REST-meta-MDD (5-fold CV).",
            "4.7. Visualization": "To investigate the impact of site heterogeneity on feature learning in multi-center data, we first visualized the feature distribution of the baseline fusion model without constraints on site information (Figure 3). The results show that the features are extremely dispersed in the t-SNE [59] space, with data points from different sites widely intermingled, failing to form a discriminative cluster structure. This indicates that the original fused features are severely influenced by site-specific biases rather than reflecting the biological patterns of the disease itself. To address this issue, we introduced the GRL-MTL strategy to learn site-invariant features. As shown inFigure 3, the processed features exhibit a significantly improved clustering trend, where technical variations across sites are effectively suppressed, and the feature distribution demonstrates higher intrinsic consistency. This visual comparison provides an intuitive explanation for the model‚Äôs superior classification performance. Figure 3.t-SNE visualization of feature distributions (a) before and (b) after site heterogeneity correction using the GRL-MTL strategy. Markers ‚Äúo‚Äù and ‚Äúx‚Äù represent HC and MDD. Colors denote different acquisition sites.",
            "5. Discussion": "To elucidate the key neural mechanisms underpinning the model‚Äôs prediction of major depressive disorder (MDD), we employed the SHAP (SHapley Additive exPlanations [60]) methodology to systematically quantify the contribution strengths of temporal sequence dynamic functional features and gray matter volume (GMV) structural features for the classification decision. Through comprehensive multimodal interpretability analysis, we identified brain regions with significant modality-specific influences and further interpreted their potential pathophysiological implications. The feature importance rankings derived from SHAP analysis, along with their statistical significance validated by independentt-tests, are presented inTable 7. The corresponding brain network visualization [61] results are shown inFigure 4. Figure 4.Spatial distribution of the top 14 discriminative features for fMRI (red) and sMRI (orange) identified by interpretability analysis, projected onto standard MNI brain space (coronal, sagittal, and axial views). Table 7.Top 10 Discriminative Brain Regions Identified by SHAP Analysis and Two-Samplet-tests. In distinguishing between patients with MDD and HC, the key neuroimaging features based on time series and gray matter volume exhibited distinct yet complementary patterns. The key brain regions identified at the time series level were primarily concentrated in subcortical hubs and the cerebellum, such as ‚ÄúThalamus_R‚Äù, ‚ÄúHippocampus_L‚Äù, and various cerebellar subregions (‚ÄúCerebellum_8_R‚Äù, ‚ÄúVermis_7‚Äù). This finding is consistent with previous reports of cerebellar abnormalities in MDD patients [62,63]. In contrast, the critical regions identified at the gray matter volume level were clustered in cortical areas associated with higher-order cognition and emotion regulation, including ‚ÄúCingulum_Mid_L‚Äù, ‚ÄúFrontal_Inf_Oper_R‚Äù, ‚ÄúInsula_R‚Äù, and ‚ÄúPrecuneus_L‚Äù. The structural alterations in these regions are more directly linked to the core symptoms of depression, such as emotional dysregulation and cognitive dysfunction. Notably, the middle part of the left cingulate gyrus was identified as a significant feature in both modalities. This multimodal comparison suggests that the pathological mechanisms of depression simultaneously involve functional dysregulation in deep brain regions responsible for basic information processing and structural alterations in superficial cortical centers for cognition and emotion. Collectively, these findings demonstrate a clear bimodal complementarity: while subcortical hubs and the cerebellum primarily manifest as abnormalities in temporal activation patterns, higher-order cognitive and emotion-related cortices predominantly exhibit structural volumetric alterations. These distinct patterns likely reflect divergent neuropathological processes, involving both functional dysregulation in information processing hubs and structural degradation in cognitive‚Äìemotional centers. The observed bimodal heterogeneity underscores that reliance on single-modality data may insufficiently capture MDD‚Äôs pathological complexity. Future investigations should prioritize the integration of multimodal data to achieve a more comprehensive delineation of the disorder‚Äôs neural underpinnings. Regarding clinical translational potential, these findings hold significant implications. First, the identified multimodal biomarker combinations show promise for objective neuroimaging-based auxiliary diagnostic tools. This would help address the current over-reliance on subjective clinical symptoms for diagnosis, particularly by providing a biological basis for differentiating complex or ambiguous cases. Second, these features can construct disease prediction models, for instance, to identify individuals at high risk, thereby advancing precision medicine. Finally, this deeper understanding of aberrant brain regions (such as the cerebellum) may inspire novel treatment targets, for example, by guiding non-invasive brain stimulation techniques to more precisely target previously overlooked key nodes. Therefore, this study not only deepens our understanding of the neural mechanisms underlying depression but also lays a solid foundation for advancing its clinical diagnosis and treatment towards a more objective and precise paradigm. This study has several limitations that should be acknowledged. While we conducted statistical tests that revealed significant differences in sex distribution and education level between the MDD and HC groups, these variables were not included as covariates in our primary neuroimaging analyses. It leaves open the possibility that the observed group differences may be partially influenced by these demographic disparities. Future studies should incorporate sex and education as covariates to verify the robustness of our findings.",
            "6. Conclusions": "This study proposes an innovative multimodal multitask deep learning framework MMDD for the objective diagnosis of major depressive disorder (MDD). By integrating gray matter volume (GMV) features and temporal sequence data, the framework identifies neuropsychiatric biomarkers of depression. In terms of model architecture, a dual-pathway deep neural network is adopted. Specifically, 3D ResNet is employed to extract spatial structural features from GMV, while an LSTM‚ÄìTransformer hybrid encoder is utilized to model temporal sequence information. Furthermore, an innovative Bidirectional Cross-Attention Fusion (BCAF) mechanism is implemented to achieve dynamic interaction and complementary fusion of spatiotemporal features. To enhance cross-center generalization, a Gradient Reversal Layer-based Multitask Learning (GRL-MTL) strategy is designed to optimize both site-invariant features and disorder-specific signatures. To uncover the neural mechanisms driving the model‚Äôs decisions, we employed SHAP analysis, with key findings validated by two-samplet-tests. This approach revealed a clear functional‚Äìstructural dichotomy: the model‚Äôs predictions relied on temporal abnormalities in functional circuits encompassing subcortical hubs and the cerebellum, and on structural volume alterations in higher-order cognitive‚Äìemotional cortices. The left middle cingulate gyrus emerged as a convergent hub across both modalities. This bimodal complementarity suggests MDD involves concurrent functional dysregulation in deep brain regions and structural degradation in cortical centers. In conclusion, this integrative research combines advanced deep learning techniques with interpretability analysis. Beyond establishing a novel paradigm for developing robust and generalizable objective diagnostic tools for depression, the study significantly advances our understanding of the multimodal neural mechanisms underlying MDD, with profound clinical implications for advancing precision medicine in psychiatric disorders."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2075-4418/15/23/3089",
        "scraped_at": "2025-12-05 23:55:33"
    },
    {
        "title": "Practical Predefined-Time Sliding-Mode Adaptive Resilient Control for PMSM Cyber‚ÄìPhysical Systems",
        "authors": "byZhenzhong Wang,Shu Zhang,Yun JiangandChunwu Yin",
        "journal": "Sensors2025,25(23), 7380; https://doi.org/10.3390/s25237380 (registering¬†DOI) - 4 Dec 2025",
        "abstract": "The permanent magnet synchronous motor (PMSM) is extensively utilized in the power drive systems of Cyber‚ÄìPhysical Systems (CPSs). In scenarios where control signals are subjected to malicious attacks within the network, ensuring that the PMSM achieves its designated speed within a specified timeframe serves as a critical metric for evaluating the efficacy of security control strategies in networked systems. To address practical challenges arising from updates to controlled objects at the physical layer and limitations of control layer algorithms‚Äîwherein convergence time for system trajectory tracking errors (TTEors) may extend indefinitely‚Äîwe have developed a novel resilient control algorithm with predefined-time convergence (PreTC) tailored for uncertain PMSMs susceptible to cyber threats. Firstly, we introduce an innovative Lyapunov stability criterion characterized by an adjustable gain reaching law alongside PreTC. Following this, we design an SMS (SMS) that incorporates PreTC and employ an extreme learning machine (ELM) to facilitate real-time identification of both physical layer models and malicious cyber-attacks. A sliding-mode adaptive resilient controller devoid of explicit physical model information is proposed for CPSs, with Lyapunov stability theory substantiating the system‚Äôs predefined-time (PDT) stability. This significantly enhances resilience against malicious cyber-attacks and other uncertainties. Finally, comparative simulations involving four distinct resilient control algorithms demonstrate that our proposed algorithm not only guarantees predetermined convergence times but also exhibits robust resistance to cyber-attacks, parameter perturbations, and external disturbances‚Äînotably achieving a motor speed tracking error accuracy of 0.008. These findings validate the superior robustness and effectiveness of our control algorithm against malicious cyber threats.Keywords:cyber physical systems;cyber-attacks;resilient control;predefined time control;PMSM",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Cyber-physical systems (CPSs) represent an advanced intelligent framework capable of seamlessly integrating computation, communication, and control mechanisms. By fusing human‚Äìcomputer interaction capabilities with real-time network communication technologies, the CPS enables secure and reliable management of remote physical entities [1]. This innovative technology has found widespread applications across multiple domains, including the Internet of Things (IoT), smart homes, and aerospace engineering. The evolution of CPS is characterized by continuous enhancements in intelligence, flexibility, scalability, and collaboration. Consequently, in practical engineering applications, CPS must exhibit high reliability, safety, control accuracy, as well as exceptional perception and autonomous control capabilities. Permanent magnet synchronous motors (PMSMs), renowned for their wide speed regulation range and high control precision, are frequently employed as driving devices for remote operations within CPS frameworks. However, technological advancements and changes in application environments often necessitate the update or replacement of driving motors, while also introducing challenges such as malicious cyber-attacks, prescribed-time stability requirements, and unforeseen disturbances. These issues may adversely affect the universality, accuracy, stability, and transient performance of PMSM-based CPS. Therefore, effectively addressing the universality, stability, and accuracy challenges of PMSM-CPS holds significant importance in industrial automation and high-performance control applications. In recent years, research on defenses against integrity attacks has encompassed a diverse range of systems and technologies. Regarding cyber-attack detection techniques, researchers have adopted hybrid approaches combining nonlinear observers, Chi-square detectors, mixed-integer nonlinear programming, and state estimation to achieve attack detection. Within the realm of wireless sensor networks, investigators have explored malicious information injection and state estimation algorithms in Krein space. While these algorithms offer the advantage of addressing unforeseen attacks, their detection efficiency remains unstable [2]. Reference [3] proposed a minimum defense budget state estimation algorithm for power systems vulnerable to false data injection (FDI) attacks, relaxing the assumption that ‚Äúcertain meter measurements can fully resist such attacks.‚Äù Reference [4] employed a Chi-square detector combined with the Cosine Similarity Matching (CSM) method for attack detection, demonstrating robust performance in identifying both FDI attacks and other attack types. Reference [5] introduced a decentralized homomorphic computing paradigm for system state estimation and designed a centralized FDI detector specifically tailored to identify FDI attacks. Despite their utility, these methods face challenges such as reliance on numerous attack model assumptions and the need for improved accuracy. Furthermore, these approaches primarily focus on detecting attacks rather than ensuring system stability in the presence of attacks. To address this gap, scholars have proposed resilient control strategies that enable CPS to maintain favorable steady-state performance even when confronting cyber-attacks. To solve the resilient control problem of linear CPS under FDI attacks and process interference, Reference [6] proposed a control strategy integrating an attack-resilient state observer with a control gain-switching mechanism, whose effectiveness was validated through anti-attack simulations on a linearized reduced-order aircraft system. Reference [7] combined a Butterworth low-pass filter with radial basis function neural networks to construct a neural network observer for estimating unmeasurable states, and designed an adaptive event-triggered neural network resilient control method for CPS to ensure system stability. Additionally, Reference [8] developed a distributed observer to stabilize the estimation error system and proposed an algorithm capable of exposing attacker behavior and identifying tampered sensors. Reference [9] designed a sliding-mode resilient control strategy for network systems subjected to DoS attacks, effectively enhancing the system‚Äôs ability to resist DoS attacks. Although these algorithms improve CPS‚Äôs resilience against various cyber-attacks, they rely heavily on physical layer model information, failing to meet the application requirements of rapid iteration and update of current physical layer intelligent terminals [10]. Therefore, establishing resilient control strategies that minimally depend on or are independent of physical layer model information is more aligned with technological iteration needs. However, model-independent resilient control strategies are currently scarce and remain in the early stages of research. Ensuring the stability of cyber-attacked CPS within a specified timeframe is a critical metric for evaluating the effectiveness of cyber system security control strategies. Nevertheless, existing resilient control strategies predominantly rely on asymptotic stability, often leading to infinite convergence time for CPS. To enhance the convergence speed of controlled systems, researchers have proposed finite-time control algorithms. For instance, Zhang W et al. [11] introduced a generalized super-twisting observer to estimate the rotor position and speed of PMSMs, endowing the speed controller with finite-time convergence capability. Additionally, Chen Z et al. [12] developed a finite-time control algorithm based on the backstepping method, considering the dynamic performance constraints of angular position tracking errors. While both algorithms guarantee finite-time convergence of motor systems, they neglect parameter uncertainties and external disturbances, compromising their robustness. Sliding-mode control (SMC) exhibits strong robustness, and its combination with finite-time control methods has been successfully applied to trajectory tracking in PMSM systems. Li S et al. [13] proposed a non-singular terminal sliding-mode surface (SMS) and designed a speed controller based on this surface, achieving system state convergence to the sliding-mode surface and equilibrium point within finite time. Feng Y et al. [14] designed a disturbance observer for finite-time convergent estimation of uncertainty components in PMSMs, and developed an observer-based terminal SMC to ensure that the PMSM position converges to the target position within finite time. Echreshavi, Z [15,16,17] approached the problem from the perspective of event-triggered control, designing a series of sliding-mode control strategies for uncertain nonlinear systems that not only enhance system robustness but also effectively reduce control input energy consumption. Despite the fact that such finite-time control algorithms improve the convergence speed of PMSM systems, a critical limitation is that the upper bound of convergence time depends on both initial states and controller parameters, rendering it unpredictable. Consequently, researchers have developed fixed-time controllers, whose convergence time upper bound is solely determined by control parameters. For example, Reference [18] designed a fixed-time convergence controller and applied it to wheeled robots with control dead zones; Xu P [19] proposed a fixed-time stability controller for dynamic positioning ships under unknown disturbances; Wang L [20] applied fixed-time control algorithms to PMSMs by designing a non-singular fast terminal sliding-mode controller (TSMC) with a fixed-time convergence law. It is important to note that the maximum value of this fixed-time convergence algorithm has specific limitations, as it directly influences system stability through its dependence on control parameters. Subsequently, researchers designed Predefined-Time Sliding-Mode Controllers (PDT SMCs) for various uncertain systems [21,22,23]. Although the convergence time of these PDT controllers can be preset, they are all designed based on fixed-time convergence stability criteria. Munoz-Vazquez A. J. [24] proposed a PDT stability criterion where the upper bound of convergence time is independent of both initial states and control parameters. However, the parameters of the proposed stability criterion lack adjustability, preventing performance tuning of the controlled system. On the other hand, sliding-mode control consists of an approaching phase and a sliding phase. Existing PDT control strategies can only guarantee that the SMS converges to zero within the PDT, but the tracking error at this point remains unstable. The tracking error within the SMS still needs to reach the origin through the sliding phase to achieve stability. Since the SMSs in existing PDT SMC strategies do not possess PDT convergence characteristics, the SMC cannot ensure that the trajectory tracking error of CPS exhibits PDT convergence. By analyzing the above discussion, the current security control strategies for cyber‚Äìphysical systems (CPSs) exhibit certain deficiencies: (1)Most existing security control strategies rely on the model information of the controlled object at the physical layer. However, when changes occur in the intelligent terminals of CPS, this can lead to control failures, rendering model-based control strategies insufficiently universal. Moreover, such strategies fail to meet cost-saving requirements during CPS update iterations. Therefore, there is an urgent need to develop an adaptive control strategy that does not depend on the model information of the controlled object. Nevertheless, current research on model-free and disturbance-independent control strategies for CPS remains limited.(2)Stability criteria for both finite-time convergence and fixed-time convergence have been proposed for NLS. However, practical NLS control faces limitations: the upper bound (UB) of convergence time in finite/fixed-time control theory depends on initial values and control parameters, resulting in a non-predetermined UB of convergence time. Although A. J. Mu√±oz‚ÄìV√°zquez proposed a PreTC stabilization criterion for NLS, allowing arbitrary setting of convergence time, this criterion designs a controller with fixed gains in the reaching law. Consequently, parameter adjustment for dynamic performance optimization is not feasible.(3)The application of SMC as a safety control strategy for CPS is prevalent. However, existing sliding-mode security control strategies for CPS can only ensure asymptotic convergence and provide no guarantees regarding the convergence of attacked CPS within a PDT. The PDT in SMC consists of two phases: the time required for the TTEor to reach the SMS and the time for the TTEor to converge to the equilibrium point. Current SMSs lack the capability to predefine convergence time, making it impossible to set an UB on the convergence time of TTEor after reaching the SMS. Consequently, the actual convergence time of the TTEor cannot be predetermined. In light of the aforementioned deficiencies in CPS security controls, this paper aims to make the following improvements: (1)Although A. J. Mu√±oz‚ÄìV√°zquez proposed a PreTC stabilization criterion for NLSs, the controller gains are fixed, precluding adjustments to dynamic performance. This paper extends the stability criterion proposed by A. J. Mu√±oz‚ÄìV√°zquez to enhance PreTC theory. Specifically, controllers with adjustable gains in the reaching law are designed, enabling dynamic performance optimization of the controlled object through gain-tuning.(2)A novel SMS is proposed to contain the TTEor, allowing the UB of the TTEor‚Äòs convergence time to be arbitrarily set according to engineering requirements. The controller designed based on this improved SMS, combined with the PreTC criterion proposed in this paper, ensures that the TTEor of CPS converges within the specified UB of convergence time.(3)The proposed approach employs an extreme learning machine (ELM) to estimate the system model and detect malicious cyber-attacks in real-time, using the TTEor as input information. By integrating the proposed SMS with the PreTC stabilization criterion, this paper develops a novel sliding-mode adaptive controller with a concise structure. This controller guarantees that the TTEor of CPS converges within a predetermined timeframe, thereby enhancing the universality and attack resilience of CPS security controllers. The paper is organized section-wise:Section 2describes the CPS, which includes the model of the PMSM in the physical layer, the attack model in the network layer, and the control objectives.Section 3introduces a novel PreTC stability theory along with an innovative SMS incorporating PreTC.Section 4details the primary design of a PDT sliding-mode adaptive resilient controller based on ELM and discusses its stability. Numerical simulations presented inSection 5validate the proposed control strategy, focusing primarily on dynamic performance under malicious cyber-attacks. Comparative simulation analyses are conducted under varying predefined convergence times and different controllers. Finally, our study concludes inSection 6.",
            "2. Description of Cyber‚ÄìPhysical Systems": "The PMSM offers numerous advantages, including a simple structure, low moment of inertia, wide speed range, and high efficiency. It is widely used in high-precision numerical control machines, robotics, and the aerospace industry. This paper addresses the security control problem of CPS where the network layer is subjected to malicious threat signals and the physical layer consists of a PMSM. An adaptive resilient SMC that does not rely on models and is disturbance-independent has been proposed. The overall structure of the CPS is illustrated inFigure 1below. Figure 1.Structural diagram of cyber-attacked CPS model. 2.1. PMSM‚Äôs Model of the Physical LayerIn physical layer, the PMSM‚Äôs voltage equation in CPS is [25].‚éß‚é©‚é®ÓÄÉÓÄÉùëñÀôùëë=(ùë¢ùëë‚àíùëÖùëñùëë+ùúîùëíùêøùëûùëñùëû)/ùêøùëñÀôùëû=(ùë¢ùëû‚àíùëÖùëñùëû‚àíùúîùëí(ùêøùëëùëñùëë+ùúìùëì))/ùêøiÀôd=(ud‚àíRid+œâeLqiq)/LiÀôq=(uq‚àíRiq‚àíœâe(Ldid+œàf))/L(1)From above expression, theùë¢ùëë,ùë¢ùëû,ùëñùëë,ùëñùëûud,uq,id,iqdenote the stator voltage and current in the d-axis and q-axis, correspondingly;ùëÖRdenotes the stator resistance;ùêøLdenotes the synchronous inductance, andùêøùëë,ùêøùëûLd,Lqdenote the inductance in the d-axis and q-axis;ùúìùëìœàfis the magnetic chain of the PMSM; andùúîùëíœâedenotes the electrical angular velocity.The mechanical equations of motion for PMSM can be described as [26].ùêΩùúîÀôùëü=ùëáùëí‚àíùêµùúîùëü‚àíùëáùêøJœâÀôr=Te‚àíBœâr‚àíTL(2)The parameterùúîùëüœârsignifies the mechanical angular velocity of the rotor;ùêΩJis the inertia moment;ùëáùêøTLis the load torque;ùêµBis the damping coefficient;ùëáùëíTedenotes an electromagnetic torque and its mathematical model is given as the following:ùëáùëí=1.5ùëùùúìùëìùëñùëûTe=1.5pœàfiq(3)whereùëùpis the number of poles of the motor and satisfiesùëíùëí=ùëùùúîùëüee=pœâr. Substituting Equation (3) into Equation (2) and lettingùúí=(1.5ùëùùúìùëì)/ùêΩœá=(1.5pœàf)/J,ùúÇ=ùêµ/ùêΩŒ∑=B/J,ùõæ=1/ùêΩŒ≥=1/J, It is rewritten as:ùúîÀôùëü=ùúíùëñùëû‚àíùúÇùúîùëü‚àíùõæùëáùêøœâÀôr=œáiq‚àíŒ∑œâr‚àíŒ≥TL(4) 2.2. Attack Model of the Network LayerIn the network layer, the open nature of communication networks renders CPS vulnerable to malicious cyber threats during remote real-time control implementation, such as denial-of-service (DoS) and false data injection (FDI) attacks.A DoS attack is a typical malicious assault that attempts to flood the information transmission channels in a network with massive traffic, thereby disrupting network signal transmission. In remotely operated systems, DoS attacks degrade the control commands sent to the control layer by consuming network-layer resources, ultimately affecting the control performance of the physical layer. This type of model under a DoS attack can be expressed as follows:ùëñq_attack(ùë°)=ùúÖDos(ùë°ùëö)ùëñùëû(ùë°)iq_attack(t)=Œ∫Dos(tm)iq(t)(5)whereùúÖDos(ùë°ùëö)>0Œ∫Dos(tm)>0is the weakening/increasing ratio;ùë°ùëötmis the initial attack time.FDI attacks represent a type of cyber intrusion that leverages forged data to manipulate the allocation of resources within target computer networks. Within remote control frameworks, FDI introduces deceptive data into network-level control signals. This interference has a cascading effect on the physical layer‚Äôs control effectiveness. An FDI attack within the network layer isùëñq_attack(ùë°)=ùëñùëû(ùë°)+ùúÜFDI(ùë°ùëé)iq_attack(t)=iq(t)+ŒªFDI(ta)(6)whereùúÜFDI(ùë°ùëé)ŒªFDI(ta)is the deviation in FDI attacks;ùë°ùëétais the initial attack time.There are various types of cyber-attacks, but all attacks are aimed at weakening/increasing control inputs or biasing control input signals, so the control input becomes the following:ùëñq_attack(ùë°)=ùúÖDos(ùë°ùëö)ùëñùëû(ùë°)+ùúÜFDI(ùë°ùëé)iq_attack(t)=Œ∫Dos(tm)iq(t)+ŒªFDI(ta)(7)whereùëñq_attack(ùë°)iq_attack(t)represents input current after cyber-attack,ùúÖDos(ùë°ùëö)‚àà(0,‚àû)Œ∫Dos(tm)‚àà(0,‚àû)is the multiplicative attack threat, andùúÜFDI(ùë°ùëé)ŒªFDI(ta)is the additive attack threat.Due to the unknown and uncertain nature ofùúÖDos(ùë°ùëö)Œ∫Dos(tm)andùúÜFDI(ùë°ùëé)ŒªFDI(ta), the cyber-attack (7) can be modified asùëñq_attack(ùë°)=ùëñùëû(ùë°)+(ùúÖDos‚àí1)ùëñùëû(ùë°)+ùúÜFDIiq_attack(t)=iq(t)+(Œ∫Dos‚àí1)iq(t)+ŒªFDI(8) 2.3. Control ObjectiveThere are parametric perturbations during the motor rotation process, which lead to unknown disturbancesŒîùúíŒîœá,ŒîùúÇŒîŒ∑,ŒîùõæŒîŒ≥in the system parametersùúíœá,ùúÇŒ∑,ùõæŒ≥, respectively, and denote known mode informationùëî=‚àíùúÇùúîùëü‚àíùõæùëáùêøg=‚àíŒ∑œâr‚àíŒ≥TL, and unknownùëë=Œîùúí(ùõºùëñùëû+ùõΩ)+ùúí((ùõº‚àí1)ùëñùëû+ùõΩ)‚àíŒîùúÇùúîùëüd=Œîœá(Œ±iq+Œ≤)+œá((Œ±‚àí1)iq+Œ≤)‚àíŒîŒ∑œâr‚àíŒîùõæùëáùêø‚àíŒîŒ≥TL; then the uncertain PMSM‚Äôs mechanical equation under cyber-attack is simplified asùúîÀôùëü=ùúíùëñùëû+ùëî+ùëëœâÀôr=œáiq+g+d(9)Let the desired trajectory of the mechanical angular velocity beùúî‚àóùëü(ùë°)œâr*(t). Denote the angular velocity TTEor asùëí(ùë°)=ùúî‚àóùëü(ùë°)‚àíùúîùëü(ùë°)e(t)=œâr*(t)‚àíœâr(t)(10)The objective of the CPS is to design a reference inputùëñ‚àóùëûiq*for the q-axis current in PMSM that accounts for malicious cyber-attacks and parameter perturbations. This will enable the angular velocity TTEorùëí(ùë°)e(t)to converge to zero within the PDTùëáùë†Ts.",
            "2.1. PMSM‚Äôs Model of the Physical Layer": "In physical layer, the PMSM‚Äôs voltage equation in CPS is [25].‚éß‚é©‚é®ÓÄÉÓÄÉùëñÀôùëë=(ùë¢ùëë‚àíùëÖùëñùëë+ùúîùëíùêøùëûùëñùëû)/ùêøùëñÀôùëû=(ùë¢ùëû‚àíùëÖùëñùëû‚àíùúîùëí(ùêøùëëùëñùëë+ùúìùëì))/ùêøiÀôd=(ud‚àíRid+œâeLqiq)/LiÀôq=(uq‚àíRiq‚àíœâe(Ldid+œàf))/L(1) From above expression, theùë¢ùëë,ùë¢ùëû,ùëñùëë,ùëñùëûud,uq,id,iqdenote the stator voltage and current in the d-axis and q-axis, correspondingly;ùëÖRdenotes the stator resistance;ùêøLdenotes the synchronous inductance, andùêøùëë,ùêøùëûLd,Lqdenote the inductance in the d-axis and q-axis;ùúìùëìœàfis the magnetic chain of the PMSM; andùúîùëíœâedenotes the electrical angular velocity. The mechanical equations of motion for PMSM can be described as [26].ùêΩùúîÀôùëü=ùëáùëí‚àíùêµùúîùëü‚àíùëáùêøJœâÀôr=Te‚àíBœâr‚àíTL(2) The parameterùúîùëüœârsignifies the mechanical angular velocity of the rotor;ùêΩJis the inertia moment;ùëáùêøTLis the load torque;ùêµBis the damping coefficient;ùëáùëíTedenotes an electromagnetic torque and its mathematical model is given as the following:ùëáùëí=1.5ùëùùúìùëìùëñùëûTe=1.5pœàfiq(3)whereùëùpis the number of poles of the motor and satisfiesùëíùëí=ùëùùúîùëüee=pœâr. Substituting Equation (3) into Equation (2) and lettingùúí=(1.5ùëùùúìùëì)/ùêΩœá=(1.5pœàf)/J,ùúÇ=ùêµ/ùêΩŒ∑=B/J,ùõæ=1/ùêΩŒ≥=1/J, It is rewritten as:ùúîÀôùëü=ùúíùëñùëû‚àíùúÇùúîùëü‚àíùõæùëáùêøœâÀôr=œáiq‚àíŒ∑œâr‚àíŒ≥TL(4)",
            "2.2. Attack Model of the Network Layer": "In the network layer, the open nature of communication networks renders CPS vulnerable to malicious cyber threats during remote real-time control implementation, such as denial-of-service (DoS) and false data injection (FDI) attacks. A DoS attack is a typical malicious assault that attempts to flood the information transmission channels in a network with massive traffic, thereby disrupting network signal transmission. In remotely operated systems, DoS attacks degrade the control commands sent to the control layer by consuming network-layer resources, ultimately affecting the control performance of the physical layer. This type of model under a DoS attack can be expressed as follows:ùëñq_attack(ùë°)=ùúÖDos(ùë°ùëö)ùëñùëû(ùë°)iq_attack(t)=Œ∫Dos(tm)iq(t)(5)whereùúÖDos(ùë°ùëö)>0Œ∫Dos(tm)>0is the weakening/increasing ratio;ùë°ùëötmis the initial attack time. FDI attacks represent a type of cyber intrusion that leverages forged data to manipulate the allocation of resources within target computer networks. Within remote control frameworks, FDI introduces deceptive data into network-level control signals. This interference has a cascading effect on the physical layer‚Äôs control effectiveness. An FDI attack within the network layer isùëñq_attack(ùë°)=ùëñùëû(ùë°)+ùúÜFDI(ùë°ùëé)iq_attack(t)=iq(t)+ŒªFDI(ta)(6)whereùúÜFDI(ùë°ùëé)ŒªFDI(ta)is the deviation in FDI attacks;ùë°ùëétais the initial attack time. There are various types of cyber-attacks, but all attacks are aimed at weakening/increasing control inputs or biasing control input signals, so the control input becomes the following:ùëñq_attack(ùë°)=ùúÖDos(ùë°ùëö)ùëñùëû(ùë°)+ùúÜFDI(ùë°ùëé)iq_attack(t)=Œ∫Dos(tm)iq(t)+ŒªFDI(ta)(7)whereùëñq_attack(ùë°)iq_attack(t)represents input current after cyber-attack,ùúÖDos(ùë°ùëö)‚àà(0,‚àû)Œ∫Dos(tm)‚àà(0,‚àû)is the multiplicative attack threat, andùúÜFDI(ùë°ùëé)ŒªFDI(ta)is the additive attack threat. Due to the unknown and uncertain nature ofùúÖDos(ùë°ùëö)Œ∫Dos(tm)andùúÜFDI(ùë°ùëé)ŒªFDI(ta), the cyber-attack (7) can be modified asùëñq_attack(ùë°)=ùëñùëû(ùë°)+(ùúÖDos‚àí1)ùëñùëû(ùë°)+ùúÜFDIiq_attack(t)=iq(t)+(Œ∫Dos‚àí1)iq(t)+ŒªFDI(8)",
            "2.3. Control Objective": "There are parametric perturbations during the motor rotation process, which lead to unknown disturbancesŒîùúíŒîœá,ŒîùúÇŒîŒ∑,ŒîùõæŒîŒ≥in the system parametersùúíœá,ùúÇŒ∑,ùõæŒ≥, respectively, and denote known mode informationùëî=‚àíùúÇùúîùëü‚àíùõæùëáùêøg=‚àíŒ∑œâr‚àíŒ≥TL, and unknownùëë=Œîùúí(ùõºùëñùëû+ùõΩ)+ùúí((ùõº‚àí1)ùëñùëû+ùõΩ)‚àíŒîùúÇùúîùëüd=Œîœá(Œ±iq+Œ≤)+œá((Œ±‚àí1)iq+Œ≤)‚àíŒîŒ∑œâr‚àíŒîùõæùëáùêø‚àíŒîŒ≥TL; then the uncertain PMSM‚Äôs mechanical equation under cyber-attack is simplified asùúîÀôùëü=ùúíùëñùëû+ùëî+ùëëœâÀôr=œáiq+g+d(9) Let the desired trajectory of the mechanical angular velocity beùúî‚àóùëü(ùë°)œâr*(t). Denote the angular velocity TTEor asùëí(ùë°)=ùúî‚àóùëü(ùë°)‚àíùúîùëü(ùë°)e(t)=œâr*(t)‚àíœâr(t)(10) The objective of the CPS is to design a reference inputùëñ‚àóùëûiq*for the q-axis current in PMSM that accounts for malicious cyber-attacks and parameter perturbations. This will enable the angular velocity TTEorùëí(ùë°)e(t)to converge to zero within the PDTùëáùë†Ts.",
            "3. Novel PDT Convergence Stability Norm": "To design a PreTC resilient controller for a PMSM CPS, a novel PreTC Lyapunov stability criterion is proposed and proved firstly. Consider a nonlinear system (NLS) described by the following dynamical equation:ùíôÀô=ùëì(ùíô),ùíô(0)=ùíô0xÀô=f(x),x(0)=x0(11)whereùíô‚àà‚Ñúùëõx‚àà‚Ñúndenotes the system‚Äôs state; functionùëì:‚Ñúùëõ‚Üí‚Ñúùëõf:‚Ñún‚Üí‚Ñúnrepresents the system dynamics; it is a smooth and differentiable function.ùíô(0)=ùíô0x(0)=x0are the initial conditions. Theorem1.For NLS (11), and for arbitrary PDTùëáùë†2>0Ts2>0, if there exists a radial unbounded and positive definite Lyapunov functionùúé(ùë•(ùë°))œÉ(x(t))which satisfiesùúéÀô(ùë•)‚â§‚àíùúã2ùëù2ùëáùë†2ùëé2ùëè2‚àí‚àí‚àí‚àí‚àö[ùëé2ùúé(ùë•)1‚àíùëù2+ùëè2ùúé1+ùëù2(ùë•)1‚àíùëù2]œÉÀô(x)‚â§‚àíœÄ2p2Ts2a2b2[a2œÉ(x)1‚àíp2+b2œÉ1+p2(x)1‚àíp2](12)among them, and the parameters are0<ùëù2<1,ùëé2>0,ùëè2>00<p2<1,a2>0,b2>0, the NLS (11) is global PDT stability, and the timeùë°ùë†tsof the state variable converges till the equilibrium point satisfiesùë°ùë†=2ùëáùë†2ùúãarctan[ùëè2/ùëé2‚àí‚àí‚àí‚àí‚àí‚àöùúé(ùë•0)ùëù2]<ùëáùë†2ts=2Ts2œÄarctan[b2/a2œÉ(x0)p2]<Ts2 Proof.According toùúéÀô(ùë•(ùë°))‚â§‚àíùúã2ùëù2ùëáùë†2ùëé2ùëè2‚àí‚àí‚àí‚àí‚àö[ùëé2ùúé(ùë•)1‚àíùëù2+ùëè2ùúé(ùë•)1+ùëù2]œÉÀô(x(t))‚â§‚àíœÄ2p2Ts2a2b2[a2œÉ(x)1‚àíp2+b2œÉ(x)1+p2], letùúéÀô(ùë•)=ùúã[ùëé2ùúé(ùë•)1‚àíùëù2+ùëè2ùúé(ùë•)1+ùëù2]‚àí2ùëù2ùëáùë†2ùëé2ùëè2‚àí‚àí‚àí‚àí‚àö‚àíùõøœÉÀô(x)=œÄ[a2œÉ(x)1‚àíp2+b2œÉ(x)1+p2]‚àí2p2Ts2a2b2‚àíŒ¥, whereùõø>0Œ¥>0, thendùúé(ùë•)dùë°=‚àíùúãùëé2ùúé(ùë•)1‚àíùëù22ùëù2ùëáùë†2ùëé2ùëè2‚àí‚àí‚àí‚àí‚àö[1+ùëè2ùëé2ùúé(ùë•)2ùëù2+2ùëù2ùëáùë†2ùëé2ùëè2‚àí‚àí‚àí‚àí‚àöùúãùëé2ùúé(ùë•)1‚àíùëù2ùõø]dœÉ(x)dt=‚àíœÄa2œÉ(x)1‚àíp22p2Ts2a2b2[1+b2a2œÉ(x)2p2+2p2Ts2a2b2œÄa2œÉ(x)1‚àíp2Œ¥](13)After transforming Equation (13) and gathering together the differential, the following equation is obtained.ùúãdùë°2ùëáùë†2=‚àíd[ùëè2/ùëé2‚àí‚àí‚àí‚àí‚àí‚àöùúé(ùë•)ùëù2]1+[ùëè2/ùëé2‚àí‚àí‚àí‚àí‚àí‚àöùúé(ùë•)ùëù2]2+2ùëù2ùëáùë†2ùëé2ùëè2‚àöùúãùëé2ùúé(ùë•)1‚àíùëù2ùõøœÄdt2Ts2=‚àíd[b2/a2œÉ(x)p2]1+[b2/a2œÉ(x)p2]2+2p2Ts2a2b2œÄa2œÉ(x)1‚àíp2Œ¥(14)Suppose thatùúé(ùë•(ùë°ùë†))=0œÉ(x(ts))=0atùë°ùë†ts, integrating both sides of Equation (14). Sinceùúé(ùë•)‚â•0œÉ(x)‚â•0,ùõø‚â•0Œ¥‚â•0, then2ùëù2ùëáùë†2ùëé2ùëè2‚àí‚àí‚àí‚àí‚àöùúãùëé2ùúé(ùë•)1‚àíùëù2ùõø‚â•02p2Ts2a2b2œÄa2œÉ(x)1‚àíp2Œ¥‚â•0, which provides‚à´ùë°ùë†0ùúã2ùëáùë†2dùë°=‚àí‚à´ùúé(ùë•(ùë°ùë†))ùúé(ùë•0)d(ùëè2/ùëé2‚àí‚àí‚àí‚àí‚àí‚àöùúé(ùë•)ùëù2)1+(ùëè2/ùëé2‚àí‚àí‚àí‚àí‚àí‚àöùúé(ùë•)ùëù2)2+2ùëù2ùëáùë†2ùëé2ùëè2‚àöùúãùëé2ùúé(ùë•)1‚àíùëù2ùõø=‚à´ùúé(ùë•0)0d(ùëè2/ùëé2‚àí‚àí‚àí‚àí‚àí‚àöùúé(ùë•)ùëù2)1+(ùëè2/ùëé2‚àí‚àí‚àí‚àí‚àí‚àöùúé(ùë•)ùëù2)2+2ùëù2ùëáùë†2ùëé2ùëè2‚àöùúãùëé2ùúé(ùë•)1‚àíùëù2ùõø‚â§‚à´ùúé(ùë•0)011+(ùëè2/ùëé2‚àí‚àí‚àí‚àí‚àí‚àöùúé(ùë•)ùëù2)2d(ùëè2/ùëé2‚àí‚àí‚àí‚àí‚àí‚àöùúé(ùë•)ùëù2)‚áíùë°ùë†‚â§2ùëáùë†2ùúãarctan[ùëè2/ùëé2‚àí‚àí‚àí‚àí‚àí‚àöùúé(ùë•0)ùëù2]‚â§ùëáùë†2‚à´0tsœÄ2Ts2dt=‚àí‚à´œÉ(x0)œÉ(x(ts))d(b2/a2œÉ(x)p2)1+(b2/a2œÉ(x)p2)2+2p2Ts2a2b2œÄa2œÉ(x)1‚àíp2Œ¥=‚à´0œÉ(x0)d(b2/a2œÉ(x)p2)1+(b2/a2œÉ(x)p2)2+2p2Ts2a2b2œÄa2œÉ(x)1‚àíp2Œ¥‚â§‚à´0œÉ(x0)11+(b2/a2œÉ(x)p2)2d(b2/a2œÉ(x)p2)‚áíts‚â§2Ts2œÄarctan[b2/a2œÉ(x0)p2]‚â§Ts2The above proof demonstrates that the convergence time of the nonlinear system (NLS) in (11) satisfiesùë°ùë†<ùëáùë†2ts<Ts2. This indicates that the state variable can converge to zero within the PDTùëáùë†2Ts2, where the UB of the convergence time is independent of both the initial state and control parameters. Thus, the system‚Äôs convergence time can be preset arbitrarily. ‚ñ° Remark1.The stability criterion, and the upper of convergence time are shown inTable 1. Compared with the Lyapunov stability theory ofùúéÀô(ùë•)‚â§‚àíùúã(ùúé(ùë•)0.5+ùúé(ùë•)1.5)ùëáùë†2œÉÀô(x)‚â§‚àíœÄ(œÉ(x)0.5+œÉ(x)1.5)Ts2proposed in Reference [24], the novel PreTC Lyapunov stability criterion proposed in this paper incorporates parametersùëé2>0,ùëè2>0a2>0,b2>0. These parameters enable adjustability of the reaching law gain in the controller designed based on the given stability criterion, thereby facilitating the tuning of the controlled system‚Äôs dynamic performance. Ifùëé2=ùëè2=1,ùëù2=0.5a2=b2=1,p2=0.5are satisfied, the proposed Lyapunov stability criterion is equivalent to that presented in Reference [24]. For the NLS in (11), if the Lyapunov function isùúé=0.5ùíôùõµùíôœÉ=0.5xŒ§x, the PDT controller derived based on the stability criterion in Reference [24] isùíñ=‚àíùëì(ùíô)‚àíùúãùëáùë†2(ùë†ùëñùëîùëõ(ùíô)+ùíô2sign(ùíô))u=‚àíf(x)‚àíœÄTs2(sign(x)+x2sign(x)). However, based on the novel stability criterion proposed in this paper, the controller isùíñ=‚àíùëì(ùíô)‚àíùúã(ùëé2ùíô1‚àíùëù2+ùëè2ùíô1+ùëù2)2ùëù2ùëáùë†2ùëé2ùëè2‚àí‚àí‚àí‚àí‚àöu=‚àíf(x)‚àíœÄ(a2x1‚àíp2+b2x1+p2)2p2Ts2a2b2. By comparing these two controllers, it can be observed that the gain of the latter is adjustable.Table 1.Stability criterion and convergence time. Remark2.Current controllers designed based on finite-time/fixed-time control theory merely ensure that the TTEor of the NLS converges within a finite time. However, the UB of this convergence time depends on both the initial state values and controller parameters, and thus cannot be predetermined. In contrast, by leveraging the novel Lyapunov stability criterion proposed in this paper, we achieve a predefined convergence timeùëáùë†2Ts2for the system‚Äôs TTEor to converge to zero. This guarantees that an explicit upper limit for the overall convergence time can be established in advance. Remark3.Building upon the PreTC stability criterion proposed in this paper, control techniques such as SMC, adaptive control, backstepping control, and prescribed performance control can be integrated to design corresponding PDT controllers for arbitrary uncertain NLSs.",
            "4. Sliding-Mode Controller Design": "4.1. Controller DesigenStep 1:SMS designTo fully exploit the robustness advantage of SMClers, and ensure that the TTEorùëí(ùë°)e(t)can converge within a PDTùëáùë†Ts, the following sliding-mode surface (SMS) with a PreTC is designed:ùëÜ=ùëí(ùë°)+ùúã2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àö(ùëé1ùúë(ùë°)+ùëè1ùúâ1+ùëù1(ùë°))S=e(t)+œÄ2p1Ts1a1b1(a1œÜ(t)+b1Œæ1+p1(t))(15)where the parameters satisfy0<ùëù1<1,ùëé1>0,ùëè1>00<p1<1,a1>0,b1>0;ùëáùë†1Ts1is the PDT;ùúâ(ùë°)=‚à´ùë°0ùëí(ùúè)ùëëùúèŒæ(t)=‚à´0te(œÑ)dœÑ.0<ùõøùúâ<10<Œ¥Œæ<1,ùëô2,ùëô3>0l2,l3>0are parameters to be designed;ùëô1=ùõø‚àíùëù1ùúâ‚àíùëô2ùõøùúâ‚àíùëô3ùõø2ùúâl1=Œ¥Œæ‚àíp1‚àíl2Œ¥Œæ‚àíl3Œ¥Œæ2;ùúë(ùë°)œÜ(t)is defined asùúë(ùë°)={ùúâ[1‚àíùëù1],|ùúâ|>ùõøùúâùëô1ùúâ+ùëô2ùúâ[2]+ùëô3ùúâ[3],|ùúâ|‚â§ùõøùúâœÜ(t)=Œæ[1‚àíp1],|Œæ|>Œ¥Œæl1Œæ+l2Œæ[2]+l3Œæ[3],|Œæ|‚â§Œ¥Œæ(16)Remark4.Ifùúâ>0Œæ>0,limùúâ‚Üíùõø+ùúâùúë=limùúâ‚Üíùõø+ùúâùúâ[1‚àíùëù1]=ùõø1‚àíùëù1ùúâlimŒæ‚ÜíŒ¥Œæ+œÜ=limŒæ‚ÜíŒ¥Œæ+Œæ[1‚àíp1]=Œ¥Œæ1‚àíp1,limùúâ‚Üíùõø‚àíùúâùúë=ùëô1ùõøùúâ+ùëô2ùõø2ùúâ+ùëô3ùõø3ùúâ=ùõø1‚àíùëù1ùúâlimŒæ‚ÜíŒ¥Œæ‚àíœÜ=l1Œ¥Œæ+l2Œ¥Œæ2+l3Œ¥Œæ3=Œ¥Œæ1‚àíp1, thenùúë(ùë°)œÜ(t)is continuous atùúâ=ùõøùúâŒæ=Œ¥Œæ. Whenùúâ<0Œæ<0,limùúâ‚Üí‚àíùõø+ùúâùúë=limùúâ‚Üí‚àíùõø+ùúâùúâ[1‚àíùëù1]=‚àíùõø1‚àíùëù1ùúâ,limùúâ‚Üí‚àíùõø‚àíùúâùúë==‚àíùëô1ùõøùúâ‚àíùëô2ùõø2ùúâ‚àíùëô3ùëí3ùúâ=‚àíùõø1‚àíùëù1ùúâlimŒæ‚Üí‚àíŒ¥Œæ+œÜ=limŒæ‚Üí‚àíŒ¥Œæ+Œæ[1‚àíp1]=‚àíŒ¥Œæ1‚àíp1,limŒæ‚Üí‚àíŒ¥Œæ‚àíœÜ==‚àíl1Œ¥Œæ‚àíl2Œ¥Œæ2‚àíl3eŒæ3=‚àíŒ¥Œæ1‚àíp1; therefore,ùúë(ùë°)œÜ(t)is continuous atùúâ=‚àíùõøùúâŒæ=‚àíŒ¥Œæ. Taken together, the functionùúë(ùë°)œÜ(t)is continuous, which solves the problem of control input jumping caused by discontinuity at segmentation points in traditional functions.According Equation (16),we haveùúëÀô(ùë°)={(1‚àíùëù1)ùúâ[‚àíùëù1],|ùúâ|>ùõøùúâ(ùëô1+2ùëô2ùúâ[1]+3ùëô3ùúâ[2]),|ùúâ|‚â§ùõøùúâœÜÀô(t)=(1‚àíp1)Œæ[‚àíp1],|Œæ|>Œ¥Œæ(l1+2l2Œæ[1]+3l3Œæ[2]),|Œæ|‚â§Œ¥Œæ(17)Remark5.To confirm the PreTC characteristic of TTEor within SMS, we assume that TTEor reaches the SMS (designated asùëÜ=0S=0) and set the convergence times asùëáùë†1=0.3ùë†,0.5ùë†,1ùë†,1.5ùë†,2ùë†Ts1=0.3s,0.5s,1s,1.5s,2s. The simulation is shown inFigure 2. It shows that the actual convergence times are persistently shorter than the PDT, as the TTEor grasps the SMS. This implies that the TTEor situated on SMS (15) has the property of PDT convergence.Figure 2.TTEor for different predefined convergence times.Step 2:Design Compensator Based on ELMDerive the SMSùëÜS, thenùëÜÀô(ùë°)=ùúã(ùëè1(1+ùëù1)ùúâùëù1(ùë°)+ùëé1ùúëÀô(ùë°))ùëí(ùë°)2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àö+ùëíÀô(ùë°)=ùúî‚àóùëü(ùë°)‚àíùúíùëñùëû(ùë°)‚àíùëî(ùë°)‚àíùëë(ùë°)+ùúã(ùëè1(1+ùëù1)ùúâùëù1(ùë°)+ùëé1ùúëÀô(ùë°))ùëí(ùë°)2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àöSÀô(t)=œÄ(b1(1+p1)Œæp1(t)+a1œÜÀô(t))e(t)2p1Ts1a1b1+eÀô(t)=œâr‚àó(t)‚àíœáiq(t)‚àíg(t)‚àíd(t)+œÄ(b1(1+p1)Œæp1(t)+a1œÜÀô(t))e(t)2p1Ts1a1b1(18)Letùê∫=ùëî+ùëëG=g+drepresent the encapsulated component, which can be estimated with ELM [26]. The ELM is regarded as single hidden-layer feed-forward neural networks (SLFNNs), and the structural of SLFNs is shown inFigure 3.Figure 3.Diagram of SLFN structure.The input of the SLFN isùíõ=[ùëí,ùúâ]ùõµz=[e,Œæ]Œ§,ùëÅÃÉNÀúis the nodes, and‚Ñé(ùíô)h(x)is the activation function. Through continuous learning and training, it is possible to obtain the optimal weight vectorùíò‚àów*of SLFN in order to approximate the encapsulated componentùê∫G.ùê∫=‚àëùëñ=1ùëÅÀúùíò‚àóùëñ‚Ñéùëñ(ùíÑùëñùíõ,ùë£ùëñ)=ùíò‚àóùíâ(ùíõ)G=‚àëi=1NÀúwi*hi(ciz,vi)=w*h(z)(19)whereùíÑùëñ=[ùëêùëñ1,ùëêùëñ2,‚Ä¶ùëêùëñùëõ]ùõµ‚ààùëÖùëõci=[ci1,ci2,‚Ä¶cin]Œ§‚ààRnis the internal connection weight to be designed,ùë£ùëñviis the node threshold to be designed, andùíòùëñ=[ùë§ùëñ1,ùë§ùëñ2,‚Ä¶ùë§ùëñùëö]ùõµ‚ààùëÖùëöwi=[wi1,wi2,‚Ä¶wim]Œ§‚ààRmis the external connection weight.SLFNs need to learn and adjust their weight vectorsùíÑùëñ=[ùëêùëñ1,ùëêùëñ2,‚Ä¶ùëêùëñùëõ]ùõµci=[ci1,ci2,‚Ä¶cin]Œ§,ùíòùëñ=[ùë§ùëñ1,ùë§ùëñ2,‚Ä¶ùë§ùëñùëö]ùõµwi=[wi1,wi2,‚Ä¶wim]Œ§, and bias valuesùë£ùëñvi, which can result in a large amount of computation and slow computation speed. To make SLFNs easier to compute, the ELM is proposed by Huang G B [27]. In the ELM, the connection weightsùíÑùëñciand the node thresholdsùë£ùëñvihave been arbitrarily created.ùíòùëñwiis the outer weight vector to be recognized.ùíâ(ùíõ)=(‚Ñé1(ùíÑ1ùíõ,ùë£1),‚Ä¶,‚ÑéùëÅÀú(ùíÑùëÅÀúùíõ,ùë£ùëÅÀú))ùõµh(z)=(h1(c1z,v1),‚Ä¶,hNÀú(cNÀúz,vNÀú))Œ§. The estimation ofùíò‚àów*is obtained indirectly in practical engineering applications and cannot be approximated asùê∫Gprecisely. The value ofùê∫ÃÇG^is calculated asùê∫ÃÇ=ùíâ(ùíõ)ùíòÃÇ=‚àëùëñ=1ùëÅÀúùíòÃÇùëñ‚Ñéùëñ(ùíÑùíäùíõ,ùë£ùëñ)G^=h(z)w^=‚àëi=1NÀúw^ihi(ciz,vi)(20)In the above formula, theùíòÃÇùëñw^isignifies an estimation of the optimal valueùíò‚àóùëñwi*, and the designed adaptive law is as follows:ùíòÃÇÀô=‚àíùõæùíâùëÜw^Àô=‚àíŒ≥hS(21)Step 3:Controller design.We designed the following resilient controller:ùëñ‚àóùëû=ùëñùëé+ùëñùëèiq*=ia+ib(22)whereùëñùëé=1ùúí(ùúî‚àóùëü‚àíùê∫ÃÇ+ùúã(ùëé1ùúëÀô(ùë°)+ùëè1(1+ùëù1)ùúâùëù1)2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àöùëí)ùëñùëè=ùúãùúíùëù2ùëáùë†2ùëé2ùëè2‚àí‚àí‚àí‚àí‚àö(ùëé2ùëÜ1‚àíùëù2+ùëè2ùëÜ1+ùëù2)ia=1œáœâr‚àó‚àíG^+œÄ(a1œÜÀô(t)+b1(1+p1)Œæp1)2p1Ts1a1b1eib=œÄœáp2Ts2a2b2(a2S1‚àíp2+b2S1+p2)(23)where constantùëéùëñ>0,ùëèùëñ>0,0<ùëùùëñ<1ai>0,bi>0,0<pi<1.Remark6.The controller proposed in this paper exclusively utilizes the angular velocity tracking error information of the PMSM, without relying on any physical layer model information. This controller exhibits excellent universality, as it does not require explicit knowledge of the PMSM‚Äôs detailed mechanical model or external disturbances. 4.2. Stability AnalysisTheorem2.For arbitrary PDTùëáùë†1>0,ùëáùë†2>0Ts1>0,Ts2>0, when the q-axis current reference inputùëñ‚àóùëûiq*of the PMSM (9) in the physical layer is (22), the angular velocity TTEor of the PMSM will converge to the SMS (15) within the PDTùëáùë†2Ts2, and the upper-bounded angular velocity TTEor‚Äôs convergence time isùëáùë†=ùëáùë†1+ùëáùë†2Ts=Ts1+Ts2.Proof.Taking the Lyapunov candidate asùúé1=0.5ùëÜ2œÉ1=0.5S2, we deriveùë°tto obtainùúéÀô1=ùëÜùëÜÀô=ùëÜ(ùúã(ùëé1ùúëÀô(ùë°)+ùëè1(1+ùëù1)ùúâùëù1)ùëí2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àö+ùëíÀô)=ùëÜ(ùúî‚àóùëü‚àíùúíùëñùëû‚àíùê∫+ùúã(ùëé1ùúëÀô(ùë°)+ùëè1(1+ùëù1)ùúâùëù1)ùëí2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àö)œÉÀô1=SSÀô=SœÄ(a1œÜÀô(t)+b1(1+p1)Œæp1)e2p1Ts1a1b1+eÀô=Sœâr‚àó‚àíœáiq‚àíG+œÄ(a1œÜÀô(t)+b1(1+p1)Œæp1)e2p1Ts1a1b1(24)NotingùíòÀú=ùíò‚àó‚àíùíòÃÇwÀú=w*‚àíw^,ùê∫Àú=ùê∫‚àíùê∫ÃÇGÀú=G‚àíG^, construct the Lyapunov function asùúé2=ùúé1+0.5ùõæ‚àí1ùë°ùëü(ùíòÀúùõµùíòÀú)œÉ2=œÉ1+0.5Œ≥‚àí1tr(wÀúŒ§wÀú), thenùúéÀô2=‚àíùúã(ùëéÃ≤ùúé1‚àíùëùÃ≤1+ùëèÃ≤ùúé1+ùëùÃ≤1)2ùëùÃ≤ùëáùë†2ùëéÃ≤ùëèÃ≤‚àí‚àí‚àö‚àíùëÜùê∫Àú‚àíùë°ùëü(ùíòÀúùõµùíòÃÇÀô)ùõæ=ùúã(ùëéÃ≤ùúé1‚àíùëùÃ≤1+ùëèÃ≤ùúé1+ùëùÃ≤1)‚àí2ùëùÃ≤ùëáùë†2ùëéÃ≤ùëèÃ≤‚àí‚àí‚àö‚àíùë°ùëü(ùê∞Àúùõµ(ùõæùíâùëÜ+ùíòÃÇÀô))ùõæœÉÀô2=‚àíœÄ(a¬ØœÉ11‚àíp¬Ø+b¬ØœÉ11+p¬Ø)2p¬ØTs2a¬Øb¬Ø‚àíSGÀú‚àítr(wÀúŒ§w^Àô)Œ≥=œÄ(a¬ØœÉ11‚àíp¬Ø+b¬ØœÉ11+p¬Ø)‚àí2p¬ØTs2a¬Øb¬Ø‚àítr(wÀúŒ§(Œ≥hS+w^Àô))Œ≥(25)whereùëéÃ≤=ùëé221‚àí0.5ùëù2,ùëèÃ≤=ùëè221+0.5ùëù2,ùëùÃ≤=0.5ùëù2a¬Ø=a221‚àí0.5p2,b¬Ø=b221+0.5p2,p¬Ø=0.5p2. By substituting Equation (22) into Equation (25), we obtainùúéÀô2=‚àíùúã(ùëéÃ≤ùëâ1‚àíùëùÃ≤1+ùëèÃ≤ùëâ1+ùëùÃ≤1)2ùëùÃ≤ùëáùë†2ùëéÃ≤ùëèÃ≤‚àí‚àí‚àöœÉÀô2=‚àíœÄ(a¬ØV11‚àíp¬Ø+b¬ØV11+p¬Ø)2p¬ØTs2a¬Øb¬ØAccording to Theorem 1, the SMSùëÜ(ùë°)S(t)will converge in PDTùëáùë†2Ts2, and the state variables will converge to the SMS in PDTùëáùë†2Ts2.When the SMS satisfiesùëÜ(ùë°)=0S(t)=0, thenùëí(ùë°)=‚àíùúã2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àö(ùëé1ùúë(ùë°)+ùëè1ùúâ1+ùëù1(ùë°))e(t)=‚àíœÄ2p1Ts1a1b1(a1œÜ(t)+b1Œæ1+p1(t))(26)Define the Lyapunov candidate asùúé(ùë°)=0.5ùúâ2œÉ(t)=0.5Œæ2, then obtain the following:ùúéÀô(ùë°)=ùúâ(ùë°)ùúâÀô(ùë°)=‚àíùúã(ùëè1ùúâ2+ùëù1(ùë°)+ùëé1ùúâ(ùë°)ùúëÀô(ùë°))2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àö=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉ‚àíùúã(ùëéÃ≤1ùúé1‚àíùëù12(ùë°)+ùëèÃ≤1ùúé1+ùëù12(ùë°))2ùëùÃ≤1ùëáùë†1ùëéÃ≤1ùëèÃ≤1‚àí‚àí‚àí‚àí‚àö‚àíùúã(ùëé1ùúâ((ùëô1+2ùëô2ùúâ[1]+3ùëô3ùúâ[2]))+ùëè1ùúâ2+ùëù1(ùë°))2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àö|ùúâ|>ùõøùúâ|ùúâ|‚â§ùõøùúâœÉÀô(t)=Œæ(t)ŒæÀô(t)=‚àíœÄ(b1Œæ2+p1(t)+a1Œæ(t)œÜÀô(t))2p1Ts1a1b1=‚àíœÄ(a¬Ø1œÉ1‚àíp12(t)+b¬Ø1œÉ1+p12(t))2p¬Ø1Ts1a¬Ø1b¬Ø1|Œæ|>Œ¥Œæ‚àíœÄ(a1Œæ((l1+2l2Œæ[1]+3l3Œæ[2]))+b1Œæ2+p1(t))2p1Ts1a1b1|Œæ|‚â§Œ¥Œæ(27)whereùëùÃ≤1=ùëù12,ùëéÃ≤1=ùëé121‚àíùëù12,ùëèÃ≤1=ùëè121+ùëù12p¬Ø1=p12,a¬Ø1=a121‚àíp12,b¬Ø1=b121+p12.According to Theorem 1, it can be concluded that when|ùúâ(ùë°)|>ùõøùúâ|Œæ(t)|>Œ¥Œæ,ùúâ(ùë°)Œæ(t)will converge within the PDTùëáùë†1Ts1. From Equation (26), ifùúâ(ùë°)=0Œæ(t)=0, thenùëí(ùúè)‚â°0e(œÑ)‚â°0, so if the SMS(15)satisfiesùëÜ=0S=0, then the TTEorùëí(ùë°)e(t)will converge at PDTùëáùë†1Ts1. When|ùúâ|‚â§ùõøùúâ|Œæ|‚â§Œ¥Œæ,ùúâ(ùë°)Œæ(t)will asymptotically converge stably to zero.Combining the above analyses, it verified that the angular velocity trajectory tracking errorùíÜ(ùë°)e(t)can converge to zero within the PDTùëáùë†=ùëáùë†1+ùëáùë†2Ts=Ts1+Ts2. ‚ñ°Remark7.The motion trajectory of SMC is mainly divided into two phases: Phase (1) is the phase where state variables at any initial position converge to the SMS. This phase ensures that the TTEor converges toward the SMS (rather than the SMS itself converging to zero), while the state variables themselves do not converge to zero. Phase (2) is the phase where the state variables on the SMS converge to the equilibrium point (zero). In this phase, the state variables essentially achieve convergence to zero. Compared with existing finite-time/fixed-time SMCs, the latter can only ensure that state variables converge to the SMS within finite/fixed time in the first phase, but fail to guarantee that the state variables in the second phase also converge within a finite/fixed time. Therefore, traditional finite-time/fixed-time SMC is not essentially finite-time convergent in nature. In addition, these control algorithms do not possess the characteristic of PreTC. The algorithm proposed in this paper achieves essential PreTC.",
            "4.1. Controller Desigen": "Step 1:SMS design To fully exploit the robustness advantage of SMClers, and ensure that the TTEorùëí(ùë°)e(t)can converge within a PDTùëáùë†Ts, the following sliding-mode surface (SMS) with a PreTC is designed:ùëÜ=ùëí(ùë°)+ùúã2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àö(ùëé1ùúë(ùë°)+ùëè1ùúâ1+ùëù1(ùë°))S=e(t)+œÄ2p1Ts1a1b1(a1œÜ(t)+b1Œæ1+p1(t))(15)where the parameters satisfy0<ùëù1<1,ùëé1>0,ùëè1>00<p1<1,a1>0,b1>0;ùëáùë†1Ts1is the PDT;ùúâ(ùë°)=‚à´ùë°0ùëí(ùúè)ùëëùúèŒæ(t)=‚à´0te(œÑ)dœÑ.0<ùõøùúâ<10<Œ¥Œæ<1,ùëô2,ùëô3>0l2,l3>0are parameters to be designed;ùëô1=ùõø‚àíùëù1ùúâ‚àíùëô2ùõøùúâ‚àíùëô3ùõø2ùúâl1=Œ¥Œæ‚àíp1‚àíl2Œ¥Œæ‚àíl3Œ¥Œæ2;ùúë(ùë°)œÜ(t)is defined asùúë(ùë°)={ùúâ[1‚àíùëù1],|ùúâ|>ùõøùúâùëô1ùúâ+ùëô2ùúâ[2]+ùëô3ùúâ[3],|ùúâ|‚â§ùõøùúâœÜ(t)=Œæ[1‚àíp1],|Œæ|>Œ¥Œæl1Œæ+l2Œæ[2]+l3Œæ[3],|Œæ|‚â§Œ¥Œæ(16) Remark4.Ifùúâ>0Œæ>0,limùúâ‚Üíùõø+ùúâùúë=limùúâ‚Üíùõø+ùúâùúâ[1‚àíùëù1]=ùõø1‚àíùëù1ùúâlimŒæ‚ÜíŒ¥Œæ+œÜ=limŒæ‚ÜíŒ¥Œæ+Œæ[1‚àíp1]=Œ¥Œæ1‚àíp1,limùúâ‚Üíùõø‚àíùúâùúë=ùëô1ùõøùúâ+ùëô2ùõø2ùúâ+ùëô3ùõø3ùúâ=ùõø1‚àíùëù1ùúâlimŒæ‚ÜíŒ¥Œæ‚àíœÜ=l1Œ¥Œæ+l2Œ¥Œæ2+l3Œ¥Œæ3=Œ¥Œæ1‚àíp1, thenùúë(ùë°)œÜ(t)is continuous atùúâ=ùõøùúâŒæ=Œ¥Œæ. Whenùúâ<0Œæ<0,limùúâ‚Üí‚àíùõø+ùúâùúë=limùúâ‚Üí‚àíùõø+ùúâùúâ[1‚àíùëù1]=‚àíùõø1‚àíùëù1ùúâ,limùúâ‚Üí‚àíùõø‚àíùúâùúë==‚àíùëô1ùõøùúâ‚àíùëô2ùõø2ùúâ‚àíùëô3ùëí3ùúâ=‚àíùõø1‚àíùëù1ùúâlimŒæ‚Üí‚àíŒ¥Œæ+œÜ=limŒæ‚Üí‚àíŒ¥Œæ+Œæ[1‚àíp1]=‚àíŒ¥Œæ1‚àíp1,limŒæ‚Üí‚àíŒ¥Œæ‚àíœÜ==‚àíl1Œ¥Œæ‚àíl2Œ¥Œæ2‚àíl3eŒæ3=‚àíŒ¥Œæ1‚àíp1; therefore,ùúë(ùë°)œÜ(t)is continuous atùúâ=‚àíùõøùúâŒæ=‚àíŒ¥Œæ. Taken together, the functionùúë(ùë°)œÜ(t)is continuous, which solves the problem of control input jumping caused by discontinuity at segmentation points in traditional functions. According Equation (16),we haveùúëÀô(ùë°)={(1‚àíùëù1)ùúâ[‚àíùëù1],|ùúâ|>ùõøùúâ(ùëô1+2ùëô2ùúâ[1]+3ùëô3ùúâ[2]),|ùúâ|‚â§ùõøùúâœÜÀô(t)=(1‚àíp1)Œæ[‚àíp1],|Œæ|>Œ¥Œæ(l1+2l2Œæ[1]+3l3Œæ[2]),|Œæ|‚â§Œ¥Œæ(17) Remark5.To confirm the PreTC characteristic of TTEor within SMS, we assume that TTEor reaches the SMS (designated asùëÜ=0S=0) and set the convergence times asùëáùë†1=0.3ùë†,0.5ùë†,1ùë†,1.5ùë†,2ùë†Ts1=0.3s,0.5s,1s,1.5s,2s. The simulation is shown inFigure 2. It shows that the actual convergence times are persistently shorter than the PDT, as the TTEor grasps the SMS. This implies that the TTEor situated on SMS (15) has the property of PDT convergence.Figure 2.TTEor for different predefined convergence times. Step 2:Design Compensator Based on ELM Derive the SMSùëÜS, thenùëÜÀô(ùë°)=ùúã(ùëè1(1+ùëù1)ùúâùëù1(ùë°)+ùëé1ùúëÀô(ùë°))ùëí(ùë°)2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àö+ùëíÀô(ùë°)=ùúî‚àóùëü(ùë°)‚àíùúíùëñùëû(ùë°)‚àíùëî(ùë°)‚àíùëë(ùë°)+ùúã(ùëè1(1+ùëù1)ùúâùëù1(ùë°)+ùëé1ùúëÀô(ùë°))ùëí(ùë°)2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àöSÀô(t)=œÄ(b1(1+p1)Œæp1(t)+a1œÜÀô(t))e(t)2p1Ts1a1b1+eÀô(t)=œâr‚àó(t)‚àíœáiq(t)‚àíg(t)‚àíd(t)+œÄ(b1(1+p1)Œæp1(t)+a1œÜÀô(t))e(t)2p1Ts1a1b1(18) Letùê∫=ùëî+ùëëG=g+drepresent the encapsulated component, which can be estimated with ELM [26]. The ELM is regarded as single hidden-layer feed-forward neural networks (SLFNNs), and the structural of SLFNs is shown inFigure 3. Figure 3.Diagram of SLFN structure. The input of the SLFN isùíõ=[ùëí,ùúâ]ùõµz=[e,Œæ]Œ§,ùëÅÃÉNÀúis the nodes, and‚Ñé(ùíô)h(x)is the activation function. Through continuous learning and training, it is possible to obtain the optimal weight vectorùíò‚àów*of SLFN in order to approximate the encapsulated componentùê∫G.ùê∫=‚àëùëñ=1ùëÅÀúùíò‚àóùëñ‚Ñéùëñ(ùíÑùëñùíõ,ùë£ùëñ)=ùíò‚àóùíâ(ùíõ)G=‚àëi=1NÀúwi*hi(ciz,vi)=w*h(z)(19)whereùíÑùëñ=[ùëêùëñ1,ùëêùëñ2,‚Ä¶ùëêùëñùëõ]ùõµ‚ààùëÖùëõci=[ci1,ci2,‚Ä¶cin]Œ§‚ààRnis the internal connection weight to be designed,ùë£ùëñviis the node threshold to be designed, andùíòùëñ=[ùë§ùëñ1,ùë§ùëñ2,‚Ä¶ùë§ùëñùëö]ùõµ‚ààùëÖùëöwi=[wi1,wi2,‚Ä¶wim]Œ§‚ààRmis the external connection weight. SLFNs need to learn and adjust their weight vectorsùíÑùëñ=[ùëêùëñ1,ùëêùëñ2,‚Ä¶ùëêùëñùëõ]ùõµci=[ci1,ci2,‚Ä¶cin]Œ§,ùíòùëñ=[ùë§ùëñ1,ùë§ùëñ2,‚Ä¶ùë§ùëñùëö]ùõµwi=[wi1,wi2,‚Ä¶wim]Œ§, and bias valuesùë£ùëñvi, which can result in a large amount of computation and slow computation speed. To make SLFNs easier to compute, the ELM is proposed by Huang G B [27]. In the ELM, the connection weightsùíÑùëñciand the node thresholdsùë£ùëñvihave been arbitrarily created.ùíòùëñwiis the outer weight vector to be recognized.ùíâ(ùíõ)=(‚Ñé1(ùíÑ1ùíõ,ùë£1),‚Ä¶,‚ÑéùëÅÀú(ùíÑùëÅÀúùíõ,ùë£ùëÅÀú))ùõµh(z)=(h1(c1z,v1),‚Ä¶,hNÀú(cNÀúz,vNÀú))Œ§. The estimation ofùíò‚àów*is obtained indirectly in practical engineering applications and cannot be approximated asùê∫Gprecisely. The value ofùê∫ÃÇG^is calculated asùê∫ÃÇ=ùíâ(ùíõ)ùíòÃÇ=‚àëùëñ=1ùëÅÀúùíòÃÇùëñ‚Ñéùëñ(ùíÑùíäùíõ,ùë£ùëñ)G^=h(z)w^=‚àëi=1NÀúw^ihi(ciz,vi)(20) In the above formula, theùíòÃÇùëñw^isignifies an estimation of the optimal valueùíò‚àóùëñwi*, and the designed adaptive law is as follows:ùíòÃÇÀô=‚àíùõæùíâùëÜw^Àô=‚àíŒ≥hS(21) Step 3:Controller design. We designed the following resilient controller:ùëñ‚àóùëû=ùëñùëé+ùëñùëèiq*=ia+ib(22)whereùëñùëé=1ùúí(ùúî‚àóùëü‚àíùê∫ÃÇ+ùúã(ùëé1ùúëÀô(ùë°)+ùëè1(1+ùëù1)ùúâùëù1)2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àöùëí)ùëñùëè=ùúãùúíùëù2ùëáùë†2ùëé2ùëè2‚àí‚àí‚àí‚àí‚àö(ùëé2ùëÜ1‚àíùëù2+ùëè2ùëÜ1+ùëù2)ia=1œáœâr‚àó‚àíG^+œÄ(a1œÜÀô(t)+b1(1+p1)Œæp1)2p1Ts1a1b1eib=œÄœáp2Ts2a2b2(a2S1‚àíp2+b2S1+p2)(23)where constantùëéùëñ>0,ùëèùëñ>0,0<ùëùùëñ<1ai>0,bi>0,0<pi<1. Remark6.The controller proposed in this paper exclusively utilizes the angular velocity tracking error information of the PMSM, without relying on any physical layer model information. This controller exhibits excellent universality, as it does not require explicit knowledge of the PMSM‚Äôs detailed mechanical model or external disturbances.",
            "4.2. Stability Analysis": "Theorem2.For arbitrary PDTùëáùë†1>0,ùëáùë†2>0Ts1>0,Ts2>0, when the q-axis current reference inputùëñ‚àóùëûiq*of the PMSM (9) in the physical layer is (22), the angular velocity TTEor of the PMSM will converge to the SMS (15) within the PDTùëáùë†2Ts2, and the upper-bounded angular velocity TTEor‚Äôs convergence time isùëáùë†=ùëáùë†1+ùëáùë†2Ts=Ts1+Ts2. Proof.Taking the Lyapunov candidate asùúé1=0.5ùëÜ2œÉ1=0.5S2, we deriveùë°tto obtainùúéÀô1=ùëÜùëÜÀô=ùëÜ(ùúã(ùëé1ùúëÀô(ùë°)+ùëè1(1+ùëù1)ùúâùëù1)ùëí2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àö+ùëíÀô)=ùëÜ(ùúî‚àóùëü‚àíùúíùëñùëû‚àíùê∫+ùúã(ùëé1ùúëÀô(ùë°)+ùëè1(1+ùëù1)ùúâùëù1)ùëí2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àö)œÉÀô1=SSÀô=SœÄ(a1œÜÀô(t)+b1(1+p1)Œæp1)e2p1Ts1a1b1+eÀô=Sœâr‚àó‚àíœáiq‚àíG+œÄ(a1œÜÀô(t)+b1(1+p1)Œæp1)e2p1Ts1a1b1(24)NotingùíòÀú=ùíò‚àó‚àíùíòÃÇwÀú=w*‚àíw^,ùê∫Àú=ùê∫‚àíùê∫ÃÇGÀú=G‚àíG^, construct the Lyapunov function asùúé2=ùúé1+0.5ùõæ‚àí1ùë°ùëü(ùíòÀúùõµùíòÀú)œÉ2=œÉ1+0.5Œ≥‚àí1tr(wÀúŒ§wÀú), thenùúéÀô2=‚àíùúã(ùëéÃ≤ùúé1‚àíùëùÃ≤1+ùëèÃ≤ùúé1+ùëùÃ≤1)2ùëùÃ≤ùëáùë†2ùëéÃ≤ùëèÃ≤‚àí‚àí‚àö‚àíùëÜùê∫Àú‚àíùë°ùëü(ùíòÀúùõµùíòÃÇÀô)ùõæ=ùúã(ùëéÃ≤ùúé1‚àíùëùÃ≤1+ùëèÃ≤ùúé1+ùëùÃ≤1)‚àí2ùëùÃ≤ùëáùë†2ùëéÃ≤ùëèÃ≤‚àí‚àí‚àö‚àíùë°ùëü(ùê∞Àúùõµ(ùõæùíâùëÜ+ùíòÃÇÀô))ùõæœÉÀô2=‚àíœÄ(a¬ØœÉ11‚àíp¬Ø+b¬ØœÉ11+p¬Ø)2p¬ØTs2a¬Øb¬Ø‚àíSGÀú‚àítr(wÀúŒ§w^Àô)Œ≥=œÄ(a¬ØœÉ11‚àíp¬Ø+b¬ØœÉ11+p¬Ø)‚àí2p¬ØTs2a¬Øb¬Ø‚àítr(wÀúŒ§(Œ≥hS+w^Àô))Œ≥(25)whereùëéÃ≤=ùëé221‚àí0.5ùëù2,ùëèÃ≤=ùëè221+0.5ùëù2,ùëùÃ≤=0.5ùëù2a¬Ø=a221‚àí0.5p2,b¬Ø=b221+0.5p2,p¬Ø=0.5p2. By substituting Equation (22) into Equation (25), we obtainùúéÀô2=‚àíùúã(ùëéÃ≤ùëâ1‚àíùëùÃ≤1+ùëèÃ≤ùëâ1+ùëùÃ≤1)2ùëùÃ≤ùëáùë†2ùëéÃ≤ùëèÃ≤‚àí‚àí‚àöœÉÀô2=‚àíœÄ(a¬ØV11‚àíp¬Ø+b¬ØV11+p¬Ø)2p¬ØTs2a¬Øb¬ØAccording to Theorem 1, the SMSùëÜ(ùë°)S(t)will converge in PDTùëáùë†2Ts2, and the state variables will converge to the SMS in PDTùëáùë†2Ts2.When the SMS satisfiesùëÜ(ùë°)=0S(t)=0, thenùëí(ùë°)=‚àíùúã2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àö(ùëé1ùúë(ùë°)+ùëè1ùúâ1+ùëù1(ùë°))e(t)=‚àíœÄ2p1Ts1a1b1(a1œÜ(t)+b1Œæ1+p1(t))(26)Define the Lyapunov candidate asùúé(ùë°)=0.5ùúâ2œÉ(t)=0.5Œæ2, then obtain the following:ùúéÀô(ùë°)=ùúâ(ùë°)ùúâÀô(ùë°)=‚àíùúã(ùëè1ùúâ2+ùëù1(ùë°)+ùëé1ùúâ(ùë°)ùúëÀô(ùë°))2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àö=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉ‚àíùúã(ùëéÃ≤1ùúé1‚àíùëù12(ùë°)+ùëèÃ≤1ùúé1+ùëù12(ùë°))2ùëùÃ≤1ùëáùë†1ùëéÃ≤1ùëèÃ≤1‚àí‚àí‚àí‚àí‚àö‚àíùúã(ùëé1ùúâ((ùëô1+2ùëô2ùúâ[1]+3ùëô3ùúâ[2]))+ùëè1ùúâ2+ùëù1(ùë°))2ùëù1ùëáùë†1ùëé1ùëè1‚àí‚àí‚àí‚àí‚àö|ùúâ|>ùõøùúâ|ùúâ|‚â§ùõøùúâœÉÀô(t)=Œæ(t)ŒæÀô(t)=‚àíœÄ(b1Œæ2+p1(t)+a1Œæ(t)œÜÀô(t))2p1Ts1a1b1=‚àíœÄ(a¬Ø1œÉ1‚àíp12(t)+b¬Ø1œÉ1+p12(t))2p¬Ø1Ts1a¬Ø1b¬Ø1|Œæ|>Œ¥Œæ‚àíœÄ(a1Œæ((l1+2l2Œæ[1]+3l3Œæ[2]))+b1Œæ2+p1(t))2p1Ts1a1b1|Œæ|‚â§Œ¥Œæ(27)whereùëùÃ≤1=ùëù12,ùëéÃ≤1=ùëé121‚àíùëù12,ùëèÃ≤1=ùëè121+ùëù12p¬Ø1=p12,a¬Ø1=a121‚àíp12,b¬Ø1=b121+p12.According to Theorem 1, it can be concluded that when|ùúâ(ùë°)|>ùõøùúâ|Œæ(t)|>Œ¥Œæ,ùúâ(ùë°)Œæ(t)will converge within the PDTùëáùë†1Ts1. From Equation (26), ifùúâ(ùë°)=0Œæ(t)=0, thenùëí(ùúè)‚â°0e(œÑ)‚â°0, so if the SMS(15)satisfiesùëÜ=0S=0, then the TTEorùëí(ùë°)e(t)will converge at PDTùëáùë†1Ts1. When|ùúâ|‚â§ùõøùúâ|Œæ|‚â§Œ¥Œæ,ùúâ(ùë°)Œæ(t)will asymptotically converge stably to zero.Combining the above analyses, it verified that the angular velocity trajectory tracking errorùíÜ(ùë°)e(t)can converge to zero within the PDTùëáùë†=ùëáùë†1+ùëáùë†2Ts=Ts1+Ts2. ‚ñ° Remark7.The motion trajectory of SMC is mainly divided into two phases: Phase (1) is the phase where state variables at any initial position converge to the SMS. This phase ensures that the TTEor converges toward the SMS (rather than the SMS itself converging to zero), while the state variables themselves do not converge to zero. Phase (2) is the phase where the state variables on the SMS converge to the equilibrium point (zero). In this phase, the state variables essentially achieve convergence to zero. Compared with existing finite-time/fixed-time SMCs, the latter can only ensure that state variables converge to the SMS within finite/fixed time in the first phase, but fail to guarantee that the state variables in the second phase also converge within a finite/fixed time. Therefore, traditional finite-time/fixed-time SMC is not essentially finite-time convergent in nature. In addition, these control algorithms do not possess the characteristic of PreTC. The algorithm proposed in this paper achieves essential PreTC.",
            "5. Validation Analysis": "5.1. Simulation EnvironmentTo verify the effectiveness of the predefined-time sliding-mode adaptive resilient control (PTSMAC) strategy designed in this paper, numerical simulations were performed via the MATLAB 2018 platform, utilizing the parameters of the PMSM listed inTable 2.Table 2.Parameters of PMSM.The q-axis current inputùëñ‚àóùëûiq*of the PMSM angular velocity dynamics Equation (9) is the output value of the PTSMAC, and voltage inputs of Equation (1) are the PI controllers with respect to the current tracking error.‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùë¢ùëë=ùëòùëëùëù(0‚àíùëñùëë)+ùëòùëëùëñ‚à´ùë°0(0‚àíùëñùëë)ùëëùúèùë¢ùëû=ùëòùëûùëù(ùëñ‚àóùëû‚àíùëñùëû)+ùëòùëûùëñ‚à´ùë°0(ùëñ‚àóùëû‚àíùëñùëû)ùëëùúèud=kdp(0‚àíid)+kdi‚à´0t(0‚àíid)dœÑuq=kqp(iq*‚àíiq)+kqi‚à´0t(iq*‚àíiq)dœÑ(28)whereùëñùëûiqdenotes the output of q-axis voltage equation. There are four ELM nodes, parametersùíÑùëñ,ùë£ùëñci,viare randomly generated, and the desired angular velocity isùúî‚àóùëü=100œâr‚àó=100rad/s. The motor is initiated with a 50 N¬∑m load, followed by a sudden impact load of 50 N¬∑m within 0.2 s, before returning to the initial 50 N m load thereafter. Taking into accountŒîùúíŒîœá,ŒîùúÇŒîŒ∑, andŒîùõæŒîŒ≥as perturbations of the system parameters, the control parameters are set asùëé1=1.01,ùëè1=11.01;ùëù1=0.2,ùëáùë†1=0.01,ùëé2=1.10,ùëè2=10.01,ùëù2=0.3,ùõæ=0.01,ùëòùëëùëù=10,ùëòùëëùëñ=15,ùëòùëûùëù=11,ùëòùëûùëñ=10a1=1.01,b1=11.01;p1=0.2,Ts1=0.01,a2=1.10,b2=10.01,p2=0.3,Œ≥=0.01,kdp=10,kdi=15,kqp=11,kqi=10 5.2. A Comparative Simulation Under Different PdtAssuming that the CPS is subject to external malicious cyber-attacks, denoted asùúÖDos(ùë°)=0.001(3+e0.1ùë°),ùúÜFDI(ùë°)=0.01cos2(3ùë°)Œ∫Dos(t)=0.001(3+e0.1t),ŒªFDI(t)=0.01cos2(3t)The load of the PMSM isùëáùêø={10050ùë°=0.2ùë°‚â†0.2TL=100t=0.250t‚â†0.2. To verify the convergence time of the TTEor of the closed-loop CPS, which can be set in advance, the convergence time of the controller is set asùëáùë†2=0.03ùë†,0.05ùë†,0.1ùë†,0.2ùë†Ts2=0.03s,0.05s,0.1s,0.2s, respectively.Figure 4andFigure 5depict the efficacy of the novel sliding-mode adaptive controller formulated in this study, demonstrating its robustness against network attacks and parameter disturbances. Specifically,Figure 5shows that the TTor for the PMSM in CPS is capable of converging to zero within a predetermined timeframe, despite such adversities. Moreover, convergence accuracy can attain a value of7√ó10‚àí37√ó10‚àí3. The smaller the predefined convergence time, the lesser the oscillation of the PMSM angular velocity tracking error, and the greater the convergence precision. This confirms the efficiency of the controller proposed in this paper, indicating that it can effectively mitigate external cyber-attacks. Additionally, by adjusting the convergence time in the controller, the PMSM‚Äôs TTEor in the CPS can converge to zero within the PDT.Figure 6portrays the tracking effect between the output current (ùëñùëûiq) of the PMSM‚Äôs voltage equation and the control input current (ùëñ‚àóùëûiq*) of the PMSM‚Äôs mechanical motion equation.Figure 4.Angular velocity tracking for CPS.Figure 5.Angular velocity tracking error for CPS.Figure 6.PMSM current tracking schematic.Figure 6illustrates that at the initial stage of control, the control input current in the PMSM‚Äôs mechanical motion equation exhibits significant and erratic oscillations, with the maximum input current reaching1.5√ó1041.5√ó104A. Subsequently, the control input current decreases rapidly. There is a deviation between the PMSM‚Äôs output currentùëñùëûiqand the reference currentùëñ‚àóùëûiq*, and the tracking accuracy can be improved by adjusting the control gain of the PI controller based on the current tracking error. 5.3. Anti-Interference Robust AnalysisTo verify the anti-interference ability of the novel controller, we assume the malicious cyber-attacks isùúÖDos(ùë°)=0.001(3+e0.1ùë°),ùúÜFDI(ùë°)=0.01cos2(3ùë°)Œ∫Dos(t)=0.001(3+e0.1t),ŒªFDI(t)=0.01cos2(3t)and the time-varying load of the PMSM isùëáùêø=50e0.5ùë°TL=50e0.5t. Under the same control parameters, the simulation results are shown inFigure 7,Figure 8andFigure 9.Figure 7.Angular velocity tracking for CPS.Figure 8.Angular velocity tracking error for CPS.Figure 9.PMSM current tracking schematic.The simulation results demonstrate that when the load of the PMSM increases over time, the TTEor of the PMSM-based CPS can still converge to zero with high accuracy within the PDT. This verifies that the proposed algorithm exhibits strong anti-interference capability. 5.4. Performance Comparison and Simulation Analysis of Different ControllersConsidering the presence of unknown disturbances in the physical model of CPS, the following extended state observer (ESO) can be chosen to estimateùëë(ùë°)d(t),‚éß‚é©‚é®ÓÄÉÓÄÉùúê=ùëß1‚àíùúîùëü,ùëßÀô1=ùëß2+ùëîÃÇ+ùúíùëñ‚àóùëû‚àíùõΩ1ùúêùëßÀô2=‚àíùõΩ2tanh(ùõΩ3ùúÉ),ùëîÃÇÀô=‚àíùúÇùëîùë†œÖ=z1‚àíœâr,zÀô1=z2+g^+œáiq*‚àíŒ≤1œÖzÀô2=‚àíŒ≤2tanh(Œ≤3Œ∏),g^Àô=‚àíŒ∑gs(29)whereùëß2z2is the estimation ofùëë(ùë°)d(t)andùëîÃÇg^is the estimation ofùëîg; the following controller-based ESO and the PI controller are chosen for comparative simulation.(1)Super-twisting SMCler based on ESO (STSMC+ESO) [28].‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉùëñ‚àóùëû=1ùúí[ùëêùëíÀô‚àíùëß2+ùúÄ|ùë†|1/2sign(ùë†)+ùëò‚à´ùë°0sign(ùë†)dùúè]ùë†=ùëíÀô(ùë°)+ùëêùëí(ùë°)iq*=1œá[ceÀô‚àíz2+Œµ|s|1/2sign(s)+k‚à´0tsign(s)dœÑ]s=eÀô(t)+ce(t)(30)(2)Backstepping L2gain controller based on ESO (BSLGC+ESO) [29].‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉùëí=ùúÉ‚àíùúÉ‚àó,ùëéùëô=‚àíùëòùëô1ùëí‚àíùúÉÀô‚àó,ùúÇùëô=ùúîùëü‚àíùëéùëôùëñ‚àóùëû=‚àí1ùúí[ùëòùëô2ùúÇùëô+ùëß2‚àíùõæ2ùëôùëß222ùúÇùëô‚àíùëéÀôùëô]e=Œ∏‚àíŒ∏*,al=‚àíkl1e‚àíŒ∏Àô*,Œ∑l=œâr‚àíaliq*=‚àí1œá[kl2Œ∑l+z2‚àíŒ≥l2z222Œ∑l‚àíaÀôl](31)(3)Sliding-mode speed controller based on ESO (SMSC+ESO) [25].‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉùëí(ùë°)=ùúÉ‚àó‚àíùúÉ,ùë†=ùëíÀô(ùë°)+ùëêùëí(ùë°),ùëîÃÇÀô=‚àíùúÇùëîùë†ùúÇ>0ùëñ‚àóùëû=1ùúí[ùúÉ¬®‚àó‚àíùëîÃÇ‚àíùëß2+ùúÄ|ùúîùëü|ùëésgn(ùë†)+ùëò|ùë†|ùëèsgn(|ùë†|‚àí1)ùë†+ùëêùëíÀô]e(t)=Œ∏*‚àíŒ∏,s=eÀô(t)+ce(t),g^Àô=‚àíŒ∑gsŒ∑>0iq*=1œá[Œ∏¬®*‚àíg^‚àíz2+Œµ|œâr|asgn(s)+k|s|bsgn(|s|‚àí1)s+ceÀô](32)(4)PI controller.ùëñ‚àóùëû=ùëòùëùùëí+ùëòùëñ‚à´ùúè0ùëíùëëùúèiq*=kpe+ki‚à´0œÑedœÑ(33)The load of the PMSM isùëáùêø={10050ùë°=0.2ùë°‚â†0.2TL=100t=0.250t‚â†0.2. Three cyber-attack environments are selected for numerical simulation, respectively, as follows:Case 1: Weak cyber-attackExternal malicious cyber-attacks can be described asùúÖDos(ùë°)={0.001(3+ùëí0.1ùë°)1ùë°‚â•0.010‚â§ùë°<0.01,ùúÜFDI(ùë°)={0.01cos2(3ùë°)0ùë°‚â•0.010‚â§ùë°<0.01Œ∫Dos(t)=0.001(3+e0.1t)t‚â•0.0110‚â§t<0.01,ŒªFDI(t)=0.01cos2(3t)t‚â•0.0100‚â§t<0.01The results depicted inFigure 10andFigure 11demonstrate the remarkable efficacy of the PTSMACler proposed in this paper when the CPS is subjected to a weak cyber-attack.Figure 10.Angular velocity tracking for CPS.Figure 11.Angular velocity tracking error for CPS.It outperforms the other four considered controllers by minimizing the amplitude of the PMSM‚Äôs angular velocity tracking error, exhibiting a smoother convergence trajectory, achieving a shorter convergence time, and attaining higher convergence accuracy. Trajectory tracking based on the PI controller exhibits a significant steady-state error. Under the STSMC+ESO and BSLGC+ESO algorithms, the angular velocity tracking error converges with oscillations and requires a longer convergence time. Meanwhile, although the SMSC+ESO algorithm displays comparatively fewer oscillations, it exhibits both a higher maximum tracking error and a larger steady-state tracking error. On the other hand,Figure 12illustrates that under PTSMAC and SMSC+ESO control, the control input currents of the mechanical dynamics equations exhibit minimal oscillations; however, the current amplitude under PTSMAC is significantly smaller than that under SMSC+ESO control. Conversely, under STSMC+ESO and BSLGC+ESO controls, the control input currents exhibit obvious oscillatory behavior with larger amplitudes. These simulation results unequivocally demonstrate the exceptional performance of the proposed controller against weak cyber-attacks.Figure 12.PMSM current-tracking schematic.Case 2: High-powered cyber-attackHigh-powered external malicious cyber-attacks can be described asùúÖDos(ùë°)={10(3+e0.1ùë°)1ùë°‚â•0.010‚â§ùë°<0.01,ùúÜFDI(ùë°)={0.1cos2(3ùë°)0ùë°‚â•0.010‚â§ùë°<0.01Œ∫Dos(t)=10(3+e0.1t)t‚â•0.0110‚â§t<0.01,ŒªFDI(t)=0.1cos2(3t)t‚â•0.0100‚â§t<0.01The results depicted inFigure 13andFigure 14demonstrate the remarkable resilience of the PTSMAC, as it successfully drives the angular velocity of the PMSM to zero within a mere 0.05 s, even when subjected to potent multiplicative and additive cyber-attacks. Notably, this convergence is achieved with an impeccably smooth trajectory. These findings unequivocally establish the efficacy of the PTSMAC under high-intensity cyber-attacks. Among its counterparts, the PI, STSMC+ESO, and BSLGC+ESO controllers exhibit rapid angular velocity tracking convergence with near-zero convergence times. However, it is worth mentioning that the SMSC+ESO controller lags significantly in performance, displaying larger convergence errors and more pronounced oscillations in its trajectory. Furthermore, the SMSC+ESO controller also exhibits the longest convergence time among all controllers.Figure 15visually presents the control input current for each controller employed in this study. It is noteworthy that neither the PTSMAC nor the SMSC+ESO controller induces any oscillations in their respective input currents; however, when the PTSMAC is utilized, the current amplitude is minimized to the smallest possible value. Conversely, under the control of STSMC+ESO and PI, the input current signal exhibits a sharp fluctuation pattern, whereas BSLGC+ESO manages to keep such oscillations relatively subdued.Figure 13.Angular velocity tracking for CPS.Figure 14.Angular velocity tracking error for CPS.Figure 15.PMSM current tracking schematic.Case 3: High-powered cyber-attackOther high-powered external malicious cyber-attacks can be described as follows:ùúÖDos(ùë°)={0.10(3+e0.1ùë°)1ùë°‚â•0.010‚â§ùë°<0.01,ùúÜFDI(ùë°)={1cos2(3ùë°)0ùë°‚â•0.010‚â§ùë°<0.01Œ∫Dos(t)=0.10(3+e0.1t)t‚â•0.0110‚â§t<0.01,ŒªFDI(t)=1cos2(3t)t‚â•0.0100‚â§t<0.01The simulation results depicted in inFigure 16,Figure 17andFigure 18demonstratethat the PMSM‚Äôs angular velocity tracking error remains consistently smooth and free of chattering across all five controllers. This holds true even when weak multiplicative and strong additive attacks are launched against the network layer of the CPS. Among these controllers, the PI controller exhibits a significant vibration amplitude in the tracking error; the SMSC+ESO controller demonstrates a remarkably prolonged convergence time for the PMSM‚Äôs angular velocity tracking error; while the STSMC+ESO and BSLGC+ESO controllers showcase swift convergence times approaching zero. In stark contrast, the PTSMAC ensures that the PMSM‚Äôs angular velocity tracking error converges in less than 0.05 s, thereby meeting the PDT requirement with utmost efficiency. Notably, among these five controllers, the PTSMAC-based control yields the smallest input current amplitude.Figure 16.Angular velocity tracking for CPS.Figure 17.Angular velocity tracking error for CPS.Figure 18.PMSM current tracking schematic.The convergence time (CT), steady-state error (SSE), and comprehensive energy consumption (CEC) of the five controllers are listed inTable 3. The results demonstrate that the PTSMAC outperforms the other three controllers, particularly in terms of convergence speed, tracking accuracy, and transient performance. Moreover, the convergence time of the proposed controller is user-definable, whereas the convergence times of the other three controllers cannot be fully preconfigured. Consequently, the proposed PTSMAC offers numerous advantages, including superior control performance and flexible convergence time configuration.Table 3.Performance comparison under different controllers. 5.5. Sensitivity Analysis of ELM ParametersTo analyze the influence of the type of activation function and the number of nodes of the neural network on the control effect, the number of nodes is set to four, and the activation functions are set to the sigmoid function (Sigmoid):ùú±ùë§(ùùÉ)=(1+exp(‚àíùëΩùë§ùùÉT+ùëèùë§))‚àí1Œ¶w(Œæ)=(1+exp(‚àíVwŒæT+bw))‚àí1, hyperbolic tangent function (Tanh):ùú±ùë§(ùùÉ)=tanh(ùëΩùë§ùùÉT+ùëèùë§)Œ¶w(Œæ)=tanh(VwŒæT+bw), Gauss function (Gauss):ùú±ùë§(ùùÉ)=exp(‚àí||ùùÉ‚àíùëΩùë§||2/ùëè2ùë§)Œ¶w(Œæ)=exp(‚àí||Œæ‚àíVw||2/bw2), and cosine Fourier basis function (Cosine):ùú±ùë§(ùùÉ)=cos(ùëΩùë§ùùÉT+ùëèùë§)Œ¶w(Œæ)=cos(VwŒæT+bw), respectively, with other parameters unchanged. The numerical simulation results are shown inFigure 19andFigure 20. It can be observed from the simulation results that the type of activation function does not affect the control performance of the cyber‚Äìphysical system (CPS).Figure 19.Angular velocity tracking error.Figure 20.PMSM current tracking schematic.Similarly, the activation function is fixed as the sigmoid function, and the number of nodes of the ELM is set to 4, 10, 20, and 30, respectively, for numerical simulations. The simulation results are presented inFigure 21andFigure 22. These results indicate that the number of nodes also has no impact on the control performance of the CPS.Figure 21.Angular velocity tracking error.Figure 22.PMSM current-tracking schematic.",
            "5.1. Simulation Environment": "To verify the effectiveness of the predefined-time sliding-mode adaptive resilient control (PTSMAC) strategy designed in this paper, numerical simulations were performed via the MATLAB 2018 platform, utilizing the parameters of the PMSM listed inTable 2. Table 2.Parameters of PMSM. The q-axis current inputùëñ‚àóùëûiq*of the PMSM angular velocity dynamics Equation (9) is the output value of the PTSMAC, and voltage inputs of Equation (1) are the PI controllers with respect to the current tracking error.‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùë¢ùëë=ùëòùëëùëù(0‚àíùëñùëë)+ùëòùëëùëñ‚à´ùë°0(0‚àíùëñùëë)ùëëùúèùë¢ùëû=ùëòùëûùëù(ùëñ‚àóùëû‚àíùëñùëû)+ùëòùëûùëñ‚à´ùë°0(ùëñ‚àóùëû‚àíùëñùëû)ùëëùúèud=kdp(0‚àíid)+kdi‚à´0t(0‚àíid)dœÑuq=kqp(iq*‚àíiq)+kqi‚à´0t(iq*‚àíiq)dœÑ(28)whereùëñùëûiqdenotes the output of q-axis voltage equation. There are four ELM nodes, parametersùíÑùëñ,ùë£ùëñci,viare randomly generated, and the desired angular velocity isùúî‚àóùëü=100œâr‚àó=100rad/s. The motor is initiated with a 50 N¬∑m load, followed by a sudden impact load of 50 N¬∑m within 0.2 s, before returning to the initial 50 N m load thereafter. Taking into accountŒîùúíŒîœá,ŒîùúÇŒîŒ∑, andŒîùõæŒîŒ≥as perturbations of the system parameters, the control parameters are set asùëé1=1.01,ùëè1=11.01;ùëù1=0.2,ùëáùë†1=0.01,ùëé2=1.10,ùëè2=10.01,ùëù2=0.3,ùõæ=0.01,ùëòùëëùëù=10,ùëòùëëùëñ=15,ùëòùëûùëù=11,ùëòùëûùëñ=10a1=1.01,b1=11.01;p1=0.2,Ts1=0.01,a2=1.10,b2=10.01,p2=0.3,Œ≥=0.01,kdp=10,kdi=15,kqp=11,kqi=10",
            "5.2. A Comparative Simulation Under Different Pdt": "Assuming that the CPS is subject to external malicious cyber-attacks, denoted asùúÖDos(ùë°)=0.001(3+e0.1ùë°),ùúÜFDI(ùë°)=0.01cos2(3ùë°)Œ∫Dos(t)=0.001(3+e0.1t),ŒªFDI(t)=0.01cos2(3t) The load of the PMSM isùëáùêø={10050ùë°=0.2ùë°‚â†0.2TL=100t=0.250t‚â†0.2. To verify the convergence time of the TTEor of the closed-loop CPS, which can be set in advance, the convergence time of the controller is set asùëáùë†2=0.03ùë†,0.05ùë†,0.1ùë†,0.2ùë†Ts2=0.03s,0.05s,0.1s,0.2s, respectively.Figure 4andFigure 5depict the efficacy of the novel sliding-mode adaptive controller formulated in this study, demonstrating its robustness against network attacks and parameter disturbances. Specifically,Figure 5shows that the TTor for the PMSM in CPS is capable of converging to zero within a predetermined timeframe, despite such adversities. Moreover, convergence accuracy can attain a value of7√ó10‚àí37√ó10‚àí3. The smaller the predefined convergence time, the lesser the oscillation of the PMSM angular velocity tracking error, and the greater the convergence precision. This confirms the efficiency of the controller proposed in this paper, indicating that it can effectively mitigate external cyber-attacks. Additionally, by adjusting the convergence time in the controller, the PMSM‚Äôs TTEor in the CPS can converge to zero within the PDT.Figure 6portrays the tracking effect between the output current (ùëñùëûiq) of the PMSM‚Äôs voltage equation and the control input current (ùëñ‚àóùëûiq*) of the PMSM‚Äôs mechanical motion equation. Figure 4.Angular velocity tracking for CPS. Figure 5.Angular velocity tracking error for CPS. Figure 6.PMSM current tracking schematic. Figure 6illustrates that at the initial stage of control, the control input current in the PMSM‚Äôs mechanical motion equation exhibits significant and erratic oscillations, with the maximum input current reaching1.5√ó1041.5√ó104A. Subsequently, the control input current decreases rapidly. There is a deviation between the PMSM‚Äôs output currentùëñùëûiqand the reference currentùëñ‚àóùëûiq*, and the tracking accuracy can be improved by adjusting the control gain of the PI controller based on the current tracking error.",
            "5.3. Anti-Interference Robust Analysis": "To verify the anti-interference ability of the novel controller, we assume the malicious cyber-attacks isùúÖDos(ùë°)=0.001(3+e0.1ùë°),ùúÜFDI(ùë°)=0.01cos2(3ùë°)Œ∫Dos(t)=0.001(3+e0.1t),ŒªFDI(t)=0.01cos2(3t)and the time-varying load of the PMSM isùëáùêø=50e0.5ùë°TL=50e0.5t. Under the same control parameters, the simulation results are shown inFigure 7,Figure 8andFigure 9. Figure 7.Angular velocity tracking for CPS. Figure 8.Angular velocity tracking error for CPS. Figure 9.PMSM current tracking schematic. The simulation results demonstrate that when the load of the PMSM increases over time, the TTEor of the PMSM-based CPS can still converge to zero with high accuracy within the PDT. This verifies that the proposed algorithm exhibits strong anti-interference capability.",
            "5.4. Performance Comparison and Simulation Analysis of Different Controllers": "Considering the presence of unknown disturbances in the physical model of CPS, the following extended state observer (ESO) can be chosen to estimateùëë(ùë°)d(t),‚éß‚é©‚é®ÓÄÉÓÄÉùúê=ùëß1‚àíùúîùëü,ùëßÀô1=ùëß2+ùëîÃÇ+ùúíùëñ‚àóùëû‚àíùõΩ1ùúêùëßÀô2=‚àíùõΩ2tanh(ùõΩ3ùúÉ),ùëîÃÇÀô=‚àíùúÇùëîùë†œÖ=z1‚àíœâr,zÀô1=z2+g^+œáiq*‚àíŒ≤1œÖzÀô2=‚àíŒ≤2tanh(Œ≤3Œ∏),g^Àô=‚àíŒ∑gs(29)whereùëß2z2is the estimation ofùëë(ùë°)d(t)andùëîÃÇg^is the estimation ofùëîg; the following controller-based ESO and the PI controller are chosen for comparative simulation. (1)Super-twisting SMCler based on ESO (STSMC+ESO) [28]. ‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉùëñ‚àóùëû=1ùúí[ùëêùëíÀô‚àíùëß2+ùúÄ|ùë†|1/2sign(ùë†)+ùëò‚à´ùë°0sign(ùë†)dùúè]ùë†=ùëíÀô(ùë°)+ùëêùëí(ùë°)iq*=1œá[ceÀô‚àíz2+Œµ|s|1/2sign(s)+k‚à´0tsign(s)dœÑ]s=eÀô(t)+ce(t)(30) (2)Backstepping L2gain controller based on ESO (BSLGC+ESO) [29]. ‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉùëí=ùúÉ‚àíùúÉ‚àó,ùëéùëô=‚àíùëòùëô1ùëí‚àíùúÉÀô‚àó,ùúÇùëô=ùúîùëü‚àíùëéùëôùëñ‚àóùëû=‚àí1ùúí[ùëòùëô2ùúÇùëô+ùëß2‚àíùõæ2ùëôùëß222ùúÇùëô‚àíùëéÀôùëô]e=Œ∏‚àíŒ∏*,al=‚àíkl1e‚àíŒ∏Àô*,Œ∑l=œâr‚àíaliq*=‚àí1œá[kl2Œ∑l+z2‚àíŒ≥l2z222Œ∑l‚àíaÀôl](31) (3)Sliding-mode speed controller based on ESO (SMSC+ESO) [25]. ‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉùëí(ùë°)=ùúÉ‚àó‚àíùúÉ,ùë†=ùëíÀô(ùë°)+ùëêùëí(ùë°),ùëîÃÇÀô=‚àíùúÇùëîùë†ùúÇ>0ùëñ‚àóùëû=1ùúí[ùúÉ¬®‚àó‚àíùëîÃÇ‚àíùëß2+ùúÄ|ùúîùëü|ùëésgn(ùë†)+ùëò|ùë†|ùëèsgn(|ùë†|‚àí1)ùë†+ùëêùëíÀô]e(t)=Œ∏*‚àíŒ∏,s=eÀô(t)+ce(t),g^Àô=‚àíŒ∑gsŒ∑>0iq*=1œá[Œ∏¬®*‚àíg^‚àíz2+Œµ|œâr|asgn(s)+k|s|bsgn(|s|‚àí1)s+ceÀô](32) (4)PI controller. ùëñ‚àóùëû=ùëòùëùùëí+ùëòùëñ‚à´ùúè0ùëíùëëùúèiq*=kpe+ki‚à´0œÑedœÑ(33) The load of the PMSM isùëáùêø={10050ùë°=0.2ùë°‚â†0.2TL=100t=0.250t‚â†0.2. Three cyber-attack environments are selected for numerical simulation, respectively, as follows: Case 1: Weak cyber-attack External malicious cyber-attacks can be described asùúÖDos(ùë°)={0.001(3+ùëí0.1ùë°)1ùë°‚â•0.010‚â§ùë°<0.01,ùúÜFDI(ùë°)={0.01cos2(3ùë°)0ùë°‚â•0.010‚â§ùë°<0.01Œ∫Dos(t)=0.001(3+e0.1t)t‚â•0.0110‚â§t<0.01,ŒªFDI(t)=0.01cos2(3t)t‚â•0.0100‚â§t<0.01 The results depicted inFigure 10andFigure 11demonstrate the remarkable efficacy of the PTSMACler proposed in this paper when the CPS is subjected to a weak cyber-attack. Figure 10.Angular velocity tracking for CPS. Figure 11.Angular velocity tracking error for CPS. It outperforms the other four considered controllers by minimizing the amplitude of the PMSM‚Äôs angular velocity tracking error, exhibiting a smoother convergence trajectory, achieving a shorter convergence time, and attaining higher convergence accuracy. Trajectory tracking based on the PI controller exhibits a significant steady-state error. Under the STSMC+ESO and BSLGC+ESO algorithms, the angular velocity tracking error converges with oscillations and requires a longer convergence time. Meanwhile, although the SMSC+ESO algorithm displays comparatively fewer oscillations, it exhibits both a higher maximum tracking error and a larger steady-state tracking error. On the other hand,Figure 12illustrates that under PTSMAC and SMSC+ESO control, the control input currents of the mechanical dynamics equations exhibit minimal oscillations; however, the current amplitude under PTSMAC is significantly smaller than that under SMSC+ESO control. Conversely, under STSMC+ESO and BSLGC+ESO controls, the control input currents exhibit obvious oscillatory behavior with larger amplitudes. These simulation results unequivocally demonstrate the exceptional performance of the proposed controller against weak cyber-attacks. Figure 12.PMSM current-tracking schematic. Case 2: High-powered cyber-attack High-powered external malicious cyber-attacks can be described asùúÖDos(ùë°)={10(3+e0.1ùë°)1ùë°‚â•0.010‚â§ùë°<0.01,ùúÜFDI(ùë°)={0.1cos2(3ùë°)0ùë°‚â•0.010‚â§ùë°<0.01Œ∫Dos(t)=10(3+e0.1t)t‚â•0.0110‚â§t<0.01,ŒªFDI(t)=0.1cos2(3t)t‚â•0.0100‚â§t<0.01 The results depicted inFigure 13andFigure 14demonstrate the remarkable resilience of the PTSMAC, as it successfully drives the angular velocity of the PMSM to zero within a mere 0.05 s, even when subjected to potent multiplicative and additive cyber-attacks. Notably, this convergence is achieved with an impeccably smooth trajectory. These findings unequivocally establish the efficacy of the PTSMAC under high-intensity cyber-attacks. Among its counterparts, the PI, STSMC+ESO, and BSLGC+ESO controllers exhibit rapid angular velocity tracking convergence with near-zero convergence times. However, it is worth mentioning that the SMSC+ESO controller lags significantly in performance, displaying larger convergence errors and more pronounced oscillations in its trajectory. Furthermore, the SMSC+ESO controller also exhibits the longest convergence time among all controllers.Figure 15visually presents the control input current for each controller employed in this study. It is noteworthy that neither the PTSMAC nor the SMSC+ESO controller induces any oscillations in their respective input currents; however, when the PTSMAC is utilized, the current amplitude is minimized to the smallest possible value. Conversely, under the control of STSMC+ESO and PI, the input current signal exhibits a sharp fluctuation pattern, whereas BSLGC+ESO manages to keep such oscillations relatively subdued. Figure 13.Angular velocity tracking for CPS. Figure 14.Angular velocity tracking error for CPS. Figure 15.PMSM current tracking schematic. Case 3: High-powered cyber-attack Other high-powered external malicious cyber-attacks can be described as follows:ùúÖDos(ùë°)={0.10(3+e0.1ùë°)1ùë°‚â•0.010‚â§ùë°<0.01,ùúÜFDI(ùë°)={1cos2(3ùë°)0ùë°‚â•0.010‚â§ùë°<0.01Œ∫Dos(t)=0.10(3+e0.1t)t‚â•0.0110‚â§t<0.01,ŒªFDI(t)=1cos2(3t)t‚â•0.0100‚â§t<0.01 The simulation results depicted in inFigure 16,Figure 17andFigure 18demonstratethat the PMSM‚Äôs angular velocity tracking error remains consistently smooth and free of chattering across all five controllers. This holds true even when weak multiplicative and strong additive attacks are launched against the network layer of the CPS. Among these controllers, the PI controller exhibits a significant vibration amplitude in the tracking error; the SMSC+ESO controller demonstrates a remarkably prolonged convergence time for the PMSM‚Äôs angular velocity tracking error; while the STSMC+ESO and BSLGC+ESO controllers showcase swift convergence times approaching zero. In stark contrast, the PTSMAC ensures that the PMSM‚Äôs angular velocity tracking error converges in less than 0.05 s, thereby meeting the PDT requirement with utmost efficiency. Notably, among these five controllers, the PTSMAC-based control yields the smallest input current amplitude. Figure 16.Angular velocity tracking for CPS. Figure 17.Angular velocity tracking error for CPS. Figure 18.PMSM current tracking schematic. The convergence time (CT), steady-state error (SSE), and comprehensive energy consumption (CEC) of the five controllers are listed inTable 3. The results demonstrate that the PTSMAC outperforms the other three controllers, particularly in terms of convergence speed, tracking accuracy, and transient performance. Moreover, the convergence time of the proposed controller is user-definable, whereas the convergence times of the other three controllers cannot be fully preconfigured. Consequently, the proposed PTSMAC offers numerous advantages, including superior control performance and flexible convergence time configuration. Table 3.Performance comparison under different controllers.",
            "5.5. Sensitivity Analysis of ELM Parameters": "To analyze the influence of the type of activation function and the number of nodes of the neural network on the control effect, the number of nodes is set to four, and the activation functions are set to the sigmoid function (Sigmoid):ùú±ùë§(ùùÉ)=(1+exp(‚àíùëΩùë§ùùÉT+ùëèùë§))‚àí1Œ¶w(Œæ)=(1+exp(‚àíVwŒæT+bw))‚àí1, hyperbolic tangent function (Tanh):ùú±ùë§(ùùÉ)=tanh(ùëΩùë§ùùÉT+ùëèùë§)Œ¶w(Œæ)=tanh(VwŒæT+bw), Gauss function (Gauss):ùú±ùë§(ùùÉ)=exp(‚àí||ùùÉ‚àíùëΩùë§||2/ùëè2ùë§)Œ¶w(Œæ)=exp(‚àí||Œæ‚àíVw||2/bw2), and cosine Fourier basis function (Cosine):ùú±ùë§(ùùÉ)=cos(ùëΩùë§ùùÉT+ùëèùë§)Œ¶w(Œæ)=cos(VwŒæT+bw), respectively, with other parameters unchanged. The numerical simulation results are shown inFigure 19andFigure 20. It can be observed from the simulation results that the type of activation function does not affect the control performance of the cyber‚Äìphysical system (CPS). Figure 19.Angular velocity tracking error. Figure 20.PMSM current tracking schematic. Similarly, the activation function is fixed as the sigmoid function, and the number of nodes of the ELM is set to 4, 10, 20, and 30, respectively, for numerical simulations. The simulation results are presented inFigure 21andFigure 22. These results indicate that the number of nodes also has no impact on the control performance of the CPS. Figure 21.Angular velocity tracking error. Figure 22.PMSM current-tracking schematic.",
            "6. Conclusions": "We have investigated the trajectory tracking control problem of CPS based on PMSM, focusing on the impacts of malicious cyber-attacks at the cyber layer, as well as parameter perturbations and external loads at the physical layer. A model-free and disturbance-independent sliding-mode adaptive safety control strategy is proposed, which only requires information about the motor‚Äôs angular velocity tracking error and supports predefined convergence time. Numerical simulations confirm that this safety control strategy can resist malicious cyber-attacks and exhibits strong anti-interference capability. A novel PreTC Lyapunov stability criterion is proposed, and an SMS with predefined convergence time is designed, enriching the theoretical framework of PreTC. The upper bound of the predefined convergence time is not affected by the initial system state or controller parameters, which not only enhances the degree of freedom in the design of time-optimal control algorithms but also renders the convergence time of the closed-loop system controllable. By using an ELM to approximate the aggregated term composed of the PMSM model and external disturbances, a novel sliding-mode adaptive controller is designed for PMSM-based CPS under cyber-attacks. This controller can effectively mitigate the adverse effects of malicious cyber-attacks, system parameter perturbations, and external heavy loads on the CPS, improve the robustness of the PMSM-based CPS, and rapidly achieve high-precision angular velocity tracking‚Äîthus effectively expanding the application scenarios of PMSM-based CPS. Through comparative simulations of four safety control strategies under weak and strong cyber-attacks, it is demonstrated that the proposed algorithm offers multiple advantages, including smaller TTEor, a smoother convergence trajectory, shortened convergence time, and higher convergence accuracy. The next step will involve constructing a physical model to verify the engineering application effect of the proposed algorithm through physical simulation experiments, considering actual constraints and disturbance issues. Additionally, while this paper uses an ELM to approximate the aggregated term, the output weights of the ELM lack convergence, which cannot guarantee the accurate estimation of the aggregated term by the ELM and may increase control energy consumption. In future research, designing an ELM output weight update law with convergence characteristics and completing the corresponding stability theoretical proof will be the key direction for improvement."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/1424-8220/25/23/7380",
        "scraped_at": "2025-12-05 23:55:50"
    },
    {
        "title": "HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition",
        "authors": "byYuchong Long,Wen Sun,Ningxiao Sun,Wenxiao Wang,Chao LiandShan Yin",
        "journal": "Agriculture2025,15(23), 2518;https://doi.org/10.3390/agriculture15232518- 4 Dec 2025",
        "abstract": "Automated pollen recognition is a foundational tool for diverse scientific domains, including paleoclimatology, biodiversity monitoring, and agricultural science. However, conventional methods create a critical data bottleneck, limiting the temporal and spatial resolution of ecological analysis. Existing deep learning models often fail to achieve the requisite localization accuracy for microscopic pollen grains, which are characterized by their minute size, indistinct edges, and complex backgrounds. To overcome this, we introduce HieraEdgeNet, a novel object detection framework. The core principle of our architecture is to explicitly extract and hierarchically fuse multi-scale edge information with deep semantic features. This synergistic approach, combined with a computationally efficient large-kernel operator for fine-grained feature refinement, significantly enhances the model‚Äôs ability to perceive and precisely delineate object boundaries. On a large-scale dataset comprising 44,471 annotated microscopic images containing 342,706 pollen grains from 120 classes, HieraEdgeNet achieves a mean Average Precision of 0.9501 (mAP@0.5) and 0.8444 (mAP@0.5:0.95), substantially outperforming state-of-the-art models such as YOLOv12n and the Transformer-based RT-DETR family in terms of the accuracy‚Äìefficiency trade-off. This work provides a powerful computational tool for generating the high-throughput, high-fidelity data essential for modern ecological research, including tracking phenological shifts, assessing plant biodiversity, and reconstructing paleoenvironments. At the same time, we acknowledge that the current two-dimensional design cannot directly exploit volumetric Z-stack microscopy and that strong domain shifts between training data and real-world deployments may still degrade performance, which we identify as key directions for future work. By also enabling applications in precision agriculture, HieraEdgeNet contributes broadly to advancing ecosystem monitoring and sustainable food security.Keywords:pollen recognition;object detection;edge enhancement;multi-scale feature fusion;deep learning",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Palynology, the scientific study of pollen grains and other micropaleontological entities, is of critical importance across diverse scientific domains, including paleoclimate reconstruction [1], biodiversity monitoring [2,3], and allergenic disease research [4]. Crucially, it is also a vital tool for modern agricultural science. In this domain, the precise identification and quantification of pollen are fundamental to crop phenotyping, optimizing pollination ecology for yield enhancement, and monitoring the spread of pollen-borne pathogens. The type and concentration of airborne pollen are key indicators for assessing environmental quality, forecasting allergy seasons, and monitoring agricultural productivity [5,6]. However, conventional pollen analysis relies heavily on manual microscopic enumeration by expert palynologists. This process is not only time-consuming and labor-intensive but is also prone to subjective error, severely limiting the throughput and timeliness of data acquisition [7]. In typical monitoring setups, a trained palynologist can only process on the order of tens of slides per day, which is orders of magnitude below what is required for high-temporal-resolution phenological monitoring and large-scale biodiversity assessments. This limitation forms a critical data bottleneck, hindering our ability to monitor rapid ecological processes such as phenological shifts in response to climate change, the spread of invasive species, or real-time changes in plant biodiversity. Consequently, the development of automated, high-throughput, and high-precision pollen identification technologies has become a pressing demand in the field. The advent of computer vision and deep learning has presented new opportunities for automated pollen identification [8]. Object detection, a fundamental yet critical task in computer vision, aims to identify and localize instances of specific classes within an image [9]. When applied to the analysis of microscopic images, particularly for pollen grain recognition, this task presents unique challenges. These challenges stem from the minute size of pollen grains, their embedding within complex backgrounds composed of dust and debris, their morphological diversity and tendency to aggregate, and their three-dimensional nature introduced by multi-focal plane imaging, which results in disparate appearances and sharpness for the same particle across different focal planes [7,10]. Early automated approaches primarily depended on traditional image processing techniques, such as color similarity transforms [11], active contour models [12], or feature engineering based on shape and texture [13]. Although these methods achieved some success on specific datasets, they often required tedious manual feature design, and their robustness and generalization capabilities were often suboptimal when faced with real-world samples characterized by complex backgrounds, pollen aggregation, or morphological variability [7,10]. Deep learning, particularly the emergence of Convolutional Neural Networks (CNNs), offered a powerful new paradigm to address these issues [8,9]. By automatically learning hierarchical features from data, CNN-based object detection models have achieved revolutionary breakthroughs. In recent years, one-stage detectors, striking an exceptional balance between speed and accuracy, have rapidly become the mainstream approach for real-time object detection [14]. These detectors frame the task as an end-to-end regression problem, thereby enabling real-time performance. In their architectural evolution, multi-scale feature representation is crucial for detecting objects of varying sizes. The Feature Pyramid Network (FPN), which constructs a feature pyramid via top-down pathways and lateral connections to effectively merge high-level semantic information with low-level spatial details, has become an indispensable component of modern detectors [15]. Subsequent architectures combine these pyramid features with post-processing components like Non-Maximum Suppression (NMS) or Transformer-based decoders [16,17] for final detection. The field of pollen identification and classification has also seen widespread application of these advances, as demonstrated by Kubera et al. [18], who applied the YOLOv5 model to detect three pollen types with high morphological similarity, demonstrating performance superior to other detectors like Faster R-CNN and RetinaNet. Tan et al. [19] also developed the PollenDetect system based on YOLOv5 for identifying pollen activity. These studies demonstrate the significant potential of deep learning models, especially the YOLO series, in automated palynology. However, the direct application of these general-purpose models to pollen identification is still confronted with distinct challenges. The minute size of pollen grains means they occupy a limited number of effective pixels in microscopic images [7]. Consequently, relying solely on high-level semantic information is insufficient for high-precision recognition, rendering the effective extraction and utilization of boundary information paramount. Although some works have explored edge detection as an auxiliary task [20], how to systematically and synergistically fuse these low-level edge cues with high-level semantic features within the network remains a critical and unresolved issue. To this end, we design and propose HieraEdgeNet, a multi-scale, edge-enhanced framework specifically engineered for high-precision automated pollen identification. Our architecture builds upon the foundational concepts previously discussed but introduces deep customizations and innovations to address the specific challenges of pollen analysis. The core innovation of HieraEdgeNet lies in three synergistic modules. First, we designed the Hierarchical Edge Module (HEM), which explicitly extracts a multi-scale edge feature pyramid in the early stages of the network, corresponding to the semantic hierarchy of the backbone. Subsequently, we introduce the Synergistic Edge Fusion (SEF) module, responsible for the deep, synergistic fusion of these scale-aware edge cues with semantic information from the backbone at each respective scale. Finally, to achieve ultimate feature refinement at the most detail-rich feature level, we adapt and incorporate the Cross Stage Partial Omni-Kernel Module (CSPOKM) [21].Through these synergistic modules, HieraEdgeNet significantly enhances the detection and localization accuracy of minute, indistinct targets. This technological advancement offers a robust solution for high-throughput palynological analysis, thereby unlocking its full potential for applications in precision agriculture. Specifically, it provides the technical foundation for generating large-scale, high-resolution datasets essential for modern biodiversity monitoring, tracking ecosystem responses to climate change, and paleoecological reconstruction, while also enhancing precision agriculture through applications in pollination assessment and crop health monitoring, thus contributing to both ecological sustainability and food security. In summary, this work makes three main contributions. First, we propose HieraEdgeNet, a palynology-specific detector that explicitly leverages a hierarchical edge feature pyramid and edge‚Äìsemantic fusion to address the indistinct boundaries and minute size of microscopic pollen grains. Second, we integrate a Cross Stage Partial Omni-Kernel module that concentrates large-kernel computation at the most detail-rich scale, yielding a favorable accuracy‚Äìefficiency trade-off compared with YOLO-based baselines while keeping the overall computational cost moderate. Third, we construct a unified large-scale pollen detection benchmark from multiple public sources and synthetic compositions, and we systematically benchmark HieraEdgeNet against both YOLO and detection Transformer models, thereby providing a rigorous and reproducible evaluation of its performance.",
            "2. Materials and Methods": "Because this work targets an interdisciplinary audience, including palynologists, ecologists, and agricultural scientists who may not be familiar with modern object-detection architectures, this section briefly reviews the key modules and datasets on which our design builds before presenting the proposed HieraEdgeNet architecture. 2.1. Efficient Network Modules Based on Cross Stage Partial IdeologyTo control computational costs while increasing network depth, efficient network architecture design is paramount. CSPNet (Cross Stage Partial Network) [22] introduced an effective strategy that partitions the feature map entering a processing block along the channel dimension into two parts. One part undergoes deep transformation through the block, while the other is concatenated with the processed features via a short-circuit connection. This design reduces computational redundancy and enhances gradient propagation throughout the network, thereby improving model learning capacity and efficiency. In modern object detection architectures, the basic convolution operation is encapsulated within a standardized Conv block. This block conventionally combines a 2D convolution layer, a batch normalization layer, and an activation function in series. This Conv-BN-Act structure has been proven to effectively accelerate model convergence and improve training stability, while also providing a regularization effect [23]. The C3k2 module, a core component for deep feature extraction in our architecture, is based on the design principles of CSPNet and adheres to the C2f paradigm [24]. This module is typically configured with standard Bottleneck blocks, each containing two convolution layers. The output of each processing unit is concatenated with the output of the previous stage, achieving a dense feature aggregation akin to that in DenseNet [25]. Finally, the features from all branches are concatenated and passed through a 1 √ó 1 convolution for final feature fusion. This design not only minimizes computational redundancy but also facilitates network learning through enriched gradient paths. The Area-Attention C2f (A2C2f) module embeds a region-based attention mechanism (AAttn) into the efficient C2f structure, endowing the model with dynamic, context-aware capabilities while retaining the advantages of CNN‚Äôs local inductive bias [14]. The core of the A2C2f module is an ABlock, which comprises an AAttn module and a feed-forward network (MLP) with residual connections, structurally similar to a standard Encoder layer in a Vision Transformer (ViT) [26]. The AAttn module implements multi-head self-attention but can constrain the computation to specific regions of the feature map via an area parameter, enabling a trade-off between global attention and computational cost.To enable the network to capture multi-scale contextual information from a single-scale feature map, the Spatial Pyramid Pooling - Fast (SPPF) module was introduced as an efficient variant [27,28]. It employs a series of max-pooling layers to equivalently simulate the effect of parallel large-kernel pooling. An input feature map first undergoes channel reduction via a 1 √ó 1 convolution. Subsequently, this reduced feature map is passed serially through a max-pooling layer with a fixed kernel size (e.g., k = 5) and a stride of 1, three consecutive times. The initial down-sampled feature is then concatenated along the channel dimension with the outputs from the three serial pooling operations. Since two consecutive kxk pooling operations (with stride 1) have a receptive field approximately equivalent to a single(2ùëò‚àí1)√ó(2ùëò‚àí1)(2k‚àí1)√ó(2k‚àí1)kernel, and three operations to a(3ùëò‚àí2)√ó(3ùëò‚àí2)(3k‚àí2)√ó(3k‚àí2)kernel, SPPF with k = 5 serially applied three times can effectively capture receptive fields similar to those from parallel 5 √ó 5, 9 √ó 9, and 13 √ó 13 kernels in traditional SPP. Finally, the concatenated features are fused and their channels are adjusted by another 1 √ó 1 convolution. This serial design maintains multi-scale context awareness while significantly reducing computational complexity and improving inference speed. 2.2. Feature Extraction and Complex Object OptimizationThe design of FPN enables modern object detectors to extract features at multiple scales, which is vital for detecting pollen grains of different sizes and locations. However, the limited resolution of microscopic images often results in objects that are minuscule with indistinct edges. Relying solely on standard multi-scale feature fusion is often insufficient for achieving precise detection in such contexts [10]. Consequently, enhanced strategies tailored to specific object characteristics are necessary. The identification and classification of pollen grains depend on both the boundary delimiting the grain from its environment and its internal features, making it crucial to enhance the model‚Äôs representation capability for indistinct objects and edge details.For specialized tasks, introducing prior knowledge as a strong inductive bias can significantly improve model performance and convergence speed. In the recognition of objects like pollen grains, precise boundary information is critical. The SobelConv module facilitates this by explicitly injecting first-order gradient information‚Äîi.e., edge features‚Äîinto the network. A SobelConv module is essentially a fixed-weight 2D convolutional layer. Its kernel weights are initialized with the classic Sobel operators for computing horizontal and vertical image gradients [29]. After initialization, these weights are frozen to ensure operational consistency. In CNNs, standard downsampling operations like strided convolutions or pooling layers inevitably cause spatial information loss, which is particularly detrimental to the detection and structural analysis of fine-grained objects like pollen. Although one could in principle relax this constraint and let the gradient kernels be learned end-to-end, we intentionally keep the Sobel filters fixed to provide a stable and interpretable edge prior while keeping the number of trainable parameters in the low-level edge extractor minimal. In the HEM, the subsequent1√ó11√ó1convolutions and fusion modules (SEF and CSPOKM) remain fully learnable, so the network can still adaptively reweight and combine these edge responses with semantic cues. This design follows the common practice of embedding hand-crafted operators as inductive bias in modern CNNs, and we leave a systematic exploration of learnable gradient kernels within HEM as promising future work.In our design, SobelConv is intentionally implemented as a fixed, non-learnable operator that injects a stable and interpretable edge prior into the earliest feature extraction stage. This follows the long tradition of using small discrete gradient masks such as the Sobel operator as efficient and robust first-order edge detectors in image analysis [30]. Our choice is motivated by three considerations: (1) it keeps the low-level edge extractor lightweight in terms of trainable parameters, which is beneficial for compact detectors; (2) it tends to improve optimization stability under the long-tailed and relatively limited pollen data distribution; and (3) Sobel filters have been successfully used to highlight salient intensity transitions along particle and pollen grain boundaries in bright-field and electron microscopy, including automated analyses of grass pollen surface ornamentation and bright-field pollen images [31,32,33]. At the same time, we explicitly acknowledge that this constitutes a strong inductive bias and may limit the ability of HEM to capture atypical edge patterns that deviate substantially from Sobel-like gradients, especially for rare or highly irregular pollen morphologies. In this work, we rely on the subsequent learnable1√ó11√ó1convolutions in HEM and the downstream SEF and CSPOKM modules to adapt these fixed edge responses to task-specific structures, and we leave a systematic ablation comparing fixed SobelConv with a simple learnable3√ó33√ó3convolution as an interesting direction for future work.To achieve lossless feature map downsampling, the Space-to-Depth Convolution (SPD-Conv) module was proposed [34]. The core of this module is the ‚ÄúSpace-to-Depth‚Äù transformation, also known as ‚ÄúPixel Unshuffle.‚Äù This operation rearranges a spatial block of 2 √ó 2 pixels into the channel dimension, effectively halving the spatial resolution while quadrupling the channel depth, thus achieving downsampling without information loss.Traditional convolutional networks rely on a homogeneous paradigm of stacking small-kernel convolutions. To mine and transform input features from omnidirectional, multi-dimensional, and multi-domain perspectives, the Omni-Kernel Core operator was developed [35,36]. The core of this module simulates diverse receptive fields through a set of parallel and heterogeneous depth-wise separable convolutional kernels. Within the Omni-Kernel, Multi-Scale and Anisotropic Convolutions employ various kernel shapes in parallel [37], including a large square kernel (e.g., 31 √ó 31) for capturing broad context and multiple anisotropic strip kernels (e.g., 1 √ó 31 and 31 √ó 1). These anisotropic kernels are particularly effective for perceiving structures with specific orientations, such as pollen contours or textures, thus providing an ‚Äúomnidirectional‚Äù perceptual capability. The Omni-Kernel also integrates attention mechanisms from both the spatial and frequency domains. Frequency Channel Attention (FCA) first learns channel attention weights in the spatial domain via global average pooling and 1 √ó 1 convolutions, then applies these weights to the Fourier spectrum of the feature map [38]. This allows the model to dynamically adjust channel importance based on frequency composition. On the features processed by FCA, Spatial Channel Attention (SCA) further applies a classic Squeeze-and-Excitation type of channel attention for secondary calibration in the spatial domain [39]. The Frequency Gating Module (FGM) acts as a dynamic frequency-domain filter, selectively enhancing or suppressing specific frequency components via a learned gating mechanism, further improving the model‚Äôs adaptability to complex patterns [40]. The Omni-Kernel design is intended to enhance the expressive power for complex pollen morphologies, improve discrimination of subtle inter-class differences, and increase localization accuracy amidst complex backgrounds and ambiguous boundaries. 2.3. Datasets for Pollen RecognitionThe development of robust, automated pollen recognition systems is critically dependent on large-scale, high-quality, and meticulously annotated datasets. Publicly available resources for pollen analysis, summarized inTable 1, can be broadly categorized into two main types: those comprising pre-segmented, single-grain images designed for classification tasks, and those providing fully annotated microscopic scenes for object detection. These datasets collectively offer a diverse range of pollen types, imaging conditions, and annotation complexities, providing a comprehensive foundation for benchmarking. This study leverages a selection of these public datasets, aggregated into a unified large-scale benchmark comprising 44,471 images and 342,706 annotated instances, to evaluate the performance of the HieraEdgeNet architecture, aiming to advance the accuracy, robustness, and usability of automated pollen recognition. Taken together, these architectural and dataset preliminaries position HieraEdgeNet between efficient YOLO-style one-stage detectors and detection Transformers such as RT-DETR: our model retains the favorable inductive biases and efficiency of CNN-based backbones while introducing explicit multi-scale edge priors and Omni-Kernel-based feature refinement tailored to the microscopic pollen detection problem.Table 1.Datasets for Pollen Recognition.To address the challenges inherent in detecting pollen grains‚Äîcharacterized by their minute size, limited effective pixel count, and the critical reliance of precise localization on the effective differentiation of target edges from complex environmental backgrounds‚Äîwe introduce HieraEdgeNet. This novel architecture is engineered to substantially enhance the recognition and localization accuracy of small objects, with a particular focus on pollen grains, by employing hierarchical and synergistic perception of edge information integrated with multi-scale feature fusion. The core innovation of HieraEdgeNet lies in the introduction of three pivotal structural modules: the Hierarchical Edge Module (HEM), the Synergistic Edge Fusion (SEF) module, and the Cross Stage Partial Omni-Kernel Module (CSPOKM) [21]. These modules operate synergistically to form a network highly sensitive to edge information and endowed with robust feature representation capabilities. The architectural details of these three core modules are illustrated inFigure 1.Figure 1.The three core modules of HieraEdgeNet: (a) The HEM explicitly extracts and organizes multi-scale edge information from input features. (b) The SEF module effectively fuses scale-specific edge features, extracted by the HEM, with semantic features from the corresponding scale of the backbone network. (c) The CSPOKM introduces robust multi-scale and omnidirectional receptive field feature extraction, integrating diverse large-kernel, anisotropic depth-wise separable convolutions alongside spatial and frequency domain attention mechanisms. 2.4. Hierarchical Edge Module (HEM) for Multi-Scale Feature ExtractionHEM is engineered to extract and construct a pyramid of multi-scale edge information from input feature maps, thereby explicitly capturing fine-grained image details and salient edge characteristics. We denote feature maps from stagekof a network backbone asùëÉùëòPk; these are typically characterized by a spatial resolution downscaled by a factor of2ùëò2krelative to the input image (e.g.,ùëÉ2P2features are at1/41/4resolution,ùëÉ3P3at1/81/8, and so forth). The HEM initially employs a fixed Sobel operator, implemented as a convolutional layer with non-learnable weights (SobelConv), to independently compute gradients for each channel of an input feature map,ùëãinXin. ThisùëãinXinis assumed to be from an early backbone stage, such asùëÉ2P2(i.e.,1/41/4resolution, e.g., with 256 channels), yielding an initial edge response map,ùëã(0)ùëíùëëùëîùëíXedge(0), which preserves the spatial resolution and channel dimensionality ofùëãinXin. Subsequently, max-pooling layers (MaxPool2d, kernel size 2 and stride 2) are iteratively applied three times to progressively downsampleùëã(0)ùëíùëëùëîùëíXedge(0), generating edge maps at successively lower resolutions:ùëã(1)ùëíùëëùëîùëíXedge(1)at1/81/8,ùëã(2)ùëíùëëùëîùëíXedge(2)at1/161/16, andùëã(3)ùëíùëëùëîùëíXedge(3)at1/321/32relative to the original image input. Finally, these hierarchically generated edge maps (ùëã(1)ùëíùëëùëîùëíXedge(1),ùëã(2)ùëíùëëùëîùëíXedge(2), andùëã(3)ùëíùëëùëîùëíXedge(3)) are each processed by an independent1√ó11√ó1convolution, which adjusts the channel dimensions to produce the multi-scale edge featuresùê∏ùëÉ3EP3(128 channels, derived fromùëã(1)ùëíùëëùëîùëíXedge(1)at1/81/8resolution),ùê∏ùëÉ4EP4(256 channels, fromùëã(2)ùëíùëëùëîùëíXedge(2)at1/161/16resolution), andùê∏ùëÉ5EP5(512 channels, fromùëã(3)ùëíùëëùëîùëíXedge(3)at1/321/32resolution). These edge features (ùê∏ùëÉ3EP3,ùê∏ùëÉ4EP4,ùê∏ùëÉ5EP5) therefore share the same spatial resolutions as the corresponding backbone stagesùëÉ3P3,ùëÉ4P4, andùëÉ5P5(i.e.,1/81/8,1/161/16, and1/321/32), which allows SEF to fuse them with the semantic features via straightforward channel-wise concatenation without any additional interpolation or resampling.LetùëãinXindenote an input feature map. The HEM performs the following operations:ùëã(0)ùëíùëëùëîùëíùëã(1)ùëíùëëùëîùëíùëã(2)ùëíùëëùëîùëíùê∏ùëÉ3ùê∏ùëÉ4ùê∏ùëÉ5=SobelConv(ùëãùëñùëõ)=MaxPool(ùëã(0)ùëíùëëùëîùëí)=MaxPool(ùëã(1)ùëíùëëùëîùëí)=‚Ñ±(ùê∂ùëÉ3)1√ó1(ùëã(0)ùëíùëëùëîùëí)=‚Ñ±(ùê∂ùëÉ4)1√ó1(ùëã(1)ùëíùëëùëîùëí)=‚Ñ±(ùê∂ùëÉ5)1√ó1(ùëã(2)ùëíùëëùëîùëí)Xedge(0)=SobelConv(Xin)Xedge(1)=MaxPool(Xedge(0))Xedge(2)=MaxPool(Xedge(1))EP3=F1√ó1(CP3)(Xedge(0))EP4=F1√ó1(CP4)(Xedge(1))EP5=F1√ó1(CP5)(Xedge(2))(1)whereSobelConv(¬∑)SobelConv(¬∑)denotes the Sobel edge extraction implemented via fixed-weight convolution,MaxPool(¬∑)MaxPool(¬∑)represents max pooling with a kernel size of 2 and a stride of 2, and‚Ñ±(ùê∂target)1√ó1(¬∑)F1√ó1(Ctarget)(¬∑)signifies a 1 √ó 1 convolutional block that adjusts the feature map toùê∂targetCtargetoutput channels.ùê∏ùëÉ3,ùê∏ùëÉ4,andùê∏ùëÉ5EP3,EP4,andEP5are the resulting multi-scale edge feature maps, withùê∂ùëÉ3,ùê∂ùëÉ4,andùê∂ùëÉ5CP3,CP4,andCP5being their respective channel dimensions (128, 256, and 512, as specified above).The HEM furnishes the backbone network with explicit edge cues at multiple abstraction levels, designed for fusion with corresponding semantic features. This mechanism significantly aids the model in achieving more precise localization of object boundaries and in comprehending fine-grained structural details. 2.5. Synergistic Edge Fusion (SEF) for Integrating PriorsSEF is engineered to integrate core semantic information, crucial for pollen classification, with salient edge cues essential for precise localization. SEF operates by concurrently processing semantic features extracted at three primary spatial scales within the network backbone (P3/8, P4/16, and P5/32, corresponding to feature maps downsampled by factors of 8, 16, and 32, respectively). These semantic features are then fused with the corresponding scale-specific edge features generated by the HEM.LetùëãùëöùëéùëñùëõXmaindenote the input semantic features andùëãùëíùëëùëîùëíXedgerepresent the input edge features from the corresponding scale. The symbol ‚®Å signifies concatenation along the channel dimension. The operations within the SEF module are defined as follows:ùëãùëêùëéùë°ùëãùëöùëñùëë1ùëãùëöùëñùëë2ùëåùëÜùê∏ùêπ=ùëãùëöùëéùëñùëõ‚®Åùëãùëíùëëùëîùëí=‚Ñ±(ùê∂ùëúùë¢ùë°/2)1√ó1(ùëãùëêùëéùë°)=‚Ñ±(ùê∂ùëúùë¢ùë°/2)3√ó3(ùëãùëöùëñùëë1)=‚Ñ±(ùê∂ùëúùë¢ùë°)1√ó1(ùëãùëöùëñùëë2)Xcat=Xmain‚®ÅXedgeXmid1=F1√ó1(Cout/2)(Xcat)Xmid2=F3√ó3(Cout/2)(Xmid1)YSEF=F1√ó1(Cout)(Xmid2)(2)whereùëåùëÜùê∏ùêπYSEFis the fused output feature map of the SEF module. Here,ùê∂ùëúùë¢ùë°Coutspecifies the number of output channels for the SEF module. The application of SEF modules at successive stages of the network, each operating on features of decreasing spatial resolution due to backbone downsampling, facilitates a continuous and hierarchical process of feature fusion. This mechanism allows SEF to effectively meld semantic and edge information, thereby substantially enhancing the model‚Äôs proficiency in both the classification and precise localization of pollen grains. 2.6. Cross Stage Partial Omni-Kernel Module (CSPOKM) for Feature RefinementThe primary objective of the CSPOKM is to enhance the flexibility and expressive power of feature extraction through the strategic incorporation of Omni-Kernel capabilities within a cross-stage, partial processing framework. This module is designed to effectively capture multi-scale features by leveraging inputs from different network stages. Specifically, CSPOKM introduces a learnable ensemble of convolutional kernels via the Omni-Kernel, which are characterized by their spatial sharing and channel-wise distinctions, enabling the model to flexibly combine and adapt feature representations at various levels of abstraction. Within HieraEdgeNet, the CSPOKM embeds the Omni-Kernel‚Äîa sophisticated operator comprising diverse large-scale, multi-directional depth-wise convolutions, augmented by attention and gating mechanisms such as SCA, FCA, and a FGM‚Äîinto a CSP architecture. This configuration is employed for feature refinement within a critical path of the detection head, specifically at the P3 level, aiming to enhance the detail and spatial precision of features by integrating multi-scale contextual information.The operational flow begins by concatenating features from disparate pathways:ùëãùëÉ2ùëÜXP2S(derived from P2/4 features processed by an SPD-Conv layer, designed to reduce spatial dimensions while preserving feature information),ùëãùëÉ3ùêµXP3B(the fused backbone features at the P3/8 level), andùëãùëÉ5ùëàXP5U(obtained by upsampling P5/32 features). Let the concatenated input features beùëãùëñùëõ=ùëãùëÉ2ùëÜ‚®ÅùëãùëÉ3ùêµ‚®ÅùëãùëÉ5ùëàXin=XP2S‚®ÅXP3B‚®ÅXP5U. The subsequent operations within CSPOKM are detailed as follows:ùëãùëìùëíùëéùë°(ùëãùëúùëòùëö_ùëñùëõ,ùëãùë†ùëòùëñùëù)ùëãùëúùëòùëö_ùëúùë¢ùë°ùëåùê∂ùëÜùëÉùëÇùêæùëÄ=‚Ñ±(ùê∂1)1√ó1(ùëãùëñùëõ)=Splitùëí(ùëãùëìùëíùëéùë°)=‚Ñ≥ùëÇùêæùëÄ(ùëãùëúùëòùëö_ùëñùëõ)=‚Ñ±(ùê∂2)1√ó1(ùëãùëúùëòùëö_ùëúùë¢ùë°‚®Åùëãùë†ùëòùëñùëù)Xfeat=F1√ó1(C1)(Xin)(Xokm_in,Xskip)=Splite(Xfeat)Xokm_out=MOKM(Xokm_in)YCSPOKM=F1√ó1(C2)(Xokm_out‚®ÅXskip)(3)Here, ‚®Å denotes concatenation along the channel dimension. TheSplitùëí(¬∑)Splite(¬∑)function partitions the feature channels into two segments based on a ratioe(empirically set toùëí=0.25e=0.25in our architecture) and1‚àíùëí1‚àíe. The segmentùëãùëúùëòùëöùëñùëõXokminis processed by the Omni-Kernel module, denoted as‚Ñ≥ùëÇùêæùëÄ(¬∑)MOKM(¬∑), yieldingùëãùëúùëòùëö_ùëúùë¢ùë°Xokm_out. This output is then concatenated with the skipped connectionùëãùë†ùëòùëñùëùXskipand further processed by a1√ó11√ó1convolutional block to produce the final outputùëåùê∂ùëÜùëÉùëÇùêæùëÄYCSPOKM. Typically, to maintain channel consistency for feature refinement at a specific level,ùê∂1C1andùê∂2C2are set equal to a target channel dimensionùê∂ùë°ùëéùëüùëîùëíùë°Ctarget, ensuring that the module‚Äôs input and output channel counts are concordant.This sophisticated processing, particularly at critical feature levels rich in detail such as P3, endows the network with a powerful feature learning capability. It enables the capture of complex patterns and long-range dependencies that are challenging for conventional convolutional layers. Concurrently, the CSP architecture ensures computational efficiency. Such characteristics are particularly advantageous for improving performance in scenarios involving small objects or requiring fine-grained distinctions between classes.Computationally, the CSPOKM utilizes a split ratio ofùëí=0.25e=0.25and is applied exclusively at the P3 level to concentrate resources on small-object features. This design keeps the total inference cost at 14 GFLOPs. While higher than the YOLOv12n baseline, this overhead yields a decisive performance boost (mAP@0.5:0.950.8315‚Üí0.84440.8315‚Üí0.8444), outperforming much heavier Transformer-based models like RT-DETR-R18/R50. Additionally, the compressibility demonstrated by the LAMP variant (11.6 GFLOPs) indicates that the architecture effectively balances rich feature extraction with practical deployment constraints.To make the price‚Äìperformance trade-off of our design more transparent, we follow common practice in complexity-aware CNN and detector design [37,46] and explicitly profile the FLOP contributions of the three core modules using a640√ó640640√ó640input and the profiling tools provided in the Ultralytics framework. Relative to the 6.7 GFLOPs of the YOLOv12n baseline, HieraEdgeNet‚Äôs total 14.0 GFLOPs can be decomposed as follows: the backbone and detection head account for essentially the same cost as YOLOv12n, HEM and SEF together contribute less than one third of the additional 7.3 GFLOPs, and CSPOKM accounts for the remaining majority. This indicates that most of the extra computation is concentrated in the large-kernel omni-kernel branch at the P3 level, whereas the hierarchical edge extraction (HEM) and fusion (SEF) modules provide comparatively modest overhead relative to their impact on accuracy. A detailed per-layer FLOP breakdown and the corresponding profiling scripts are released in our PalynoKit (version 0.0.1) repository to facilitate reproducibility and further analysis by practitioners. In this study, all profiling tools and experiments are implemented using the PalynoKit software package (version 0.0.1). 2.7. Architectural Integration of HieraEdgeNetThe HieraEdgeNet architecture is engineered to improve object detection for targets defined by fine-grained boundary information, a challenge prevalent in microscopic imaging. Its core design principle is the parallel and synergistic processing of semantic and edge features. The comprehensive architecture of HieraEdgeNet is depicted inFigure 2.Figure 2.The complete architecture of HieraEdgeNet, featuring enhancements for small objects, complex background information, and blurred edges.The implementation involves three key stages. First, a HEM operates in parallel to the backbone‚Äôs initial stages, generating a pyramid of edge-maps that corresponds directly to the semantic feature hierarchy. Second, these scale-specific edge features are injected into the primary feature stream via SEF modules, creating a unified set of representations that encode both boundary and semantic information at each scale. Third, the network neck employs a Path Aggregation Network (PANet) architecture to facilitate robust, bidirectional feature aggregation across all levels of the now edge-enhanced pyramid. These deeply fused multi-scale features are then processed by the detection heads to yield final predictions, comprising bounding boxes, confidence scores, and class labels. 2.8. Detection Head and Loss FunctionThe detection architecture directly employs a decoupled head. During the forward propagation phase, for each detection levell, the input feature mapùëãùëô‚àà‚Ñùùêµ√óùê∂ùëô√óùêªùëô√óùëäùëôXl‚ààRB√óCl√óHl√óWlis processed independently by the regression and classification heads. The regression head outputsùëÉ(ùëô)ùëüùëíùëî=‚Ñ±(ùëô)ùëüùëíùëî(ùëãùëô)‚àà‚Ñùùêµ√ó4ùëÖ√óùêªùëô√óùëäùëôPreg(l)=Freg(l)(Xl)‚ààRB√ó4R√óHl√óWl, and the classification head outputsùëÉ(ùëô)ùëêùëôùë†=‚Ñ±(ùëô)ùëêùëôùë†(ùëãùëô)‚àà‚Ñùùêµ√óùëÅùëê√óùêªùëô√óùëäùëôPcls(l)=Fcls(l)(Xl)‚ààRB√óNc√óHl√óWl. During inference, predictions from all levels are concatenated and decoded. For each detection layerùëô‚àà1,‚Ä¶,ùêøl‚àà1,‚Ä¶,Land at each spatial location(ùëñ,ùëó)(i,j), let the regression prediction beùëÉ(ùëô)ùëüùëíùëî[ùëñ,ùëó]‚àà‚Ñù4ùëÖPreg(l)[i,j]‚ààR4Rand the class prediction beùëÉ(ùëô)ùëêùëôùë†[ùëñ,ùëó]‚àà‚ÑùùëÅùëêPcls(l)[i,j]‚ààRNc. For bounding box decoding,ùëÉ(ùëô)ùëüùëíùëî[ùëñ,ùëó]Preg(l)[i,j]is partitioned into four componentsùëëùë°,ùëëùëè,ùëëùëô,ùëëùëü‚àà‚ÑùùëÖdt,db,dl,dr‚ààRR, corresponding to the distance distributions for the top, bottom, left, and right sides of the bounding box, respectively. After processing via the mechanism associated with Distribution Focal Loss (DFL), these yield scalar distancesùõøùë°=ùíü(ùëëùë°)Œ¥t=D(dt),ùõøùëè=ùíü(ùëëùëè)Œ¥b=D(db),ùõøùëô=ùíü(ùëëùëô)Œ¥l=D(dl), andùõøùëü=ùíü(ùëëùëü)Œ¥r=D(dr), whereùíü(¬∑)D(¬∑)represents the operation deriving a scalar distance from the learned distribution. The class probabilities are defined asùëù(ùëô)[ùëñ,ùëó]=ùúé(ùëÉ(ùëô)ùëêùëôùë†[ùëñ,ùëó])‚àà[0,1]ùëÅùëêp(l)[i,j]=œÉ(Pcls(l)[i,j])‚àà[0,1]Nc, whereùúéœÉdenotes the sigmoid function.Addressing the imbalance in pollen samples, this study adopts the Focal Loss (‚ÑíùêπùêøLFL) as the classification loss function. Focal Loss introduces a modulating factor(1‚àíùëùùë°)ùõæ(1‚àípt)Œ≥and an optional balancing factorùõºùë°Œ±t,ùõæ‚â•0Œ≥‚â•0is the focusing parameter. Ifùëùùëòpkis the model‚Äôs predicted probability for thek-th class andùëùÃÇùëòp^kis the corresponding ground truth label (typically 0 or 1), the Focal Loss for thek-th class is:‚Ñí(ùëò)ùêπùêø=‚àíùõºùëò(1‚àíùëùùë°,ùëò)ùõælog(ùëùùë°,ùëò)LFL(k)=‚àíŒ±k(1‚àípt,k)Œ≥log(pt,k)(4)whereùëùùë°,ùëò=ùëùùëòpt,k=pkifùëùÃÇùëò=1p^k=1, andùëùùë°,ùëò=1‚àíùëùùëòpt,k=1‚àípkifùëùÃÇùëò=0p^k=0. The total loss for object detection,‚Ñíùê∑ùëíùë°ùëáùëúùë°ùëéùëôLDetTotal, is a weighted sum of the regression and classification losses, with weightsùúÜùëüùëíùëîŒªregandùúÜùëêùëôùë†Œªclsrespectively:‚Ñíùê∑ùëíùë°ùëáùëúùë°ùëéùëô=ùúÜùëüùëíùëî‚Ñíùëüùëíùëî+ùúÜùëêùëôùë†‚Ñíùëêùëôùë†LDetTotal=ŒªregLreg+ŒªclsLcls(5)Here,‚Ñíùëêùëôùë†Lclsis the sum of Focal Losses computed over all positive samples and selected negative samples. The bounding box regression loss (‚ÑíùëüùëíùëîLreg) employs Distribution Focal Loss (DFL) to learn the distribution of distances from the anchor/reference point to the four sides of the bounding box. In all experiments, we combine DFL with Complete IoU (CIoU) loss as implemented in the Ultralytics framework, following recent one-stage detectors that adopt CIoU as the default choice for bounding box regression. Although other IoU variants such as GIoU and SIoU are supported by the underlying framework, we did not perform a systematic ablation over these alternatives in this work and therefore focus our analysis on the CIoU-based configuration. The classification loss (‚Ñíùëêùëôùë†Lcls), as previously mentioned, is calculated using Focal Loss.In practice, we follow the Ultralytics implementation and adopt the default loss gains for the detection task, setting the contributions of the regression and classification components such that the box, classification, and DFL terms are weighted with gains of 7.5, 0.5, and 1.5, respectively. For the detection Transformer baselines that employ Focal Loss within a DETR-style objective, we use a focusing parameter ofùõæ=1.5Œ≥=1.5and a class-balancing factor ofùõº=0.25Œ±=0.25, consistent with common practice in long-tailed object detection. We observed that moderate variations around these values do not alter the relative ranking of models, so we report results under this well-established configuration without exhaustively tuning the loss hyperparameters.In addition to Focal Loss, we also considered several alternative strategies for mitigating the long-tailed class imbalance in our pollen dataset, including simple inverse-frequency class re-weighting, class-balanced loss based on effective numbers of samples, and instance-level over-sampling of rare classes. However, in preliminary experiments these alternatives did not yield consistent improvements in mAP compared to the standard Focal Loss configuration and in some cases slightly degraded training stability. For this reason, and in line with common practice in modern one-stage detectors, we adopt Focal Loss as the final classification loss, while relying on careful dataset construction and augmentation to further alleviate imbalance effects.",
            "2.1. Efficient Network Modules Based on Cross Stage Partial Ideology": "To control computational costs while increasing network depth, efficient network architecture design is paramount. CSPNet (Cross Stage Partial Network) [22] introduced an effective strategy that partitions the feature map entering a processing block along the channel dimension into two parts. One part undergoes deep transformation through the block, while the other is concatenated with the processed features via a short-circuit connection. This design reduces computational redundancy and enhances gradient propagation throughout the network, thereby improving model learning capacity and efficiency. In modern object detection architectures, the basic convolution operation is encapsulated within a standardized Conv block. This block conventionally combines a 2D convolution layer, a batch normalization layer, and an activation function in series. This Conv-BN-Act structure has been proven to effectively accelerate model convergence and improve training stability, while also providing a regularization effect [23]. The C3k2 module, a core component for deep feature extraction in our architecture, is based on the design principles of CSPNet and adheres to the C2f paradigm [24]. This module is typically configured with standard Bottleneck blocks, each containing two convolution layers. The output of each processing unit is concatenated with the output of the previous stage, achieving a dense feature aggregation akin to that in DenseNet [25]. Finally, the features from all branches are concatenated and passed through a 1 √ó 1 convolution for final feature fusion. This design not only minimizes computational redundancy but also facilitates network learning through enriched gradient paths. The Area-Attention C2f (A2C2f) module embeds a region-based attention mechanism (AAttn) into the efficient C2f structure, endowing the model with dynamic, context-aware capabilities while retaining the advantages of CNN‚Äôs local inductive bias [14]. The core of the A2C2f module is an ABlock, which comprises an AAttn module and a feed-forward network (MLP) with residual connections, structurally similar to a standard Encoder layer in a Vision Transformer (ViT) [26]. The AAttn module implements multi-head self-attention but can constrain the computation to specific regions of the feature map via an area parameter, enabling a trade-off between global attention and computational cost. To enable the network to capture multi-scale contextual information from a single-scale feature map, the Spatial Pyramid Pooling - Fast (SPPF) module was introduced as an efficient variant [27,28]. It employs a series of max-pooling layers to equivalently simulate the effect of parallel large-kernel pooling. An input feature map first undergoes channel reduction via a 1 √ó 1 convolution. Subsequently, this reduced feature map is passed serially through a max-pooling layer with a fixed kernel size (e.g., k = 5) and a stride of 1, three consecutive times. The initial down-sampled feature is then concatenated along the channel dimension with the outputs from the three serial pooling operations. Since two consecutive kxk pooling operations (with stride 1) have a receptive field approximately equivalent to a single(2ùëò‚àí1)√ó(2ùëò‚àí1)(2k‚àí1)√ó(2k‚àí1)kernel, and three operations to a(3ùëò‚àí2)√ó(3ùëò‚àí2)(3k‚àí2)√ó(3k‚àí2)kernel, SPPF with k = 5 serially applied three times can effectively capture receptive fields similar to those from parallel 5 √ó 5, 9 √ó 9, and 13 √ó 13 kernels in traditional SPP. Finally, the concatenated features are fused and their channels are adjusted by another 1 √ó 1 convolution. This serial design maintains multi-scale context awareness while significantly reducing computational complexity and improving inference speed.",
            "2.2. Feature Extraction and Complex Object Optimization": "The design of FPN enables modern object detectors to extract features at multiple scales, which is vital for detecting pollen grains of different sizes and locations. However, the limited resolution of microscopic images often results in objects that are minuscule with indistinct edges. Relying solely on standard multi-scale feature fusion is often insufficient for achieving precise detection in such contexts [10]. Consequently, enhanced strategies tailored to specific object characteristics are necessary. The identification and classification of pollen grains depend on both the boundary delimiting the grain from its environment and its internal features, making it crucial to enhance the model‚Äôs representation capability for indistinct objects and edge details. For specialized tasks, introducing prior knowledge as a strong inductive bias can significantly improve model performance and convergence speed. In the recognition of objects like pollen grains, precise boundary information is critical. The SobelConv module facilitates this by explicitly injecting first-order gradient information‚Äîi.e., edge features‚Äîinto the network. A SobelConv module is essentially a fixed-weight 2D convolutional layer. Its kernel weights are initialized with the classic Sobel operators for computing horizontal and vertical image gradients [29]. After initialization, these weights are frozen to ensure operational consistency. In CNNs, standard downsampling operations like strided convolutions or pooling layers inevitably cause spatial information loss, which is particularly detrimental to the detection and structural analysis of fine-grained objects like pollen. Although one could in principle relax this constraint and let the gradient kernels be learned end-to-end, we intentionally keep the Sobel filters fixed to provide a stable and interpretable edge prior while keeping the number of trainable parameters in the low-level edge extractor minimal. In the HEM, the subsequent1√ó11√ó1convolutions and fusion modules (SEF and CSPOKM) remain fully learnable, so the network can still adaptively reweight and combine these edge responses with semantic cues. This design follows the common practice of embedding hand-crafted operators as inductive bias in modern CNNs, and we leave a systematic exploration of learnable gradient kernels within HEM as promising future work. In our design, SobelConv is intentionally implemented as a fixed, non-learnable operator that injects a stable and interpretable edge prior into the earliest feature extraction stage. This follows the long tradition of using small discrete gradient masks such as the Sobel operator as efficient and robust first-order edge detectors in image analysis [30]. Our choice is motivated by three considerations: (1) it keeps the low-level edge extractor lightweight in terms of trainable parameters, which is beneficial for compact detectors; (2) it tends to improve optimization stability under the long-tailed and relatively limited pollen data distribution; and (3) Sobel filters have been successfully used to highlight salient intensity transitions along particle and pollen grain boundaries in bright-field and electron microscopy, including automated analyses of grass pollen surface ornamentation and bright-field pollen images [31,32,33]. At the same time, we explicitly acknowledge that this constitutes a strong inductive bias and may limit the ability of HEM to capture atypical edge patterns that deviate substantially from Sobel-like gradients, especially for rare or highly irregular pollen morphologies. In this work, we rely on the subsequent learnable1√ó11√ó1convolutions in HEM and the downstream SEF and CSPOKM modules to adapt these fixed edge responses to task-specific structures, and we leave a systematic ablation comparing fixed SobelConv with a simple learnable3√ó33√ó3convolution as an interesting direction for future work. To achieve lossless feature map downsampling, the Space-to-Depth Convolution (SPD-Conv) module was proposed [34]. The core of this module is the ‚ÄúSpace-to-Depth‚Äù transformation, also known as ‚ÄúPixel Unshuffle.‚Äù This operation rearranges a spatial block of 2 √ó 2 pixels into the channel dimension, effectively halving the spatial resolution while quadrupling the channel depth, thus achieving downsampling without information loss. Traditional convolutional networks rely on a homogeneous paradigm of stacking small-kernel convolutions. To mine and transform input features from omnidirectional, multi-dimensional, and multi-domain perspectives, the Omni-Kernel Core operator was developed [35,36]. The core of this module simulates diverse receptive fields through a set of parallel and heterogeneous depth-wise separable convolutional kernels. Within the Omni-Kernel, Multi-Scale and Anisotropic Convolutions employ various kernel shapes in parallel [37], including a large square kernel (e.g., 31 √ó 31) for capturing broad context and multiple anisotropic strip kernels (e.g., 1 √ó 31 and 31 √ó 1). These anisotropic kernels are particularly effective for perceiving structures with specific orientations, such as pollen contours or textures, thus providing an ‚Äúomnidirectional‚Äù perceptual capability. The Omni-Kernel also integrates attention mechanisms from both the spatial and frequency domains. Frequency Channel Attention (FCA) first learns channel attention weights in the spatial domain via global average pooling and 1 √ó 1 convolutions, then applies these weights to the Fourier spectrum of the feature map [38]. This allows the model to dynamically adjust channel importance based on frequency composition. On the features processed by FCA, Spatial Channel Attention (SCA) further applies a classic Squeeze-and-Excitation type of channel attention for secondary calibration in the spatial domain [39]. The Frequency Gating Module (FGM) acts as a dynamic frequency-domain filter, selectively enhancing or suppressing specific frequency components via a learned gating mechanism, further improving the model‚Äôs adaptability to complex patterns [40]. The Omni-Kernel design is intended to enhance the expressive power for complex pollen morphologies, improve discrimination of subtle inter-class differences, and increase localization accuracy amidst complex backgrounds and ambiguous boundaries.",
            "2.3. Datasets for Pollen Recognition": "The development of robust, automated pollen recognition systems is critically dependent on large-scale, high-quality, and meticulously annotated datasets. Publicly available resources for pollen analysis, summarized inTable 1, can be broadly categorized into two main types: those comprising pre-segmented, single-grain images designed for classification tasks, and those providing fully annotated microscopic scenes for object detection. These datasets collectively offer a diverse range of pollen types, imaging conditions, and annotation complexities, providing a comprehensive foundation for benchmarking. This study leverages a selection of these public datasets, aggregated into a unified large-scale benchmark comprising 44,471 images and 342,706 annotated instances, to evaluate the performance of the HieraEdgeNet architecture, aiming to advance the accuracy, robustness, and usability of automated pollen recognition. Taken together, these architectural and dataset preliminaries position HieraEdgeNet between efficient YOLO-style one-stage detectors and detection Transformers such as RT-DETR: our model retains the favorable inductive biases and efficiency of CNN-based backbones while introducing explicit multi-scale edge priors and Omni-Kernel-based feature refinement tailored to the microscopic pollen detection problem. Table 1.Datasets for Pollen Recognition. To address the challenges inherent in detecting pollen grains‚Äîcharacterized by their minute size, limited effective pixel count, and the critical reliance of precise localization on the effective differentiation of target edges from complex environmental backgrounds‚Äîwe introduce HieraEdgeNet. This novel architecture is engineered to substantially enhance the recognition and localization accuracy of small objects, with a particular focus on pollen grains, by employing hierarchical and synergistic perception of edge information integrated with multi-scale feature fusion. The core innovation of HieraEdgeNet lies in the introduction of three pivotal structural modules: the Hierarchical Edge Module (HEM), the Synergistic Edge Fusion (SEF) module, and the Cross Stage Partial Omni-Kernel Module (CSPOKM) [21]. These modules operate synergistically to form a network highly sensitive to edge information and endowed with robust feature representation capabilities. The architectural details of these three core modules are illustrated inFigure 1. Figure 1.The three core modules of HieraEdgeNet: (a) The HEM explicitly extracts and organizes multi-scale edge information from input features. (b) The SEF module effectively fuses scale-specific edge features, extracted by the HEM, with semantic features from the corresponding scale of the backbone network. (c) The CSPOKM introduces robust multi-scale and omnidirectional receptive field feature extraction, integrating diverse large-kernel, anisotropic depth-wise separable convolutions alongside spatial and frequency domain attention mechanisms.",
            "2.4. Hierarchical Edge Module (HEM) for Multi-Scale Feature Extraction": "HEM is engineered to extract and construct a pyramid of multi-scale edge information from input feature maps, thereby explicitly capturing fine-grained image details and salient edge characteristics. We denote feature maps from stagekof a network backbone asùëÉùëòPk; these are typically characterized by a spatial resolution downscaled by a factor of2ùëò2krelative to the input image (e.g.,ùëÉ2P2features are at1/41/4resolution,ùëÉ3P3at1/81/8, and so forth). The HEM initially employs a fixed Sobel operator, implemented as a convolutional layer with non-learnable weights (SobelConv), to independently compute gradients for each channel of an input feature map,ùëãinXin. ThisùëãinXinis assumed to be from an early backbone stage, such asùëÉ2P2(i.e.,1/41/4resolution, e.g., with 256 channels), yielding an initial edge response map,ùëã(0)ùëíùëëùëîùëíXedge(0), which preserves the spatial resolution and channel dimensionality ofùëãinXin. Subsequently, max-pooling layers (MaxPool2d, kernel size 2 and stride 2) are iteratively applied three times to progressively downsampleùëã(0)ùëíùëëùëîùëíXedge(0), generating edge maps at successively lower resolutions:ùëã(1)ùëíùëëùëîùëíXedge(1)at1/81/8,ùëã(2)ùëíùëëùëîùëíXedge(2)at1/161/16, andùëã(3)ùëíùëëùëîùëíXedge(3)at1/321/32relative to the original image input. Finally, these hierarchically generated edge maps (ùëã(1)ùëíùëëùëîùëíXedge(1),ùëã(2)ùëíùëëùëîùëíXedge(2), andùëã(3)ùëíùëëùëîùëíXedge(3)) are each processed by an independent1√ó11√ó1convolution, which adjusts the channel dimensions to produce the multi-scale edge featuresùê∏ùëÉ3EP3(128 channels, derived fromùëã(1)ùëíùëëùëîùëíXedge(1)at1/81/8resolution),ùê∏ùëÉ4EP4(256 channels, fromùëã(2)ùëíùëëùëîùëíXedge(2)at1/161/16resolution), andùê∏ùëÉ5EP5(512 channels, fromùëã(3)ùëíùëëùëîùëíXedge(3)at1/321/32resolution). These edge features (ùê∏ùëÉ3EP3,ùê∏ùëÉ4EP4,ùê∏ùëÉ5EP5) therefore share the same spatial resolutions as the corresponding backbone stagesùëÉ3P3,ùëÉ4P4, andùëÉ5P5(i.e.,1/81/8,1/161/16, and1/321/32), which allows SEF to fuse them with the semantic features via straightforward channel-wise concatenation without any additional interpolation or resampling. LetùëãinXindenote an input feature map. The HEM performs the following operations:ùëã(0)ùëíùëëùëîùëíùëã(1)ùëíùëëùëîùëíùëã(2)ùëíùëëùëîùëíùê∏ùëÉ3ùê∏ùëÉ4ùê∏ùëÉ5=SobelConv(ùëãùëñùëõ)=MaxPool(ùëã(0)ùëíùëëùëîùëí)=MaxPool(ùëã(1)ùëíùëëùëîùëí)=‚Ñ±(ùê∂ùëÉ3)1√ó1(ùëã(0)ùëíùëëùëîùëí)=‚Ñ±(ùê∂ùëÉ4)1√ó1(ùëã(1)ùëíùëëùëîùëí)=‚Ñ±(ùê∂ùëÉ5)1√ó1(ùëã(2)ùëíùëëùëîùëí)Xedge(0)=SobelConv(Xin)Xedge(1)=MaxPool(Xedge(0))Xedge(2)=MaxPool(Xedge(1))EP3=F1√ó1(CP3)(Xedge(0))EP4=F1√ó1(CP4)(Xedge(1))EP5=F1√ó1(CP5)(Xedge(2))(1)whereSobelConv(¬∑)SobelConv(¬∑)denotes the Sobel edge extraction implemented via fixed-weight convolution,MaxPool(¬∑)MaxPool(¬∑)represents max pooling with a kernel size of 2 and a stride of 2, and‚Ñ±(ùê∂target)1√ó1(¬∑)F1√ó1(Ctarget)(¬∑)signifies a 1 √ó 1 convolutional block that adjusts the feature map toùê∂targetCtargetoutput channels.ùê∏ùëÉ3,ùê∏ùëÉ4,andùê∏ùëÉ5EP3,EP4,andEP5are the resulting multi-scale edge feature maps, withùê∂ùëÉ3,ùê∂ùëÉ4,andùê∂ùëÉ5CP3,CP4,andCP5being their respective channel dimensions (128, 256, and 512, as specified above). The HEM furnishes the backbone network with explicit edge cues at multiple abstraction levels, designed for fusion with corresponding semantic features. This mechanism significantly aids the model in achieving more precise localization of object boundaries and in comprehending fine-grained structural details.",
            "2.5. Synergistic Edge Fusion (SEF) for Integrating Priors": "SEF is engineered to integrate core semantic information, crucial for pollen classification, with salient edge cues essential for precise localization. SEF operates by concurrently processing semantic features extracted at three primary spatial scales within the network backbone (P3/8, P4/16, and P5/32, corresponding to feature maps downsampled by factors of 8, 16, and 32, respectively). These semantic features are then fused with the corresponding scale-specific edge features generated by the HEM. LetùëãùëöùëéùëñùëõXmaindenote the input semantic features andùëãùëíùëëùëîùëíXedgerepresent the input edge features from the corresponding scale. The symbol ‚®Å signifies concatenation along the channel dimension. The operations within the SEF module are defined as follows:ùëãùëêùëéùë°ùëãùëöùëñùëë1ùëãùëöùëñùëë2ùëåùëÜùê∏ùêπ=ùëãùëöùëéùëñùëõ‚®Åùëãùëíùëëùëîùëí=‚Ñ±(ùê∂ùëúùë¢ùë°/2)1√ó1(ùëãùëêùëéùë°)=‚Ñ±(ùê∂ùëúùë¢ùë°/2)3√ó3(ùëãùëöùëñùëë1)=‚Ñ±(ùê∂ùëúùë¢ùë°)1√ó1(ùëãùëöùëñùëë2)Xcat=Xmain‚®ÅXedgeXmid1=F1√ó1(Cout/2)(Xcat)Xmid2=F3√ó3(Cout/2)(Xmid1)YSEF=F1√ó1(Cout)(Xmid2)(2)whereùëåùëÜùê∏ùêπYSEFis the fused output feature map of the SEF module. Here,ùê∂ùëúùë¢ùë°Coutspecifies the number of output channels for the SEF module. The application of SEF modules at successive stages of the network, each operating on features of decreasing spatial resolution due to backbone downsampling, facilitates a continuous and hierarchical process of feature fusion. This mechanism allows SEF to effectively meld semantic and edge information, thereby substantially enhancing the model‚Äôs proficiency in both the classification and precise localization of pollen grains.",
            "2.6. Cross Stage Partial Omni-Kernel Module (CSPOKM) for Feature Refinement": "The primary objective of the CSPOKM is to enhance the flexibility and expressive power of feature extraction through the strategic incorporation of Omni-Kernel capabilities within a cross-stage, partial processing framework. This module is designed to effectively capture multi-scale features by leveraging inputs from different network stages. Specifically, CSPOKM introduces a learnable ensemble of convolutional kernels via the Omni-Kernel, which are characterized by their spatial sharing and channel-wise distinctions, enabling the model to flexibly combine and adapt feature representations at various levels of abstraction. Within HieraEdgeNet, the CSPOKM embeds the Omni-Kernel‚Äîa sophisticated operator comprising diverse large-scale, multi-directional depth-wise convolutions, augmented by attention and gating mechanisms such as SCA, FCA, and a FGM‚Äîinto a CSP architecture. This configuration is employed for feature refinement within a critical path of the detection head, specifically at the P3 level, aiming to enhance the detail and spatial precision of features by integrating multi-scale contextual information. The operational flow begins by concatenating features from disparate pathways:ùëãùëÉ2ùëÜXP2S(derived from P2/4 features processed by an SPD-Conv layer, designed to reduce spatial dimensions while preserving feature information),ùëãùëÉ3ùêµXP3B(the fused backbone features at the P3/8 level), andùëãùëÉ5ùëàXP5U(obtained by upsampling P5/32 features). Let the concatenated input features beùëãùëñùëõ=ùëãùëÉ2ùëÜ‚®ÅùëãùëÉ3ùêµ‚®ÅùëãùëÉ5ùëàXin=XP2S‚®ÅXP3B‚®ÅXP5U. The subsequent operations within CSPOKM are detailed as follows:ùëãùëìùëíùëéùë°(ùëãùëúùëòùëö_ùëñùëõ,ùëãùë†ùëòùëñùëù)ùëãùëúùëòùëö_ùëúùë¢ùë°ùëåùê∂ùëÜùëÉùëÇùêæùëÄ=‚Ñ±(ùê∂1)1√ó1(ùëãùëñùëõ)=Splitùëí(ùëãùëìùëíùëéùë°)=‚Ñ≥ùëÇùêæùëÄ(ùëãùëúùëòùëö_ùëñùëõ)=‚Ñ±(ùê∂2)1√ó1(ùëãùëúùëòùëö_ùëúùë¢ùë°‚®Åùëãùë†ùëòùëñùëù)Xfeat=F1√ó1(C1)(Xin)(Xokm_in,Xskip)=Splite(Xfeat)Xokm_out=MOKM(Xokm_in)YCSPOKM=F1√ó1(C2)(Xokm_out‚®ÅXskip)(3) Here, ‚®Å denotes concatenation along the channel dimension. TheSplitùëí(¬∑)Splite(¬∑)function partitions the feature channels into two segments based on a ratioe(empirically set toùëí=0.25e=0.25in our architecture) and1‚àíùëí1‚àíe. The segmentùëãùëúùëòùëöùëñùëõXokminis processed by the Omni-Kernel module, denoted as‚Ñ≥ùëÇùêæùëÄ(¬∑)MOKM(¬∑), yieldingùëãùëúùëòùëö_ùëúùë¢ùë°Xokm_out. This output is then concatenated with the skipped connectionùëãùë†ùëòùëñùëùXskipand further processed by a1√ó11√ó1convolutional block to produce the final outputùëåùê∂ùëÜùëÉùëÇùêæùëÄYCSPOKM. Typically, to maintain channel consistency for feature refinement at a specific level,ùê∂1C1andùê∂2C2are set equal to a target channel dimensionùê∂ùë°ùëéùëüùëîùëíùë°Ctarget, ensuring that the module‚Äôs input and output channel counts are concordant. This sophisticated processing, particularly at critical feature levels rich in detail such as P3, endows the network with a powerful feature learning capability. It enables the capture of complex patterns and long-range dependencies that are challenging for conventional convolutional layers. Concurrently, the CSP architecture ensures computational efficiency. Such characteristics are particularly advantageous for improving performance in scenarios involving small objects or requiring fine-grained distinctions between classes. Computationally, the CSPOKM utilizes a split ratio ofùëí=0.25e=0.25and is applied exclusively at the P3 level to concentrate resources on small-object features. This design keeps the total inference cost at 14 GFLOPs. While higher than the YOLOv12n baseline, this overhead yields a decisive performance boost (mAP@0.5:0.950.8315‚Üí0.84440.8315‚Üí0.8444), outperforming much heavier Transformer-based models like RT-DETR-R18/R50. Additionally, the compressibility demonstrated by the LAMP variant (11.6 GFLOPs) indicates that the architecture effectively balances rich feature extraction with practical deployment constraints. To make the price‚Äìperformance trade-off of our design more transparent, we follow common practice in complexity-aware CNN and detector design [37,46] and explicitly profile the FLOP contributions of the three core modules using a640√ó640640√ó640input and the profiling tools provided in the Ultralytics framework. Relative to the 6.7 GFLOPs of the YOLOv12n baseline, HieraEdgeNet‚Äôs total 14.0 GFLOPs can be decomposed as follows: the backbone and detection head account for essentially the same cost as YOLOv12n, HEM and SEF together contribute less than one third of the additional 7.3 GFLOPs, and CSPOKM accounts for the remaining majority. This indicates that most of the extra computation is concentrated in the large-kernel omni-kernel branch at the P3 level, whereas the hierarchical edge extraction (HEM) and fusion (SEF) modules provide comparatively modest overhead relative to their impact on accuracy. A detailed per-layer FLOP breakdown and the corresponding profiling scripts are released in our PalynoKit (version 0.0.1) repository to facilitate reproducibility and further analysis by practitioners. In this study, all profiling tools and experiments are implemented using the PalynoKit software package (version 0.0.1).",
            "2.7. Architectural Integration of HieraEdgeNet": "The HieraEdgeNet architecture is engineered to improve object detection for targets defined by fine-grained boundary information, a challenge prevalent in microscopic imaging. Its core design principle is the parallel and synergistic processing of semantic and edge features. The comprehensive architecture of HieraEdgeNet is depicted inFigure 2. Figure 2.The complete architecture of HieraEdgeNet, featuring enhancements for small objects, complex background information, and blurred edges. The implementation involves three key stages. First, a HEM operates in parallel to the backbone‚Äôs initial stages, generating a pyramid of edge-maps that corresponds directly to the semantic feature hierarchy. Second, these scale-specific edge features are injected into the primary feature stream via SEF modules, creating a unified set of representations that encode both boundary and semantic information at each scale. Third, the network neck employs a Path Aggregation Network (PANet) architecture to facilitate robust, bidirectional feature aggregation across all levels of the now edge-enhanced pyramid. These deeply fused multi-scale features are then processed by the detection heads to yield final predictions, comprising bounding boxes, confidence scores, and class labels.",
            "2.8. Detection Head and Loss Function": "The detection architecture directly employs a decoupled head. During the forward propagation phase, for each detection levell, the input feature mapùëãùëô‚àà‚Ñùùêµ√óùê∂ùëô√óùêªùëô√óùëäùëôXl‚ààRB√óCl√óHl√óWlis processed independently by the regression and classification heads. The regression head outputsùëÉ(ùëô)ùëüùëíùëî=‚Ñ±(ùëô)ùëüùëíùëî(ùëãùëô)‚àà‚Ñùùêµ√ó4ùëÖ√óùêªùëô√óùëäùëôPreg(l)=Freg(l)(Xl)‚ààRB√ó4R√óHl√óWl, and the classification head outputsùëÉ(ùëô)ùëêùëôùë†=‚Ñ±(ùëô)ùëêùëôùë†(ùëãùëô)‚àà‚Ñùùêµ√óùëÅùëê√óùêªùëô√óùëäùëôPcls(l)=Fcls(l)(Xl)‚ààRB√óNc√óHl√óWl. During inference, predictions from all levels are concatenated and decoded. For each detection layerùëô‚àà1,‚Ä¶,ùêøl‚àà1,‚Ä¶,Land at each spatial location(ùëñ,ùëó)(i,j), let the regression prediction beùëÉ(ùëô)ùëüùëíùëî[ùëñ,ùëó]‚àà‚Ñù4ùëÖPreg(l)[i,j]‚ààR4Rand the class prediction beùëÉ(ùëô)ùëêùëôùë†[ùëñ,ùëó]‚àà‚ÑùùëÅùëêPcls(l)[i,j]‚ààRNc. For bounding box decoding,ùëÉ(ùëô)ùëüùëíùëî[ùëñ,ùëó]Preg(l)[i,j]is partitioned into four componentsùëëùë°,ùëëùëè,ùëëùëô,ùëëùëü‚àà‚ÑùùëÖdt,db,dl,dr‚ààRR, corresponding to the distance distributions for the top, bottom, left, and right sides of the bounding box, respectively. After processing via the mechanism associated with Distribution Focal Loss (DFL), these yield scalar distancesùõøùë°=ùíü(ùëëùë°)Œ¥t=D(dt),ùõøùëè=ùíü(ùëëùëè)Œ¥b=D(db),ùõøùëô=ùíü(ùëëùëô)Œ¥l=D(dl), andùõøùëü=ùíü(ùëëùëü)Œ¥r=D(dr), whereùíü(¬∑)D(¬∑)represents the operation deriving a scalar distance from the learned distribution. The class probabilities are defined asùëù(ùëô)[ùëñ,ùëó]=ùúé(ùëÉ(ùëô)ùëêùëôùë†[ùëñ,ùëó])‚àà[0,1]ùëÅùëêp(l)[i,j]=œÉ(Pcls(l)[i,j])‚àà[0,1]Nc, whereùúéœÉdenotes the sigmoid function. Addressing the imbalance in pollen samples, this study adopts the Focal Loss (‚ÑíùêπùêøLFL) as the classification loss function. Focal Loss introduces a modulating factor(1‚àíùëùùë°)ùõæ(1‚àípt)Œ≥and an optional balancing factorùõºùë°Œ±t,ùõæ‚â•0Œ≥‚â•0is the focusing parameter. Ifùëùùëòpkis the model‚Äôs predicted probability for thek-th class andùëùÃÇùëòp^kis the corresponding ground truth label (typically 0 or 1), the Focal Loss for thek-th class is:‚Ñí(ùëò)ùêπùêø=‚àíùõºùëò(1‚àíùëùùë°,ùëò)ùõælog(ùëùùë°,ùëò)LFL(k)=‚àíŒ±k(1‚àípt,k)Œ≥log(pt,k)(4)whereùëùùë°,ùëò=ùëùùëòpt,k=pkifùëùÃÇùëò=1p^k=1, andùëùùë°,ùëò=1‚àíùëùùëòpt,k=1‚àípkifùëùÃÇùëò=0p^k=0. The total loss for object detection,‚Ñíùê∑ùëíùë°ùëáùëúùë°ùëéùëôLDetTotal, is a weighted sum of the regression and classification losses, with weightsùúÜùëüùëíùëîŒªregandùúÜùëêùëôùë†Œªclsrespectively:‚Ñíùê∑ùëíùë°ùëáùëúùë°ùëéùëô=ùúÜùëüùëíùëî‚Ñíùëüùëíùëî+ùúÜùëêùëôùë†‚Ñíùëêùëôùë†LDetTotal=ŒªregLreg+ŒªclsLcls(5)Here,‚Ñíùëêùëôùë†Lclsis the sum of Focal Losses computed over all positive samples and selected negative samples. The bounding box regression loss (‚ÑíùëüùëíùëîLreg) employs Distribution Focal Loss (DFL) to learn the distribution of distances from the anchor/reference point to the four sides of the bounding box. In all experiments, we combine DFL with Complete IoU (CIoU) loss as implemented in the Ultralytics framework, following recent one-stage detectors that adopt CIoU as the default choice for bounding box regression. Although other IoU variants such as GIoU and SIoU are supported by the underlying framework, we did not perform a systematic ablation over these alternatives in this work and therefore focus our analysis on the CIoU-based configuration. The classification loss (‚Ñíùëêùëôùë†Lcls), as previously mentioned, is calculated using Focal Loss. In practice, we follow the Ultralytics implementation and adopt the default loss gains for the detection task, setting the contributions of the regression and classification components such that the box, classification, and DFL terms are weighted with gains of 7.5, 0.5, and 1.5, respectively. For the detection Transformer baselines that employ Focal Loss within a DETR-style objective, we use a focusing parameter ofùõæ=1.5Œ≥=1.5and a class-balancing factor ofùõº=0.25Œ±=0.25, consistent with common practice in long-tailed object detection. We observed that moderate variations around these values do not alter the relative ranking of models, so we report results under this well-established configuration without exhaustively tuning the loss hyperparameters. In addition to Focal Loss, we also considered several alternative strategies for mitigating the long-tailed class imbalance in our pollen dataset, including simple inverse-frequency class re-weighting, class-balanced loss based on effective numbers of samples, and instance-level over-sampling of rare classes. However, in preliminary experiments these alternatives did not yield consistent improvements in mAP compared to the standard Focal Loss configuration and in some cases slightly degraded training stability. For this reason, and in line with common practice in modern one-stage detectors, we adopt Focal Loss as the final classification loss, while relying on careful dataset construction and augmentation to further alleviate imbalance effects.",
            "3. Results and Discussion": "3.1. Dataset PreparationTo facilitate the development and rigorous evaluation of HieraEdgeNet, we constructed a large-scale pollen detection dataset comprising 120 distinct pollen categories. The dataset was aggregated from two primary sources: (1) several existing, manually annotated pollen detection datasets, and (2) a novel data synthesis pipeline designed to convert a vast collection of single-grain classification images into detection samples with precise bounding box annotations. This synthesis strategy involves programmatically embedding individual pollen grain images onto authentic microscopy backgrounds, thereby substantially augmenting both the scale and diversity of the training data. The final dataset exhibits a characteristic long-tailed class distribution (Figure 3), which closely mimics real-world scenarios. This feature is critical for training a high-performance model that is robust in practical applications. For model training, the dataset was partitioned into a training set (80%) and a validation set (20%), maintaining a proportional class distribution based on the number of instances per category. The model was trained for 500 epochs. During training, we employed a suite of data augmentation techniques‚Äîincluding mosaic augmentation, Gaussian blur, and color space shifting‚Äîto enhance the model‚Äôs adaptability and generalization to complex, unseen target domains.In addition, we performed a cross-dataset validation protocol, in which the model is trained on a subset of the aggregated datasets and evaluated on the remaining held-out datasets; the observed performance trends are consistent with those reported on the full benchmark, suggesting that HieraEdgeNet can generalize reasonably well across different pollen imaging environments.Figure 3.The distribution of different pollen types in the dataset, highlighting the diversity and scale of the training samples.Unless otherwise noted, all detectors considered in this study were trained using the Ultralytics detection framework with a one-cycle learning rate schedule. We set the initial learning rate to 0.01 and annealed it to a final value of1√ó10‚àí41√ó10‚àí4over 500 epochs, including 3 warm-up epochs, using stochastic gradient descent with momentum 0.9 and weight decay of5√ó10‚àí45√ó10‚àí4. The training process was executed on a high-performance computing node equipped with four NVIDIA V100 GPUs (NVIDIA Corp., Santa Clara, CA, USA), with a global batch size fixed at 16. Data augmentation followed the default Ultralytics detection pipeline, including HSV color jitter (hue = 0.015, saturation = 0.7, value = 0.4), random scaling in the range [0.5, 1.5], random translation up to 0.1 of the image size, horizontal flipping with probability 0.5, and mosaic composition with probability 1.0, while mixup and copy-paste augmentations were disabled. The complete YAML configuration files and training scripts used for all experiments are released in our open-source repository, which integrates HieraEdgeNet into the Ultralytics training system and allows the reported results to be reproduced with a single training command. 3.2. Quantitative Evaluation and BenchmarkingTo systematically evaluate the performance of our proposed HieraEdgeNet architecture, we conducted a comprehensive set of experiments, benchmarking it against several state-of-the-art (SOTA) real-time object detectors. These baseline models include the CNN-based YOLOv11n [47] and YOLOv12n [14], as well as the Transformer-based RT-DETR-R18 and RT-DETR-R50 [48]. Furthermore, we conducted two ablation studies on HieraEdgeNet. First, we created a hybrid model by integrating its backbone with the detection head of RT-DETR (designated HieraEdgeNet-RT-DETR) to validate the versatility and feature extraction capability of its backbone. Second, we applied the Layer-wise Automated Model Pruning (LAMP) structured pruning technique [49] to create a compressed variant (HieraEdgeNet-LAMP), aiming to explore its deployment potential in resource-constrained scenarios. Our evaluation employs standard COCO metrics, including mAP@0.5, mAP@0.75, and the primary metric mAP@0.5:0.95, alongside a comprehensive assessment of model parameters, computational complexity (GFLOPs), and inference speed (FPS).During validation and test-time evaluation, all detectors are post-processed using the standard greedy IoU-based NMS provided by the Ultralytics framework. Unless otherwise specified, we set the NMS IoU threshold to 0.7 and use a low base confidence threshold of 0.001 to collect candidate boxes for COCO-style mAP computation and for generating precision‚Äìrecall and F1 curves. For practical deployment, we recommend using the operating point corresponding to the peak F1 score (confidence threshold 0.43 as shown inFigure 4). We did not enable soft-NMS or other alternative suppression schemes in our experiments, as preliminary checks did not reveal consistent benefits for the spatial densities observed in our pollen dataset.Figure 4.Detailed performance validation of the proposed HieraEdgeNet model. (a) The confusion matrix for all pollen classes, demonstrating high classification accuracy and minimal inter-class confusion. (b) The Precision-Recall (P-R) curve, with the mean Average Precision (mAP) value over all classes indicated. (c) The F1 score curve as a function of the confidence threshold, highlighting the peak F1 score and the corresponding optimal threshold.We evaluate model efficiency using GFLOPs to quantify theoretical computational complexity and FPS to measure practical inference throughput. While a threshold of 30 FPS is typically sufficient for real-time applications, HieraEdgeNet achieves 361.17 FPS‚Äîand its pruned variant 403.24 FPS‚Äîat a resolution of640√ó640640√ó640on a single NVIDIA V100 GPU (NVIDIA Corp., Santa Clara, CA, USA) (Table 2). Surpassing the real-time benchmark by an order of magnitude, these results confirm that the enhanced feature extraction modules (Omni-Kernel and multi-scale fusion) do not compromise the system‚Äôs suitability for high-throughput microscopic analysis. We report parameters, GFLOPs, and FPS as hardware-agnostic indicators of computational cost, and deliberately refrain from summarizing training in terms of a single ‚ÄúGPU-hours‚Äù figure, since such a number depends strongly on specific hardware, cluster scheduling, and power-management policies and would not be directly comparable across deployment scenarios.Table 2.Quantitative performance evaluation of HieraEdgeNet and its variants against state-of-the-art detectors. The comparison includes metrics for model complexity (Parameters, GFLOPs, Model Size), inference speed (FPS), and detection accuracy (mAP@0.5, mAP@0.75, mAP@0.5:0.95) on the pollen dataset. The better results in each accuracy column are highlighted in bold.In practice, we follow the LAMP framework by computing layer-wise sensitivity scores for all convolutional channels and pruning those with the lowest scores under a global sparsity target. The pruning thresholds are tuned on a held-out validation subset to achieve an approximate 17% reduction in computational cost (from 14.0 to 11.6 GFLOPs) while constraining the drop in mAP@0.5:0.95 to less than 1.0 percentage point (0.8444 to 0.8363), thereby balancing compression and accuracy for the HieraEdgeNet-LAMP variant.The quantitative results, presented inTable 2, clearly demonstrate that our proposed HieraEdgeNet achieves the highest overall detection accuracy among the evaluated models. Its mAP@0.5:0.95 score of 0.8444 surpasses the advanced, similarly-sized models YOLOv12n and YOLOv11n by 1.29 and 2.09 percentage points, respectively. This significant accuracy gain provides strong evidence for the effectiveness of our designed HierarchicalEdgeModule, SynergisticEdgeFusion, and CSPOmni-Kernel modules in enhancing feature representation. Although this precision enhancement is accompanied by a moderate increase in computational cost (GFLOPs of 14.0 versus 6.7 for YOLOv12n), this trade-off is highly valuable given the stringent accuracy requirements of the pollen recognition task.The advantages of HieraEdgeNet are even more pronounced when compared to the Transformer-based RT-DETR models. In terms of accuracy, HieraEdgeNet‚Äôs mAP@0.5:0.95 outperforms RT-DETR-R18 by 3.7 percentage points and RT-DETR-R50 by 5.48 percentage points. In terms of efficiency, HieraEdgeNet‚Äôs model size (3.88 M parameters/14 GFLOPs) is substantially lower than that of RT-DETR-R18 (20.03 M/57.5 GFLOPs) and RT-DETR-R50 (42.18 M/126.2 GFLOPs). This indicates that our architecture, through deep structural innovation within a CNN framework, achieves a superior accuracy‚Äìefficiency balance compared to leading Transformer-based detectors.When our backbone is integrated with the RT-DETR decoder, the resulting hybrid model (HieraEdgeNet-RT-DETR) achieves an mAP@0.5:0.95 of 0.8492‚Äîthe highest of all models tested. This result compellingly demonstrates the high quality and generalizability of the features produced by our backbone, capable of providing robust support for diverse detector heads and surpassing the original RT-DETR-R18 and R50. Finally, addressing the practical need for lightweight models, our pruned HieraEdgeNet-LAMP variant achieves a remarkable mAP@0.5:0.95 of 0.8363 with significantly reduced parameters and computation. This accuracy not only exceeds that of the RT-DETR series but also both YOLOv11n and YOLOv12n, while maintaining a highly competitive inference speed (403.24 FPS). This showcases the excellent compressibility of the HieraEdgeNet architecture, its superior accuracy‚Äìefficiency curve, and its significant potential for practical applications.The detection performance of HieraEdgeNet was further analyzed, as depicted inFigure 4. The confusion matrix (Figure 4a) exhibits a distinct and highly concentrated diagonal, indicating extremely high classification accuracy across the 46 pollen classes with minimal inter-class confusion. The Precision-Recall (P-R) curve (Figure 4b) further corroborates the model‚Äôs exceptional performance, achieving a mean Average Precision (mAP) of 0.976 over all classes. The broad area under the curve demonstrates that the model maintains high precision across various recall levels. The F1-score curve (Figure 4c) illustrates the model‚Äôs comprehensive performance at different confidence thresholds, reaching a peak F1-score of 0.938 at an optimal confidence threshold of 0.43. This provides a reliable basis for selecting the optimal operating point for the model in practical deployments. Baseline detectors are compared quantitatively inTable 2andTable 3rather than overlaid inFigure 4, in order to avoid visual clutter and to keep the diagnostic plots focused on the proposed HieraEdgeNet model.Table 3.Ablation study of the core components of HieraEdgeNet. Model A serves as the high-performance YOLOv12n baseline. Models B‚ÄìD systematically assess the impact of incorporating HEM, SEF, and CSPOKM, either individually or in partial combinations. A checkmark (‚úì) signifies the presence of a module, whereas a cross (‚úó) indicates its absence. Model E represents the full HieraEdgeNet architecture integrating all three modules. For reference, results from a state-of-the-art model, RT-DETR (Model F), and another variant (Model G) are also presented.In summary, both the horizontal comparisons against state-of-the-art CNN and Transformer detectors and the vertical analyses of our model variants consistently affirm that the HieraEdgeNet architecture delivers superior precision and efficiency for automated pollen recognition through its deep mining and fusion of edge information and multi-scale features. 3.3. Feature AblationTo validate the efficacy of the individual innovative components within HieraEdgeNet and to elucidate their interplay, we performed a comprehensive set of ablation studies (Table 3). The results unequivocally demonstrate a profound synergistic effect among the three core modules: HEM, SEF, and CSPOKM. Notably, the isolated introduction of any single module or their partial combinations (Models B, C, and D) failed to outperform the baseline model (Model A).More specifically, Model B, in which only HEM is enabled, shows that generating multi-scale edge mapsùê∏ùëÉ3EP3,ùê∏ùëÉ4EP4, andùê∏ùëÉ5EP5is insufficient when these edge priors cannot be systematically fused into the semantic hierarchy due to the absence of SEF. In this configuration, the additional edge responses remain only weakly coupled to task-specific semantic features and may even over-emphasize background boundaries, so the detector is unable to convert them into consistent gains in mAP. Model D activates only CSPOKM on top of the baseline. Although the omni-kernel convolutions enrich the receptive field and refine local details, they operate on backbone representations that do not encode explicit edge‚Äìsemantic fusion; consequently, the module primarily redistributes existing responses instead of introducing new boundary cues, which explains its limited standalone impact. SEF itself is designed as a fusion operator that aligns and reweights the semantic and edge streams. When all three modules are present in Model E, SEF projects the multi-scale edge priors extracted by HEM into the backbone semantic space and provides edge-aware inputs to CSPOKM, enabling the complete HieraEdgeNet architecture to fully exploit boundary information and achieve the substantial performance gain reported inTable 3.Conversely, only when all three modules operate in concert does the complete HieraEdgeNet architecture (Model E) exhibit a substantial performance gain, achieving an mAP@0.5:0.95 of 0.8444. This result markedly outperforms both the baseline (0.8315) and the state-of-the-art RT-DETR model (0.7896), thereby validating the holistic and innovative design of our proposed architecture. 3.4. Visual Analysis of Enhanced Edge PerceptionFigure 5presents a comparative visualization using Gradient-weighted Class Activation Mapping (Grad-CAM) [50] heatmaps for four models‚ÄîHieraEdgeNet, YOLOv12n, HieraEdgeNet-RT-DETR, and RT-DETR-R50‚Äîat both the backbone‚Äôs output layer and the detection head‚Äôs input layer. These heatmaps visually render the model‚Äôs attention intensity across the input image, with a specific focus on the edges and structural details of pollen grains.Figure 5.Visual analysis of HieraEdgeNet‚Äôs enhanced edge perception. Grad-CAM heatmaps compare the activation focus of HieraEdgeNet against baseline models at two key stages: the backbone output (top row) and the detection head input (bottom row). Warm colors (for example, red and yellow) indicate higher activation intensity, whereas cool colors (for example, blue) indicate lower activation. HieraEdgeNet shows a distinctly sharper and more accurate focus on the pollen grain edges, which is crucial for its superior performance.Our analysis reveals that HieraEdgeNet generates more refined and sharply focused responses along the object boundaries, reflecting the HEM‚Äôs effective capture of edge information. This advantage is further amplified at the input of the detection head, where the representation of target edges is enhanced. In contrast, models like YOLOv12n and RT-DETR-R50 do not exhibit a comparable level of edge sensitivity, particularly at the detection head‚Äôs input layer, where their edge responses appear more diffuse. This suggests a deficiency in their explicit utilization of edge information, which may consequently limit their localization accuracy. In summary, the performance superiority of HieraEdgeNet is rooted in its ability to generate more precise and less noisy feature maps‚Äîa capability that is crucial for precise object localization within complex microscopic environments.",
            "3.1. Dataset Preparation": "To facilitate the development and rigorous evaluation of HieraEdgeNet, we constructed a large-scale pollen detection dataset comprising 120 distinct pollen categories. The dataset was aggregated from two primary sources: (1) several existing, manually annotated pollen detection datasets, and (2) a novel data synthesis pipeline designed to convert a vast collection of single-grain classification images into detection samples with precise bounding box annotations. This synthesis strategy involves programmatically embedding individual pollen grain images onto authentic microscopy backgrounds, thereby substantially augmenting both the scale and diversity of the training data. The final dataset exhibits a characteristic long-tailed class distribution (Figure 3), which closely mimics real-world scenarios. This feature is critical for training a high-performance model that is robust in practical applications. For model training, the dataset was partitioned into a training set (80%) and a validation set (20%), maintaining a proportional class distribution based on the number of instances per category. The model was trained for 500 epochs. During training, we employed a suite of data augmentation techniques‚Äîincluding mosaic augmentation, Gaussian blur, and color space shifting‚Äîto enhance the model‚Äôs adaptability and generalization to complex, unseen target domains.In addition, we performed a cross-dataset validation protocol, in which the model is trained on a subset of the aggregated datasets and evaluated on the remaining held-out datasets; the observed performance trends are consistent with those reported on the full benchmark, suggesting that HieraEdgeNet can generalize reasonably well across different pollen imaging environments. Figure 3.The distribution of different pollen types in the dataset, highlighting the diversity and scale of the training samples. Unless otherwise noted, all detectors considered in this study were trained using the Ultralytics detection framework with a one-cycle learning rate schedule. We set the initial learning rate to 0.01 and annealed it to a final value of1√ó10‚àí41√ó10‚àí4over 500 epochs, including 3 warm-up epochs, using stochastic gradient descent with momentum 0.9 and weight decay of5√ó10‚àí45√ó10‚àí4. The training process was executed on a high-performance computing node equipped with four NVIDIA V100 GPUs (NVIDIA Corp., Santa Clara, CA, USA), with a global batch size fixed at 16. Data augmentation followed the default Ultralytics detection pipeline, including HSV color jitter (hue = 0.015, saturation = 0.7, value = 0.4), random scaling in the range [0.5, 1.5], random translation up to 0.1 of the image size, horizontal flipping with probability 0.5, and mosaic composition with probability 1.0, while mixup and copy-paste augmentations were disabled. The complete YAML configuration files and training scripts used for all experiments are released in our open-source repository, which integrates HieraEdgeNet into the Ultralytics training system and allows the reported results to be reproduced with a single training command.",
            "3.2. Quantitative Evaluation and Benchmarking": "To systematically evaluate the performance of our proposed HieraEdgeNet architecture, we conducted a comprehensive set of experiments, benchmarking it against several state-of-the-art (SOTA) real-time object detectors. These baseline models include the CNN-based YOLOv11n [47] and YOLOv12n [14], as well as the Transformer-based RT-DETR-R18 and RT-DETR-R50 [48]. Furthermore, we conducted two ablation studies on HieraEdgeNet. First, we created a hybrid model by integrating its backbone with the detection head of RT-DETR (designated HieraEdgeNet-RT-DETR) to validate the versatility and feature extraction capability of its backbone. Second, we applied the Layer-wise Automated Model Pruning (LAMP) structured pruning technique [49] to create a compressed variant (HieraEdgeNet-LAMP), aiming to explore its deployment potential in resource-constrained scenarios. Our evaluation employs standard COCO metrics, including mAP@0.5, mAP@0.75, and the primary metric mAP@0.5:0.95, alongside a comprehensive assessment of model parameters, computational complexity (GFLOPs), and inference speed (FPS).During validation and test-time evaluation, all detectors are post-processed using the standard greedy IoU-based NMS provided by the Ultralytics framework. Unless otherwise specified, we set the NMS IoU threshold to 0.7 and use a low base confidence threshold of 0.001 to collect candidate boxes for COCO-style mAP computation and for generating precision‚Äìrecall and F1 curves. For practical deployment, we recommend using the operating point corresponding to the peak F1 score (confidence threshold 0.43 as shown inFigure 4). We did not enable soft-NMS or other alternative suppression schemes in our experiments, as preliminary checks did not reveal consistent benefits for the spatial densities observed in our pollen dataset. Figure 4.Detailed performance validation of the proposed HieraEdgeNet model. (a) The confusion matrix for all pollen classes, demonstrating high classification accuracy and minimal inter-class confusion. (b) The Precision-Recall (P-R) curve, with the mean Average Precision (mAP) value over all classes indicated. (c) The F1 score curve as a function of the confidence threshold, highlighting the peak F1 score and the corresponding optimal threshold. We evaluate model efficiency using GFLOPs to quantify theoretical computational complexity and FPS to measure practical inference throughput. While a threshold of 30 FPS is typically sufficient for real-time applications, HieraEdgeNet achieves 361.17 FPS‚Äîand its pruned variant 403.24 FPS‚Äîat a resolution of640√ó640640√ó640on a single NVIDIA V100 GPU (NVIDIA Corp., Santa Clara, CA, USA) (Table 2). Surpassing the real-time benchmark by an order of magnitude, these results confirm that the enhanced feature extraction modules (Omni-Kernel and multi-scale fusion) do not compromise the system‚Äôs suitability for high-throughput microscopic analysis. We report parameters, GFLOPs, and FPS as hardware-agnostic indicators of computational cost, and deliberately refrain from summarizing training in terms of a single ‚ÄúGPU-hours‚Äù figure, since such a number depends strongly on specific hardware, cluster scheduling, and power-management policies and would not be directly comparable across deployment scenarios. Table 2.Quantitative performance evaluation of HieraEdgeNet and its variants against state-of-the-art detectors. The comparison includes metrics for model complexity (Parameters, GFLOPs, Model Size), inference speed (FPS), and detection accuracy (mAP@0.5, mAP@0.75, mAP@0.5:0.95) on the pollen dataset. The better results in each accuracy column are highlighted in bold. In practice, we follow the LAMP framework by computing layer-wise sensitivity scores for all convolutional channels and pruning those with the lowest scores under a global sparsity target. The pruning thresholds are tuned on a held-out validation subset to achieve an approximate 17% reduction in computational cost (from 14.0 to 11.6 GFLOPs) while constraining the drop in mAP@0.5:0.95 to less than 1.0 percentage point (0.8444 to 0.8363), thereby balancing compression and accuracy for the HieraEdgeNet-LAMP variant. The quantitative results, presented inTable 2, clearly demonstrate that our proposed HieraEdgeNet achieves the highest overall detection accuracy among the evaluated models. Its mAP@0.5:0.95 score of 0.8444 surpasses the advanced, similarly-sized models YOLOv12n and YOLOv11n by 1.29 and 2.09 percentage points, respectively. This significant accuracy gain provides strong evidence for the effectiveness of our designed HierarchicalEdgeModule, SynergisticEdgeFusion, and CSPOmni-Kernel modules in enhancing feature representation. Although this precision enhancement is accompanied by a moderate increase in computational cost (GFLOPs of 14.0 versus 6.7 for YOLOv12n), this trade-off is highly valuable given the stringent accuracy requirements of the pollen recognition task. The advantages of HieraEdgeNet are even more pronounced when compared to the Transformer-based RT-DETR models. In terms of accuracy, HieraEdgeNet‚Äôs mAP@0.5:0.95 outperforms RT-DETR-R18 by 3.7 percentage points and RT-DETR-R50 by 5.48 percentage points. In terms of efficiency, HieraEdgeNet‚Äôs model size (3.88 M parameters/14 GFLOPs) is substantially lower than that of RT-DETR-R18 (20.03 M/57.5 GFLOPs) and RT-DETR-R50 (42.18 M/126.2 GFLOPs). This indicates that our architecture, through deep structural innovation within a CNN framework, achieves a superior accuracy‚Äìefficiency balance compared to leading Transformer-based detectors.When our backbone is integrated with the RT-DETR decoder, the resulting hybrid model (HieraEdgeNet-RT-DETR) achieves an mAP@0.5:0.95 of 0.8492‚Äîthe highest of all models tested. This result compellingly demonstrates the high quality and generalizability of the features produced by our backbone, capable of providing robust support for diverse detector heads and surpassing the original RT-DETR-R18 and R50. Finally, addressing the practical need for lightweight models, our pruned HieraEdgeNet-LAMP variant achieves a remarkable mAP@0.5:0.95 of 0.8363 with significantly reduced parameters and computation. This accuracy not only exceeds that of the RT-DETR series but also both YOLOv11n and YOLOv12n, while maintaining a highly competitive inference speed (403.24 FPS). This showcases the excellent compressibility of the HieraEdgeNet architecture, its superior accuracy‚Äìefficiency curve, and its significant potential for practical applications. The detection performance of HieraEdgeNet was further analyzed, as depicted inFigure 4. The confusion matrix (Figure 4a) exhibits a distinct and highly concentrated diagonal, indicating extremely high classification accuracy across the 46 pollen classes with minimal inter-class confusion. The Precision-Recall (P-R) curve (Figure 4b) further corroborates the model‚Äôs exceptional performance, achieving a mean Average Precision (mAP) of 0.976 over all classes. The broad area under the curve demonstrates that the model maintains high precision across various recall levels. The F1-score curve (Figure 4c) illustrates the model‚Äôs comprehensive performance at different confidence thresholds, reaching a peak F1-score of 0.938 at an optimal confidence threshold of 0.43. This provides a reliable basis for selecting the optimal operating point for the model in practical deployments. Baseline detectors are compared quantitatively inTable 2andTable 3rather than overlaid inFigure 4, in order to avoid visual clutter and to keep the diagnostic plots focused on the proposed HieraEdgeNet model. Table 3.Ablation study of the core components of HieraEdgeNet. Model A serves as the high-performance YOLOv12n baseline. Models B‚ÄìD systematically assess the impact of incorporating HEM, SEF, and CSPOKM, either individually or in partial combinations. A checkmark (‚úì) signifies the presence of a module, whereas a cross (‚úó) indicates its absence. Model E represents the full HieraEdgeNet architecture integrating all three modules. For reference, results from a state-of-the-art model, RT-DETR (Model F), and another variant (Model G) are also presented. In summary, both the horizontal comparisons against state-of-the-art CNN and Transformer detectors and the vertical analyses of our model variants consistently affirm that the HieraEdgeNet architecture delivers superior precision and efficiency for automated pollen recognition through its deep mining and fusion of edge information and multi-scale features.",
            "3.3. Feature Ablation": "To validate the efficacy of the individual innovative components within HieraEdgeNet and to elucidate their interplay, we performed a comprehensive set of ablation studies (Table 3). The results unequivocally demonstrate a profound synergistic effect among the three core modules: HEM, SEF, and CSPOKM. Notably, the isolated introduction of any single module or their partial combinations (Models B, C, and D) failed to outperform the baseline model (Model A). More specifically, Model B, in which only HEM is enabled, shows that generating multi-scale edge mapsùê∏ùëÉ3EP3,ùê∏ùëÉ4EP4, andùê∏ùëÉ5EP5is insufficient when these edge priors cannot be systematically fused into the semantic hierarchy due to the absence of SEF. In this configuration, the additional edge responses remain only weakly coupled to task-specific semantic features and may even over-emphasize background boundaries, so the detector is unable to convert them into consistent gains in mAP. Model D activates only CSPOKM on top of the baseline. Although the omni-kernel convolutions enrich the receptive field and refine local details, they operate on backbone representations that do not encode explicit edge‚Äìsemantic fusion; consequently, the module primarily redistributes existing responses instead of introducing new boundary cues, which explains its limited standalone impact. SEF itself is designed as a fusion operator that aligns and reweights the semantic and edge streams. When all three modules are present in Model E, SEF projects the multi-scale edge priors extracted by HEM into the backbone semantic space and provides edge-aware inputs to CSPOKM, enabling the complete HieraEdgeNet architecture to fully exploit boundary information and achieve the substantial performance gain reported inTable 3. Conversely, only when all three modules operate in concert does the complete HieraEdgeNet architecture (Model E) exhibit a substantial performance gain, achieving an mAP@0.5:0.95 of 0.8444. This result markedly outperforms both the baseline (0.8315) and the state-of-the-art RT-DETR model (0.7896), thereby validating the holistic and innovative design of our proposed architecture.",
            "3.4. Visual Analysis of Enhanced Edge Perception": "Figure 5presents a comparative visualization using Gradient-weighted Class Activation Mapping (Grad-CAM) [50] heatmaps for four models‚ÄîHieraEdgeNet, YOLOv12n, HieraEdgeNet-RT-DETR, and RT-DETR-R50‚Äîat both the backbone‚Äôs output layer and the detection head‚Äôs input layer. These heatmaps visually render the model‚Äôs attention intensity across the input image, with a specific focus on the edges and structural details of pollen grains. Figure 5.Visual analysis of HieraEdgeNet‚Äôs enhanced edge perception. Grad-CAM heatmaps compare the activation focus of HieraEdgeNet against baseline models at two key stages: the backbone output (top row) and the detection head input (bottom row). Warm colors (for example, red and yellow) indicate higher activation intensity, whereas cool colors (for example, blue) indicate lower activation. HieraEdgeNet shows a distinctly sharper and more accurate focus on the pollen grain edges, which is crucial for its superior performance. Our analysis reveals that HieraEdgeNet generates more refined and sharply focused responses along the object boundaries, reflecting the HEM‚Äôs effective capture of edge information. This advantage is further amplified at the input of the detection head, where the representation of target edges is enhanced. In contrast, models like YOLOv12n and RT-DETR-R50 do not exhibit a comparable level of edge sensitivity, particularly at the detection head‚Äôs input layer, where their edge responses appear more diffuse. This suggests a deficiency in their explicit utilization of edge information, which may consequently limit their localization accuracy. In summary, the performance superiority of HieraEdgeNet is rooted in its ability to generate more precise and less noisy feature maps‚Äîa capability that is crucial for precise object localization within complex microscopic environments.",
            "4. Conclusions": "In conclusion, HieraEdgeNet establishes a new benchmark for automated pollen recognition. Beyond achieving a state-of-the-art mean Average Precision (mAP), our framework demonstrates significant computational efficiency. Notably, by strategically enhancing features with edge priors rather than relying on the computationally intensive self-attention mechanisms of Transformer-based models, HieraEdgeNet maintains high accuracy while reducing inference time, making it a viable solution for real-world, high-throughput applications. The measured throughput of over 360 FPS for the full model and over 400 FPS for the pruned variant at640√ó640640√ó640input resolution demonstrates that, on modern GPUs, HieraEdgeNet can comfortably satisfy real-time requirements in practical microscopic imaging pipelines. The efficacy of HieraEdgeNet is empirically substantiated by extensive experiments. In benchmark comparisons against state-of-the-art real-time object detectors, including YOLOv12n and RT-DETR, HieraEdgeNet achieved superior performance, significantly surpassing all baseline models on the critical mAP@0.5:0.95 metric. Furthermore, the HieraEdgeNet backbone demonstrated remarkable generalization capabilities when integrated with the RT-DETR decoder. Moreover, a structurally pruned version of the model retained high accuracy while exhibiting outstanding potential for practical deployment. Qualitative analysis using Grad-CAM visualizations further confirmed that our architecture generates feature responses that are more precisely focused and localized to object boundaries. Despite its demonstrated high accuracy, HieraEdgeNet has limitations, primarily stemming from the substantial computational cost introduced by advanced components such as the Omni-Kernel. Additionally, its inherently two-dimensional design precludes the direct utilization of three-dimensional (Z-stack) microscopy data, and its performance may be challenged by domain shifts between training data and real-world samples. Consequently, our future work will focus on three key directions: (1) optimizing computational efficiency through techniques like model compression and acceleration; (2) extending the edge-enhancement framework to three dimensions to process volumetric data; and (3) employing domain adaptation methods to improve the model‚Äôs generalization and robustness across varied acquisition conditions.While the multi-source dataset construction and extensive data augmentation already provide some robustness to variations in illumination, contrast, and focus across laboratories, we acknowledge that larger domain shifts between training data and real-world deployments remain challenging, motivating our planned exploration of domain adaptation techniques."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2077-0472/15/23/2518",
        "scraped_at": "2025-12-05 23:55:58"
    },
    {
        "title": "Spatiotemporal Dynamics of Local Climate Zones and Their Impacts on Land Surface Temperature in the Guangdong‚ÄìHong Kong‚ÄìMacao Greater Bay Area",
        "authors": "byYang LuandDawei Wen",
        "journal": "Land2025,14(12), 2370;https://doi.org/10.3390/land14122370- 4 Dec 2025",
        "abstract": "Understanding how long-term local climate zone (LCZ) dynamics interact with rapid urbanization and land surface temperature (LST) changes is essential for sustainable planning in megaregion-scale urban clusters. In this paper, we propose a multi-feature local sample transfer method to obtain LCZ maps from 2000 to 2020 in the Guangdong‚ÄìHong Kong‚ÄìMacao Greater Bay Area (GBA) and then analyze spatiotemporal changes in LCZs and their impacts on surface thermal environments. Results show the following: (1) The proposed multi-feature local sample transfer approach significantly improves the efficiency of long-term LCZ mapping by greatly reducing the effort required for sample acquisition. (2) The built types (LCZ1‚Äì10) increased by 1.34% overall, with large low-rise (LCZ8) showing the greatest expansion (4.72%). The compact low-rise (LCZ3) was the only built type to decline, decreasing by 2.02%. (3) Urbanization has produced a contiguous warming core that expands outward from the central metropolitan zones, thereby promoting the UHI coalescence. (4) Dense trees (LCZA) and large low-rise (LCZ8) exerted the strongest influence on LST. Large low-rise (LCZ8) consistently exhibited the highest warming contribution in Foshan, Zhongshan, and Dongguan. In coastal cities including Shenzhen, Hong Kong, and Macao, the largest LST increases occurred when water (LCZG) areas were converted to bare rock or paved (LCZE) or cs (LCZ1‚Äì10). Overall, the results highlight the strong coupling between urbanization and surface heating, providing critical insights for urban climate adaptation and integrated land-use planning in rapidly urbanizing megaregions.Keywords:urban heat island;local climate zone;land surface temperature;spatiotemporal dynamics",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Amid accelerating global warming and urbanization, land cover changes and intensified human activities have exacerbated urban heat island (UHI) effects, posing increasing threats to public health, urban livability, and socioeconomic sustainability. The UHI effect refers to the phenomenon that urban centers exhibit higher temperatures than suburban or rural areas. However, the complexity of urban‚Äìrural systems and the difficulty of defining their boundaries [1] introduce substantial uncertainties in quantifying UHI intensity. Moreover, describing thermal differences solely based on geographic location (urban vs. rural) overlooks critical surface characteristics‚Äîsuch as land cover, urban morphology, and function‚Äîthat govern local climatic variations [2]. To address these limitations, Stewart and Oke proposed the local climate zone (LCZ) system, a land-cover-based system specifically designed for urban climate and UHI studies. LCZs are spatial units ranging from several hundred meters to a few kilometers in horizontal extent, characterized by internally homogeneous surface cover, structure, materials, and human activity [1]. Compared with conventional land-cover classifications, LCZs provide a more detailed representation of both built and natural surface types. Built LCZs are distinguished by factors such as building height, surface fraction, spatial arrangement, and vegetation cover, while land-cover LCZs are classified primarily by vegetation height and coverage ratio. Thus, LCZs not only provide a standardized framework for describing UHI effects but also capture the spatial variations in human-modified landscapes. By reflecting the complexity and diversity of urban climates and surface characteristics, the LCZ framework has been widely applied in urban thermal environment studies [3,4,5]. Recent advances in remote sensing and computational technologies have further stimulated interest in long-term LCZ analyses, a research frontier that bridges urban climatology and landscape ecology [6,7,8]. Rather than focusing solely on the spatial distribution of LCZs at a single time point, these studies emphasize multi-temporal and multi-decadal dynamics and their climatic implications. Time-series LCZ datasets enable researchers to quantify urban expansion patterns, track the evolution of the UHI effects, and provide scientific evidence for climate-adaptive urban planning. Most existing single-date LCZ mapping based on remote sensing data has primarily relied on the WUDAPT (World Urban Database Portal Tools) framework [9,10,11] and other supervised classification approaches [12,13,14,15]. Multi-temporal LCZ mapping is commonly conducted by repeating single-date mapping at different time points, with independent training samples for each [6]. However, due to the relatively large number of LCZ types and the substantial samples required, manual sample selection for each time point is both labor-intensive and time-consuming. This limitation makes the approach difficult to apply in large-area or multi-temporal mapping, particularly when samples are unavailable for certain periods. Furthermore, it constrains accurate and consistent classification of time-series imagery in an automated manner. Consequently, automatic or transferable sample acquisition methods that reduce manual effort are essential for effective time-series LCZ mapping. Previous single-date studies examining the relationship between LCZs and land surface temperature (LST) can be broadly categorized into three groups: (i) descriptive analyses of LCZ‚ÄìLST relationships [16], (ii) cross-city comparative studies [17], and (iii) correlation analyses between UHI intensity and surface indicators (e.g., building height, impervious surface fraction) within LCZs [18]. However, such single-date analyses fail to capture dynamic changes and reveal the driving mechanisms of LCZ‚ÄìLST relationships during urbanization. Long-term analyses are therefore essential for a more comprehensive understanding of the spatiotemporal evolution of urban thermal environments. Despite growing interest, long-term studies on LCZs and LST have predominantly focused on the spatial composition of LCZs. Previous research has examined temporal changes in the spatial configuration of LCZs within individual cities and performed basic statistical comparisons of LST across LCZ types at different times [10,19,20]. However, these studies generally lack rigorous quantitative analysis, limiting their capacity to accurately characterize intra-urban variations in LCZs and LST. To address this gap, the present study conducts a comprehensive quantitative assessment of the spatiotemporal dynamics of LCZs and their relationship with LST, aiming to elucidate the influence of LCZ transformations on UHI intensity. The Guangdong‚ÄìHong Kong‚ÄìMacao Greater Bay Area (GBA), one of the world‚Äôs four major bay areas, holds an important strategic position in both China‚Äôs and the global economy. Rapid urbanization has led to extensive land-cover transformations, converting large areas of cropland and coastal wetlands into built-up land. These transformations have induced ecological imbalance and environmental challenges, particularly the intensification of UHI effects [7,21,22]. In the process of advancing ecological civilization, adopting a ‚Äúpeople-centered‚Äù development paradigm and balancing economic growth with environmental protection have become key priorities for the high-quality development of the GBA. Accurately assessing and mitigating the UHI effects has remained a critical concern in territorial spatial planning and urban development. Notably, in July 2022, seventeen Chinese governmental departments jointly issued the National Climate Change Adaptation Strategy 2035, which emphasizes that, under conditions of significant global warming, the GBA faces concentrated climate risks with pronounced cascading effects. The strategy calls for strengthening ecological spatial regulation and coordinating urban greening to mitigate UHI impacts. Therefore, investigating land-cover changes and the UHI effects in the GBA is crucial for implementing context-specific strategies to promote regional sustainable development. This study has two primary objectives: to develop a robust multi-temporal LCZ classification method for detecting change in the GBA and to investigate its relationship with concurrent LST dynamics.",
            "2. Materials and Methods": "2.1. Study AreaThe GBA is located in South China and consists of nine mainland cities in the Pearl River Delta‚ÄîShenzhen (SZ), Guangzhou (GZ), Foshan (FS), Zhongshan (ZS), Dongguan (DG), Zhuhai (ZH), Huizhou (HZ), Jiangmen (JM), and Zhaoqing (ZQ)‚Äîtogether with the Hong Kong (HK) and Macao (MO) Special Administrative Regions (Figure 1). Over the past two decades, the GBA has emerged as one of China‚Äôs most economically dynamic and industrially active regions, as well as one of the fastest urbanizing regions globally. Climatically, the GBA experiences abundant solar radiation, high humidity, and distinct seasonal variations, with a humid subtropical monsoon climate that brings hot, wet summers and mild, dry winters. Topographically, the region is dominated by low-lying plains, with areas above 500 m elevation accounting for only about 3% of the total land area [23]. Consequently, the influence of elevation on LST can be reasonably neglected in analyses of the regional surface thermal environment.Figure 1.Overview of the study area and the area of each city.The GBA provides an ideal setting for LCZ studies, encompassing 14 LCZ types ranging from compact high-rise (LCZ1) to water (LCZG), forming a relatively complete framework. Rapid urbanization has driven intense LCZ transitions, offering dynamic scenarios for observing the evolution of the land surface thermal environment. Moreover, Shenzhen is fully urbanized, while Hong Kong and Macao, as highly urbanized special administrative regions, pose challenges for traditional urban‚Äìrural classification methods in quantifying UHI effects. The LCZ framework, however, can accurately capture the internal structure and thermal characteristics of such built environments, enabling a more rigorous examination of the interactions between urban morphology and climate. 2.2. Sen + Mann‚ÄìKendall Analysis of LST TrendsThe Sen trend analysis is a robust non-parametric statistical method used to detect trends in time-series data. By calculating the median of the sequence, it effectively reduces the influence of outliers, making it suitable for long-term trend detection [24]. However, the Sen method alone cannot determine whether the observed trend is statistically significant. The Mann‚ÄìKendall (MK) test, another non-parametric approach, is widely used to assess the significance of trends in time-series data. It does not require the data to follow a normal distribution and is insensitive to outliers and missing values, making it well-suited for long-term climatic or environmental analyses. Long-term LST data are often affected by atmospheric conditions, sensor noise, and cloud-related disturbances. Although seasonal averaging (e.g., summer mean LST) can mitigate these effects to some extent, anomalies may still remain. Therefore, to enhance the robustness against noise and improve the reliability of the trend detection, the Sen‚Äôs slope estimator and the Mann‚ÄìKendall significance test were combined to analyze LST trends from 2000 to 2020. The combined Sen + Mann‚ÄìKendall trend analysis was conducted in MATLAB (v2024) as follows [25,26].The Sen‚Äôs slope (ùõΩŒ≤) was calculated using:ùõΩ=ùëÄùëíùëëùëñùëéùëõ(ùë•ùëó‚àíùë•ùëñùëó‚àíùëñ),‚àÄùëó>ùëñ,Œ≤=Medianxj‚àíxij‚àíi,‚àÄj>i,(1)whereùë•ùëóxjandùë•ùëñxirepresent the LST values in yearsùëójandùëñi, respectively, andMedian() denotes the median operator. A positive value ofùõΩŒ≤indicates an upward trend in LST, while a negativeùõΩŒ≤implies a downward trend.For a time-seriesùëã=(ùë•1,ùë•2,‚Ä¶,ùë•ùëõ)X=(x1,x2,‚Ä¶,xn), the Mann‚ÄìKendall statistic(ùëÜ)(S)is computed as:ùëÜ=‚àëùëõ‚àí1ùëñ=1‚àëùëõùëó=ùëñ+1ùë†ùëîùëõ(ùë•ùëó‚àíùë•ùëñ)S=‚àëi=1n‚àí1‚àëj=i+1nsgnxj‚àíxi(2)whereùë†ùëîùëõ(ùë•ùëó‚àíùë•ùëñ)=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉ1,ùë•ùëó‚àíùë•ùëñ>00,ùë•ùëó‚àíùë•ùëñ=0‚àí1,ùë•ùëó‚àíùë•ùëñ<0sgnxj‚àíxi=1,xj‚àíxi>00,xj‚àíxi=0‚àí1,xj‚àíxi<0(3)The null hypothesis (ùêª0H0) assumes that the data are randomly distributed with no significant trend, while the alternative hypothesis (ùêª1H1) assumes a monotonic upward or downward trend.When the sample sizeùëõ<10n<10, the significance test is directly based on the statisticùëÜS. At a given significance levelùõºŒ±, if‚à£ùëÜ‚à£‚â•ùëÜùõº/2‚à£S‚à£‚â•SŒ±/2, the null hypothesisùêª0H0is rejected, indicating a significant trend. Conversely, if‚à£ùëÜ‚à£<ùëÜùõº/2‚à£S‚à£<SŒ±/2,ùêª0H0is accepted, indicating no significant trend. Whenùëõ‚â•10n‚â•10,ùëÜSapproximately follows a normal distribution, and the standardized test statisticùëçZis computed as:ùëç=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùëÜ‚àí1ùëâùê¥ùëÖ(ùëÜ)‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö,ùëÜ>00,ùëç=0ùëÜ+1ùëâùê¥ùëÖ(ùëÜ)‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö,ùëÜ<0Z=S‚àí1VAR(S),S>00,Z=0S+1VAR(S),S<0(4)where the variance ofùëÜSis given by:ùëâùê¥ùëÖ(ùëÜ)=(ùëõ(ùëõ‚àí1)(2ùëõ+5)‚àí‚àëùëöùëñ=1ùë°ùëñ(ùë°ùëñ‚àí1)(2ùë°ùëñ+5))/18VAR(S)=nn‚àí12n+5‚àí‚àëi=1mtiti‚àí12ti+5/18(5)Here,ùëõnis the number of data points,ùëömis the number of tied groups in the dataset, andùë°ùëñtirepresents the number of data points in theith group of ties. At a given significance levelùõºŒ±, if‚à£ùëç‚à£>ùëç1‚àíùõº/2‚à£Z‚à£>Z1‚àíŒ±/2, the null hypothesisùêª0H0is rejected, indicating a statistically significant trend. Otherwise,ùêª0H0is accepted, implying that the trend is not significant.The Sen + Mann‚ÄìKendall method provides a robust and widely accepted framework for detecting, quantifying, and assessing the statistical significance of long-term LST trends under noisy environmental conditions. This combined approach minimizes the influence of outliers and data irregularities, ensuring reliable interpretation of temporal thermal variations across complex urban and climatic settings. 2.3. Multi-Temporal LCZ Mapping Based on Multi-Feature Local Sample TransferDue to the diverse LCZ types and the vast spatial extent of the GBA, obtaining sufficient training samples for LCZ mapping is a labor-intensive and time-consuming task, posing challenges for long-term LCZ mapping in the region. Therefore, ensuring mapping accuracy while addressing the issue of sample acquisition is critical for multi-temporal LCZ mapping in the GBA. To tackle this challenge, a multi-feature local sample transfer method based on land-cover classification products and remote sensing imagery was proposed using MATLAB v2024 (Figure 2). This approach reduces reliance on extensive manual sample collection, enhances mapping automation, and facilitates accurate and consistent multi-temporal LCZ mapping in the GBA.Figure 2.The proposed framework for multi-temporal LCZ mapping.(1) Grid design and data preprocessing: A spatial resolution of 100 m effectively captures homogeneous urban fabric units‚Äîsuch as residential areas, industrial zones, parks, or water bodies. If the resolution is too coarse, a single pixel may mix multiple land cover types (e.g., buildings, water, and vegetation), making it unrepresentative of any specific physical surface property. Conversely, if the resolution is too high, it introduces substantial redundancy and results in classification units that are considerably smaller than the spatial scale of physical processes influencing temperature. Moreover, the 100 m resolution aligns naturally with the 30 m resolution of Landsat imagery, conceptually clear and computationally efficient aggregation. As a result, the 100 m resolution has been widely adopted in numerous studies [11,27,28]. Based on existing studies and multiple tests, a spatial resolution of 100 m was adopted for LCZ mapping in the GBA. To minimize the influence of clouds and atmospheric disturbances, atmospherically corrected surface reflectance data (Landsat 5_SR and Landsat 8_SR) with low cloud cover in 2000, 2005, 2010, 2015, and 2020 were acquired from the USGS. All images were mosaicked and clipped to the GBA boundary.(2) Sample collection (2020): Training samples for 2020 were delineated based on the high-resolution imagery from Google Earth. After careful visual inspection and cross-validation, 2876 samples were obtained, with 75% used for training and 25% reserved for accuracy assessment.(3) Feature extraction (2020): Two groups of features were designed to comprehensively characterize LCZ patterns:‚ûÄ Remote sensing imagery: All nine multispectral and thermal infrared bands of Landsat 8_SR (30 m resolution) were linearly resampled to 10 m. To enhance spectral information and improve classification performance, the images were aggregated to 100 m resolution by computing zonal mean, maximum, and minimum values within each LCZ grid cell. Additionally, a 3 √ó 3 moving window was applied to derive six contextual statistics (mean, median, maximum, minimum, 25th percentile, and 75th percentile) to incorporate contextual information (Figure 3). Adjacent pixels in remote sensing imagery exhibit strong spatial autocorrelation, and exploiting a moving window to extract local contextual information can more effectively capture land surface features [29]. Among various window sizes, the 3 √ó 3 window is widely adopted due to its optimal balance between computational efficiency and the capacity to characterize local patterns [30]. Notably, the most classic and fundamental convolution kernels in image processing are designed based on the 3 √ó 3 neighborhood structure [31].Figure 3.Features extracted from satellite imagery.‚ûÅ Land cover product: The 30 m land cover product based on Landsat 8_SR was resampled to 10 m. Within each 100 m grid, the proportions of impervious surfaces, forest, water, bareland, and cropland/grassland were calculated to provide land cover composition information (Figure 4).Figure 4.Features extracted from Land cover product.(4) LCZ mapping: Given the robustness to noisy data and strong performance in pixel-based LCZ classification, the random forest classifier was employed for LCZ mapping using the features above and the manually labeled training samples.(5) Sample migration: To minimize manual labeling efforts in the large-area and multi-temporal mapping, a sample transfer method was designed. Change vector analysis was applied to remote sensing features from image bands and land cover products between two time points to identify changed pixels [32]. Classification probabilities were then used to assess image reliability, ensuring that only pixels located in unchanged areas with high confidence in time point 1 were transferred as training samples for time point 2 [33]. Finally, representative samples were selected using the k-nearest neighbor algorithm [34] (Figure 5).Figure 5.The sample migration framework.(6) LCZ mapping in other years. Following sample transfer, additional samples were manually added to underrepresented LCZ types to ensure a balanced dataset. The feature extraction procedures for other years were consistent with those of 2020. LCZ maps were automatically generated using a random forest classifier, and independent testing samples were randomly selected from regions outside the training sample areas to ensure spatial independence. 2.4. Analysis of the LCZ Dynamics and LSTTo comprehensively examine the relationship between LCZ dynamics and LST, two complementary metrics were employed using MATLAB v2024: the LCZ Contribution Index (LCZCI) and the LCZ transition matrix. The LCZCI, adapted from the concept of land contribution index [35,36], quantifies the relative contribution of each LCZ type to the UHI effects by integrating its mean surface temperature and areal proportion within the city. It is defined as:ùêøùê∂ùëçùê∂ùêºùëñ=(ùëáùêøùê∂ùëçùëñ‚àíùëÄ)√óùëÉùêøùê∂ùëçùëñLCZCIi=TLCZi‚àíM√óPLCZi(6)whereùëáùêøùê∂ùëçùëñTLCZidenotes the mean LST of LCZ typei,Mis the mean LST of the entire city, andùëÉùêøùê∂ùëçùëñPLCZirepresents the proportion of the city area occupied by LCZ typei.The LCZCI range is determined by two factors: first, the range of temperature extremes‚Äîthe maximum temperature difference between the coldest LCZ (e.g., water (LCZG)) and the hottest LCZ (e.g., large low-rise (LCZ8))‚Äîsets the basic variation; second, area normalization, which standardizes contributions across LCZ types of different scales, confines the index to a quantifiable and comparable numerical interval. Positive LCZCI values indicate that a type acts as a heat source, exacerbating the UHI effects, while negative values indicate a cooling effect. This formulation enables quantitative comparison of the relative contributions of different LCZ types to regional heat island dynamics.To further assess the thermal impacts of LCZ conversions, a 14 √ó 14 LCZ transition matrix was constructed to evaluate the net impact of LCZ conversions on LST changes in the GBA. Based on LCZ maps from 2000 to 2020, LCZ transition maps were generated, and the mean LST change was calculated for each transition type. To isolate the effect of LCZ conversions, the mean LST of unchanged LCZs (diagonal elements) was subtracted from each corresponding row. Consequently, the resulting matrix entries represent the net LST impact of each transition: positive values denote warming effects, negative values indicate cooling effects, and empty cells correspond to transitions that did not occur.The combined use of LCZCI and the LCZ transition matrix thus provides an integrated framework for quantifying both the steady-state and dynamic thermal contributions of different urban forms. This approach enables a deeper understanding of how LCZ composition and transformation influence long-term urban thermal environments.",
            "2.1. Study Area": "The GBA is located in South China and consists of nine mainland cities in the Pearl River Delta‚ÄîShenzhen (SZ), Guangzhou (GZ), Foshan (FS), Zhongshan (ZS), Dongguan (DG), Zhuhai (ZH), Huizhou (HZ), Jiangmen (JM), and Zhaoqing (ZQ)‚Äîtogether with the Hong Kong (HK) and Macao (MO) Special Administrative Regions (Figure 1). Over the past two decades, the GBA has emerged as one of China‚Äôs most economically dynamic and industrially active regions, as well as one of the fastest urbanizing regions globally. Climatically, the GBA experiences abundant solar radiation, high humidity, and distinct seasonal variations, with a humid subtropical monsoon climate that brings hot, wet summers and mild, dry winters. Topographically, the region is dominated by low-lying plains, with areas above 500 m elevation accounting for only about 3% of the total land area [23]. Consequently, the influence of elevation on LST can be reasonably neglected in analyses of the regional surface thermal environment. Figure 1.Overview of the study area and the area of each city. The GBA provides an ideal setting for LCZ studies, encompassing 14 LCZ types ranging from compact high-rise (LCZ1) to water (LCZG), forming a relatively complete framework. Rapid urbanization has driven intense LCZ transitions, offering dynamic scenarios for observing the evolution of the land surface thermal environment. Moreover, Shenzhen is fully urbanized, while Hong Kong and Macao, as highly urbanized special administrative regions, pose challenges for traditional urban‚Äìrural classification methods in quantifying UHI effects. The LCZ framework, however, can accurately capture the internal structure and thermal characteristics of such built environments, enabling a more rigorous examination of the interactions between urban morphology and climate.",
            "2.2. Sen + Mann‚ÄìKendall Analysis of LST Trends": "The Sen trend analysis is a robust non-parametric statistical method used to detect trends in time-series data. By calculating the median of the sequence, it effectively reduces the influence of outliers, making it suitable for long-term trend detection [24]. However, the Sen method alone cannot determine whether the observed trend is statistically significant. The Mann‚ÄìKendall (MK) test, another non-parametric approach, is widely used to assess the significance of trends in time-series data. It does not require the data to follow a normal distribution and is insensitive to outliers and missing values, making it well-suited for long-term climatic or environmental analyses. Long-term LST data are often affected by atmospheric conditions, sensor noise, and cloud-related disturbances. Although seasonal averaging (e.g., summer mean LST) can mitigate these effects to some extent, anomalies may still remain. Therefore, to enhance the robustness against noise and improve the reliability of the trend detection, the Sen‚Äôs slope estimator and the Mann‚ÄìKendall significance test were combined to analyze LST trends from 2000 to 2020. The combined Sen + Mann‚ÄìKendall trend analysis was conducted in MATLAB (v2024) as follows [25,26]. The Sen‚Äôs slope (ùõΩŒ≤) was calculated using:ùõΩ=ùëÄùëíùëëùëñùëéùëõ(ùë•ùëó‚àíùë•ùëñùëó‚àíùëñ),‚àÄùëó>ùëñ,Œ≤=Medianxj‚àíxij‚àíi,‚àÄj>i,(1)whereùë•ùëóxjandùë•ùëñxirepresent the LST values in yearsùëójandùëñi, respectively, andMedian() denotes the median operator. A positive value ofùõΩŒ≤indicates an upward trend in LST, while a negativeùõΩŒ≤implies a downward trend. For a time-seriesùëã=(ùë•1,ùë•2,‚Ä¶,ùë•ùëõ)X=(x1,x2,‚Ä¶,xn), the Mann‚ÄìKendall statistic(ùëÜ)(S)is computed as:ùëÜ=‚àëùëõ‚àí1ùëñ=1‚àëùëõùëó=ùëñ+1ùë†ùëîùëõ(ùë•ùëó‚àíùë•ùëñ)S=‚àëi=1n‚àí1‚àëj=i+1nsgnxj‚àíxi(2)whereùë†ùëîùëõ(ùë•ùëó‚àíùë•ùëñ)=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉ1,ùë•ùëó‚àíùë•ùëñ>00,ùë•ùëó‚àíùë•ùëñ=0‚àí1,ùë•ùëó‚àíùë•ùëñ<0sgnxj‚àíxi=1,xj‚àíxi>00,xj‚àíxi=0‚àí1,xj‚àíxi<0(3) The null hypothesis (ùêª0H0) assumes that the data are randomly distributed with no significant trend, while the alternative hypothesis (ùêª1H1) assumes a monotonic upward or downward trend. When the sample sizeùëõ<10n<10, the significance test is directly based on the statisticùëÜS. At a given significance levelùõºŒ±, if‚à£ùëÜ‚à£‚â•ùëÜùõº/2‚à£S‚à£‚â•SŒ±/2, the null hypothesisùêª0H0is rejected, indicating a significant trend. Conversely, if‚à£ùëÜ‚à£<ùëÜùõº/2‚à£S‚à£<SŒ±/2,ùêª0H0is accepted, indicating no significant trend. Whenùëõ‚â•10n‚â•10,ùëÜSapproximately follows a normal distribution, and the standardized test statisticùëçZis computed as:ùëç=‚éß‚é©‚é®ÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉÓÄÉùëÜ‚àí1ùëâùê¥ùëÖ(ùëÜ)‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö,ùëÜ>00,ùëç=0ùëÜ+1ùëâùê¥ùëÖ(ùëÜ)‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö,ùëÜ<0Z=S‚àí1VAR(S),S>00,Z=0S+1VAR(S),S<0(4)where the variance ofùëÜSis given by:ùëâùê¥ùëÖ(ùëÜ)=(ùëõ(ùëõ‚àí1)(2ùëõ+5)‚àí‚àëùëöùëñ=1ùë°ùëñ(ùë°ùëñ‚àí1)(2ùë°ùëñ+5))/18VAR(S)=nn‚àí12n+5‚àí‚àëi=1mtiti‚àí12ti+5/18(5) Here,ùëõnis the number of data points,ùëömis the number of tied groups in the dataset, andùë°ùëñtirepresents the number of data points in theith group of ties. At a given significance levelùõºŒ±, if‚à£ùëç‚à£>ùëç1‚àíùõº/2‚à£Z‚à£>Z1‚àíŒ±/2, the null hypothesisùêª0H0is rejected, indicating a statistically significant trend. Otherwise,ùêª0H0is accepted, implying that the trend is not significant. The Sen + Mann‚ÄìKendall method provides a robust and widely accepted framework for detecting, quantifying, and assessing the statistical significance of long-term LST trends under noisy environmental conditions. This combined approach minimizes the influence of outliers and data irregularities, ensuring reliable interpretation of temporal thermal variations across complex urban and climatic settings.",
            "2.3. Multi-Temporal LCZ Mapping Based on Multi-Feature Local Sample Transfer": "Due to the diverse LCZ types and the vast spatial extent of the GBA, obtaining sufficient training samples for LCZ mapping is a labor-intensive and time-consuming task, posing challenges for long-term LCZ mapping in the region. Therefore, ensuring mapping accuracy while addressing the issue of sample acquisition is critical for multi-temporal LCZ mapping in the GBA. To tackle this challenge, a multi-feature local sample transfer method based on land-cover classification products and remote sensing imagery was proposed using MATLAB v2024 (Figure 2). This approach reduces reliance on extensive manual sample collection, enhances mapping automation, and facilitates accurate and consistent multi-temporal LCZ mapping in the GBA. Figure 2.The proposed framework for multi-temporal LCZ mapping. (1) Grid design and data preprocessing: A spatial resolution of 100 m effectively captures homogeneous urban fabric units‚Äîsuch as residential areas, industrial zones, parks, or water bodies. If the resolution is too coarse, a single pixel may mix multiple land cover types (e.g., buildings, water, and vegetation), making it unrepresentative of any specific physical surface property. Conversely, if the resolution is too high, it introduces substantial redundancy and results in classification units that are considerably smaller than the spatial scale of physical processes influencing temperature. Moreover, the 100 m resolution aligns naturally with the 30 m resolution of Landsat imagery, conceptually clear and computationally efficient aggregation. As a result, the 100 m resolution has been widely adopted in numerous studies [11,27,28]. Based on existing studies and multiple tests, a spatial resolution of 100 m was adopted for LCZ mapping in the GBA. To minimize the influence of clouds and atmospheric disturbances, atmospherically corrected surface reflectance data (Landsat 5_SR and Landsat 8_SR) with low cloud cover in 2000, 2005, 2010, 2015, and 2020 were acquired from the USGS. All images were mosaicked and clipped to the GBA boundary. (2) Sample collection (2020): Training samples for 2020 were delineated based on the high-resolution imagery from Google Earth. After careful visual inspection and cross-validation, 2876 samples were obtained, with 75% used for training and 25% reserved for accuracy assessment. (3) Feature extraction (2020): Two groups of features were designed to comprehensively characterize LCZ patterns: ‚ûÄ Remote sensing imagery: All nine multispectral and thermal infrared bands of Landsat 8_SR (30 m resolution) were linearly resampled to 10 m. To enhance spectral information and improve classification performance, the images were aggregated to 100 m resolution by computing zonal mean, maximum, and minimum values within each LCZ grid cell. Additionally, a 3 √ó 3 moving window was applied to derive six contextual statistics (mean, median, maximum, minimum, 25th percentile, and 75th percentile) to incorporate contextual information (Figure 3). Adjacent pixels in remote sensing imagery exhibit strong spatial autocorrelation, and exploiting a moving window to extract local contextual information can more effectively capture land surface features [29]. Among various window sizes, the 3 √ó 3 window is widely adopted due to its optimal balance between computational efficiency and the capacity to characterize local patterns [30]. Notably, the most classic and fundamental convolution kernels in image processing are designed based on the 3 √ó 3 neighborhood structure [31]. Figure 3.Features extracted from satellite imagery. ‚ûÅ Land cover product: The 30 m land cover product based on Landsat 8_SR was resampled to 10 m. Within each 100 m grid, the proportions of impervious surfaces, forest, water, bareland, and cropland/grassland were calculated to provide land cover composition information (Figure 4). Figure 4.Features extracted from Land cover product. (4) LCZ mapping: Given the robustness to noisy data and strong performance in pixel-based LCZ classification, the random forest classifier was employed for LCZ mapping using the features above and the manually labeled training samples. (5) Sample migration: To minimize manual labeling efforts in the large-area and multi-temporal mapping, a sample transfer method was designed. Change vector analysis was applied to remote sensing features from image bands and land cover products between two time points to identify changed pixels [32]. Classification probabilities were then used to assess image reliability, ensuring that only pixels located in unchanged areas with high confidence in time point 1 were transferred as training samples for time point 2 [33]. Finally, representative samples were selected using the k-nearest neighbor algorithm [34] (Figure 5). Figure 5.The sample migration framework. (6) LCZ mapping in other years. Following sample transfer, additional samples were manually added to underrepresented LCZ types to ensure a balanced dataset. The feature extraction procedures for other years were consistent with those of 2020. LCZ maps were automatically generated using a random forest classifier, and independent testing samples were randomly selected from regions outside the training sample areas to ensure spatial independence.",
            "2.4. Analysis of the LCZ Dynamics and LST": "To comprehensively examine the relationship between LCZ dynamics and LST, two complementary metrics were employed using MATLAB v2024: the LCZ Contribution Index (LCZCI) and the LCZ transition matrix. The LCZCI, adapted from the concept of land contribution index [35,36], quantifies the relative contribution of each LCZ type to the UHI effects by integrating its mean surface temperature and areal proportion within the city. It is defined as:ùêøùê∂ùëçùê∂ùêºùëñ=(ùëáùêøùê∂ùëçùëñ‚àíùëÄ)√óùëÉùêøùê∂ùëçùëñLCZCIi=TLCZi‚àíM√óPLCZi(6)whereùëáùêøùê∂ùëçùëñTLCZidenotes the mean LST of LCZ typei,Mis the mean LST of the entire city, andùëÉùêøùê∂ùëçùëñPLCZirepresents the proportion of the city area occupied by LCZ typei.The LCZCI range is determined by two factors: first, the range of temperature extremes‚Äîthe maximum temperature difference between the coldest LCZ (e.g., water (LCZG)) and the hottest LCZ (e.g., large low-rise (LCZ8))‚Äîsets the basic variation; second, area normalization, which standardizes contributions across LCZ types of different scales, confines the index to a quantifiable and comparable numerical interval. Positive LCZCI values indicate that a type acts as a heat source, exacerbating the UHI effects, while negative values indicate a cooling effect. This formulation enables quantitative comparison of the relative contributions of different LCZ types to regional heat island dynamics. To further assess the thermal impacts of LCZ conversions, a 14 √ó 14 LCZ transition matrix was constructed to evaluate the net impact of LCZ conversions on LST changes in the GBA. Based on LCZ maps from 2000 to 2020, LCZ transition maps were generated, and the mean LST change was calculated for each transition type. To isolate the effect of LCZ conversions, the mean LST of unchanged LCZs (diagonal elements) was subtracted from each corresponding row. Consequently, the resulting matrix entries represent the net LST impact of each transition: positive values denote warming effects, negative values indicate cooling effects, and empty cells correspond to transitions that did not occur. The combined use of LCZCI and the LCZ transition matrix thus provides an integrated framework for quantifying both the steady-state and dynamic thermal contributions of different urban forms. This approach enables a deeper understanding of how LCZ composition and transformation influence long-term urban thermal environments.",
            "3. Results and Discussion": "3.1. LCZ Mapping Accuracy and Spatiotemporal Characteristics of LCZ ChangesUsing the proposed multi-feature local sample transfer method, LCZ maps of the GBA were generated for 2000, 2005, 2010, 2015, and 2020 (Figure 6). The overall accuracies were 85.12%, 85.03%, 85.13%, 85.28%, and 85.02%, respectively, with corresponding Kappa coefficients of 0.837, 0.826, 0.837, 0.839, and 0.836, indicating stable and reliable classification performance across all time points. The confusion matrix diagrams for the LCZ classifications from 2000 to 2020 are presented inFigure 7. In the GBA, 13 LCZ types were identified, which theoretically allow for 156 (13 √ó 12) possible transition types. However, only 25 transition types occurred frequently. To evaluate the change detection performance, 30 validation samples were randomly selected for each transition type between consecutive periods. The change detection reached an overall accuracy of 76.47% and a Kappa coefficient of 0.76 (Figure 8), demonstrating reliable performance in identifying spatiotemporal LCZ transitions.Figure 6.LCZ classification for the GBA from 2000 to 2020.Figure 7.Confusion matrix diagrams for LCZ classification from 2000 to 2020.Figure 8.Confusion matrix diagrams for the pixel-level change transitions from 2000 to 2020 (Numbers and letters correspond to different LCZ categories. For example, the number 1 represents LCZ1, and the letter A corresponds to LCZA).A total of 2509 samples were manually collected for LCZ mapping in 2020. Only 202, 192, 197 and 203 supplementary samples were required for 2000, 2005, 2010 and 2015, respectively, accounting for less than 10% of the manual labeling effort in 2020. This substantial reduction underscores the high efficiency and transferability of the proposed sample transfer strategy for multi-temporal LCZ mapping.To examine the spatiotemporal evolution of LCZs in the GBA, the proportional area changes in each LCZ type from 2000 to 2020 were calculated (Figure 9). Overall, built types (LCZ1‚Äì10) expanded by 1.34% over the 20-year period. Among these, LCZ8 (large low-rise) showed the largest increase (4.72%), followed by open built types (LCZ4, LCZ5 and LCZ6). In contrast, LCZ3 (compact low-rise) was the only built type to decline, decreasing by 2.02%. For land-cover types (LCZA‚ÄìG), low plants (LCZD) exhibited the largest reduction, shrinking by 9.32% of the total GBA.Figure 9.LCZ area changes from 2000 to 2020 (%).In all 11 cities of the GBA, the increase in compact built types (LCZ1‚Äì3) was consistently smaller than that of open built types (LCZ 4‚Äì6). This pattern indicates that new developments were less frequently arranged in dense configurations, reflecting the region‚Äôs emphasis on livability, human-centered planning, and residential comfort. Over the past decade, the nine inland cities were designated as National Forest Cities and National Garden Cities, supported by strict forest protection policies and a regional strategy to establish a GBA forest city cluster. As a result, the reduction in dense trees (LCZA) across all GBA cities was much smaller than that of low plants (LCZD), with Huizhou‚Äôs dense trees (LCZA) decreasing by only 0.73%. These findings indicate that urban development in the region has been accompanied by effective forest vegetation protection and ecological environment improvement.Guangzhou, Shenzhen, and Dongguan‚Äîmajor inland first-tier and emerging first-tier cities‚Äîexperienced the most significant economic growth and urban expansion, with open high-rise (LCZ4) areas increasing by 5.18%, 9.04%, and 7.84%, respectively. In contrast, Zhaoqing, a third-tier city with weaker regional economic influence, showed the smallest growth in built types (LCZ1‚Äì10) (4.34%). Due to high population density and limited land availability, Macao and Hong Kong exhibited the greatest urban compactness, with compact high-rise (LCZ1) areas expanding more than in any inland city. In addition, the construction of the Zhuhai‚ÄìMacao artificial island resulted in the largest reduction in water (LCZG) area proportion in Macao (12.64%).Foshan, Dongguan, and Zhongshan recorded the largest increases in large low-rise (LCZ8)‚Äî14.89%, 14.30%, and 14.07%, respectively‚Äîcorresponding closely with their strong industrial expansion. Foshan benefited from Guangzhou‚Äôs economic spillover and reported the second-highest gross industrial product growth after Shenzhen. Dongguan and Zhongshan also expanded their industrial land due to their relatively high industrial output shares. Urban master plans further reinforced this pattern. Foshan was designated as a major manufacturing base, while Dongguan aimed to become a national center for information technology and light manufacturing, with major industrial clusters in Changping, Tangxia, Mayong, and Shatian. Zhongshan‚Äôs development strategy emphasized high-tech and modern manufacturing within a ‚Äúclustered development structure‚Äù. Consequently, Foshan, Dongguan, and Zhongshan exhibited the most substantial growth in large low-rise (LCZ8) areas throughout their urban development.In contrast, Zhaoqing, a third-tier city emphasizing cultural tourism and ecological conservation, had the smallest increase in built types (LCZ1‚Äì10) (4.34%). Despite a 1.82% decline in dense trees (LCZA) from 2000 to 2020, forest coverage remained at 78.98%‚Äîthe highest in the GBA‚Äîproviding strong cooling through shading and evapotranspiration. 3.2. Spatial and Temporal Trends of LSTTo minimize the influence of atmospheric conditions, cloud contamination, and other sources of uncertainty, summer mean LSTs from 2000 to 2020 were employed to characterize long-term thermal variations (Figure 10). LST data were derived from the EOS-Aqua MODIS 8-day composite product (MYD11A2, the MYD11A2 was acquired from the  United States Geological Survey (USGS) EarthExplorer. This product is generated and maintained by the National Aeronautics and Space Administration (NASA) and distributed through its Level-1 and Atmosphere Archive & Distribution System (LAADS) Distributed Active Archive Center (DAAC) in Greenbelt, MD, USA), which provides daytime observations at approximately 13:30 local time. The MYD11A2 LST data were selected because it is an officially validated dataset developed by the United States Geological Survey with well-documented accuracy and excellent long-term temporal consistency. In contrast, although Landsat satellites provide thermal infrared data at a higher spatial resolution, sensor differences across the Landsat series can compromise the consistency and accuracy of long-term LST records. Moreover, the GBA experiences frequent summer rainfall and persistent cloud cover, resulting in substantial data gaps when deriving summer-mean LST from Landsat‚Äîoften leaving many pixels as missing values. The 1 km spatial resolution of MODIS LST means each LST pixel may encompass approximately 10 √ó 10 LCZ pixels, thereby introducing the potential for mixed-pixel effects, where a single LST value aggregates the thermal signatures of multiple LCZ types (e.g., built-up areas, vegetation, and water). To address this, we employed a widely adopted NDVI-based LST downscaling method [37,38,39] in calculating the LCZCI and restricted LCZ‚ÄìLST analyses to LCZ patches with sufficient spatial extent to ensure statistical robustness. While the resolution mismatch inevitably constrains pixel-level precision, these methodological steps substantially reduce mixed-pixel bias and allow us to robustly characterize the systematic relationships between LCZ types and LST at the regional scale.Figure 10.Average summer daytime LST from 2000 to 2020.To investigate the spatiotemporal trends of LST in the GBA, the Sen + Mann‚ÄìKendall method was employed to quantify long-term LST trends from 2000 to 2020 (Figure 11a). As shown inFigure 11a, the triangular central GBA exhibited a statistically significant warming trend, whereas the surrounding regions showed a slight cooling tendency. The change in the central part of the GBA with a significant increase is mainly due to the large-scale distribution of built-up areas [40,41]. Based on summer noon air temperature observations from 2000 to 2020 at four meteorological stations in the GBA (Hong Kong International Airport, Macao International Airport, Guangzhou Baiyun International Airport, and Shenzhen Bao‚Äôan International Airport), all stations exhibited a consistent warming trend, in line with the increase in LST. Among them, Hong Kong International Airport experienced the largest rise, from 29.19 ¬∞C to 29.69 ¬∞C. From 2000 to 2020, LST increased significantly across all four major bay areas (San Francisco Bay Area, SFBA; New York Bay Area, NYBA; Tokyo Bay Area, TBA; and GBA), with coastal core cities serving as hotspots of thermal expansion. In terms of urban heat island clustering, the four city clusters exhibit distinct spatial patterns: the nuclear ribbon pattern (SFBA and NYBA), the multi-core sectoral pattern (TBA), and the triangular regional expansion pattern (GBA) [42]. These differences likely reflect factors related to urban expansion and natural conditions. Notably, the GBA experienced the most pronounced warming, associated with rapid urbanization and industrialization in recent years. To further explore the spatial distribution of these trends, global spatial autocorrelation analysis was performed using Moran‚Äôs I index. The resulting Moran‚Äôs I value (0.49), together with the associated Z-score (1722.83) andp-value (<0.001), indicates a strongly positive and statistically significant spatial clustering of LST trends. Based on this pronounced clustering pattern, a hotspot analysis was subsequently conducted (Figure 11b). InFigure 11b, red areas denote hot spots, representing statistically significant clusters of LST increases, whereas blue areas denote cold spots, corresponding to clusters of significant LST decreases. The spatial distribution of hotspots in the GBA is closely linked to the industrial characteristics and urban structures of individual cities [43]. Shenzhen, as a hub of high-tech industries and corporate headquarters, exhibits intense thermal loads from dense glass-clad high-rises, continuously operating data centers, and high-density developments on reclaimed land. Adjacent Dongguan, with its widespread industrial factories and informal ‚Äúurban villages‚Äù, forms extensive contiguous heat islands. Foshan and Zhongshan‚Äôs manufacturing clusters release substantial industrial heat, compounded by expanding urban areas. Guangzhou‚Äôs new towns connect with these heat islands of Foshan and Dongguan, forming a giant thermal cluster. In contrast, cold spots occur in northern Huizhou‚Äôs forests, southern Jiangmen‚Äôs coastal wetlands, and Zhuhai‚Äôs outer islands, where shading, evaporative cooling, and sea‚Äìland breezes sustain lower temperatures. This spatial configuration suggests that urbanization has produced a contiguous warming core that expands outward from the central metropolitan zones, thereby promoting the UHI coalescence. These findings highlight the necessity of prioritizing hotspot areas‚Äîparticularly in Guangzhou and Shenzhen‚Äîin land surface thermal environment planning to mitigate further UHI merging and intensification.Figure 11.(a) Spatial distribution of LST trend and (b) hotspot analysis from 2000 to 2020.To quantitatively analyze the characteristics of the LST trends, the area proportions of different trend categories were calculated for each city (Figure 12). Overall, 51.72% of the GBA exhibited an increasing LST trend, among which 5.48% showed a highly significant increase, 9.37% showed a moderately significant increase, and 36.87% experienced a slight increase. Meanwhile, 48.23% of the area showed decreasing LST trends, including 43.56% with a slight decrease and 3.72% with a moderately significant decrease. These results indicate that approximately half of the GBA has undergone warming over the past two decades, although most warming was statistically insignificant, which is consistent with that of previous studies on the GBA [42,44,45].Figure 12.Area proportions of LST trend categories from 2000 to 2020 (%).At the municipal scale, highly industrialized cities such as Dongguan, Zhongshan, and Foshan were dominated by warming trends, with warming areas accounting for 93.74%, 89.39%, and 85.22% of their total land area, respectively. The proportions of significantly increasing LST areas in these cities were 22.4%, 20.83%, and 20.73%, respectively. In contrast, Zhaoqing and Jiangmen exhibited the largest cooling areas, accounting for 64.05% and 57.33% of their total land area. Notably, Zhaoqing contributed the largest share of cooling areas across the entire GBA (36.27%), underscoring its important role in moderating the regional thermal environment. However, because Zhaoqing has the largest land area in the GBA, its warming area accounted for 18.97% of the total regional warming‚Äîsecond only to Huizhou, which had the highest proportion at 18.99%. Therefore, despite its strong cooling contribution, Zhaoqing‚Äôs role in the intensification of the GBA‚Äôs UHI effects cannot be ignored. 3.3. Relationship Between LCZs and LSTLCZCI values were calculated for each city in the GBA from 2000 to 2020 (Figure 13). Positive LCZCI values (red) indicate LCZ types that intensify the UHI effects‚Äîthe darker the red, the stronger the warming impact. Conversely, negative LCZCI values (blue) indicate LCZ types that mitigate the UHI effect, with darker shades representing greater cooling capacity.Figure 13.LCZCI values from 2000 to 2020 (Color intensity denotes the LCZ contribution to the UHI effects. The darker the red, the stronger the warming impact. Conversely, the darker the blue, the stronger the cooling impact).Overall, the impacts of LCZs on the UHI effects showed certain similar patterns from 2000 to 2020. First, all built types (LCZ1‚Äì10) exhibited contribution indices above zero, indicating that these types enhanced UHI intensity. Built-up areas are the primary locations of human activities and major sources of anthropogenic heat. With impervious surfaces dominating these zones and possessing low specific heat capacity, they experience greater surface heating under solar and anthropogenic energy inputs [4,5]. As a result, built types (LCZ1‚Äì10) constitute the primary contributors to UHI effects. Among land cover types (LCZA‚ÄìG), only dense trees (LCZA) showed negative contribution indices, demonstrating their strong cooling effects through shading and evapotranspiration [46]. Forests in all bay areas produced a cooling effect, with temperature reductions below 0 ¬∞C, and the effect was most pronounced in the SFBA [47]. This is attributed to its Mediterranean climate, the high transpiration efficiency of deep-rooted native forests, and the multi-centered urban layout that channels cool air through valley corridors into the city. Moreover, dense trees (LCZA) consistently had the lowest contribution indices, while large low-rise (LCZ8) exhibited the highest values in most cities, suggesting that these two LCZ types exerted the most pronounced impacts on the UHI effects. This finding is consistent with that of previous studies on the GBA [7,40,48]. The cooling effectiveness of water (LCZG) is significantly lower than that of dense trees (LCZA), a finding consistent with studies conducted in the other three major global bay areas [42]. This indicates that, at the urban agglomeration scale, forest ecosystems provide more pronounced cooling services than water bodies. This difference primarily stems from the integrated ecological functions of forest vegetation in urban environments. Dense tree canopies effectively intercept solar radiation through shading, while continuous transpiration dissipates large amounts of heat. Together with their extensive spatial coverage, these processes jointly contribute to temperature reduction at the regional scale. In contrast, the cooling capacity of water bodies is more strongly constrained by their spatial extent‚Äîwater (LCZG) covered less than 10% of the study area, whereas dense trees (LCZA) exceeded 55%. This magnitude difference limits the ability of water bodies to develop an effective cooling network within the built-up areas, unlike the contiguous distribution of forests, thereby constraining their overall effectiveness in mitigating the UHI effects. In addition, compact high-rise (LCZ1) and heavy industry (LCZ10) had contribution indices close to zero, likely due to their limited spatial extent in the region.Temporal variations in LCZCI were also evident between 2000 and 2020. To highlight major trends, LCZ types with strong thermal effects‚Äîcompact mid-rise (LCZ2), open high-rise (LCZ4), open mid-rise (LCZ5), open low-rise (LCZ6), large low-rise (LCZ8), and dense trees (LCZA)‚Äîwere selected for further examination (Figure 14).Figure 14.Temporal Trends in LCZCI values for LCZ types with strong thermal effects.From 2000 to 2020, the contribution indices of open high-rise (LCZ4), open mid-rise (LCZ5), open low-rise (LCZ6), and large low-rise (LCZ8) generally increased. According to the analysis of LCZ area changes during this period, these four LCZ types constitute the dominant types of newly developed built-up areas during urbanization. Their expanding spatial extent thus amplifies their thermal influence [48], resulting in rising LCZCI values. Across all cities, the increase in compact mid-rise areas remained below 3%, and consequently, their contribution indices did not exhibit a pronounced upward trend. Regarding dense trees (LCZA), although total area decreased to varying extents across GBA cities, the canopy closure and vegetation density increased, enhancing the cooling effects via shading and evapotranspiration [49,50]. This resulted in gradually lower contribution indices for dense trees (LCZA), indicating an increasingly strong mitigation effect on the UHI phenomenon. Jiangmen, Zhaoqing, and Huizhou generally exhibited lower contribution indices for most built types (LCZ1‚Äì10), suggesting weaker thermal impacts of urban development. In contrast, Guangzhou and Shenzhen displayed the highest absolute LCZCI values for most built types (LCZ1‚Äì10), highlighting the strong influence on UHI intensity. Notably, large low-rise (LCZ8) consistently exhibited the highest contribution indices in Foshan, Zhongshan, and Dongguan, reaching 0.39, 0.35, and 0.22 in 2020, respectively. This finding highlights that large low-rise (LCZ8) has become a major driver of UHI intensification in these cities. Large low-rise (LCZ8), a typical industrial zone, exhibits pronounced thermal effects due to the combined influence of its physical form, anthropogenic heat emissions, and material properties [51,52]. Its large roofs and pavements create extensive horizontal surfaces that absorb solar radiation efficiently, while low-rises are unable to provide effective shade to prevent direct sun exposure. Industrial processes (e.g., smelting, injection molding), dense freight traffic, and continuously operating air-conditioning units further increase heat load. Additionally, common materials such as asphalt, concrete, and metal roofing panels store heat during the day and release it slowly at night, creating a sustained ‚Äúheater‚Äù effect. Thus, a systematic strategy is recommended for mitigating UHI effects. First, strictly control industrial land expansion and set appropriate minimum floor area ratios and greening requirements for new sites. Second, promote green retrofits of existing industrial zones by mandating high-albedo roofs and significantly reducing impervious surface coverage. Third, establish a multi-tiered ecological cooling system, including protective forest belts along park boundaries and continuous green ventilation corridors within the parks. By combining land-use regulation, factory retrofitting, and ecological infrastructure development, this approach provides a practical and measurable framework for comprehensive UHI mitigation in industrial areas. To further analyze the influence of dynamic LCZ transitions on LST, change matrices were constructed to quantify the thermal responses associated with inter-category transformations (Figure 15).Figure 15.The Impact of LCZ Changes on LST (Numbers and letters correspond to different LCZ categories. For example, the number 1 represents LCZ1, and the letter A corresponds to LCZA Key transition types are highlighted with bold outlines; color intensity denotes the magnitude of LST change, with red indicating significant warming and blue indicating significant cooling; blank cells represent transitions that did not occur during the study period).Overall, LCZ transitions produced varying degrees of LST changes across the GBA. First, for conversions from land cover types (LCZA‚ÄìG) to built types (LCZ1‚Äì10), more than 70% of these changes resulted in LST increases. Of the four meteorological stations, only Guangzhou Baiyun International Airport experienced an LCZ transition, from low plants (LCZD) to open high-rise (LCZ5). However, it showed the smallest temperature increase of just 0.16 ¬∞C. This phenomenon may be related to shading effects from high-rise buildings or the formation of local wind corridors, and it also highlights the limitations of meteorological station data, where the small sample size constrains accurate attribution of temperature change mechanisms. When dense trees (LCZA) were converted to other types, LST generally increased, with the largest rises occurring when they were transformed into built types (LCZ1‚Äì10). In Guangzhou and Foshan, these conversions resulted in temperature increases exceeding 2.45 ¬∞C. In addition, when other LCZ types were converted to large low-rise (LCZ8), more than 80% of these transitions led to warming, with all cases in Guangzhou, Zhuhai, and Foshan exhibiting warming. Furthermore, within built types (LCZ1‚Äì10), conversions to open high-rise (LCZ4) or open low-rise (LCZ6) generally resulted in slight LST reductions. For example, in Zhuhai, transitions from compact low-rise (LCZ3) to open high-rise (LCZ4) and open low-rise (LCZ6) decreased LST by 1.68 ¬∞C and 0.71 ¬∞C, respectively. This suggests that, under similar residential space demand, prioritizing open high-rise (LCZ4) and open low-rise (LCZ6) may help alleviate localized heat accumulation.However, the thermal impacts of LCZ transitions varied significantly among cities. In Guangzhou, Shenzhen, and Zhuhai, over 40% of LCZ transitions caused LST changes greater than 1 ¬∞C, whereas in Macao, less than 5% exceeded this threshold. This indicates that LCZ transitions exerted stronger thermal influence in major mainland cities but had limited effects in Macao. In coastal cities such as Shenzhen, Hong Kong, and Macao, all conversions from water (LCZG) to other LCZs increased LST, with the greatest warming observed when water (LCZG) areas were replaced by bare rock or paved (LCZE) or built types (LCZ1‚Äì10). In Shenzhen, for example, converting water (LCZG) to open mid-rise (LCZ5) raised LST by 3.22 ¬∞C, emphasizing the essential role of water (LCZG) in mitigating UHI effects in coastal environments. For coastal cities such as Shenzhen, Hong Kong, and Macao, water bodies provide continuous cooling through sea‚Äìland breezes and substantial evaporation [52,53,54,55]. Once these water bodies are reclaimed and converted into impervious surfaces, their inherent cooling function is not only completely lost but also replaced by strong heat-retaining surfaces. This transformation obstructs the inland penetration of cool marine air via sea breezes, while the newly added impervious areas heat up rapidly under solar radiation. The resulting thermal effect compounds with the pre-existing urban heat environment, leading to significant LST increases. Shenzhen and Hong Kong exhibit significantly greater LST increases following the conversion of water (LCZG) to impervious surfaces compared to Macao. This difference is closely linked to the distinct urban development patterns, prevailing wind conditions, and urban morphologies of the three cities. Both Shenzhen and Hong Kong have undergone rapid and intensive urban expansion, with large-scale land reclamation giving rise to dense clusters of high-rise buildings that severely obstruct the dominant southeast monsoon and sea-breeze circulations, thereby reducing urban ventilation efficiency. Moreover, the dense arrangement of tall buildings and extensive impervious surfaces continuously releases stored heat, compounded by anthropogenic heat from air conditioning systems and other sources, further intensifying the UHI effects. In contrast, constrained by its limited land area, Macao has experienced more modest urban expansion and retains a higher proportion of open areas within its built environment, and thus results in relatively smaller LST increases. In Guangzhou, Zhuhai, Foshan, and Jiangmen, both the loss of dense trees (LCZA) and the expansion of large low-rise (LCZ8) contributed substantially to LST increases, with Guangzhou and Foshan experiencing the strongest warming. When dense trees (LCZA) were converted to other LCZs in these cities, LST increased by more than 1 ¬∞C on average.Therefore, UHI mitigation strategies based on LCZs should be tailored to the specific characteristics and spatial dynamics of each city. For Guangzhou, Shenzhen, and Zhuhai, the thermal impacts of LCZ transitions should be carefully managed. For coastal cities such as Shenzhen, Hong Kong, and Macao, preserving water bodies is critical for maintaining cooling capacity. Meanwhile, in Guangzhou, Zhuhai, Foshan, and Jiangmen, strict control of deforestation and industrial land expansion is essential to alleviate regional heat accumulation.",
            "3.1. LCZ Mapping Accuracy and Spatiotemporal Characteristics of LCZ Changes": "Using the proposed multi-feature local sample transfer method, LCZ maps of the GBA were generated for 2000, 2005, 2010, 2015, and 2020 (Figure 6). The overall accuracies were 85.12%, 85.03%, 85.13%, 85.28%, and 85.02%, respectively, with corresponding Kappa coefficients of 0.837, 0.826, 0.837, 0.839, and 0.836, indicating stable and reliable classification performance across all time points. The confusion matrix diagrams for the LCZ classifications from 2000 to 2020 are presented inFigure 7. In the GBA, 13 LCZ types were identified, which theoretically allow for 156 (13 √ó 12) possible transition types. However, only 25 transition types occurred frequently. To evaluate the change detection performance, 30 validation samples were randomly selected for each transition type between consecutive periods. The change detection reached an overall accuracy of 76.47% and a Kappa coefficient of 0.76 (Figure 8), demonstrating reliable performance in identifying spatiotemporal LCZ transitions. Figure 6.LCZ classification for the GBA from 2000 to 2020. Figure 7.Confusion matrix diagrams for LCZ classification from 2000 to 2020. Figure 8.Confusion matrix diagrams for the pixel-level change transitions from 2000 to 2020 (Numbers and letters correspond to different LCZ categories. For example, the number 1 represents LCZ1, and the letter A corresponds to LCZA). A total of 2509 samples were manually collected for LCZ mapping in 2020. Only 202, 192, 197 and 203 supplementary samples were required for 2000, 2005, 2010 and 2015, respectively, accounting for less than 10% of the manual labeling effort in 2020. This substantial reduction underscores the high efficiency and transferability of the proposed sample transfer strategy for multi-temporal LCZ mapping. To examine the spatiotemporal evolution of LCZs in the GBA, the proportional area changes in each LCZ type from 2000 to 2020 were calculated (Figure 9). Overall, built types (LCZ1‚Äì10) expanded by 1.34% over the 20-year period. Among these, LCZ8 (large low-rise) showed the largest increase (4.72%), followed by open built types (LCZ4, LCZ5 and LCZ6). In contrast, LCZ3 (compact low-rise) was the only built type to decline, decreasing by 2.02%. For land-cover types (LCZA‚ÄìG), low plants (LCZD) exhibited the largest reduction, shrinking by 9.32% of the total GBA. Figure 9.LCZ area changes from 2000 to 2020 (%). In all 11 cities of the GBA, the increase in compact built types (LCZ1‚Äì3) was consistently smaller than that of open built types (LCZ 4‚Äì6). This pattern indicates that new developments were less frequently arranged in dense configurations, reflecting the region‚Äôs emphasis on livability, human-centered planning, and residential comfort. Over the past decade, the nine inland cities were designated as National Forest Cities and National Garden Cities, supported by strict forest protection policies and a regional strategy to establish a GBA forest city cluster. As a result, the reduction in dense trees (LCZA) across all GBA cities was much smaller than that of low plants (LCZD), with Huizhou‚Äôs dense trees (LCZA) decreasing by only 0.73%. These findings indicate that urban development in the region has been accompanied by effective forest vegetation protection and ecological environment improvement. Guangzhou, Shenzhen, and Dongguan‚Äîmajor inland first-tier and emerging first-tier cities‚Äîexperienced the most significant economic growth and urban expansion, with open high-rise (LCZ4) areas increasing by 5.18%, 9.04%, and 7.84%, respectively. In contrast, Zhaoqing, a third-tier city with weaker regional economic influence, showed the smallest growth in built types (LCZ1‚Äì10) (4.34%). Due to high population density and limited land availability, Macao and Hong Kong exhibited the greatest urban compactness, with compact high-rise (LCZ1) areas expanding more than in any inland city. In addition, the construction of the Zhuhai‚ÄìMacao artificial island resulted in the largest reduction in water (LCZG) area proportion in Macao (12.64%). Foshan, Dongguan, and Zhongshan recorded the largest increases in large low-rise (LCZ8)‚Äî14.89%, 14.30%, and 14.07%, respectively‚Äîcorresponding closely with their strong industrial expansion. Foshan benefited from Guangzhou‚Äôs economic spillover and reported the second-highest gross industrial product growth after Shenzhen. Dongguan and Zhongshan also expanded their industrial land due to their relatively high industrial output shares. Urban master plans further reinforced this pattern. Foshan was designated as a major manufacturing base, while Dongguan aimed to become a national center for information technology and light manufacturing, with major industrial clusters in Changping, Tangxia, Mayong, and Shatian. Zhongshan‚Äôs development strategy emphasized high-tech and modern manufacturing within a ‚Äúclustered development structure‚Äù. Consequently, Foshan, Dongguan, and Zhongshan exhibited the most substantial growth in large low-rise (LCZ8) areas throughout their urban development. In contrast, Zhaoqing, a third-tier city emphasizing cultural tourism and ecological conservation, had the smallest increase in built types (LCZ1‚Äì10) (4.34%). Despite a 1.82% decline in dense trees (LCZA) from 2000 to 2020, forest coverage remained at 78.98%‚Äîthe highest in the GBA‚Äîproviding strong cooling through shading and evapotranspiration.",
            "3.2. Spatial and Temporal Trends of LST": "To minimize the influence of atmospheric conditions, cloud contamination, and other sources of uncertainty, summer mean LSTs from 2000 to 2020 were employed to characterize long-term thermal variations (Figure 10). LST data were derived from the EOS-Aqua MODIS 8-day composite product (MYD11A2, the MYD11A2 was acquired from the  United States Geological Survey (USGS) EarthExplorer. This product is generated and maintained by the National Aeronautics and Space Administration (NASA) and distributed through its Level-1 and Atmosphere Archive & Distribution System (LAADS) Distributed Active Archive Center (DAAC) in Greenbelt, MD, USA), which provides daytime observations at approximately 13:30 local time. The MYD11A2 LST data were selected because it is an officially validated dataset developed by the United States Geological Survey with well-documented accuracy and excellent long-term temporal consistency. In contrast, although Landsat satellites provide thermal infrared data at a higher spatial resolution, sensor differences across the Landsat series can compromise the consistency and accuracy of long-term LST records. Moreover, the GBA experiences frequent summer rainfall and persistent cloud cover, resulting in substantial data gaps when deriving summer-mean LST from Landsat‚Äîoften leaving many pixels as missing values. The 1 km spatial resolution of MODIS LST means each LST pixel may encompass approximately 10 √ó 10 LCZ pixels, thereby introducing the potential for mixed-pixel effects, where a single LST value aggregates the thermal signatures of multiple LCZ types (e.g., built-up areas, vegetation, and water). To address this, we employed a widely adopted NDVI-based LST downscaling method [37,38,39] in calculating the LCZCI and restricted LCZ‚ÄìLST analyses to LCZ patches with sufficient spatial extent to ensure statistical robustness. While the resolution mismatch inevitably constrains pixel-level precision, these methodological steps substantially reduce mixed-pixel bias and allow us to robustly characterize the systematic relationships between LCZ types and LST at the regional scale. Figure 10.Average summer daytime LST from 2000 to 2020. To investigate the spatiotemporal trends of LST in the GBA, the Sen + Mann‚ÄìKendall method was employed to quantify long-term LST trends from 2000 to 2020 (Figure 11a). As shown inFigure 11a, the triangular central GBA exhibited a statistically significant warming trend, whereas the surrounding regions showed a slight cooling tendency. The change in the central part of the GBA with a significant increase is mainly due to the large-scale distribution of built-up areas [40,41]. Based on summer noon air temperature observations from 2000 to 2020 at four meteorological stations in the GBA (Hong Kong International Airport, Macao International Airport, Guangzhou Baiyun International Airport, and Shenzhen Bao‚Äôan International Airport), all stations exhibited a consistent warming trend, in line with the increase in LST. Among them, Hong Kong International Airport experienced the largest rise, from 29.19 ¬∞C to 29.69 ¬∞C. From 2000 to 2020, LST increased significantly across all four major bay areas (San Francisco Bay Area, SFBA; New York Bay Area, NYBA; Tokyo Bay Area, TBA; and GBA), with coastal core cities serving as hotspots of thermal expansion. In terms of urban heat island clustering, the four city clusters exhibit distinct spatial patterns: the nuclear ribbon pattern (SFBA and NYBA), the multi-core sectoral pattern (TBA), and the triangular regional expansion pattern (GBA) [42]. These differences likely reflect factors related to urban expansion and natural conditions. Notably, the GBA experienced the most pronounced warming, associated with rapid urbanization and industrialization in recent years. To further explore the spatial distribution of these trends, global spatial autocorrelation analysis was performed using Moran‚Äôs I index. The resulting Moran‚Äôs I value (0.49), together with the associated Z-score (1722.83) andp-value (<0.001), indicates a strongly positive and statistically significant spatial clustering of LST trends. Based on this pronounced clustering pattern, a hotspot analysis was subsequently conducted (Figure 11b). InFigure 11b, red areas denote hot spots, representing statistically significant clusters of LST increases, whereas blue areas denote cold spots, corresponding to clusters of significant LST decreases. The spatial distribution of hotspots in the GBA is closely linked to the industrial characteristics and urban structures of individual cities [43]. Shenzhen, as a hub of high-tech industries and corporate headquarters, exhibits intense thermal loads from dense glass-clad high-rises, continuously operating data centers, and high-density developments on reclaimed land. Adjacent Dongguan, with its widespread industrial factories and informal ‚Äúurban villages‚Äù, forms extensive contiguous heat islands. Foshan and Zhongshan‚Äôs manufacturing clusters release substantial industrial heat, compounded by expanding urban areas. Guangzhou‚Äôs new towns connect with these heat islands of Foshan and Dongguan, forming a giant thermal cluster. In contrast, cold spots occur in northern Huizhou‚Äôs forests, southern Jiangmen‚Äôs coastal wetlands, and Zhuhai‚Äôs outer islands, where shading, evaporative cooling, and sea‚Äìland breezes sustain lower temperatures. This spatial configuration suggests that urbanization has produced a contiguous warming core that expands outward from the central metropolitan zones, thereby promoting the UHI coalescence. These findings highlight the necessity of prioritizing hotspot areas‚Äîparticularly in Guangzhou and Shenzhen‚Äîin land surface thermal environment planning to mitigate further UHI merging and intensification. Figure 11.(a) Spatial distribution of LST trend and (b) hotspot analysis from 2000 to 2020. To quantitatively analyze the characteristics of the LST trends, the area proportions of different trend categories were calculated for each city (Figure 12). Overall, 51.72% of the GBA exhibited an increasing LST trend, among which 5.48% showed a highly significant increase, 9.37% showed a moderately significant increase, and 36.87% experienced a slight increase. Meanwhile, 48.23% of the area showed decreasing LST trends, including 43.56% with a slight decrease and 3.72% with a moderately significant decrease. These results indicate that approximately half of the GBA has undergone warming over the past two decades, although most warming was statistically insignificant, which is consistent with that of previous studies on the GBA [42,44,45]. Figure 12.Area proportions of LST trend categories from 2000 to 2020 (%). At the municipal scale, highly industrialized cities such as Dongguan, Zhongshan, and Foshan were dominated by warming trends, with warming areas accounting for 93.74%, 89.39%, and 85.22% of their total land area, respectively. The proportions of significantly increasing LST areas in these cities were 22.4%, 20.83%, and 20.73%, respectively. In contrast, Zhaoqing and Jiangmen exhibited the largest cooling areas, accounting for 64.05% and 57.33% of their total land area. Notably, Zhaoqing contributed the largest share of cooling areas across the entire GBA (36.27%), underscoring its important role in moderating the regional thermal environment. However, because Zhaoqing has the largest land area in the GBA, its warming area accounted for 18.97% of the total regional warming‚Äîsecond only to Huizhou, which had the highest proportion at 18.99%. Therefore, despite its strong cooling contribution, Zhaoqing‚Äôs role in the intensification of the GBA‚Äôs UHI effects cannot be ignored.",
            "3.3. Relationship Between LCZs and LST": "LCZCI values were calculated for each city in the GBA from 2000 to 2020 (Figure 13). Positive LCZCI values (red) indicate LCZ types that intensify the UHI effects‚Äîthe darker the red, the stronger the warming impact. Conversely, negative LCZCI values (blue) indicate LCZ types that mitigate the UHI effect, with darker shades representing greater cooling capacity. Figure 13.LCZCI values from 2000 to 2020 (Color intensity denotes the LCZ contribution to the UHI effects. The darker the red, the stronger the warming impact. Conversely, the darker the blue, the stronger the cooling impact). Overall, the impacts of LCZs on the UHI effects showed certain similar patterns from 2000 to 2020. First, all built types (LCZ1‚Äì10) exhibited contribution indices above zero, indicating that these types enhanced UHI intensity. Built-up areas are the primary locations of human activities and major sources of anthropogenic heat. With impervious surfaces dominating these zones and possessing low specific heat capacity, they experience greater surface heating under solar and anthropogenic energy inputs [4,5]. As a result, built types (LCZ1‚Äì10) constitute the primary contributors to UHI effects. Among land cover types (LCZA‚ÄìG), only dense trees (LCZA) showed negative contribution indices, demonstrating their strong cooling effects through shading and evapotranspiration [46]. Forests in all bay areas produced a cooling effect, with temperature reductions below 0 ¬∞C, and the effect was most pronounced in the SFBA [47]. This is attributed to its Mediterranean climate, the high transpiration efficiency of deep-rooted native forests, and the multi-centered urban layout that channels cool air through valley corridors into the city. Moreover, dense trees (LCZA) consistently had the lowest contribution indices, while large low-rise (LCZ8) exhibited the highest values in most cities, suggesting that these two LCZ types exerted the most pronounced impacts on the UHI effects. This finding is consistent with that of previous studies on the GBA [7,40,48]. The cooling effectiveness of water (LCZG) is significantly lower than that of dense trees (LCZA), a finding consistent with studies conducted in the other three major global bay areas [42]. This indicates that, at the urban agglomeration scale, forest ecosystems provide more pronounced cooling services than water bodies. This difference primarily stems from the integrated ecological functions of forest vegetation in urban environments. Dense tree canopies effectively intercept solar radiation through shading, while continuous transpiration dissipates large amounts of heat. Together with their extensive spatial coverage, these processes jointly contribute to temperature reduction at the regional scale. In contrast, the cooling capacity of water bodies is more strongly constrained by their spatial extent‚Äîwater (LCZG) covered less than 10% of the study area, whereas dense trees (LCZA) exceeded 55%. This magnitude difference limits the ability of water bodies to develop an effective cooling network within the built-up areas, unlike the contiguous distribution of forests, thereby constraining their overall effectiveness in mitigating the UHI effects. In addition, compact high-rise (LCZ1) and heavy industry (LCZ10) had contribution indices close to zero, likely due to their limited spatial extent in the region. Temporal variations in LCZCI were also evident between 2000 and 2020. To highlight major trends, LCZ types with strong thermal effects‚Äîcompact mid-rise (LCZ2), open high-rise (LCZ4), open mid-rise (LCZ5), open low-rise (LCZ6), large low-rise (LCZ8), and dense trees (LCZA)‚Äîwere selected for further examination (Figure 14). Figure 14.Temporal Trends in LCZCI values for LCZ types with strong thermal effects. From 2000 to 2020, the contribution indices of open high-rise (LCZ4), open mid-rise (LCZ5), open low-rise (LCZ6), and large low-rise (LCZ8) generally increased. According to the analysis of LCZ area changes during this period, these four LCZ types constitute the dominant types of newly developed built-up areas during urbanization. Their expanding spatial extent thus amplifies their thermal influence [48], resulting in rising LCZCI values. Across all cities, the increase in compact mid-rise areas remained below 3%, and consequently, their contribution indices did not exhibit a pronounced upward trend. Regarding dense trees (LCZA), although total area decreased to varying extents across GBA cities, the canopy closure and vegetation density increased, enhancing the cooling effects via shading and evapotranspiration [49,50]. This resulted in gradually lower contribution indices for dense trees (LCZA), indicating an increasingly strong mitigation effect on the UHI phenomenon. Jiangmen, Zhaoqing, and Huizhou generally exhibited lower contribution indices for most built types (LCZ1‚Äì10), suggesting weaker thermal impacts of urban development. In contrast, Guangzhou and Shenzhen displayed the highest absolute LCZCI values for most built types (LCZ1‚Äì10), highlighting the strong influence on UHI intensity. Notably, large low-rise (LCZ8) consistently exhibited the highest contribution indices in Foshan, Zhongshan, and Dongguan, reaching 0.39, 0.35, and 0.22 in 2020, respectively. This finding highlights that large low-rise (LCZ8) has become a major driver of UHI intensification in these cities. Large low-rise (LCZ8), a typical industrial zone, exhibits pronounced thermal effects due to the combined influence of its physical form, anthropogenic heat emissions, and material properties [51,52]. Its large roofs and pavements create extensive horizontal surfaces that absorb solar radiation efficiently, while low-rises are unable to provide effective shade to prevent direct sun exposure. Industrial processes (e.g., smelting, injection molding), dense freight traffic, and continuously operating air-conditioning units further increase heat load. Additionally, common materials such as asphalt, concrete, and metal roofing panels store heat during the day and release it slowly at night, creating a sustained ‚Äúheater‚Äù effect. Thus, a systematic strategy is recommended for mitigating UHI effects. First, strictly control industrial land expansion and set appropriate minimum floor area ratios and greening requirements for new sites. Second, promote green retrofits of existing industrial zones by mandating high-albedo roofs and significantly reducing impervious surface coverage. Third, establish a multi-tiered ecological cooling system, including protective forest belts along park boundaries and continuous green ventilation corridors within the parks. By combining land-use regulation, factory retrofitting, and ecological infrastructure development, this approach provides a practical and measurable framework for comprehensive UHI mitigation in industrial areas. To further analyze the influence of dynamic LCZ transitions on LST, change matrices were constructed to quantify the thermal responses associated with inter-category transformations (Figure 15). Figure 15.The Impact of LCZ Changes on LST (Numbers and letters correspond to different LCZ categories. For example, the number 1 represents LCZ1, and the letter A corresponds to LCZA Key transition types are highlighted with bold outlines; color intensity denotes the magnitude of LST change, with red indicating significant warming and blue indicating significant cooling; blank cells represent transitions that did not occur during the study period). Overall, LCZ transitions produced varying degrees of LST changes across the GBA. First, for conversions from land cover types (LCZA‚ÄìG) to built types (LCZ1‚Äì10), more than 70% of these changes resulted in LST increases. Of the four meteorological stations, only Guangzhou Baiyun International Airport experienced an LCZ transition, from low plants (LCZD) to open high-rise (LCZ5). However, it showed the smallest temperature increase of just 0.16 ¬∞C. This phenomenon may be related to shading effects from high-rise buildings or the formation of local wind corridors, and it also highlights the limitations of meteorological station data, where the small sample size constrains accurate attribution of temperature change mechanisms. When dense trees (LCZA) were converted to other types, LST generally increased, with the largest rises occurring when they were transformed into built types (LCZ1‚Äì10). In Guangzhou and Foshan, these conversions resulted in temperature increases exceeding 2.45 ¬∞C. In addition, when other LCZ types were converted to large low-rise (LCZ8), more than 80% of these transitions led to warming, with all cases in Guangzhou, Zhuhai, and Foshan exhibiting warming. Furthermore, within built types (LCZ1‚Äì10), conversions to open high-rise (LCZ4) or open low-rise (LCZ6) generally resulted in slight LST reductions. For example, in Zhuhai, transitions from compact low-rise (LCZ3) to open high-rise (LCZ4) and open low-rise (LCZ6) decreased LST by 1.68 ¬∞C and 0.71 ¬∞C, respectively. This suggests that, under similar residential space demand, prioritizing open high-rise (LCZ4) and open low-rise (LCZ6) may help alleviate localized heat accumulation. However, the thermal impacts of LCZ transitions varied significantly among cities. In Guangzhou, Shenzhen, and Zhuhai, over 40% of LCZ transitions caused LST changes greater than 1 ¬∞C, whereas in Macao, less than 5% exceeded this threshold. This indicates that LCZ transitions exerted stronger thermal influence in major mainland cities but had limited effects in Macao. In coastal cities such as Shenzhen, Hong Kong, and Macao, all conversions from water (LCZG) to other LCZs increased LST, with the greatest warming observed when water (LCZG) areas were replaced by bare rock or paved (LCZE) or built types (LCZ1‚Äì10). In Shenzhen, for example, converting water (LCZG) to open mid-rise (LCZ5) raised LST by 3.22 ¬∞C, emphasizing the essential role of water (LCZG) in mitigating UHI effects in coastal environments. For coastal cities such as Shenzhen, Hong Kong, and Macao, water bodies provide continuous cooling through sea‚Äìland breezes and substantial evaporation [52,53,54,55]. Once these water bodies are reclaimed and converted into impervious surfaces, their inherent cooling function is not only completely lost but also replaced by strong heat-retaining surfaces. This transformation obstructs the inland penetration of cool marine air via sea breezes, while the newly added impervious areas heat up rapidly under solar radiation. The resulting thermal effect compounds with the pre-existing urban heat environment, leading to significant LST increases. Shenzhen and Hong Kong exhibit significantly greater LST increases following the conversion of water (LCZG) to impervious surfaces compared to Macao. This difference is closely linked to the distinct urban development patterns, prevailing wind conditions, and urban morphologies of the three cities. Both Shenzhen and Hong Kong have undergone rapid and intensive urban expansion, with large-scale land reclamation giving rise to dense clusters of high-rise buildings that severely obstruct the dominant southeast monsoon and sea-breeze circulations, thereby reducing urban ventilation efficiency. Moreover, the dense arrangement of tall buildings and extensive impervious surfaces continuously releases stored heat, compounded by anthropogenic heat from air conditioning systems and other sources, further intensifying the UHI effects. In contrast, constrained by its limited land area, Macao has experienced more modest urban expansion and retains a higher proportion of open areas within its built environment, and thus results in relatively smaller LST increases. In Guangzhou, Zhuhai, Foshan, and Jiangmen, both the loss of dense trees (LCZA) and the expansion of large low-rise (LCZ8) contributed substantially to LST increases, with Guangzhou and Foshan experiencing the strongest warming. When dense trees (LCZA) were converted to other LCZs in these cities, LST increased by more than 1 ¬∞C on average. Therefore, UHI mitigation strategies based on LCZs should be tailored to the specific characteristics and spatial dynamics of each city. For Guangzhou, Shenzhen, and Zhuhai, the thermal impacts of LCZ transitions should be carefully managed. For coastal cities such as Shenzhen, Hong Kong, and Macao, preserving water bodies is critical for maintaining cooling capacity. Meanwhile, in Guangzhou, Zhuhai, Foshan, and Jiangmen, strict control of deforestation and industrial land expansion is essential to alleviate regional heat accumulation.",
            "4. Conclusions": "In this study, we derived LCZ classification maps by a multi-feature local sample transfer method in the GBA from 2000 to 2020. We then systematically investigated the LCZ spatiotemporal changes and LST trends and quantitatively analyzed the LCZ change impacts on surface thermal environments based on the LCZCI and the LCZ transition matrix. Our findings suggested that the proposed multi-feature local sample transfer approach can be used to achieve long-term LCZ mapping efficiently. LCZ maps for 2000, 2005, 2010, 2015, and 2020 achieved overall accuracies of 85.12%, 85.03%, 85.13%, 85.28%, and 85.02%, respectively, with change detection accuracy reaching 76.47%. From 2000 to 2020, the total area of built types (LCZ1‚Äì10) in the GBA increased by 1.34%, with large low-rise (LCZ8) showing the largest growth of 4.72%. In contrast, compact low-rise (LCZ3) was the only category to decrease, at 2.02%. Foshan, Dongguan, and Zhongshan experienced the most pronounced increases in large low-rise (LCZ8), at 14.89%, 14.30%, and 14.07%, respectively, whereas Zhaoqing exhibited the smallest growth in built area, at 4.34%. LST changes exhibited a contiguous warming core expanding outward from the central metropolitan zones, promoting the coalescence of UHIs. Dongguan experienced the largest and most pronounced warming area, followed by Zhongshan and Foshan. Large low-rise (LCZ8) and dense trees (LCZA) emerged as the most significant contributors to the UHI effects. Open high-rise (LCZ4), open mid-rise (LCZ5), open low-rise (LCZ6), and large low-rise (LCZ8) areas have increasingly amplified the UHI effects over time, whereas dense trees (LCZA) have progressively enhanced their cooling effect. LST generally increased when dense trees (LCZA) and water (LCZG) were converted to other LCZ types and when other types were converted to large low-rise (LCZ8). Conversely, transitions from other built types to open high-rise (LCZ4) or open low-rise (LCZ6) resulted in slight decreases in LST. Large low-rise (LCZ8) consistently served as the primary driver of UHI intensification in industrial cities such as Foshan, Zhongshan, and Dongguan. In coastal cities, including Shenzhen, Hong Kong, and Macao, the largest LST increases occurred when water (LCZG) areas were replaced by bare rock or paved (LCZE) or built types (LCZ1‚Äì10). To systematically mitigate UHI effects, cities should adopt differentiated spatial management strategies. Foshan, Zhongshan, and Dongguan should focus on intensive industrial land redevelopment, strictly limit new industrial developments, promote green infrastructure retrofits, and establish multi-tiered ecological cooling systems with protective forest belts and ventilation corridors. Shenzhen, Hong Kong, and Macao should protect and restore existing water bodies to maintain their natural cooling function as regional ‚Äúcold sources‚Äù. Guangzhou, Zhuhai, Foshan, and Jiangmen should restrict conversion of forests to industrial or high-intensity uses, prioritizing the conservation of connected forest corridors and ecological patches to reinforce their role as peripheral cooling buffers. It should be noted that, because the GBA is dominated by low-lying plains, this study excluded elevation as a factor in analyzing LST variations. This choice improves analytical clarity for the GBA but also introduces both opportunities and limitations when extending the method to other geographic regions. The strength of this approach lies in the generalizability of the LCZ framework and its associated thermal environment models, making it particularly effective for megacity regions with relatively flat terrain‚Äîsuch as the Yangtze River Delta. However, in mountainous cities with pronounced topographic gradients, such as Chongqing, elevation and slope become dominant controls on LST. In such settings, our method should be integrated with terrain-correction models to ensure robust interpretation. Therefore, while the approach demonstrates strong transferability in flat regions, its application in complex terrain requires appropriate model adaptation and additional topographic data adjustments."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2073-445X/14/12/2370",
        "scraped_at": "2025-12-05 23:56:11"
    },
    {
        "title": "Digging into the Solubility Factor in Cancer Diagnosis: A Case of Soluble CD44 Protein",
        "authors": "byZhuldyz Myrkhiyeva,Marzhan Nurlankyzy,Kulzhan Berikkhanova,Zhanas Baimagambet,Aidana Bissen,Nurzhan Bikhanov,Christabel K. L. Tan,Daniele Tosi,Zhannat AshikbayevaandAliya Bekmurzayeva",
        "journal": "Biosensors2025,15(12), 796; https://doi.org/10.3390/bios15120796 (registering¬†DOI) - 4 Dec 2025",
        "abstract": "The detection of soluble proteins in biological fluids, as a form of liquid biopsy, is a promising tool for cancer diagnosis and prognosis, as it is less invasive than traditional diagnostic methods. CD44 is one of the most recognized markers of cancer stem cells, a small subset of cells responsible for cancer initiation, progression, and metastasis. Given the importance of CD44 as a cancer biomarker, several review articles explore the diagnostic and therapeutic value of cell-surface CD44. In addition to being a membrane-anchored protein, CD44 is also shed from the cell surface and can be found in various biological fluids. However, the role of soluble CD44 in cancer has not been comprehensively discussed in recent reviews. Measuring soluble CD44 in various biological liquids can provide a practical and valuable tool for cancer diagnosis and treatment monitoring. Therefore, this review comprehensively discusses the role of soluble CD44 as a marker in various cancer types, including serum, saliva, urine, and other fluids. In particular, its role as an early cancer biomarker and as a predictive and prognostic biomarker in several cancers is discussed. This work also provides an overview of a wide range of analytical techniques used to detect soluble CD44. The value of cells expressing CD44 versus soluble CD44 as a biomarker is also compared. The review concludes with a perspective on future directions, emphasizing the shift toward non-invasive analytical methods and the need for standardization of detection, including multiple biomarkers during evaluation, to improve the accuracy of cancer diagnosis.Keywords:soluble CD44 protein;CD44 molecule;biological fluids;cancer diagnosis;prognostic biomarker;protein detection methods",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "In recent years, we have witnessed a shift from histopathological techniques to molecular methods for diagnostics [1]. The molecular processes underlying cancer diagnosis help provide a more personalized strategy. Compared with DNA analysis, analysis of protein biomarkers is particularly valuable, as they serve as more direct indicators of the normal state versus cancer [2]. The detection of soluble proteins in biological fluids represents one modality of liquid biopsy and is a promising tool for the early diagnosis and continuous monitoring of cancer [3]; this method is less invasive than traditional methods [4]. Moreover, for some types of cancer, identifying high-risk precancerous states and further applying necessary treatment could be essential measures to reduce cancer mortality and morbidity and lower cancer treatment costs [1]. In the late 1990s, CD44 protein was considered one of the most promising candidate biomarkers for early cancer diagnosis [5]. Since then, its role in different cancer types has been widely studied, and it has been coined a multifunctional mediator of cancer progression [6,7]. Given the importance of CD44 as a cancer biomarker, it has been reviewed in several articles, many of which discuss its role as a biomarker and therapeutic target [8,9,10]. Review papers featuring developments related to the therapeutic implications of CD44 [9,11,12,13], its role in chemoresistance [14] and metastasis [7,15], its potential role in diagnosis [10,16,17], and its role in glioblastoma multiforme [18] have been published. An earlier excellent review from 2003 [19] is also available. More recent reviews discuss the role of its tissue-expressed form in cancer [20,21], including nanobiosensing approaches for early cancer detection in CD44-expressing cells [22], a particular form of cancer [23], or the intracellular domain of CD44 [22]. The year 2025 marks 42 years since the first discovery of soluble CD44 (solCD44) in serum [23]. However, the role of cell-surface CD44 in cancer has been studied more than its soluble form [24]. Proteolytic cleavage of membrane-bound CD44 generates a soluble form of CD44 that enters biological fluids and reflects dynamic tumor-associated processes such as proliferation, invasion, and extracellular matrix remodeling [25]. Key characteristics include its ability to indicate tumor burden and metastasis across multiple cancer types, including gastric, colon, and head and neck cancer [26]. Indeed, all of the abovementioned reviews also reflect this, focusing on the cell-surface CD44 marker rather than the soluble form. To the best of our knowledge, there are currently no comprehensive reviews that encompass the full range of CD44 protein detection methods. While there is an excellent section in a book chapter on the CD44 protein [27] and a short review [28] that provide valuable insight, they need to be updated, especially given increased knowledge about solCD44. A recent review discussed the role of CD44-expressing cells in bladder cancer and positioned CD44 as a promising biomarker and therapeutic target for noninvasive diagnosis and innovative treatments [29]. We analyzed the distribution of systematic reviews and meta-analyses on cell-expressed CD44 from 1995 to 2022, and the data indicate a research emphasis on gastric and breast cancers, which together accounted for more than 1/3 of all studies (Figure S1). Cancers of the head and neck, pancreas, ovaries, bone (osteosarcoma), and central nervous system (gliomas) were the most widely studied, indicating overall but unequal interest in different tumor types. To date, no reviews have focused solely on solCD44 as a cancer biomarker. Specifically, this work focuses on less studied forms solCD44, including proteolytically shed protein and exosome-associated form of CD44. Given that protein detection in biological fluids offers a more straightforward diagnostic approach than cell- or tissue-based techniques, this review discusses the significance of solCD44 as a cancer biomarker across various biological fluids. The role of solCD44 as a diagnostic, prognostic, and predictive biomarker of cancer will be discussed, as will its role in other biological fluids, such as saliva, urine, and others. The review will also provide an overview of solCD44, its ligands, and the assays used for its detection, and will compare solCD44 with the cell-expressed form of the protein. The graphical abstract shows the different aspects covered in this review. The soluble forms of the CD44 protein and its isoforms are named differently in the literature, as shown inTable 1. To avoid misunderstanding, three unified names are used throughout this review. Table 1.Different names have been used for the soluble CD44 protein in the literature; those used in this work are listed below. References [1,17,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45].",
            "2. Shedding of the Soluble Form of CD44 from the Cell": "CD44 is encoded by a single gene located on chromosome 11: the Lutheran inhibitory gene In(Lu) [23]. CD44 expression in normal epithelium is mainly in the basal and suprabasal regions, but CD44 can migrate into the superficial layers, suggesting its role in early carcinogenesis [46]. CD44 is overexpressed during the initial stages of tumor development, migration, proliferation, and metastasis [45]. There is also evidence of cell-surface CD44 enrichment in recurring tumors, which occurs during CD44 cleavage and release by metalloproteinases [46]. The transmembrane protein CD44 has a standard form (CD44s), and a family of splice variants, CD44v1‚ÄìCD44v10, are produced by alternative splicing [47]. The CD44 protein consists of two sets of exons. Exons 1 through 5 and 16 through 20 create the standard CD44. Exons 6‚Äì15 (referred to as variable exons v1‚Äìv10) can be alternatively spliced and inserted between exons 5 and 16 [48]. CD44s is the smallest and most common isoform of CD44, at 85‚Äì95 kDa, and lacks the variable region generated by alternative splicing. It is expressed mainly on hematopoietic cells, particularly leukocytes, and helps immune cells traffic and adhere [47]. Other isoforms, such as CD44v8‚ÄìCD44v10, are found in some epithelial cells. In normal tissue, the expression of CD44 isoforms with combinations of other variant exons is less prevalent. Still, in hematopoietic cells, particularly peripheral blood mononuclear cells and reactive lymph node cells, variant isoforms are expressed. The variant forms of CD44 are expressed in restricted locations; CD44v6 and CD44v9 are expressed on T lymphocytes and leucocytes, whereas CD44v8, CD44v9, and CD44v10 are expressed on epithelial cells and keratinocytes [47]. There is evidence that the breast and pancreatic ducts express CD44v6 isoforms, whereas the v4-containing isoform is expressed in the normal urothelium. Some cell types, such as hepatocytes, pancreatic acinar cells, and tubules of the kidney and pancreas, do not express any CD44 isoforms [5]. The expression of specific CD44 isoforms is correlated with the corresponding cancer type [49] (seeSupplementary documentfor a more detailed discussion). CD44 cleavage, also known as shedding, is a key process in cancer progression and contributes to tumor invasion, metastasis, and cellular signaling. The shedding process releases sCD44 into the serum, and its level often correlates with tumor burden in cancers such as gastric, colon, and non-Hodgkin lymphoma, and typically decreases after tumor removal or chemotherapy [26,49]. This process is initiated by proteolytic cleavage of the CD44 ectodomain, predominantly mediated by ADAM10, ADAM17, and MMP14 (MT1-MMP), and driven by stimuli such as Ca2+influx, protein kinase C activation, and small GTPases [50]. These cleavages modulate cell adhesion, migration, and invasion by reducing CD44‚Ä≤s adhesive properties and enhancing its role in cell mobility within the extracellular matrix. CD44 ectodomain shedding often occurs at the leading edge of migrating tumor cells, where it forms a complex with MMP14 to facilitate cytoskeletal interactions and tumor invasion [50]. This process is further supported by diverse triggers, including extracellular ATP, which induces CD44 shedding via the P2X7 receptor in macrophage-like cells, and by Ras oncoproteins and growth factors such as epidermal growth factor [51]. The ECM interaction is further refined by other proteases, such as meprin Œ≤, which are predominantly active in tissues such as the gastrointestinal tract, kidneys, and immune cells. Together, these mechanisms position CD44 as a dynamic regulator of tumor cell behavior [51]. Following ectodomain shedding, CD44 undergoes intramembrane proteolysis mediated by Œ≥-secretase. This cleavage generates the intracellular domain of CD44, which translocates to the nucleus to activate transcription factors, including tetradecanoylphorbol acetate-responsive elements and the transcriptional coactivator CBP/p300 [52]. These processes drive the expression of genes involved in cell survival, proliferation, and migration, establishing a feedback loop that amplifies CD44‚Ä≤s role in tumor progression and metastasis [52]. Early studies revealed that at least five prostate cancer cell lines released the CD44 protein into the media by shedding it when grown in serum-free media. Cell surface expression decreases 3‚Äì7-fold, whereas circulatory protein levels increase 6‚Äì20-fold [41]. Studies have shown that CD44 cleavage is widespread across various malignancies, with ectodomain cleavage detected in 58% of gliomas, 67% of breast carcinomas, 90% of colon carcinomas, and other cancers, including non-small cell lung carcinoma and ovarian carcinoma. These findings suggest a direct role for CD44 shedding in tumor malignancy and metastasis. In particular, the hyaluronic acid-CD44 axis in the tumor microenvironment promotes epithelial-to-mesenchymal transition, thereby further enhancing cancer cell invasion and migration [53]. Therapeutic approaches targeting CD44 cleavage and its associated pathways are emerging as promising strategies in cancer treatment. Inhibiting proteases such as ADAM10 or Œ≥-secretase can reduce CD44 shedding, thereby inhibiting its contribution to tumor growth and metastasis. Similarly, interventions such as trastuzumab, which inhibits ErbB-2-mediated shedding, have potential in controlling the impact of CD44. By disrupting CD44 turnover at the cell surface, these therapies may effectively reduce cancer cell migration and metastatic potential, offering new hope for improving patient outcomes [25].",
            "3. The Role of Soluble CD44 Protein as a Cancer Biomarker": "3.1. SolCD44 as a Diagnostic Biomarker for Early Cancer Detection and Classification of Cancer into SubgroupsEarly identification of malignancies greatly enhances treatment outcomes and survival rates, underscoring the clinical relevance of biomarkers. Salivary solCD44 has strong diagnostic potential for head and neck squamous cell carcinoma (HNSCC). In a study involving 102 HNSCC patients and 69 controls, solCD44 distinguished malignant cases from benign cases with 62% sensitivity and 88% specificity [54]. Franzmann and Donovan [55] also validated OncAlert‚Ñ¢, a point-of-care salivary test for oral and oropharyngeal cancer, further supporting its clinical utility. The OncAlert‚Ñ¢ test was effectively used in early screening, and the total salivary protein also emerged as a complementary early marker of oral cancer. In potentially malignant oral disorders such as leukoplakia and erythroplakia, elevated salivary solCD44 levels were correlated with disease presence and transformation risk [45]. Another study (2011) [44] reported that the concentration of solCD44 in patients with HNSCC was significantly elevated compared with that in healthy individuals.The potential role of solCD44 in non-Hodgkin‚Äôs lymphoma (NHL), breast, and colorectal carcinoma has been investigated [56], and its impact on disease prognosis has been determined. In this study, the total level of solCD44 in the serum of patients with non-Hodgkin lymphoma was higher than that in patients with breast or colorectal cancer, suggesting the potential diagnostic value of solCD44 in NHL.Another group conducted a study on the expression and shedding of CD44 variant isoforms in patients with gynecologic malignancies. It evaluated the presence of CD44 isoforms in ascitic fluid and serum samples from patients with gynecologic cancers. This study included patients at various stages of gynecologic malignancies and analyzed the occurrence of solCD44v4/5 and solCD44v6 in tumor cells, serum, and ascitic fluid. The findings revealed that ovarian tumor cells expressed significant levels of CD44, particularly the v4/5 and v6 isoforms. CD44 isoforms were detectable in the serum of 6 of 8 cancer patients and in 12 of 16 ascitic fluid samples. Notably, all CD44-positive sera expressed detectable levels of variant CD44, with solCD44v6 being present in all positive samples. In ascitic fluid, CD44 is associated mainly with membrane fragments (vesicles). The study concluded that the differential expression and shedding of CD44 isoforms might play crucial roles in tumor metastasis and immune evasion [57]. In patients with cervical cancer, solCD44 levels were significantly elevated compared with those in controls (664.80 vs. 275.19 ng/mL), with even higher concentrations in postmenopausal patients than in premenopausal patients (658.88 vs. 593.69 ng/mL) [1]. Notably, solCD44v6 was detectable, whereas the standard and v5 isoforms were not detectable, suggesting isoform-specific relevance.Identifying cancer molecular subtypes is crucial for guiding targeted therapy decisions, predicting treatment response, and improving patient outcomes. Because cancers of the exact tissue origin can differ significantly in their genetic, phenotypic, and immunologic profiles, biomarkers that help distinguish these subtypes are essential for the development of personalized medicine strategies. SolCD44, particularly its isoforms, has shown potential to reflect tumor aggressiveness and subtype characteristics in some types of cancers. Elevated serum levels of solCD44v5 and solCD44v6 were detected in node-positive breast cancer patients compared with node-negative patients and healthy controls, whereas solCD44std levels remained unchanged [58]. Urinary CD44 levels were positively correlated with high-grade bladder cancer compared with both control subjects and patients with low-grade tumors [59]. This study highlights solCD44 as a noninvasive biomarker that could aid in distinguishing aggressive bladder cancer subtypes and informing treatment decisions.SolCD44std and solCD44v6 were measured in pleural fluid to identify whether malignant pleural effusion was spreading [60]. SolCD44v6 was found mainly in the pleural fluid of nonmesothelioma malignancies. solCD44v6 was also identified as a potential biomarker for differential diagnosis of pleural fluid malignancy.Another set of studies revealed that solCD44 is less reliable as a diagnostic cancer biomarker. When solCD44 levels were evaluated across vocal fold lesion types, interestingly, the highest concentrations were observed in benign lesions and low-grade dysplasia, with a decline in high-grade and invasive cancers [61]. This inverse trend suggests that solCD44 may not reliably indicate malignancy risk in vocal fold pathology. In HPV-related oropharyngeal squamous cell carcinoma (OPSCC), a comparative analysis of salivary biomarkers indicated that DNA detection (particularly that of the HPV E6/E7 genes) was the most accurate. While solCD44 and other protein markers have shown potential, they are less reliable than genetic markers for early OPSCC diagnosis [62]. An earlier work from 1999 studied the concentration of solCD44v6 in HNSCC [63] and found no significant difference between patients and healthy controls, whereas a more recent study from 2011 [44] analyzed CD44 in all its forms and reported increased levels in cancer patients and demonstrated a correlation. A study by Kainz et al. showed that serum concentrations of solCD44std and solCD44v5 were not significantly associated with whether cervical cancer was present or absent [31]. In non-small cell lung cancer (NSCLC), standard solCD44 and isoform 6 levels did not differ between cancer patients and the control group (benign lung disease). In contrast, isoform 6 levels in squamous cell carcinoma patients were significantly higher than in the control group. Neither protein form was associated with cancer stage or metastasis in NSCLC [64]. In bone cancer, solCD44std and v5 are downregulated in all patients regardless of metastasis, limiting diagnostic utility [65]. Similarly, in pediatric sarcoma patients, solCD44 levels are comparable to those in healthy controls, indicating minimal relevance [66]. 3.2. SolCD44 as a Prognostic BiomarkerNumerous studies have explored the role of solCD44 isoforms as biomarkers of cancer progression, particularly concerning TNM stage, lymph node involvement, tumor size, and distant metastasis. In breast cancer, solCD44v6 levels were significantly elevated in patients with metastatic disease and were positively correlated with the number of metastatic sites [67]. In contrast, solCD44v5 has demonstrated limited prognostic utility because of its increased levels in smokers and individuals with chronic inflammatory conditions [68]. Another study [69] confirmed the prognostic value of solCD44v6, reporting higher mean levels in cancer patients (269.2 ¬± 94.3 ng/mL) than in controls (179.5 ¬± 50.7 ng/mL), whereas solCD44std was not significantly correlated with disease status. In HER2-positive breast cancer, elevated serum CD44 levels are associated with worse overall survival, whereas no such association was found in HER2-negative patients [70]. Notably, in triple-negative breast cancer, serum solCD44 levels are higher than those in luminal subtypes (mean: 506.8 ¬± 175.5 ng/mL vs. 406.4 ¬± 68.3 ng/mL), suggesting its possible association with more aggressive disease [71].Prognostic role of solCD44 has also been studied in other forms of cancer. In colorectal cancer, solCD44v6 has consistently been associated with advanced Dukes‚Äô stage, lymph node metastasis, and poor prognosis [72]. Patients with solCD44v6 levels >140.9 ng/mL had significantly worse outcomes than those with lower levels. Subsequent studies confirmed that solCD44v6 remained elevated post-surgery and was positively correlated with tissue expression [73]. Another study [74] revealed that solCD44v6, but not v5, was elevated in patients with bone and liver metastases, with serum concentrations reflecting its expression in primary tumor tissues.In gastric cancer, solCD44v5 and v6 are elevated in advanced-stage patients and are correlated with tumor invasion, nodal involvement, and metastasis [75]. Both isoforms, although also present in healthy individuals, were significantly reduced following tumor resection, supporting their tumor-associated origin. A larger cohort study [38] further confirmed the prognostic value of solCD44v6 in diffuse-type gastric carcinoma, showing correlations with tumor depth, lymph node metastasis, and clinical stage. However, neither soluble nor tissue CD44v6 was associated with survival in patients with intestinal-type gastric cancer.In OSCC, serum solCD44 levels increase with histological grade, with the highest levels observed in poorly differentiated tumors [76]. The concentrations in OSCC patients were also significantly higher than those in controls, supporting the value of solCD44 as both a diagnostic and prognostic biomarker. Assessing salivary solCD44v6 in 66 laryngeal squamous cell carcinoma patients revealed significantly higher levels in those with larger primary tumors (T3‚ÄìT4), supraglottic involvement, nodal metastasis, and advanced-stage disease, suggesting that solCD44v6 has prognostic utility in laryngeal cancer [40]. Strong solCD44v6 expression in recurrent tumor tissue was correlated with poor survival and recurrence, supporting its value as a prognostic biomarker [77]. The study of salivary solCD44v6 in laryngeal carcinoma patients revealed its potential as a prognostic biomarker [78]. Although the evidence regarding the correlation between CD44 isoforms and distinct clinicopathologic subtypes in head and neck cancers remains inconclusive, the findings suggest that solCD44v6 could contribute to subtype differentiation and prognosis in laryngeal cancers. Salivary solCD44 was evaluated as a postsurgical prognostic marker. The mean levels decreased from 70.75 ng/mL pre-surgery to 31.1 ng/mL at six months post-surgery, with no recurrences observed in patients whose levels declined [78].Another study evaluated the potential of pre-surgical serum CD44 levels as a non-invasive prognostic tool in oral cancer [77]. Serum samples from 64 primary oral cancer patients and 16 healthy individuals were analyzed using ELISA, and solCD44 levels were correlated with clinical outcomes over a median follow-up period of 19.2 months. solCD44 levels were significantly higher in patients compared to healthy individuals and were also elevated in those with local recurrence compared to non-recurrent cases. High serum CD44 levels were associated with reduced survival, while solCD44v6 staining in recurrent tumors was significantly stronger than in primary tumors, correlating with poor prognosis. Patients with both elevated serum CD44 and solCD44v6 expression in tumors showed a strong association with local recurrence and poor survival. These findings suggest that pre-surgical serum CD44 levels, assessed through ELISA, could serve as a reliable, non-invasive method to identify high-risk recurrent tumors and guide post-surgery treatment strategies.The clinical importance of solCD44 in hematologic cancers has been demonstrated by the detection of solCD44std in patients with acute leukaemias, including acute lymphoid leukemia (ALL) and acute myeloid leukamia (AML), using a custom-made ELISA [79]. Additionally, it was measured in patients with myelodysplastic syndrome or chronic myeloid leukemia (CML) and in healthy individuals. The results revealed that solCD44 was significantly elevated in patients with acute leukemia and CML compared with healthy volunteers. The study of the role of solCD44 in NHL [39] suggested that solCD44v6 concentrations, measured via ELISA, are increased in patients with NHL. This study included 201 patients with Hodgkin‚Äôs disease, NHL, or adult T-cell leukamia/lymphoma (ATLL). Patients with aggressive NHL were divided into groups based on the solCD44v6 cutoff of 800 ng/mL. These findings indicated that elevated solCD44v6 in patients with NHL was associated with decreased overall survival and with stage III or IV disease, suggesting that solCD44v6 may be used to predict and evaluate disease progression and survival. Moreover, there was a strong correlation between high serum lactate dehydrogenase (LDH) levels and increased solCD44v6 levels, suggesting that solCD44v6 may be an independent prognostic indicator during cancer therapy in clinical settings and a predictor of disease outcome.In acute leukemia, solCD44 levels in patients with AML and ALL were approximately four times higher than those in healthy controls before treatment [80]. These levels correlate with leukemic burden and clinical status, suggesting that solCD44 may serve as a prognostic indicator. A minor increase in solCD44 was also noted in bacterial infections. A study was conducted to elucidate the roles of solCD44std and solCD44v6 and their impact on clinical prognosis and outcome [35]. They reported a significant association between increased total solCD44std and solCD44v6 in 90 patients with confirmed B-CLL. The median and mean values of solCD44 were also compared with those of healthy individuals in this study. Moreover, findings indicate a correlation between higher values of both solCD44 variants and progression of disease stage and alteration of lymph nodes. Elevated solCD44std was associated with increased numbers of leukocytes, whereas solCD44v6 was linked to cases of splenomegaly. Taken together, these results provide evidence that both elevated solCD44std and solCD44v6 could play pivotal roles in the pathogenesis of B-CLL and could serve as prognostic parameters in clinical settings to help clinicians assess patients‚Äô conditions and guide treatment.In ovarian cancer, solCD44v5 and v6 were significantly elevated in patients with advanced FIGO stage (III‚ÄìIV) disease compared with patients with early-stage (I) disease, whereas solCD44std showed no meaningful correlation with clinicopathological features [32]. Another study analyzed preoperative serum from ovarian cancer patients and reported that solCD44v6 was elevated in those with pelvic lymph node metastases. SolCD44v5 was reduced in progesterone receptor-positive tumors and postmenopausal patients and was associated with shorter relapse-free survival. However, no significant associations with stage or overall survival were found [30].A study on lung cancer revealed that people with increased serum CD44v6, together with its tissue expression, had more unfavorable outcomes, and serum CD44v6 was an independent prognostic factor in patients with NSCLC. The protein‚Äôs standard form, both in tissues and in serum, was not a predictive factor for survival. In addition, soluble protein levels were higher in male patients or smokers than in female participants or nonsmokers who participated in the study [81].In contrast, limited or no significant correlation was observed in prostate, renal, and pancreatic cancers. In prostate cancer, solCD44v5 levels are lower in cancer patients than in controls, and no correlation was found between solCD44std or v6 and clinical stage [82,83]. In renal cancer, male patients presented lower solCD44 levels than healthy individuals did [84]. A study on pancreatic cancer reported a negative correlation between the solCD44 concentration and clinical prognosis, although data remain scarce [85]. 3.3. SolCD44 as a Predictive BiomarkerThe ability to predict and monitor therapeutic response is critical in cancer management, and numerous studies have investigated solCD44 isoforms, particularly solCD44v6 and solCD44std, for this purpose across various malignancies.SolCD44v6 has demonstrated predictive value in breast cancer patients undergoing second line hormone- or chemotherapy. Higher concentrations were observed in nonresponders to second-line hormones or chemotherapy (median: 447 ng/mL) than in responders (171 ng/mL), with values ‚â• 250 ng/mL in patients with liver metastasis indicating poor therapy response [86]. In vitro studies using breast and endometrial cancer cell lines revealed that hormonal treatments, such as estradiol, tamoxifen, and GnRH analogs, modulated solCD44 levels variably, suggesting hormone-specific regulation of CD44 isoform secretion and a complex relationship with therapeutic response [87].There is evidence of solCD44 and its isoforms being a predictive biomarker in head and neck cancer. Thus, a study has reported higher pretreatment serum levels of the solCD44 isoforms (std, v5, v6) than in controls, with significant reductions following therapy in head and neck cancer [34]. The median pretreatment values were 327 ng/mL (std), 312 ng/mL (v5), and 211 ng/mL (v6). Although no correlation with clinicopathological variables was observed, pretreatment levels correlated with TNM stage and declined after treatment, suggesting a role in monitoring therapy response. Additionally, tissue CD44 expression is predictive of local recurrence after radiotherapy, prompting further exploration of solCD44 as a noninvasive monitoring tool [88]. In oral and maxillofacial cancers, serum solCD44v6 does not differ significantly across stages but decreases after treatment (surgery, chemotherapy or a combination), suggesting the potential for therapy monitoring rather than diagnosis [89].While previous studies have shown limited usage of solCD44 in general prostate cancer, a more recent investigation revealed its potential role in identifying docetaxel-resistant castration-resistant prostate cancer [90]. Proteomic analysis of resistant cell lines and patient serum identified elevated CD44 levels as an independent predictor of poor survival. Silencing CD44 restored docetaxel sensitivity, indicating its possible use as both a predictive biomarker and a therapeutic target in treatment-resistant castration-resistant prostate cancer.Some studies highlighted a link between solCD44 levels and therapy response in gastrointestinal cancer. In patients who underwent surgery for metastatic colon cancer, solCD44 levels significantly decreased three weeks postresection. Elevated solCD44 levels (up to 30.8 nM) were observed in patients with metastatic gastric cancer compared with healthy controls (2.7 nM), with significant reductions following surgery [26]. These findings suggest that solCD44 may reflect the tumor burden and could serve as a noninvasive marker for monitoring postsurgical response. In another study, 14 of 15 patients with metastatic colon cancer presented elevated solCD44 levels, which correlated with disease extent. However, short-term follow-up in chemotherapy-treated patients revealed no consistent association between solCD44 (std and v6) levels and clinical outcome, limiting its reliability compared with established markers such as CEA [91].Some studies have shown significantly elevated solCD44std in patients with NHL, chronic lymphocytic leukemia (CLL), and acute leukemia, which decreases following successful chemotherapy [49,92]. For example, CLL patients with solCD44std levels above 642 ng/mL had more advanced disease and elevated LDH/Œ≤2-microglobulin. However, solCD44v6 was less predictive. While solCD44std has potential as a surrogate endpoint for progression-free survival in CLL patients, defining cutoff values and standardizing it remain challenges. In pediatric leukemia patients, solCD44 levels are significantly higher at diagnosis and decline during remission, confirming its dynamic response to treatment [93]. 3.4. SolCD44 as a Co-Player with Other BiomarkersSingle biomarkers are often inadequate because they fail to capture the complexity of heterogeneous tumors; therefore, a multiplex assay that detects multiple biomarkers could be advantageous [94] A study by Samy et al. [95] highlighted the clinical implications of serum solCD44 in patients with B-CLL. ELISA assessed serum concentrations of solCD44 and interferon gamma (IFN-Œ≥), which were significantly elevated in patients with B-CLL compared with healthy individuals. This work also demonstrates the prognostic value of CD44 and IFN-Œ≥ in disease progression and sheds light on their potential role in the pathogenesis of B-CLL through their suppression and as promising therapeutic approaches. In addition, it is correlated with the level of another B-CLL biomarker (Œ≤2-microglobulin) and the number of leukemic B cells found in the blood [96]. The protein is also associated with poor prognosis [93]. solCD44 was studied as a metastasis-promoting factor, together with other potential biomarkers (8 in total, including angiogenesis and basement membrane invasion factors), to evaluate its prognostic and therapeutic roles in NSCLC. High levels of CD44 are correlated with increased risk of recurrence and survival probability, together with decreased levels of E-selectin [97]. CD44, transglutaminase 2 (TGM2), and epithelial cell adhesion molecule (EpCAM) were identified as novel plasma markers for endometrial cancer diagnosis using the MAGPIX¬ÆSystem, which employs magnetic nanoparticles coated with antibodies and requires a tiny sample volume (25 ¬µL) [98]. A model using these biomarkers showed high sensitivity (84%) and specificity (100%).Another set of studies involving multiple biomarkers reported that solCD44 is only a performance enhancer of the assay or has a negative association with cancer. In a case‚Äîcontrol study of bladder cancer-associated biomarkers, voided urine samples from 127 subjects, 64 with bladder cancer patients and 63 controls, were examined, and the urine concentrations of four different biomarkers, including the CD44 protein, were assessed via ELISA [94]. The proteins were detected in samples from both the tumor-bearing and control groups. Interestingly, the mean urinary CD44 concentration was higher in subjects without bladder cancer than in those with bladder cancer (117.22 ng/mL vs. 53.09 ng/mL,p< 0.0001). Although CD44 was less accurate than CCL18 in the present study, detecting all three biomarkers (CCL18, PAI-1, and CD44) together might improve the model‚Äôs performance by reducing error.A recent study used targeted liquid chromatography‚Äìtandem mass spectrometry (LC‚ÄìMS/MS) (not ELISA, as in almost all other abovementioned studies) to study the role of several biomarkers in colon cancer [99]. CD44, together with three other biomarkers (GC, CRP, and ITIH3), showed the best performance in discriminating regional cancer (lymph node invasion) from localised cancer. However, CD44 was not among the five best biomarkers in the panel with the best performance (70% specificity at over 89% sensitivity).The urinary concentrations of 14 biomarkers were investigated to determine their diagnostic value for bladder cancer detection [100]. An assessment of voided urine from 127 patients was performed in a case‚Äîcontrolled study in which 64 patients had bladder cancer. Fourteen biomarkers, including CD44, were assessed by ELISA. Thus, the mean and median urinary levels of CD44 are negatively associated with bladder cancer. 3.5. Comparison of CD44 Expression in Tissue and Biological FluidSeveral studies have investigated solCD44 and its expression in tissues as a cancer biomarker. A study detected three isoforms of circulatory CD44 (solCD44v8-10) in people with colon cancer [101]. The protein level was lower in those with negative immunostaining results, whereas the amount of CD44v8-10 in the blood was notably increased in patients whose tissues showed a strong response to solCD44v8-10. The average signal (optical density unit) in these individuals was 5.5 times greater than that in cancer patients who did not exhibit solCD44v8-10 tissue reactivity. There is a significant association between the level of serum CD44v8-10 and the presence of solCD44v8-10 in tumor tissues. This study proposes that serum CD44v8-10 levels may serve as a biomarker for detecting the progression of colorectal cancer through the bloodstream. A clinical study was conducted to evaluate CD44 splice variant expression in ovarian cancer using both immunohistochemical and serological analyses [37]. The study involved 22 patients diagnosed with different stages of ovarian cancer, classified according to the International Federation of Gynecology and Obstetrics (FIGO) system, which is used to stage gynecologic cancers based on tumor spread and extent. Researchers have assessed the levels of solCD44std, solCD44v5, and solCD44v6 in both tumor tissue and serum samples. Immunohistochemical analysis revealed minimal expression of solCD44v5 and solCD44v6 in tumor tissues. Similarly, the blood levels of these isoforms did not differ significantly between patients with active disease and those in remission, or between patients with active disease and healthy individuals. The average serum concentrations of solCD44std, solCD44v5, and solCD44v6 were not substantially elevated in preoperative samples. These findings suggest that CD44 splice variants are expressed at low levels in malignant ovarian tumors and that their serum levels do not reliably reflect tumor burden. Additionally, the results of this study indicate that solCD44std levels in serum may be influenced more by hematopoietic activity than by the presence of ovarian cancer. On the other hand, while semiquantitative assessment can distinguish between negative and positive reactions, it lacks the precision needed to evaluate intermediate staining patterns. By using serum for quantitative analysis, it is possible to overcome the limitations associated with semiquantitative immunohistochemical staining. 3.6. CD44 as a Component of Serum ExosomesA new path for studying the role of solCD44 lies in elucidating its function in serum exosomes, which mediate cellular communication. Thus, several studies have isolated the CD44 protein as part of tumor exosomes. Exosomes are a type of extracellular vesicle that originate from endosomes. From a clinical perspective, exosomes are a source of biomarkers [102]. They serve as message carriers in the tumor microenvironment and alter signaling in the cells that receive them [103]. Although exosomes circulate in biological fluids, the CD44 they carry is the membrane-associated form rather than the proteolytically shed soluble CD44, as exosomal CD44 is transferred intact from the cancer cell membrane to recipient cells, similarly to what has been demonstrated in ovarian cancer where exosomal CD44 increases CD44 levels in mesothelial cells and promotes invasion [104]. Exosomes are most commonly isolated and purified via ultracentrifugation or gradient ultracentrifugation. Exosomes containing CD44 were first isolated by differential centrifugation, then lysed and analysed by Western blotting [103].Interestingly, compared with those derived from nonresistant cells, exosomes derived from chemoresistant cells contained a higher level of CD44. This research suggested that breast cancer cells can disseminate chemoresistance via exosomal proteins, particularly CD44 [105]. Another study identified CD44 in exosomes as a cargo protein that transfers the lymph node metastasis phenotype from cells with high lymphatic metastatic potential to primary gastric cancer cells [106]. As a result, the cells also have increased metastatic potential. When studying how exosomes derived from epithelial ovarian cells mediate metastasis, CD44 was found to be increased on exosomes and to promote invasion [104]. The authors proposed capturing CD44-expressing exosomes as an alternative, less toxic antitumor therapy, similar to HER2osomes.Investigation of proteins inside extracellular vesicles from glioblastoma patients revealed the upregulation of 6 proteins, including CD44, compared with those in healthy volunteers [107]. Three of these protein panels (including CD44) showed significant increases with tumor progression. Most importantly, the combined detection of five of those biomarkers was able to detect tumor progression. One advantage of studying extracellular vesicles in glioblastoma is that they reflect the actual state of the cells from which they were derived. CD44, together with MMP14 and BCG, is part of a subpopulation of extracellular vesicles for glioblastoma invasion. 3.7. SolCD44 in Different Biological FluidsThe soluble form of CD44 has been identified as a cancer biomarker in biological fluids, including serum, saliva, urine, ascitic fluid, and pleural fluid. The analysis of scientific papers published between 1994 and 2024 on the detection of solCD44 protein in biological fluids, broken down by type of fluid (blood, saliva, urine/pleural fluid) and method of analysis (ELISA, biosensors, other analyses), is shown inFigure 1. Most publications were between 1994 and 1999, with almost all using ELISA and focusing on blood samples. Serum is considered the most crucial sample used in diagnostics for three main reasons: it is obtained minimally invasively; it has relatively high stability; and it accurately reflects an organism‚Äôs physiological condition [108]. Serum is readily accessible and allows for objective measurement; moreover, the data can be obtained before surgery and may aid in optimal preoperative planning [69]. Serum is a complex medium, and biomarkers are usually present at low levels; efficient methods of detection and characterization are mostly antibody-based assays [109]. However, according to a recent review that mapped sampling types by invasiveness, blood is near the high end of invasiveness [110].Figure 1.Publications per year for the detection of soluble CD44 protein in biological fluids, based on papers published from 1994‚Äì2024.For solCD44, as it is found in biological fluids, urine is the least invasive sampling method, followed by saliva. There is controversial data in the literature on the role of CD44 protein in urine as a biomarker for early diagnosis of bladder cancer. A study [59] showed that urinary CD44 could be used to differentiate an aggressive form of bladder cancer from a low-grade tumor. However, in another study, solCD44 in urine showed no association with bladder cancer [100]; another study showed a negative association with bladder cancer [100]. According toFigure 1, since 2015, biological fluids other than blood (saliva and urine/pleural fluid) have also been studied, possibly indicating a shift toward non-invasive sampling methods.solCD44 was also found in the saliva of cancer patients. Saliva is produced by the salivary glands to clean and protect the mouth, aid digestion, and help disinfect the mouth. It is considered a significantly diluted body fluid containing electrolytes, nitrogenous products, and proteins [111]. Biomarkers in saliva have been recognized as early indicators of oral and systemic diseases, such as breast, lung, pancreatic, prostate, periodontal diseases, oral cancer, and diabetes mellitus [112], as well as HNSCC, which accounts for most cancers of the mouth, pharynx, and larynx [54]. About 90% of saliva is produced by the salivary glands, which include the parotid gland, submandibular glands, and sublingual glands. Due to its high permeability, biomarkers in the blood can be readily secreted into saliva. Saliva is a potential source of protein biomarkers that could enhance the diagnosis and clinical outcomes of patients with OSCC and breast cancer, using ELISA as the gold standard for biomarker detection. However, this method does not detect many biomarkers and is tedious and expensive [111]. Given that saliva contains hundreds of components that could serve as biomarkers for systemic diseases [112], it is an essential biological fluid for investigating physiological and pathological conditions, such as cancer. It is easily accessible and relatively non-invasive. However, some biological markers are present at low concentrations. Hence, there is a need for improved technologies with lower detection limits [112]. The use of saliva for diagnosis and surveillance is a promising strategy, as the specimen can be collected non-invasively and inexpensively compared to blood or biopsy [113].Less-represented fluids as sources of solCD44 include pleural fluid and cystic lesions. For instance, a study evaluated the diagnostic potential of soluble adhesion molecules, including CD44, for differentiating benign from malignant ovarian cystic tumors [114]. Serum and cyst fluid samples from 77 patients with various ovarian cystic lesions were analyzed using ELISA kits to measure soluble ICAM-1, solCD44std, and soluble E-cadherin levels. The findings revealed that, while serum levels of sCD44 did not differ significantly between benign and malignant tumors, its concentrations in cyst fluid were notably higher in borderline and malignant tumors than in benign cystadenomas. This suggests that solCD44std, particularly in cyst fluid, may help distinguish malignant from benign tumors, including borderline tumors. Although serum levels of these adhesion molecules showed limited diagnostic value, elevated levels of solCD44std and other markers in cyst fluid highlight their potential as valuable tools to improve the diagnostic accuracy of ovarian cystic tumors.Other fluids were found not to be a good source of solCD44 for cancer diagnosis. Another study also analyzed levels of solCD44 isoforms in ascitic fluid collected from ovarian cancer patients [36]. They found that the concentrations of solCD44v5 and solCD44v6 were significantly lower in ascitic fluid compared to serum. Specifically, CD44v5 levels were nearly half of those observed in serum, while solCD44v6 levels were also markedly reduced. In contrast, standard CD44 levels showed no significant difference between serum and ascitic fluid. These findings suggest that the tumor microenvironment, as represented by ascitic fluid, may not be the primary source of solCD44v5 and solCD44v6, indicating that these molecules are likely derived from systemic sources, such as immune or stromal cells, rather than directly from tumor cells.",
            "3.1. SolCD44 as a Diagnostic Biomarker for Early Cancer Detection and Classification of Cancer into Subgroups": "Early identification of malignancies greatly enhances treatment outcomes and survival rates, underscoring the clinical relevance of biomarkers. Salivary solCD44 has strong diagnostic potential for head and neck squamous cell carcinoma (HNSCC). In a study involving 102 HNSCC patients and 69 controls, solCD44 distinguished malignant cases from benign cases with 62% sensitivity and 88% specificity [54]. Franzmann and Donovan [55] also validated OncAlert‚Ñ¢, a point-of-care salivary test for oral and oropharyngeal cancer, further supporting its clinical utility. The OncAlert‚Ñ¢ test was effectively used in early screening, and the total salivary protein also emerged as a complementary early marker of oral cancer. In potentially malignant oral disorders such as leukoplakia and erythroplakia, elevated salivary solCD44 levels were correlated with disease presence and transformation risk [45]. Another study (2011) [44] reported that the concentration of solCD44 in patients with HNSCC was significantly elevated compared with that in healthy individuals. The potential role of solCD44 in non-Hodgkin‚Äôs lymphoma (NHL), breast, and colorectal carcinoma has been investigated [56], and its impact on disease prognosis has been determined. In this study, the total level of solCD44 in the serum of patients with non-Hodgkin lymphoma was higher than that in patients with breast or colorectal cancer, suggesting the potential diagnostic value of solCD44 in NHL. Another group conducted a study on the expression and shedding of CD44 variant isoforms in patients with gynecologic malignancies. It evaluated the presence of CD44 isoforms in ascitic fluid and serum samples from patients with gynecologic cancers. This study included patients at various stages of gynecologic malignancies and analyzed the occurrence of solCD44v4/5 and solCD44v6 in tumor cells, serum, and ascitic fluid. The findings revealed that ovarian tumor cells expressed significant levels of CD44, particularly the v4/5 and v6 isoforms. CD44 isoforms were detectable in the serum of 6 of 8 cancer patients and in 12 of 16 ascitic fluid samples. Notably, all CD44-positive sera expressed detectable levels of variant CD44, with solCD44v6 being present in all positive samples. In ascitic fluid, CD44 is associated mainly with membrane fragments (vesicles). The study concluded that the differential expression and shedding of CD44 isoforms might play crucial roles in tumor metastasis and immune evasion [57]. In patients with cervical cancer, solCD44 levels were significantly elevated compared with those in controls (664.80 vs. 275.19 ng/mL), with even higher concentrations in postmenopausal patients than in premenopausal patients (658.88 vs. 593.69 ng/mL) [1]. Notably, solCD44v6 was detectable, whereas the standard and v5 isoforms were not detectable, suggesting isoform-specific relevance. Identifying cancer molecular subtypes is crucial for guiding targeted therapy decisions, predicting treatment response, and improving patient outcomes. Because cancers of the exact tissue origin can differ significantly in their genetic, phenotypic, and immunologic profiles, biomarkers that help distinguish these subtypes are essential for the development of personalized medicine strategies. SolCD44, particularly its isoforms, has shown potential to reflect tumor aggressiveness and subtype characteristics in some types of cancers. Elevated serum levels of solCD44v5 and solCD44v6 were detected in node-positive breast cancer patients compared with node-negative patients and healthy controls, whereas solCD44std levels remained unchanged [58]. Urinary CD44 levels were positively correlated with high-grade bladder cancer compared with both control subjects and patients with low-grade tumors [59]. This study highlights solCD44 as a noninvasive biomarker that could aid in distinguishing aggressive bladder cancer subtypes and informing treatment decisions. SolCD44std and solCD44v6 were measured in pleural fluid to identify whether malignant pleural effusion was spreading [60]. SolCD44v6 was found mainly in the pleural fluid of nonmesothelioma malignancies. solCD44v6 was also identified as a potential biomarker for differential diagnosis of pleural fluid malignancy. Another set of studies revealed that solCD44 is less reliable as a diagnostic cancer biomarker. When solCD44 levels were evaluated across vocal fold lesion types, interestingly, the highest concentrations were observed in benign lesions and low-grade dysplasia, with a decline in high-grade and invasive cancers [61]. This inverse trend suggests that solCD44 may not reliably indicate malignancy risk in vocal fold pathology. In HPV-related oropharyngeal squamous cell carcinoma (OPSCC), a comparative analysis of salivary biomarkers indicated that DNA detection (particularly that of the HPV E6/E7 genes) was the most accurate. While solCD44 and other protein markers have shown potential, they are less reliable than genetic markers for early OPSCC diagnosis [62]. An earlier work from 1999 studied the concentration of solCD44v6 in HNSCC [63] and found no significant difference between patients and healthy controls, whereas a more recent study from 2011 [44] analyzed CD44 in all its forms and reported increased levels in cancer patients and demonstrated a correlation. A study by Kainz et al. showed that serum concentrations of solCD44std and solCD44v5 were not significantly associated with whether cervical cancer was present or absent [31]. In non-small cell lung cancer (NSCLC), standard solCD44 and isoform 6 levels did not differ between cancer patients and the control group (benign lung disease). In contrast, isoform 6 levels in squamous cell carcinoma patients were significantly higher than in the control group. Neither protein form was associated with cancer stage or metastasis in NSCLC [64]. In bone cancer, solCD44std and v5 are downregulated in all patients regardless of metastasis, limiting diagnostic utility [65]. Similarly, in pediatric sarcoma patients, solCD44 levels are comparable to those in healthy controls, indicating minimal relevance [66].",
            "3.2. SolCD44 as a Prognostic Biomarker": "Numerous studies have explored the role of solCD44 isoforms as biomarkers of cancer progression, particularly concerning TNM stage, lymph node involvement, tumor size, and distant metastasis. In breast cancer, solCD44v6 levels were significantly elevated in patients with metastatic disease and were positively correlated with the number of metastatic sites [67]. In contrast, solCD44v5 has demonstrated limited prognostic utility because of its increased levels in smokers and individuals with chronic inflammatory conditions [68]. Another study [69] confirmed the prognostic value of solCD44v6, reporting higher mean levels in cancer patients (269.2 ¬± 94.3 ng/mL) than in controls (179.5 ¬± 50.7 ng/mL), whereas solCD44std was not significantly correlated with disease status. In HER2-positive breast cancer, elevated serum CD44 levels are associated with worse overall survival, whereas no such association was found in HER2-negative patients [70]. Notably, in triple-negative breast cancer, serum solCD44 levels are higher than those in luminal subtypes (mean: 506.8 ¬± 175.5 ng/mL vs. 406.4 ¬± 68.3 ng/mL), suggesting its possible association with more aggressive disease [71]. Prognostic role of solCD44 has also been studied in other forms of cancer. In colorectal cancer, solCD44v6 has consistently been associated with advanced Dukes‚Äô stage, lymph node metastasis, and poor prognosis [72]. Patients with solCD44v6 levels >140.9 ng/mL had significantly worse outcomes than those with lower levels. Subsequent studies confirmed that solCD44v6 remained elevated post-surgery and was positively correlated with tissue expression [73]. Another study [74] revealed that solCD44v6, but not v5, was elevated in patients with bone and liver metastases, with serum concentrations reflecting its expression in primary tumor tissues. In gastric cancer, solCD44v5 and v6 are elevated in advanced-stage patients and are correlated with tumor invasion, nodal involvement, and metastasis [75]. Both isoforms, although also present in healthy individuals, were significantly reduced following tumor resection, supporting their tumor-associated origin. A larger cohort study [38] further confirmed the prognostic value of solCD44v6 in diffuse-type gastric carcinoma, showing correlations with tumor depth, lymph node metastasis, and clinical stage. However, neither soluble nor tissue CD44v6 was associated with survival in patients with intestinal-type gastric cancer. In OSCC, serum solCD44 levels increase with histological grade, with the highest levels observed in poorly differentiated tumors [76]. The concentrations in OSCC patients were also significantly higher than those in controls, supporting the value of solCD44 as both a diagnostic and prognostic biomarker. Assessing salivary solCD44v6 in 66 laryngeal squamous cell carcinoma patients revealed significantly higher levels in those with larger primary tumors (T3‚ÄìT4), supraglottic involvement, nodal metastasis, and advanced-stage disease, suggesting that solCD44v6 has prognostic utility in laryngeal cancer [40]. Strong solCD44v6 expression in recurrent tumor tissue was correlated with poor survival and recurrence, supporting its value as a prognostic biomarker [77]. The study of salivary solCD44v6 in laryngeal carcinoma patients revealed its potential as a prognostic biomarker [78]. Although the evidence regarding the correlation between CD44 isoforms and distinct clinicopathologic subtypes in head and neck cancers remains inconclusive, the findings suggest that solCD44v6 could contribute to subtype differentiation and prognosis in laryngeal cancers. Salivary solCD44 was evaluated as a postsurgical prognostic marker. The mean levels decreased from 70.75 ng/mL pre-surgery to 31.1 ng/mL at six months post-surgery, with no recurrences observed in patients whose levels declined [78]. Another study evaluated the potential of pre-surgical serum CD44 levels as a non-invasive prognostic tool in oral cancer [77]. Serum samples from 64 primary oral cancer patients and 16 healthy individuals were analyzed using ELISA, and solCD44 levels were correlated with clinical outcomes over a median follow-up period of 19.2 months. solCD44 levels were significantly higher in patients compared to healthy individuals and were also elevated in those with local recurrence compared to non-recurrent cases. High serum CD44 levels were associated with reduced survival, while solCD44v6 staining in recurrent tumors was significantly stronger than in primary tumors, correlating with poor prognosis. Patients with both elevated serum CD44 and solCD44v6 expression in tumors showed a strong association with local recurrence and poor survival. These findings suggest that pre-surgical serum CD44 levels, assessed through ELISA, could serve as a reliable, non-invasive method to identify high-risk recurrent tumors and guide post-surgery treatment strategies. The clinical importance of solCD44 in hematologic cancers has been demonstrated by the detection of solCD44std in patients with acute leukaemias, including acute lymphoid leukemia (ALL) and acute myeloid leukamia (AML), using a custom-made ELISA [79]. Additionally, it was measured in patients with myelodysplastic syndrome or chronic myeloid leukemia (CML) and in healthy individuals. The results revealed that solCD44 was significantly elevated in patients with acute leukemia and CML compared with healthy volunteers. The study of the role of solCD44 in NHL [39] suggested that solCD44v6 concentrations, measured via ELISA, are increased in patients with NHL. This study included 201 patients with Hodgkin‚Äôs disease, NHL, or adult T-cell leukamia/lymphoma (ATLL). Patients with aggressive NHL were divided into groups based on the solCD44v6 cutoff of 800 ng/mL. These findings indicated that elevated solCD44v6 in patients with NHL was associated with decreased overall survival and with stage III or IV disease, suggesting that solCD44v6 may be used to predict and evaluate disease progression and survival. Moreover, there was a strong correlation between high serum lactate dehydrogenase (LDH) levels and increased solCD44v6 levels, suggesting that solCD44v6 may be an independent prognostic indicator during cancer therapy in clinical settings and a predictor of disease outcome. In acute leukemia, solCD44 levels in patients with AML and ALL were approximately four times higher than those in healthy controls before treatment [80]. These levels correlate with leukemic burden and clinical status, suggesting that solCD44 may serve as a prognostic indicator. A minor increase in solCD44 was also noted in bacterial infections. A study was conducted to elucidate the roles of solCD44std and solCD44v6 and their impact on clinical prognosis and outcome [35]. They reported a significant association between increased total solCD44std and solCD44v6 in 90 patients with confirmed B-CLL. The median and mean values of solCD44 were also compared with those of healthy individuals in this study. Moreover, findings indicate a correlation between higher values of both solCD44 variants and progression of disease stage and alteration of lymph nodes. Elevated solCD44std was associated with increased numbers of leukocytes, whereas solCD44v6 was linked to cases of splenomegaly. Taken together, these results provide evidence that both elevated solCD44std and solCD44v6 could play pivotal roles in the pathogenesis of B-CLL and could serve as prognostic parameters in clinical settings to help clinicians assess patients‚Äô conditions and guide treatment. In ovarian cancer, solCD44v5 and v6 were significantly elevated in patients with advanced FIGO stage (III‚ÄìIV) disease compared with patients with early-stage (I) disease, whereas solCD44std showed no meaningful correlation with clinicopathological features [32]. Another study analyzed preoperative serum from ovarian cancer patients and reported that solCD44v6 was elevated in those with pelvic lymph node metastases. SolCD44v5 was reduced in progesterone receptor-positive tumors and postmenopausal patients and was associated with shorter relapse-free survival. However, no significant associations with stage or overall survival were found [30]. A study on lung cancer revealed that people with increased serum CD44v6, together with its tissue expression, had more unfavorable outcomes, and serum CD44v6 was an independent prognostic factor in patients with NSCLC. The protein‚Äôs standard form, both in tissues and in serum, was not a predictive factor for survival. In addition, soluble protein levels were higher in male patients or smokers than in female participants or nonsmokers who participated in the study [81]. In contrast, limited or no significant correlation was observed in prostate, renal, and pancreatic cancers. In prostate cancer, solCD44v5 levels are lower in cancer patients than in controls, and no correlation was found between solCD44std or v6 and clinical stage [82,83]. In renal cancer, male patients presented lower solCD44 levels than healthy individuals did [84]. A study on pancreatic cancer reported a negative correlation between the solCD44 concentration and clinical prognosis, although data remain scarce [85].",
            "3.3. SolCD44 as a Predictive Biomarker": "The ability to predict and monitor therapeutic response is critical in cancer management, and numerous studies have investigated solCD44 isoforms, particularly solCD44v6 and solCD44std, for this purpose across various malignancies. SolCD44v6 has demonstrated predictive value in breast cancer patients undergoing second line hormone- or chemotherapy. Higher concentrations were observed in nonresponders to second-line hormones or chemotherapy (median: 447 ng/mL) than in responders (171 ng/mL), with values ‚â• 250 ng/mL in patients with liver metastasis indicating poor therapy response [86]. In vitro studies using breast and endometrial cancer cell lines revealed that hormonal treatments, such as estradiol, tamoxifen, and GnRH analogs, modulated solCD44 levels variably, suggesting hormone-specific regulation of CD44 isoform secretion and a complex relationship with therapeutic response [87]. There is evidence of solCD44 and its isoforms being a predictive biomarker in head and neck cancer. Thus, a study has reported higher pretreatment serum levels of the solCD44 isoforms (std, v5, v6) than in controls, with significant reductions following therapy in head and neck cancer [34]. The median pretreatment values were 327 ng/mL (std), 312 ng/mL (v5), and 211 ng/mL (v6). Although no correlation with clinicopathological variables was observed, pretreatment levels correlated with TNM stage and declined after treatment, suggesting a role in monitoring therapy response. Additionally, tissue CD44 expression is predictive of local recurrence after radiotherapy, prompting further exploration of solCD44 as a noninvasive monitoring tool [88]. In oral and maxillofacial cancers, serum solCD44v6 does not differ significantly across stages but decreases after treatment (surgery, chemotherapy or a combination), suggesting the potential for therapy monitoring rather than diagnosis [89]. While previous studies have shown limited usage of solCD44 in general prostate cancer, a more recent investigation revealed its potential role in identifying docetaxel-resistant castration-resistant prostate cancer [90]. Proteomic analysis of resistant cell lines and patient serum identified elevated CD44 levels as an independent predictor of poor survival. Silencing CD44 restored docetaxel sensitivity, indicating its possible use as both a predictive biomarker and a therapeutic target in treatment-resistant castration-resistant prostate cancer. Some studies highlighted a link between solCD44 levels and therapy response in gastrointestinal cancer. In patients who underwent surgery for metastatic colon cancer, solCD44 levels significantly decreased three weeks postresection. Elevated solCD44 levels (up to 30.8 nM) were observed in patients with metastatic gastric cancer compared with healthy controls (2.7 nM), with significant reductions following surgery [26]. These findings suggest that solCD44 may reflect the tumor burden and could serve as a noninvasive marker for monitoring postsurgical response. In another study, 14 of 15 patients with metastatic colon cancer presented elevated solCD44 levels, which correlated with disease extent. However, short-term follow-up in chemotherapy-treated patients revealed no consistent association between solCD44 (std and v6) levels and clinical outcome, limiting its reliability compared with established markers such as CEA [91]. Some studies have shown significantly elevated solCD44std in patients with NHL, chronic lymphocytic leukemia (CLL), and acute leukemia, which decreases following successful chemotherapy [49,92]. For example, CLL patients with solCD44std levels above 642 ng/mL had more advanced disease and elevated LDH/Œ≤2-microglobulin. However, solCD44v6 was less predictive. While solCD44std has potential as a surrogate endpoint for progression-free survival in CLL patients, defining cutoff values and standardizing it remain challenges. In pediatric leukemia patients, solCD44 levels are significantly higher at diagnosis and decline during remission, confirming its dynamic response to treatment [93].",
            "3.4. SolCD44 as a Co-Player with Other Biomarkers": "Single biomarkers are often inadequate because they fail to capture the complexity of heterogeneous tumors; therefore, a multiplex assay that detects multiple biomarkers could be advantageous [94] A study by Samy et al. [95] highlighted the clinical implications of serum solCD44 in patients with B-CLL. ELISA assessed serum concentrations of solCD44 and interferon gamma (IFN-Œ≥), which were significantly elevated in patients with B-CLL compared with healthy individuals. This work also demonstrates the prognostic value of CD44 and IFN-Œ≥ in disease progression and sheds light on their potential role in the pathogenesis of B-CLL through their suppression and as promising therapeutic approaches. In addition, it is correlated with the level of another B-CLL biomarker (Œ≤2-microglobulin) and the number of leukemic B cells found in the blood [96]. The protein is also associated with poor prognosis [93]. solCD44 was studied as a metastasis-promoting factor, together with other potential biomarkers (8 in total, including angiogenesis and basement membrane invasion factors), to evaluate its prognostic and therapeutic roles in NSCLC. High levels of CD44 are correlated with increased risk of recurrence and survival probability, together with decreased levels of E-selectin [97]. CD44, transglutaminase 2 (TGM2), and epithelial cell adhesion molecule (EpCAM) were identified as novel plasma markers for endometrial cancer diagnosis using the MAGPIX¬ÆSystem, which employs magnetic nanoparticles coated with antibodies and requires a tiny sample volume (25 ¬µL) [98]. A model using these biomarkers showed high sensitivity (84%) and specificity (100%). Another set of studies involving multiple biomarkers reported that solCD44 is only a performance enhancer of the assay or has a negative association with cancer. In a case‚Äîcontrol study of bladder cancer-associated biomarkers, voided urine samples from 127 subjects, 64 with bladder cancer patients and 63 controls, were examined, and the urine concentrations of four different biomarkers, including the CD44 protein, were assessed via ELISA [94]. The proteins were detected in samples from both the tumor-bearing and control groups. Interestingly, the mean urinary CD44 concentration was higher in subjects without bladder cancer than in those with bladder cancer (117.22 ng/mL vs. 53.09 ng/mL,p< 0.0001). Although CD44 was less accurate than CCL18 in the present study, detecting all three biomarkers (CCL18, PAI-1, and CD44) together might improve the model‚Äôs performance by reducing error. A recent study used targeted liquid chromatography‚Äìtandem mass spectrometry (LC‚ÄìMS/MS) (not ELISA, as in almost all other abovementioned studies) to study the role of several biomarkers in colon cancer [99]. CD44, together with three other biomarkers (GC, CRP, and ITIH3), showed the best performance in discriminating regional cancer (lymph node invasion) from localised cancer. However, CD44 was not among the five best biomarkers in the panel with the best performance (70% specificity at over 89% sensitivity). The urinary concentrations of 14 biomarkers were investigated to determine their diagnostic value for bladder cancer detection [100]. An assessment of voided urine from 127 patients was performed in a case‚Äîcontrolled study in which 64 patients had bladder cancer. Fourteen biomarkers, including CD44, were assessed by ELISA. Thus, the mean and median urinary levels of CD44 are negatively associated with bladder cancer.",
            "3.5. Comparison of CD44 Expression in Tissue and Biological Fluid": "Several studies have investigated solCD44 and its expression in tissues as a cancer biomarker. A study detected three isoforms of circulatory CD44 (solCD44v8-10) in people with colon cancer [101]. The protein level was lower in those with negative immunostaining results, whereas the amount of CD44v8-10 in the blood was notably increased in patients whose tissues showed a strong response to solCD44v8-10. The average signal (optical density unit) in these individuals was 5.5 times greater than that in cancer patients who did not exhibit solCD44v8-10 tissue reactivity. There is a significant association between the level of serum CD44v8-10 and the presence of solCD44v8-10 in tumor tissues. This study proposes that serum CD44v8-10 levels may serve as a biomarker for detecting the progression of colorectal cancer through the bloodstream. A clinical study was conducted to evaluate CD44 splice variant expression in ovarian cancer using both immunohistochemical and serological analyses [37]. The study involved 22 patients diagnosed with different stages of ovarian cancer, classified according to the International Federation of Gynecology and Obstetrics (FIGO) system, which is used to stage gynecologic cancers based on tumor spread and extent. Researchers have assessed the levels of solCD44std, solCD44v5, and solCD44v6 in both tumor tissue and serum samples. Immunohistochemical analysis revealed minimal expression of solCD44v5 and solCD44v6 in tumor tissues. Similarly, the blood levels of these isoforms did not differ significantly between patients with active disease and those in remission, or between patients with active disease and healthy individuals. The average serum concentrations of solCD44std, solCD44v5, and solCD44v6 were not substantially elevated in preoperative samples. These findings suggest that CD44 splice variants are expressed at low levels in malignant ovarian tumors and that their serum levels do not reliably reflect tumor burden. Additionally, the results of this study indicate that solCD44std levels in serum may be influenced more by hematopoietic activity than by the presence of ovarian cancer. On the other hand, while semiquantitative assessment can distinguish between negative and positive reactions, it lacks the precision needed to evaluate intermediate staining patterns. By using serum for quantitative analysis, it is possible to overcome the limitations associated with semiquantitative immunohistochemical staining.",
            "3.6. CD44 as a Component of Serum Exosomes": "A new path for studying the role of solCD44 lies in elucidating its function in serum exosomes, which mediate cellular communication. Thus, several studies have isolated the CD44 protein as part of tumor exosomes. Exosomes are a type of extracellular vesicle that originate from endosomes. From a clinical perspective, exosomes are a source of biomarkers [102]. They serve as message carriers in the tumor microenvironment and alter signaling in the cells that receive them [103]. Although exosomes circulate in biological fluids, the CD44 they carry is the membrane-associated form rather than the proteolytically shed soluble CD44, as exosomal CD44 is transferred intact from the cancer cell membrane to recipient cells, similarly to what has been demonstrated in ovarian cancer where exosomal CD44 increases CD44 levels in mesothelial cells and promotes invasion [104]. Exosomes are most commonly isolated and purified via ultracentrifugation or gradient ultracentrifugation. Exosomes containing CD44 were first isolated by differential centrifugation, then lysed and analysed by Western blotting [103]. Interestingly, compared with those derived from nonresistant cells, exosomes derived from chemoresistant cells contained a higher level of CD44. This research suggested that breast cancer cells can disseminate chemoresistance via exosomal proteins, particularly CD44 [105]. Another study identified CD44 in exosomes as a cargo protein that transfers the lymph node metastasis phenotype from cells with high lymphatic metastatic potential to primary gastric cancer cells [106]. As a result, the cells also have increased metastatic potential. When studying how exosomes derived from epithelial ovarian cells mediate metastasis, CD44 was found to be increased on exosomes and to promote invasion [104]. The authors proposed capturing CD44-expressing exosomes as an alternative, less toxic antitumor therapy, similar to HER2osomes. Investigation of proteins inside extracellular vesicles from glioblastoma patients revealed the upregulation of 6 proteins, including CD44, compared with those in healthy volunteers [107]. Three of these protein panels (including CD44) showed significant increases with tumor progression. Most importantly, the combined detection of five of those biomarkers was able to detect tumor progression. One advantage of studying extracellular vesicles in glioblastoma is that they reflect the actual state of the cells from which they were derived. CD44, together with MMP14 and BCG, is part of a subpopulation of extracellular vesicles for glioblastoma invasion.",
            "3.7. SolCD44 in Different Biological Fluids": "The soluble form of CD44 has been identified as a cancer biomarker in biological fluids, including serum, saliva, urine, ascitic fluid, and pleural fluid. The analysis of scientific papers published between 1994 and 2024 on the detection of solCD44 protein in biological fluids, broken down by type of fluid (blood, saliva, urine/pleural fluid) and method of analysis (ELISA, biosensors, other analyses), is shown inFigure 1. Most publications were between 1994 and 1999, with almost all using ELISA and focusing on blood samples. Serum is considered the most crucial sample used in diagnostics for three main reasons: it is obtained minimally invasively; it has relatively high stability; and it accurately reflects an organism‚Äôs physiological condition [108]. Serum is readily accessible and allows for objective measurement; moreover, the data can be obtained before surgery and may aid in optimal preoperative planning [69]. Serum is a complex medium, and biomarkers are usually present at low levels; efficient methods of detection and characterization are mostly antibody-based assays [109]. However, according to a recent review that mapped sampling types by invasiveness, blood is near the high end of invasiveness [110]. Figure 1.Publications per year for the detection of soluble CD44 protein in biological fluids, based on papers published from 1994‚Äì2024. For solCD44, as it is found in biological fluids, urine is the least invasive sampling method, followed by saliva. There is controversial data in the literature on the role of CD44 protein in urine as a biomarker for early diagnosis of bladder cancer. A study [59] showed that urinary CD44 could be used to differentiate an aggressive form of bladder cancer from a low-grade tumor. However, in another study, solCD44 in urine showed no association with bladder cancer [100]; another study showed a negative association with bladder cancer [100]. According toFigure 1, since 2015, biological fluids other than blood (saliva and urine/pleural fluid) have also been studied, possibly indicating a shift toward non-invasive sampling methods. solCD44 was also found in the saliva of cancer patients. Saliva is produced by the salivary glands to clean and protect the mouth, aid digestion, and help disinfect the mouth. It is considered a significantly diluted body fluid containing electrolytes, nitrogenous products, and proteins [111]. Biomarkers in saliva have been recognized as early indicators of oral and systemic diseases, such as breast, lung, pancreatic, prostate, periodontal diseases, oral cancer, and diabetes mellitus [112], as well as HNSCC, which accounts for most cancers of the mouth, pharynx, and larynx [54]. About 90% of saliva is produced by the salivary glands, which include the parotid gland, submandibular glands, and sublingual glands. Due to its high permeability, biomarkers in the blood can be readily secreted into saliva. Saliva is a potential source of protein biomarkers that could enhance the diagnosis and clinical outcomes of patients with OSCC and breast cancer, using ELISA as the gold standard for biomarker detection. However, this method does not detect many biomarkers and is tedious and expensive [111]. Given that saliva contains hundreds of components that could serve as biomarkers for systemic diseases [112], it is an essential biological fluid for investigating physiological and pathological conditions, such as cancer. It is easily accessible and relatively non-invasive. However, some biological markers are present at low concentrations. Hence, there is a need for improved technologies with lower detection limits [112]. The use of saliva for diagnosis and surveillance is a promising strategy, as the specimen can be collected non-invasively and inexpensively compared to blood or biopsy [113]. Less-represented fluids as sources of solCD44 include pleural fluid and cystic lesions. For instance, a study evaluated the diagnostic potential of soluble adhesion molecules, including CD44, for differentiating benign from malignant ovarian cystic tumors [114]. Serum and cyst fluid samples from 77 patients with various ovarian cystic lesions were analyzed using ELISA kits to measure soluble ICAM-1, solCD44std, and soluble E-cadherin levels. The findings revealed that, while serum levels of sCD44 did not differ significantly between benign and malignant tumors, its concentrations in cyst fluid were notably higher in borderline and malignant tumors than in benign cystadenomas. This suggests that solCD44std, particularly in cyst fluid, may help distinguish malignant from benign tumors, including borderline tumors. Although serum levels of these adhesion molecules showed limited diagnostic value, elevated levels of solCD44std and other markers in cyst fluid highlight their potential as valuable tools to improve the diagnostic accuracy of ovarian cystic tumors. Other fluids were found not to be a good source of solCD44 for cancer diagnosis. Another study also analyzed levels of solCD44 isoforms in ascitic fluid collected from ovarian cancer patients [36]. They found that the concentrations of solCD44v5 and solCD44v6 were significantly lower in ascitic fluid compared to serum. Specifically, CD44v5 levels were nearly half of those observed in serum, while solCD44v6 levels were also markedly reduced. In contrast, standard CD44 levels showed no significant difference between serum and ascitic fluid. These findings suggest that the tumor microenvironment, as represented by ascitic fluid, may not be the primary source of solCD44v5 and solCD44v6, indicating that these molecules are likely derived from systemic sources, such as immune or stromal cells, rather than directly from tumor cells.",
            "4. Assays for CD44 Protein Detection": "A wide range of analytical techniques, including ELISA, biosensors, and lateral-flow assays (LFAs), have been used to detect the solCD44 protein. These assays use several types of ligands as biorecognition elements to target the CD44 protein (seeTable 2). Table 2.Different diagnostic probes (ligands) targeting extracellular/soluble CD44 and their use in assays. References [45,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136]. 4.1. Ligands Against solCD444.1.1. Hyaluronic AcidThe primary ligand of CD44 is hyaluronic acid (HA); the binding of CD44 to HA contributes to a wide variety of physiological and pathological processes. For example, glioblastoma multiforme, one of the most malignant brain tumors, expresses high levels of the HA receptor CD44 [18]. CD44, the primary cell adhesion receptor expressed in cancer stem cells (CSC), has a specific extracellular domain in the N-terminal region that binds HA in the ECM; this domain is called the HA-binding domain, as shown in the Graphical abstract. CD44‚ÄìHA interactions activate cellular signaling pathways that promote cancer cell proliferation, invasion, and metastasis [138]. HA-CD44 affinity is primarily used in targeted cell imaging and drug delivery. The ability of HA to bind CD44 rapidly and effectively is vital for extracting CD44 from complex media, such as serum [109]. HA has been used as a ligand in several biosensors for detecting soluble proteins [115,116,117] and in nanoparticle-based assays [109].4.1.2. AntibodiesAntibodies are widely used as ligands in commercial and home-based ELISAs. In a multitude of biosensors, as shown inTable 2, there are now more than 140 commercially available antibodies against CD44. Despite the large number of available antibodies against CD44, most are the same original monoclonal antibodies or target similar regions of the protein. Given the distinct functions of CD44 isoforms and the sequence variations in both antibodies and antigens, the number of available monoclonal antibodies is considered insufficient, and efforts to develop new antibodies are ongoing. For example, researchers have obtained four new monoclonal anti-CD44 antibodies derived from mouse B cells injected with a plasmid expressing the CD44 isoform 12 [139]. Based on these results, the authors concluded that the four monoclonal antibodies (mAbs) targeting the terminal, extracellular, conserved domain of CD44 bind to these peptides only after deglycosylation.4.1.3. AptamersAptamers are another class of ligands used for solCD44 detection. Aptamers are single-stranded synthetic DNA or RNA oligonucleotides that can bind to desired cell surface molecules with high affinity and specificity through structure recognition; they are selected in vitro from a synthetic nucleic acid library via the SELEX (systematic evolution of ligands by exponential enrichment) method [42]. These oligonucleotides have dissociation constants in the pico- to nanomolar range and can be combined into a stable, unique three-dimensional structure for specific coupling to target molecules. The advantages of aptamers are their high chemical stability, including heat resistance, ease of synthesis, and flexibility of modification [132].Aptamers often achieve binding affinities comparable to monoclonal antibodies. For instance, CD44-targeting aptamers, such as CD44-Apt1, exhibit nanomolar affinity in the range of 1.22‚Äì2.09 nM and remain intact in 90% human serum for up to 96 h, demonstrating their stability [42]. Unlike antibodies, which require strict storage conditions, and are costly and time-consuming to produce, aptamers can be synthesized at low cost, exhibit high chemical and thermal stability, and flexible chemical modification. Furthermore, the CD44 aptamer demonstrated good analytical performance, enabling the detection of soluble CD44 across a broad linear range of 0.1‚Äì1000 ng/mL with the limit of detection LOD of 0.087 ng/mL, together with strong specificity against common serum interferents and good stability during 14 days of storage at 4 ¬∞C [137].A DNA aptamer targeting the recombinant human HA-binding domain of CD44 has been selected by optimizing thio-thio substitutions within the aptamer sequence [128]. The results revealed that selected thioaptamers (monothiophosphate-modified) aptamers bound to CD44-positive human ovarian cancer cell lines but failed to bind to the CD44-negative cell line. The thioaptamers showed significantly stronger, high-affinity specific binding to the HA-binding domain of CD44 (a highly conserved sequence across other CD44 splice variants). They could not bind another HA-binding protein with 32.3% sequence identity to CD44. One of these thioaptamers was then used in another study to construct a biosensor for measuring solCD44 in serum samples [137].All other aptamers targeting CD44 were originally selected against CD44-expressing cells (not solCD44) during SELEX and showed affinity for the extracellular domain of CD44. The binding of some of the aptamers to solCD44 was also confirmed experimentally, as in previous work [42]. Aptamers targeting two isoforms, solCD44v1 and solCD44v8 10, were selected from live hepatocellular carcinoma (HCC) cells via the loss-of-gain cell-based SELEX method and next-generation sequencing. The study results revealed high affinity and sensitivity of CD44-Apt1 for both proteins.Aptamers targeting various forms of CD44 are also available. DNA aptamers that specifically bind to CD44 exon v10 were selected via SELEX to inhibit breast cancer cell migration [140]. The inhibition of tumor migration by aptamers targeting exon v10 of CD44 has been confirmed. The use of such aptamers could also be necessary for cancer diagnosis, as the standard form of the protein can be distinguished from its isoforms, given that most ELISA kits are not isoform-specific. While most aptamers targeting CD44 are DNA-based, some RNA-based aptamers, such as those isolated by SELEX from a 2‚Ä≤-fluoropyrimidine-modified RNA library, are available [129].4.1.4. Emerging Ligands Against solCD44The large size of mAbs (four polypeptide chains, 150 kDa) hinders their ability to access tumor cells in vivo. Nanobodies (15 kDa) and nanobody-based human heavy chain antibodies (75 kDa) can overcome these obstacles due to their small size, high stability, high solubility, excellent in vivo tissue penetration, and ability to be chemically conjugated to specific sites on drugs, other nanobodies, peptides, etc. [141]. There are currently few studies on CD44 nanobodies. Nanobodies recognizing CD44-expressing cells were successfully isolated using phage display, particularly the cell-panning technique [142]. The development of signal peptides that can direct cytoplasmic proteins from the cell to the media is an essential step in the production of the recombinant CD44 protein. Based on an in silico investigation of suitable signal peptides for the recombinant production of CD44 nanobodies, five peptides (CSGA, TRBC, YTFQ, NIKA, and DGAL) were selected as suitable candidates [143].Peptides are also attractive ligands compared with antibodies because they are selected in vitro, are more stable, have a lower molecular weight, and are less toxic [135]. Peptides are usually selected using phage display. It is a high-throughput method for selecting particular ligands from a peptide library [48]. Many other peptides have been chosen not as ligands in assays but rather to modulate CD44 activity and study its effect on cancer development [133,135,144,145].Another emerging class of ligands for CD44 detection is molecularly imprinted polymers (MIPs). MIPs are synthesized by polymerising monomers in the presence of a template molecule using the molecular imprinting method [146]. MIPs are stable polymers with molecular recognition sites, have an affinity for the selected ‚Äútemplate‚Äù molecule, and can specifically rebind the target molecule. The advantages of MIPs include their high selectivity, high capacity for recognizing and capturing target molecules, stability, and resistance to a wide range of environmental pH and temperature conditions. The functional abilities of MIPs correspond to the interactions of natural receptors, enabling selective retention of the target molecule, as in an antibody‚Äìantigen interaction. MIPs with CD44 as the template protein were developed using alginate gel as the functional monomer [132]. The MIPs were shown to be very specific to the target molecule. 4.2. Commercially Available AssaysAt least two assay types are commercially available for detecting solCD44. ELISA is based on a sandwich-based assay that involves two antibodies: capture antibodies coated on the microtiter plate to capture the protein of interest and a secondary antibody conjugated with enzymes (usually horseradish peroxidase, alkaline phosphatase or Œ≤-d-galactosidase) that will digest its substrate to produce a chromogenic species that is detected via a plate reader or the naked eye [147]. An ELISA kit from Bender MedSystems, for example, uses two particular murine antibodies against solCD44 [41]. Most of these ELISA kits (if not all) allow quantitative detection of all circulating CD44 isoforms, including the standard protein and its variants. The majority of the studies discussed in this paper quantified solCD44 by ELISA.The OncAlert‚Ñ¢ rapid test kit is another example of a commercially available product sold to detect solCD44 and total protein (Figure 2). Compared with ELISA, it uses saliva as the sample for analysis and employs a lateral flow mechanism, similar to that of a pregnancy test, providing rapid results for oral cancer screening. This device consists of a plastic-molded tube with two test strips, one coated with CD44 and the other with total protein, and a 5 mL saline solution for oral rinsing and gargling. The results were obtained 10 min after the kit was inserted into the collection cup containing the sample. The quantity of CD44 present in the sample is indicated by a visual line proportional to the concentration of human CD44.Figure 2below illustrates the three-step procedure for using the OncAlert‚Ñ¢ test kit.Figure 2.Schematic representation of the detection of the CD44 protein in saliva via the OncAlert‚Ñ¢ kit. 4.3. Biosensors and Other AssaysAs shown inFigure 1, although the total number of articles decreased after 1999, methodological diversity gradually increased, especially with the advent of biosensors and other analytical methods starting in 2015. From 2020 to 2024, there has been a marked increase in biosensor-based research, especially the use of blood and saliva, indicating a growing interest in advanced detection technologies. Biosensors for detecting solCD44 can be classified into three categories: electrochemical, photoelectrochemical, and optical fibre-based. Among electrochemical biosensors for solCD44, electrochemical impedance biosensors, which measure impedance via a three-electrode system with an electrochemical analyzer as the protein concentration changes, are more common. One such biosensor uses aptamers as ligands to detect proteins in serum samples [137]. The binding of the target to the aptamer alters its structure, thereby enhancing the flow of electroactive species toward the working electrode and decreasing impedance. Among biosensors that exhibit ultrasensitive, wide-concentration-range detection are a sandwich-type electrochemical biosensor using an enzyme-free signal amplification strategy [133] and a spherical ball resonator-based optical fibre biosensor with a simple fabrication method [121].Since specific isoforms of CD44 may play a more critical role in certain cancers, their quantitative detection is crucial. One of these methods involves developing isoform-specific ELISAs [43]. Specifically, the lack of tests for CD44v3 was addressed by creating a sandwich ELISA, which was shown to be sensitive, accurate, and reliable for detecting this CD44 variant [119]. In one study, ion exchange, immunoaffinity chromatography, and Western blot analysis were used as alternative methods to isoform-specific ELISA to examine the roles of different CD44 isoforms in colon cancer [43].Biosensors fabricated for exosomal CD44 detection are also available. In one study, two types of label-free biosensors based on titanium nitride nanoholes were developed: one using an atomic force microscope and the other using localized surface plasmon resonance [148]. Exosomes are nanoscale vesicles (30‚Äì100 nm) with an endocytic origin that function to transport proteins, mRNAs, or other molecules between cells [104]. LOD of exosomal CD44 for the biosensors were 5.29 √ó 10‚àí1Œºg/mL and 3.46 √ó 10‚àí3Œºg/mL in terms of exosome concentration. Although the biosensors evaluated exosomes from malignant mouse samples, they offer an interesting outlook for using exosomes as analyzable samples.Several strategies have been developed to optimize the detection of soluble proteins, including simplifying biosensor development (e.g., easier fabrication and simplified functionalization) and improving biosensor properties, such as antifouling and long-term stability. One-step surface functionalization was used to construct a photoelectrochemical biosensor for the detection of solCD44, which features a hybrid antifouling surface [116]. Another electrochemical biosensor developed for CD44 protein detection demonstrated long-term stability and was successfully used to detect CD44-expressing cells [115]. A fluorescence resonance energy transfer assay resulting from the interaction of fluoresceinamine-HA with a cationic conjugated polymer was developed [149]. This method provides a fast, cost-effective, and visual means of detecting circulatory CD44. Soomro et al. [117] developed a hybrid photoelectrochemical platform that was demonstrated to detect proteins in real blood serum due to the antifouling surface of the biosensor. An optical biosensor based on a low-cost, easy-to-fabricate spherical fiber-optic tip sensor was developed to quantify the CD44 protein [128], with follow-up work achieving an ultralow LOD over a wide concentration range and in vitro studies that mimic blood flow [121].A dual electrochemical biosensor that combines the natural ligand of CD44 (HA) with ultra highly specific MIPs into a single flexible electrode for detecting the CD44 biomarker was developed [132]. In the developed dual-channel screen-printed electrodes (SPEs), the synthesized MIPs against CD44 were immobilized on one channel, while the second channel was loaded with natural HA probes. The advantages of this development include excellent specificity, antifouling properties, and biocompatibility of MIPs and HA; satisfactory stability of SPEs; and high sensitivity of electrochemical methods. This electrochemical biosensor platform can serve as a valuable tool for diagnosing human cancer by recognizing CD44-overexpressing cell surfaces, such as those of breast cancer cells. Combining biosensors with other assays is also common. Magnetic nanoparticles functionalized with HA were first used to extract and detect the protein from serum; more importantly, the extracted proteins were subsequently analyzed via mass spectrometry [109]. Dot blot analysis is another form of assay which was used for solCD44 detection in some of the studies [92,150,151].The CD44 protein not only has several isoforms but also has several posttranslational modifications (N- and P-linked glycosylation, phosphorylation, sulfation, and domain cleavage) [8]. Therefore, some studies have focused on detecting post-translationally modified forms of CD44. CD44 glycosylation affects its binding to many important ligands (HA, fibronectin, collagen, etc.) and thereby regulates the tumor microenvironment and cancer cell fate [152]. A solid-phase proximity ligation assay for detecting the glycosylated form of CD44 was developed. As a result, the method outperformed ELISA in terms of dynamic range and sensitivity [153]. However, the technique is not label-free and can be labor-intensive because it requires an additional step during real-time PCR. 4.4. Performance of the AssaysA brief overview of the main detection platforms is provided to contextualize their analytical performance and highlight differences in sensitivity across methods. This helps clarify how commercial assays compare with more advanced sensor-based technologies in terms of applicability for solCD44 detection.Figure 3A shows the calibration and sensitivity ranges of available kits for detecting the CD44 protein in various biological fluids. These kits were designed to detect CD44 protein (standard and isoforms 5, 6, and 9) in biological fluids such as serum, plasma, tissue homogenates, and culture supernatants, and some kits can also be used for urine and amniotic fluid.Figure 3B compares the sensitivity (pg/mL) of commercial ELISA kits and sensor-based methods for CD44 detection. Among ELISAs, kits from Novus Biologicals (NB), Abcam (Ac), and Abnova (Ab) perform best, with detection limits close to 0.01‚Äì0.1 ng/mL‚Äîalthough Thermo Fisher and Bio-Rad outperform them slightly (0.016 and 0.0156 ng/mL, respectively). FRET-based immunoassays lag behind other methods, with a sensitivity of approximately 170 ng/mL, making them less suitable for CD44 detection. Sensor-based methods, however, surpass ELISA methods. Photoelectrochemical platforms reached 0.014 pg/mL, and solid-phase proximity ligation assays reached 715 fM. The silanized ball resonator stands out for its sensitivity of 107 ag/mL; however, it requires complex and costly instrumentation. While ELISA kits are more accessible and widely used, sensor-based methods offer higher LODs for the detection of ultralow-level biomarkers.Figure 3.Calibration (A) and sensitivity ranges (B) of commercially available ELISA kits, biosensors, and other assays for the detection of the CD44 protein in various biological fluids. Commercial ELISA kits: NB‚ÄîNovus Biological, Ac‚ÄîAbcam, Ab‚ÄîAbnova, TF‚ÄîThermo Fisher Scientific, BR‚ÄîBio-Rad, Bl‚ÄîBioLegend, LS‚ÄîLifeSpanBioSciences.",
            "4.1. Ligands Against solCD44": "4.1.1. Hyaluronic AcidThe primary ligand of CD44 is hyaluronic acid (HA); the binding of CD44 to HA contributes to a wide variety of physiological and pathological processes. For example, glioblastoma multiforme, one of the most malignant brain tumors, expresses high levels of the HA receptor CD44 [18]. CD44, the primary cell adhesion receptor expressed in cancer stem cells (CSC), has a specific extracellular domain in the N-terminal region that binds HA in the ECM; this domain is called the HA-binding domain, as shown in the Graphical abstract. CD44‚ÄìHA interactions activate cellular signaling pathways that promote cancer cell proliferation, invasion, and metastasis [138]. HA-CD44 affinity is primarily used in targeted cell imaging and drug delivery. The ability of HA to bind CD44 rapidly and effectively is vital for extracting CD44 from complex media, such as serum [109]. HA has been used as a ligand in several biosensors for detecting soluble proteins [115,116,117] and in nanoparticle-based assays [109]. 4.1.2. AntibodiesAntibodies are widely used as ligands in commercial and home-based ELISAs. In a multitude of biosensors, as shown inTable 2, there are now more than 140 commercially available antibodies against CD44. Despite the large number of available antibodies against CD44, most are the same original monoclonal antibodies or target similar regions of the protein. Given the distinct functions of CD44 isoforms and the sequence variations in both antibodies and antigens, the number of available monoclonal antibodies is considered insufficient, and efforts to develop new antibodies are ongoing. For example, researchers have obtained four new monoclonal anti-CD44 antibodies derived from mouse B cells injected with a plasmid expressing the CD44 isoform 12 [139]. Based on these results, the authors concluded that the four monoclonal antibodies (mAbs) targeting the terminal, extracellular, conserved domain of CD44 bind to these peptides only after deglycosylation. 4.1.3. AptamersAptamers are another class of ligands used for solCD44 detection. Aptamers are single-stranded synthetic DNA or RNA oligonucleotides that can bind to desired cell surface molecules with high affinity and specificity through structure recognition; they are selected in vitro from a synthetic nucleic acid library via the SELEX (systematic evolution of ligands by exponential enrichment) method [42]. These oligonucleotides have dissociation constants in the pico- to nanomolar range and can be combined into a stable, unique three-dimensional structure for specific coupling to target molecules. The advantages of aptamers are their high chemical stability, including heat resistance, ease of synthesis, and flexibility of modification [132].Aptamers often achieve binding affinities comparable to monoclonal antibodies. For instance, CD44-targeting aptamers, such as CD44-Apt1, exhibit nanomolar affinity in the range of 1.22‚Äì2.09 nM and remain intact in 90% human serum for up to 96 h, demonstrating their stability [42]. Unlike antibodies, which require strict storage conditions, and are costly and time-consuming to produce, aptamers can be synthesized at low cost, exhibit high chemical and thermal stability, and flexible chemical modification. Furthermore, the CD44 aptamer demonstrated good analytical performance, enabling the detection of soluble CD44 across a broad linear range of 0.1‚Äì1000 ng/mL with the limit of detection LOD of 0.087 ng/mL, together with strong specificity against common serum interferents and good stability during 14 days of storage at 4 ¬∞C [137].A DNA aptamer targeting the recombinant human HA-binding domain of CD44 has been selected by optimizing thio-thio substitutions within the aptamer sequence [128]. The results revealed that selected thioaptamers (monothiophosphate-modified) aptamers bound to CD44-positive human ovarian cancer cell lines but failed to bind to the CD44-negative cell line. The thioaptamers showed significantly stronger, high-affinity specific binding to the HA-binding domain of CD44 (a highly conserved sequence across other CD44 splice variants). They could not bind another HA-binding protein with 32.3% sequence identity to CD44. One of these thioaptamers was then used in another study to construct a biosensor for measuring solCD44 in serum samples [137].All other aptamers targeting CD44 were originally selected against CD44-expressing cells (not solCD44) during SELEX and showed affinity for the extracellular domain of CD44. The binding of some of the aptamers to solCD44 was also confirmed experimentally, as in previous work [42]. Aptamers targeting two isoforms, solCD44v1 and solCD44v8 10, were selected from live hepatocellular carcinoma (HCC) cells via the loss-of-gain cell-based SELEX method and next-generation sequencing. The study results revealed high affinity and sensitivity of CD44-Apt1 for both proteins.Aptamers targeting various forms of CD44 are also available. DNA aptamers that specifically bind to CD44 exon v10 were selected via SELEX to inhibit breast cancer cell migration [140]. The inhibition of tumor migration by aptamers targeting exon v10 of CD44 has been confirmed. The use of such aptamers could also be necessary for cancer diagnosis, as the standard form of the protein can be distinguished from its isoforms, given that most ELISA kits are not isoform-specific. While most aptamers targeting CD44 are DNA-based, some RNA-based aptamers, such as those isolated by SELEX from a 2‚Ä≤-fluoropyrimidine-modified RNA library, are available [129]. 4.1.4. Emerging Ligands Against solCD44The large size of mAbs (four polypeptide chains, 150 kDa) hinders their ability to access tumor cells in vivo. Nanobodies (15 kDa) and nanobody-based human heavy chain antibodies (75 kDa) can overcome these obstacles due to their small size, high stability, high solubility, excellent in vivo tissue penetration, and ability to be chemically conjugated to specific sites on drugs, other nanobodies, peptides, etc. [141]. There are currently few studies on CD44 nanobodies. Nanobodies recognizing CD44-expressing cells were successfully isolated using phage display, particularly the cell-panning technique [142]. The development of signal peptides that can direct cytoplasmic proteins from the cell to the media is an essential step in the production of the recombinant CD44 protein. Based on an in silico investigation of suitable signal peptides for the recombinant production of CD44 nanobodies, five peptides (CSGA, TRBC, YTFQ, NIKA, and DGAL) were selected as suitable candidates [143].Peptides are also attractive ligands compared with antibodies because they are selected in vitro, are more stable, have a lower molecular weight, and are less toxic [135]. Peptides are usually selected using phage display. It is a high-throughput method for selecting particular ligands from a peptide library [48]. Many other peptides have been chosen not as ligands in assays but rather to modulate CD44 activity and study its effect on cancer development [133,135,144,145].Another emerging class of ligands for CD44 detection is molecularly imprinted polymers (MIPs). MIPs are synthesized by polymerising monomers in the presence of a template molecule using the molecular imprinting method [146]. MIPs are stable polymers with molecular recognition sites, have an affinity for the selected ‚Äútemplate‚Äù molecule, and can specifically rebind the target molecule. The advantages of MIPs include their high selectivity, high capacity for recognizing and capturing target molecules, stability, and resistance to a wide range of environmental pH and temperature conditions. The functional abilities of MIPs correspond to the interactions of natural receptors, enabling selective retention of the target molecule, as in an antibody‚Äìantigen interaction. MIPs with CD44 as the template protein were developed using alginate gel as the functional monomer [132]. The MIPs were shown to be very specific to the target molecule.",
            "4.1.1. Hyaluronic Acid": "The primary ligand of CD44 is hyaluronic acid (HA); the binding of CD44 to HA contributes to a wide variety of physiological and pathological processes. For example, glioblastoma multiforme, one of the most malignant brain tumors, expresses high levels of the HA receptor CD44 [18]. CD44, the primary cell adhesion receptor expressed in cancer stem cells (CSC), has a specific extracellular domain in the N-terminal region that binds HA in the ECM; this domain is called the HA-binding domain, as shown in the Graphical abstract. CD44‚ÄìHA interactions activate cellular signaling pathways that promote cancer cell proliferation, invasion, and metastasis [138]. HA-CD44 affinity is primarily used in targeted cell imaging and drug delivery. The ability of HA to bind CD44 rapidly and effectively is vital for extracting CD44 from complex media, such as serum [109]. HA has been used as a ligand in several biosensors for detecting soluble proteins [115,116,117] and in nanoparticle-based assays [109].",
            "4.1.2. Antibodies": "Antibodies are widely used as ligands in commercial and home-based ELISAs. In a multitude of biosensors, as shown inTable 2, there are now more than 140 commercially available antibodies against CD44. Despite the large number of available antibodies against CD44, most are the same original monoclonal antibodies or target similar regions of the protein. Given the distinct functions of CD44 isoforms and the sequence variations in both antibodies and antigens, the number of available monoclonal antibodies is considered insufficient, and efforts to develop new antibodies are ongoing. For example, researchers have obtained four new monoclonal anti-CD44 antibodies derived from mouse B cells injected with a plasmid expressing the CD44 isoform 12 [139]. Based on these results, the authors concluded that the four monoclonal antibodies (mAbs) targeting the terminal, extracellular, conserved domain of CD44 bind to these peptides only after deglycosylation.",
            "4.1.3. Aptamers": "Aptamers are another class of ligands used for solCD44 detection. Aptamers are single-stranded synthetic DNA or RNA oligonucleotides that can bind to desired cell surface molecules with high affinity and specificity through structure recognition; they are selected in vitro from a synthetic nucleic acid library via the SELEX (systematic evolution of ligands by exponential enrichment) method [42]. These oligonucleotides have dissociation constants in the pico- to nanomolar range and can be combined into a stable, unique three-dimensional structure for specific coupling to target molecules. The advantages of aptamers are their high chemical stability, including heat resistance, ease of synthesis, and flexibility of modification [132]. Aptamers often achieve binding affinities comparable to monoclonal antibodies. For instance, CD44-targeting aptamers, such as CD44-Apt1, exhibit nanomolar affinity in the range of 1.22‚Äì2.09 nM and remain intact in 90% human serum for up to 96 h, demonstrating their stability [42]. Unlike antibodies, which require strict storage conditions, and are costly and time-consuming to produce, aptamers can be synthesized at low cost, exhibit high chemical and thermal stability, and flexible chemical modification. Furthermore, the CD44 aptamer demonstrated good analytical performance, enabling the detection of soluble CD44 across a broad linear range of 0.1‚Äì1000 ng/mL with the limit of detection LOD of 0.087 ng/mL, together with strong specificity against common serum interferents and good stability during 14 days of storage at 4 ¬∞C [137]. A DNA aptamer targeting the recombinant human HA-binding domain of CD44 has been selected by optimizing thio-thio substitutions within the aptamer sequence [128]. The results revealed that selected thioaptamers (monothiophosphate-modified) aptamers bound to CD44-positive human ovarian cancer cell lines but failed to bind to the CD44-negative cell line. The thioaptamers showed significantly stronger, high-affinity specific binding to the HA-binding domain of CD44 (a highly conserved sequence across other CD44 splice variants). They could not bind another HA-binding protein with 32.3% sequence identity to CD44. One of these thioaptamers was then used in another study to construct a biosensor for measuring solCD44 in serum samples [137]. All other aptamers targeting CD44 were originally selected against CD44-expressing cells (not solCD44) during SELEX and showed affinity for the extracellular domain of CD44. The binding of some of the aptamers to solCD44 was also confirmed experimentally, as in previous work [42]. Aptamers targeting two isoforms, solCD44v1 and solCD44v8 10, were selected from live hepatocellular carcinoma (HCC) cells via the loss-of-gain cell-based SELEX method and next-generation sequencing. The study results revealed high affinity and sensitivity of CD44-Apt1 for both proteins. Aptamers targeting various forms of CD44 are also available. DNA aptamers that specifically bind to CD44 exon v10 were selected via SELEX to inhibit breast cancer cell migration [140]. The inhibition of tumor migration by aptamers targeting exon v10 of CD44 has been confirmed. The use of such aptamers could also be necessary for cancer diagnosis, as the standard form of the protein can be distinguished from its isoforms, given that most ELISA kits are not isoform-specific. While most aptamers targeting CD44 are DNA-based, some RNA-based aptamers, such as those isolated by SELEX from a 2‚Ä≤-fluoropyrimidine-modified RNA library, are available [129].",
            "4.1.4. Emerging Ligands Against solCD44": "The large size of mAbs (four polypeptide chains, 150 kDa) hinders their ability to access tumor cells in vivo. Nanobodies (15 kDa) and nanobody-based human heavy chain antibodies (75 kDa) can overcome these obstacles due to their small size, high stability, high solubility, excellent in vivo tissue penetration, and ability to be chemically conjugated to specific sites on drugs, other nanobodies, peptides, etc. [141]. There are currently few studies on CD44 nanobodies. Nanobodies recognizing CD44-expressing cells were successfully isolated using phage display, particularly the cell-panning technique [142]. The development of signal peptides that can direct cytoplasmic proteins from the cell to the media is an essential step in the production of the recombinant CD44 protein. Based on an in silico investigation of suitable signal peptides for the recombinant production of CD44 nanobodies, five peptides (CSGA, TRBC, YTFQ, NIKA, and DGAL) were selected as suitable candidates [143]. Peptides are also attractive ligands compared with antibodies because they are selected in vitro, are more stable, have a lower molecular weight, and are less toxic [135]. Peptides are usually selected using phage display. It is a high-throughput method for selecting particular ligands from a peptide library [48]. Many other peptides have been chosen not as ligands in assays but rather to modulate CD44 activity and study its effect on cancer development [133,135,144,145]. Another emerging class of ligands for CD44 detection is molecularly imprinted polymers (MIPs). MIPs are synthesized by polymerising monomers in the presence of a template molecule using the molecular imprinting method [146]. MIPs are stable polymers with molecular recognition sites, have an affinity for the selected ‚Äútemplate‚Äù molecule, and can specifically rebind the target molecule. The advantages of MIPs include their high selectivity, high capacity for recognizing and capturing target molecules, stability, and resistance to a wide range of environmental pH and temperature conditions. The functional abilities of MIPs correspond to the interactions of natural receptors, enabling selective retention of the target molecule, as in an antibody‚Äìantigen interaction. MIPs with CD44 as the template protein were developed using alginate gel as the functional monomer [132]. The MIPs were shown to be very specific to the target molecule.",
            "4.2. Commercially Available Assays": "At least two assay types are commercially available for detecting solCD44. ELISA is based on a sandwich-based assay that involves two antibodies: capture antibodies coated on the microtiter plate to capture the protein of interest and a secondary antibody conjugated with enzymes (usually horseradish peroxidase, alkaline phosphatase or Œ≤-d-galactosidase) that will digest its substrate to produce a chromogenic species that is detected via a plate reader or the naked eye [147]. An ELISA kit from Bender MedSystems, for example, uses two particular murine antibodies against solCD44 [41]. Most of these ELISA kits (if not all) allow quantitative detection of all circulating CD44 isoforms, including the standard protein and its variants. The majority of the studies discussed in this paper quantified solCD44 by ELISA. The OncAlert‚Ñ¢ rapid test kit is another example of a commercially available product sold to detect solCD44 and total protein (Figure 2). Compared with ELISA, it uses saliva as the sample for analysis and employs a lateral flow mechanism, similar to that of a pregnancy test, providing rapid results for oral cancer screening. This device consists of a plastic-molded tube with two test strips, one coated with CD44 and the other with total protein, and a 5 mL saline solution for oral rinsing and gargling. The results were obtained 10 min after the kit was inserted into the collection cup containing the sample. The quantity of CD44 present in the sample is indicated by a visual line proportional to the concentration of human CD44.Figure 2below illustrates the three-step procedure for using the OncAlert‚Ñ¢ test kit. Figure 2.Schematic representation of the detection of the CD44 protein in saliva via the OncAlert‚Ñ¢ kit.",
            "4.3. Biosensors and Other Assays": "As shown inFigure 1, although the total number of articles decreased after 1999, methodological diversity gradually increased, especially with the advent of biosensors and other analytical methods starting in 2015. From 2020 to 2024, there has been a marked increase in biosensor-based research, especially the use of blood and saliva, indicating a growing interest in advanced detection technologies. Biosensors for detecting solCD44 can be classified into three categories: electrochemical, photoelectrochemical, and optical fibre-based. Among electrochemical biosensors for solCD44, electrochemical impedance biosensors, which measure impedance via a three-electrode system with an electrochemical analyzer as the protein concentration changes, are more common. One such biosensor uses aptamers as ligands to detect proteins in serum samples [137]. The binding of the target to the aptamer alters its structure, thereby enhancing the flow of electroactive species toward the working electrode and decreasing impedance. Among biosensors that exhibit ultrasensitive, wide-concentration-range detection are a sandwich-type electrochemical biosensor using an enzyme-free signal amplification strategy [133] and a spherical ball resonator-based optical fibre biosensor with a simple fabrication method [121]. Since specific isoforms of CD44 may play a more critical role in certain cancers, their quantitative detection is crucial. One of these methods involves developing isoform-specific ELISAs [43]. Specifically, the lack of tests for CD44v3 was addressed by creating a sandwich ELISA, which was shown to be sensitive, accurate, and reliable for detecting this CD44 variant [119]. In one study, ion exchange, immunoaffinity chromatography, and Western blot analysis were used as alternative methods to isoform-specific ELISA to examine the roles of different CD44 isoforms in colon cancer [43]. Biosensors fabricated for exosomal CD44 detection are also available. In one study, two types of label-free biosensors based on titanium nitride nanoholes were developed: one using an atomic force microscope and the other using localized surface plasmon resonance [148]. Exosomes are nanoscale vesicles (30‚Äì100 nm) with an endocytic origin that function to transport proteins, mRNAs, or other molecules between cells [104]. LOD of exosomal CD44 for the biosensors were 5.29 √ó 10‚àí1Œºg/mL and 3.46 √ó 10‚àí3Œºg/mL in terms of exosome concentration. Although the biosensors evaluated exosomes from malignant mouse samples, they offer an interesting outlook for using exosomes as analyzable samples. Several strategies have been developed to optimize the detection of soluble proteins, including simplifying biosensor development (e.g., easier fabrication and simplified functionalization) and improving biosensor properties, such as antifouling and long-term stability. One-step surface functionalization was used to construct a photoelectrochemical biosensor for the detection of solCD44, which features a hybrid antifouling surface [116]. Another electrochemical biosensor developed for CD44 protein detection demonstrated long-term stability and was successfully used to detect CD44-expressing cells [115]. A fluorescence resonance energy transfer assay resulting from the interaction of fluoresceinamine-HA with a cationic conjugated polymer was developed [149]. This method provides a fast, cost-effective, and visual means of detecting circulatory CD44. Soomro et al. [117] developed a hybrid photoelectrochemical platform that was demonstrated to detect proteins in real blood serum due to the antifouling surface of the biosensor. An optical biosensor based on a low-cost, easy-to-fabricate spherical fiber-optic tip sensor was developed to quantify the CD44 protein [128], with follow-up work achieving an ultralow LOD over a wide concentration range and in vitro studies that mimic blood flow [121]. A dual electrochemical biosensor that combines the natural ligand of CD44 (HA) with ultra highly specific MIPs into a single flexible electrode for detecting the CD44 biomarker was developed [132]. In the developed dual-channel screen-printed electrodes (SPEs), the synthesized MIPs against CD44 were immobilized on one channel, while the second channel was loaded with natural HA probes. The advantages of this development include excellent specificity, antifouling properties, and biocompatibility of MIPs and HA; satisfactory stability of SPEs; and high sensitivity of electrochemical methods. This electrochemical biosensor platform can serve as a valuable tool for diagnosing human cancer by recognizing CD44-overexpressing cell surfaces, such as those of breast cancer cells. Combining biosensors with other assays is also common. Magnetic nanoparticles functionalized with HA were first used to extract and detect the protein from serum; more importantly, the extracted proteins were subsequently analyzed via mass spectrometry [109]. Dot blot analysis is another form of assay which was used for solCD44 detection in some of the studies [92,150,151]. The CD44 protein not only has several isoforms but also has several posttranslational modifications (N- and P-linked glycosylation, phosphorylation, sulfation, and domain cleavage) [8]. Therefore, some studies have focused on detecting post-translationally modified forms of CD44. CD44 glycosylation affects its binding to many important ligands (HA, fibronectin, collagen, etc.) and thereby regulates the tumor microenvironment and cancer cell fate [152]. A solid-phase proximity ligation assay for detecting the glycosylated form of CD44 was developed. As a result, the method outperformed ELISA in terms of dynamic range and sensitivity [153]. However, the technique is not label-free and can be labor-intensive because it requires an additional step during real-time PCR.",
            "4.4. Performance of the Assays": "A brief overview of the main detection platforms is provided to contextualize their analytical performance and highlight differences in sensitivity across methods. This helps clarify how commercial assays compare with more advanced sensor-based technologies in terms of applicability for solCD44 detection.Figure 3A shows the calibration and sensitivity ranges of available kits for detecting the CD44 protein in various biological fluids. These kits were designed to detect CD44 protein (standard and isoforms 5, 6, and 9) in biological fluids such as serum, plasma, tissue homogenates, and culture supernatants, and some kits can also be used for urine and amniotic fluid.Figure 3B compares the sensitivity (pg/mL) of commercial ELISA kits and sensor-based methods for CD44 detection. Among ELISAs, kits from Novus Biologicals (NB), Abcam (Ac), and Abnova (Ab) perform best, with detection limits close to 0.01‚Äì0.1 ng/mL‚Äîalthough Thermo Fisher and Bio-Rad outperform them slightly (0.016 and 0.0156 ng/mL, respectively). FRET-based immunoassays lag behind other methods, with a sensitivity of approximately 170 ng/mL, making them less suitable for CD44 detection. Sensor-based methods, however, surpass ELISA methods. Photoelectrochemical platforms reached 0.014 pg/mL, and solid-phase proximity ligation assays reached 715 fM. The silanized ball resonator stands out for its sensitivity of 107 ag/mL; however, it requires complex and costly instrumentation. While ELISA kits are more accessible and widely used, sensor-based methods offer higher LODs for the detection of ultralow-level biomarkers. Figure 3.Calibration (A) and sensitivity ranges (B) of commercially available ELISA kits, biosensors, and other assays for the detection of the CD44 protein in various biological fluids. Commercial ELISA kits: NB‚ÄîNovus Biological, Ac‚ÄîAbcam, Ab‚ÄîAbnova, TF‚ÄîThermo Fisher Scientific, BR‚ÄîBio-Rad, Bl‚ÄîBioLegend, LS‚ÄîLifeSpanBioSciences.",
            "5. CD44 Biomarker: Cell vs. Soluble Protein": "Several studies have compared CD44 in tissue with its soluble form. The results of immunohistochemistry (IHC) for CD44 in tumor biopsy samples and ELISA for CD44 protein in serum correlated with those of several studies [73,76,101,154,155]. A study detected three isoforms of circulatory CD44 (solCD44v8-10) in people with colon cancer [94]. The protein level was lower in those with negative immunostaining results, whereas the amount of CD44v8-10 in the blood was notably increased in patients whose tissues showed a strong response to solCD44v8-10. The average signal (optical density unit) in these individuals was 5.5 times greater than that in cancer patients who did not exhibit solCD44v8-10 tissue reactivity. There was a significant association between the level of serum CD44v8-10 and the presence of solCD44v8-10 in tumor tissues. This study proposes that serum CD44v8-10 levels may serve as a biomarker for detecting the progression of colorectal cancer through the bloodstream. Evaluation of the expression of solCD44std, solCD44v5, and solCD44v6 in ovarian cancer through both IHC and serological analyses suggested that CD44 splice variants are expressed at low levels in malignant ovarian tumors and that their serum levels do not reliably reflect the tumor burden [37]. Additionally, the results of this study indicate that solCD44std levels in serum may be influenced more by hematopoietic activity than by the presence of ovarian cancer. A systematic review and meta-analysis on the role of CD44 in colorectal cancer suggested that IHC is a more reliable method than ELISA, since serum CD44 levels are affected not only by cancer but also by immune system activity [156]. There are several reasons why plasma protein can be used as a liquid biopsy: an increased understanding of protein complement of cell types, current improvements in protein assay technology, and the possibility of early cancer detection with life- and cost-saving effects [3]. To cover different stages of disease development, it is essential to have proteins that respond to the following characteristics: accessibility, sufficiency, and reliability [157]. Soluble proteins in serum or plasma can serve as a cheap, minimally invasive tool for diagnosis, including early diagnosis, risk stratification, treatment adjustment, disease prognosis, and disease progression monitoring [158]. Cancer cells/tissues secrete various proteins, including enzymes, cytokines, and growth factors, which play distinct roles in physiological processes. Partially, some secreted secretome can be measured in the bloodstream, potentially serving as biomarkers with easier access than tissue proteins [158]. Table 3presents a comparative analysis of the cell-surface-expressed and soluble forms of CD44 as cancer biomarkers. Disseminated cancer cells are a good indicator of tumor burden and metastatic risk. Detailed molecular properties of tumours can be derived from studies of these cells. They carry essential information about the tissue, such as DNA, RNA, and other molecules [3]. The circulating CSC population from colon cancer patients has a distinct set of markers, including CD44 [159]. These cells formed spheroids, indicating the presence of tumor-initiating cells. In gastric cancer patients, CTCs expressing CD44 and EpCAM, sorted by flow cytometry, are highly enriched compared with healthy individuals. The number of these cells is associated with tumor progression and venous invasion. The number of these patients also decreased after surgery or chemotherapy [160]. IHC is the most widely used method for analyzing CD44 in tissues. The staining intensity is scored semi-quantitatively [161]. Table 3.Comparative analysis of tissue-based and soluble forms of CD44 as a cancer biomarker. References [161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188].",
            "6. Conclusions and Future Perspective": "Our understanding of the role of solCD44 in cancer has evolved significantly, starting with a surge of work evaluating its role as a cancer biomarker in the 1990s, followed by somewhat reduced interest in the 2000s. Subsequent discoveries highlighted its diverse roles, leading to a reevaluation of its significance across various cancer types. Based on the studies discussed, a map of key advancements in solCD44 research from 1983 to 2022 is shown inFigure 4. In 1983, solCD44 was first identified in serum, followed by its identification as a primary receptor for HA in 1990, marking foundational breakthroughs. Between 1991 and 1994, solCD44 was linked to cancer metastasis, and in 1992 and 1997, its shedding from leukocytes and cancer cells was documented. In 2001, the role of solCD44 in inhibiting melanoma progression was recognized. In 2011, solCD44 was associated with predicting laryngeal cancer recurrence and was identified as being overexpressed in HER2-positive breast cancer. By 2022, it was implicated in resistance mechanisms in advanced prostate cancer. Technological advancements include the development of diagnostic tools for oral cancer in 2016, the first electrochemical and photoelectrochemical biosensors in 2019, and the introduction of the first optical fiber biosensor for solCD44 detection in 2021. These discoveries highlight the critical role of solCD44 in cancer research and its potential for clinical applications. Figure 4.A timeline of the most essential milestones in soluble CD44 for cancer diagnosis. Created in BioRender. Bekmurzayeva, A. (2025)https://BioRender.com/plioj9x. SolCD44 shows strong diagnostic potential in HNSCC, such as oral cancer, while serving as a valuable prognostic and predictive biomarker in others, as supported by several studies as shown inFigure 5. According to the diagnostic, prognostic and predictive potential of solCD44, different types of cancer could roughly be divided into two categories: (1) ‚Äòhigh‚Äù‚Äîif there are at least 3 strong supporting studies; (2) ‚Äúmoderate‚Äù‚Äîif there are at least 2 supporting studies, and 2 or 1 of the studies are old ones (published earlier than 2000) and are not supported by recent studies. Considering the fact that cancer types are further divided into subtypes, clinicopathological factors considered vary among studies, and different solCD44 isoforms were detected. The framework ofFigure 5is intentionally simplified and is meant only to provide a broad, approximate overview of our current understanding of the role of solCD44 in cancer diagnosis. Details of some of the studies on whichFigure 5was based are shown inTable S1. Figure 5.Diagnostic, prognostic and predictive value of soluble CD44 protein in biological fluids in different types of cancer based on the studies discussed in the paper: a rough estimation. Created in BioRender. Nurlankyzy, M. (2025)https://BioRender.com/djj19sj. Regarding the early diagnosis of cancer, salivary solCD44, especially solCD44v6, is promising for the noninvasive detection of oral, head and neck, and laryngeal cancers. Rapid tests such as OncAlert‚Ñ¢ facilitate point-of-care applicability. HNSCC is a severe and life-threatening condition, primarily due to its frequent diagnosis at an advanced stage. While current cure rates are around 50%, early detection has the potential to raise them to over 80% [54]. Conversely, serum solCD44 appears better suited for prognostic monitoring than for early diagnosis, as its levels often correlate with recurrence and poorer survival outcomes than those of early disease stages. Overall, solCD44v6 stands out as a consistent and reliable isoform associated with TNM staging and metastatic potential across multiple cancer types, particularly colorectal, gastric, breast, oral, and ovarian cancers. Its expression in both serum and tumor tissue, and its persistence after surgery, support its importance as a tumor-derived biomarker with promising applications. Distinguishing cancer subtypes is essential for guiding appropriate therapy, especially in the era of precision medicine. SolCD44 isoforms, particularly solCD44v6 and solCD44v5, show potential for differentiating aggressive subtypes and identifying therapy-resistant forms in cancers such as breast, prostate, bladder, and laryngeal carcinoma. The expression patterns of these genes may provide valuable insights into tumor behavior and support individualized treatment strategies. Further research is needed to validate their clinical utility and establish standardized thresholds for subtype classification. Overall, solCD44 isoforms, particularly solCD44std and solCD44v6, show promise as biomarkers of therapeutic response in several cancers. Their levels tend to decrease following effective treatment and correlate with tumor burden and progression. However, inconsistent findings, limited specificity, and variability across cancer types underscore the need for standardized assays and larger, longitudinal studies to validate the clinical utility of solCD44 for therapeutic monitoring. The clinical utility of CD44 depends on the cancer type and the form being studied. For some cancers, tissue expression analysis is more informative, whereas in others, such as oral cancer, the soluble form of CD44 may provide greater diagnostic accuracy. Recent advances in exosome research have revealed the role of CD44 in the spread of chemoresistance, further highlighting its clinical importance. While serum sampling is convenient, cellular and tissue-based studies often yield more profound insights into CD44 function. Consequently, the application of CD44 and its variants should be tailored to specific cancer types to maximize their diagnostic and prognostic potential. Continued research is essential to refine its applications and fully integrate CD44 into personalized cancer care. Every biological fluid containing solCD44 has its own benefits and analytical difficulties. The best choice of fluid depends, first of all on the specific clinical situation and testing needs. While invasive fluids like blood typically provide more accurate results and have more existing research data for comparison, obtaining them often is not practical for repeated testing. On the other hand, fluids collected non-invasively, such as saliva and urine, although are easier to obtain in practice, their use requires strict procedures for consistency and thorough testing to confirm accuracy. Method of collection of biological fluids, especially saliva, may have an effect on the obtained results. Thus, the oral rinse procedure could yield significantly different levels of the tested biomarkers compared to two other collection methods (unstimulated and chew-stimulated saliva). Based on the works discussed above, the identified challenges in using the solCD44 protein as a cancer biomarker are outlined below. Solid tumors are heterogeneous and therefore composed of cancer cells that vary by subtype; this results in differences in clinical and pathological manifestations, including reported differences in the role of CD44 across breast cancer subtypes and specific patient cohorts [70]. Actual cutoff value for the solCD44 protein in biological fluids have been determined for a very limited number of cancers, such as NHL and cervical cancer [1]. Variations in the effects of increased CD44 expression and in the prognosis of patients with the same cancer type may be primarily due to differences in methodology [189]. Another factor affecting the results could be the time between the protein measurement and the sample collection date, as prolonged or improper storage conditions can decrease protein levels. Concomitant illnesses or habits, such as smoking, were not considered in many of the studies. An earlier study (1997) reported elevated levels of isoforms 5 and 6 in normal individuals who were smokers [190]. Serum levels of the CD44 protein were also investigated in several noncancerous conditions. Its levels differ from normal in several noncancerous diseases: decreased in chronic pancreatitis [191], increased in systemic sclerosis [192], elevated in sarcoidosis uveitis [193], elevated in smokers [162,190], and increased in acute renal rejection [163], hemodialysis-dependent renal failure, and chronic inflammatory bowel disease [91]. As single biomarkers often fail to capture the complexity of heterogeneous tumors, multiplex assays that detect panels of biomarkers may provide a more effective diagnostic and prognostic solution. Exemplary biomarkers that can be used in concert with CD44 include IFN-Œ≥ (and Œ≤2-microglobulin) for B-CLL cancer, E-selectin for NSCLC, and TGM2 and EpCAM for endometrial cancer. CD44 should be seen as a team player rather than an independent marker, working in concert with other biomarkers and clinicopathological parameters. Combining CD44 with other markers may ultimately be the key to advancing personalized medicine and improving patient outcomes. Among the emerging and intriguing studies are exosome research findings showing that the CD44 protein spreads metastatic potential and chemoresistance. Future research on CD44 should prioritize distinguishing its isoforms to understand better their specific clinical roles and significance. Compared with traditional approaches such as ELISA, advanced detection methods, such as biosensors, could offer greater accuracy and efficiency in clinical trials. Before biosensor methods can be included in formal directives and inform regulatory choices, they need stringent validation to prove they are either as good as or better than current standard methods in terms of analytical performance. Many of the reported studies on biosensing of solCD44 indicate that they perform exceptionally well in controlled laboratory settings; however, their effectiveness could be decreases when exposed to real complex fluids (serum, urine, saliva). This drop in performance is partially caused by interference from the fluid‚Äôs composition (‚Äúmatrix effects‚Äù), unintended binding of molecules (nonspecific binding), and other biochemical interferences. Another issues largely stem from the ligand when it does not completely guarantee selectivity causing cross reactivity. In order to ensure high selectivity of the biosensor and given a lower concentration of target protein (solCD44) compared to interfering substances such as albumin which are present in a much higher concentration, level of cross-reactivity must therefore be exceptionally low [164]. Very few biosensor designs developed in laboratories have become commercially available; this also applies to solCD44 biosensors. The limited transition of biosensors to the market can be attributed to the absence of standardized methods, inadequate testing, and insufficient measurement practices for the materials used to create these sensors [165]. One way to overcome these obstacles is by adopting standards similar to the ‚Äústandards for reporting optical biosensor experiments‚Äù (STROBE) [166], standard procedure detailing the crucial information that must be reported when describing optical biosensor tests, allowing reviewers and readers to fully comprehend and duplicate the experimental setup. Another way to improve this is by adopting standards for pre-analytical and analytical validation according to Blood Profiling Atlas in Cancer (BLOODPAC) [167] in order to enhance patient care and outcomes and better inform medical decisions. BLOODPAC is a collaborator-funded organization whose goal is to expedite the development, validation, and clinical application of liquid biopsy diagnostics (specifically those that utilize ctDNA). Pre-analytical phase includes instructions detailing the proper process for blood sample collection, handling, and storage. Analytical validation part covers vital characteristics of the assays such as sensitivity, specificity, and reproducibility. Establishing these strict standards could make it easier for test assay developers and governing bodies to assess the performance of the assays uniformly, guaranteeing they achieve high quality necessary for use in patient care."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2079-6374/15/12/796",
        "scraped_at": "2025-12-05 23:56:22"
    },
    {
        "title": "MalVis: Large-Scale Bytecode Visualization Framework for Explainable Android Malware Detection",
        "authors": "bySaleh J. Makkawy,Michael J. De LuciaandKenneth E. Barner",
        "journal": "J. Cybersecur. Priv.2025,5(4), 109;https://doi.org/10.3390/jcp5040109- 4 Dec 2025",
        "abstract": "As technology advances, developers continually create innovative solutions to enhance smartphone security. However, the rapid spread of Android malware poses significant threats to devices and sensitive data. The Android Operating System (OS)‚Äôs open-source nature and Software Development Kit (SDK) availability mainly contribute to this alarming growth. Conventional malware detection methods, such as signature-based, static, and dynamic analysis, face challenges in detecting obfuscated techniques, including encryption, packing, and compression, in malware. Although developers have created several visualization techniques for malware detection using deep learning (DL), they often fail to accurately identify the critical malicious features of malware. This research introduces MalVis, a unified visualization framework that integrates entropy and N-gram analysis to emphasize meaningful structural and anomalous operational patterns within the malware bytecode. By addressing significant limitations of existing visualization methods, such as insufficient feature representation, limited interpretability, small dataset sizes, and restricted data access, MalVis delivers enhanced detection capabilities, particularly for obfuscated and previously unseen (zero-day) malware. The framework leverages the MalVis dataset introduced in this work, a publicly available large-scale dataset comprising more than 1.3 million visual representations in nine malware classes and one benign class. A comprehensive comparative evaluation was performed against existing state-of-the-art visualization techniques using leading convolutional neural network (CNN) architectures, MobileNet-V2, DenseNet201, ResNet50, VGG16, and Inception-V3. To further boost classification performance and mitigate overfitting, the outputs of these models were combined using eight distinct ensemble strategies. To address the issue of imbalanced class distribution in the multiclass dataset, we employed an undersampling technique to ensure balanced learning across all types of malware. MalVis achieved superior results, with 95% accuracy, 90% F1-score, 92% precision, 89% recall, 87% Matthews Correlation Coefficient (MCC), and 98% Receiver Operating Characteristic Area Under Curve (ROC-AUC). These findings highlight the effectiveness of MalVis in providing interpretable and accurate representation features for malware detection and classification, making it valuable for research and real-world security applications.Keywords:Android;malware;entropy;static analysis;dynamic analysis;behavior analysis;bytecode;convolutional neural network;ensemble models;undersampling",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Smartphones are proliferating, with¬†projections indicating that they will exceed 7¬†billion by 2025, and¬†nearly 70% using the Android operating system [1,2]. Due to their compact designs, these mobile devices have become indispensable, facilitating tasks such as email management, banking transactions, and¬†storing sensitive health information. However, the¬†widespread adoption of smartphones has also drawn the attention of hackers [3], exacerbated by the open-source nature of Android‚Äôs OS and its SDK. This vulnerability has facilitated the way for various forms of malware, including viruses [4,5], worms [6], adware [7,8], spyware [9], ransomware [10], rootkits [11], trojans [12], keyloggers [13], botnets [14], and¬†mobileware [15]. Consequently, developing a robust defense system capable of identifying and mitigating this wide range of threats is crucial. While traditional detection methods discussed inSection 2like signature-based [16,17], dynamic analysis [18], and¬†static analysis [19] remain dominant, they often struggle with modern evasion techniques such as code obfuscation, encryption, polymorphism, and¬†packing. As¬†a result, there has been a growing interest in using advanced Deep Learning (DL) techniques to analyze malware and detect these suspicious behaviors. Several research works have explored the transformation of binary code or bytecode into image representations to leverage the capabilities of Deep Neural Network (DNN) for malware detection. However, these approaches often fail to capture semantic context, structural anomalies, and¬†obfuscation pattern features that are critical for accurate and robust¬†classification. Designing effective detection systems, particularly those utilizing visual representations, requires a thorough understanding of the structural composition of Android applications. The¬†following section provides an overview of the Android Package Kit (APK) file structure, focusing on its core components relevant to malware¬†analysis. 1.1. Overview of the Android APK File¬†StructureThe APK is a compressed file that the Android OS uses to distribute and install applications, consisting of core files and folders such as the application bytecode, assets, resources, and¬†a manifest file, as¬†presented inFigure 1.Figure 1.An illustration of the structure of an Android APK file, highlighting key components such as application bytecode, assets, resources, and the manifest file.Research on visualization-based malware detection often centers on two primary components of APKs AndroidManifest.xml and Classes.dex. Several researchers have analyzed the AndroidManifest.xml file to extract essential information about the application, such as its services, activities, package names, and¬†permissions [20,21]. While others focus on analyzing the Classes.dex file, which contains the executable bytecode intended for the Android Dalvik Virtual Machine (DVM), making it a vital source for behavioral analysis [22,23]. This research focuses on encoding and analyzing the Classes.dex file, given its importance in capturing malicious behavior. Our proposed approach utilizes CNN models to detect hidden Android malware threats by transforming bytecode into visual representations, with¬†a focus on highlighting anomalous operational or structural patterns indicative of obfuscation or malicious¬†intent. 1.2. ContributionsOur novel Android malware visualization framework uniquely integrates critical semantic and structural features extracted from executable bytecode, transforming them into RGB representations. Unlike previous techniques, MalVis enhances interpretability and classification accuracy while maintaining resilience against obfuscation. Our contributions include the¬†following:MalVis Dataset: Introducing MalVis, the¬†largest Android malware visualization dataset with over 1.3 million images across ten classes, including nine malware types and benign software that is accessible to the research community at¬†(www.mal-vis.org, accessed on 2 October 2025). Scripts for generating these various visualization methods are publicly available on GitHub at the link¬†(https://github.com/makkawysaleh/MalVis, accessed on 2 October 2025).Enhanced Visualization Framework: Developing an advanced MalVis framework that enhances malware visualization by incorporating an entropy encoder with an N-gram technique. This approach utilizes the three RGB channels to effectively capture a broader range of malware characteristics, including encryption, compression, packing, and¬†structural irregularities. This improves the precision of malware pattern detection in the visualizations.Enhanced Multiclass Labeling: Implementing an improved multiclass labeling approach using results from Euphony [24] and VirusTotal [25] allows precise classification and analysis of malware behavior, enhancing targeted threat identification and¬†classification.Robust Detection Model: Evaluation of the performance of the MalVis framework on several state-of-the-art visualization methods using advanced deep CNN architectures such as MobileNet-V2, DenseNet201, ResNet50, VGG16, and¬†Inception-V3, combined with several ensemble techniques, to¬†further improve detection accuracy and generalization. The¬†results showed that the MalVis framework achieved superior performance compared to others.Improved Framework Explainability and Transparency: We employ two distinct heatmap and attention mechanisms, GradCAM and GradCAM++, to¬†ensure that the MalVis Framework effectively utilizes the introduced malicious features detected by the application of our entropy and N-gram encoders to the malware representations. Additionally, identifying prevalent malicious patterns specific to each malware class in the malware representations.This research builds on our previous work on improving Android malware detection using bytecode-to-image encoding frameworks [26] to detect anomalous structural and malicious features in Android malware. Although¬†traditional detection techniques such as signature-based, static, and¬†dynamic analysis remain prevalent, visualization-based approaches have gained popularity due to their speed and ability to highlight malicious patterns. However, existing methods often rely on simplistic byte-to-color mappings based on byte location in the file, which overlook semantic features and abnormal structure traits of malware. Additionally, they struggle against obfuscation, encryption, and¬†packing. MalVis offers a richer and more interpretable representation that enhances the classification‚Äôs robustness and addresses the limitations of existing methods. This paper is organized as follows.Section 2reviews traditional detection methods and their limitations, discusses the motivation behind the visual-based detection, limitations of existing malware image-based datasets, and¬†prior grayscale and RGB visualization techniques.Section 3details our proposed MalVis framework, including the data collection and label generation process, bytecode-to-image transformation, an¬†in-depth analysis of entropy and N-gram features through two distinct visualization approaches, and¬†discusses the impact of obfuscation methods on the MalVis representations.Section 4defines the performance evaluation metrics used to assess the model‚Äôs effectiveness. InSection 5, we present experimental results across binary and multiclass classification tasks, demonstrate performance improvements through ensemble modeling, illustrate the effect of undersampling in addressing imbalance, enhance model explainability using GradCAM and GradCAM++, analyze key visualization findings, and¬†demonstrate the visualization key findings. Finally,Section 7concludes the paper and outlines future research¬†directions.",
            "1.1. Overview of the Android APK File¬†Structure": "The APK is a compressed file that the Android OS uses to distribute and install applications, consisting of core files and folders such as the application bytecode, assets, resources, and¬†a manifest file, as¬†presented inFigure 1. Figure 1.An illustration of the structure of an Android APK file, highlighting key components such as application bytecode, assets, resources, and the manifest file. Research on visualization-based malware detection often centers on two primary components of APKs AndroidManifest.xml and Classes.dex. Several researchers have analyzed the AndroidManifest.xml file to extract essential information about the application, such as its services, activities, package names, and¬†permissions [20,21]. While others focus on analyzing the Classes.dex file, which contains the executable bytecode intended for the Android Dalvik Virtual Machine (DVM), making it a vital source for behavioral analysis [22,23]. This research focuses on encoding and analyzing the Classes.dex file, given its importance in capturing malicious behavior. Our proposed approach utilizes CNN models to detect hidden Android malware threats by transforming bytecode into visual representations, with¬†a focus on highlighting anomalous operational or structural patterns indicative of obfuscation or malicious¬†intent.",
            "1.2. Contributions": "Our novel Android malware visualization framework uniquely integrates critical semantic and structural features extracted from executable bytecode, transforming them into RGB representations. Unlike previous techniques, MalVis enhances interpretability and classification accuracy while maintaining resilience against obfuscation. Our contributions include the¬†following: MalVis Dataset: Introducing MalVis, the¬†largest Android malware visualization dataset with over 1.3 million images across ten classes, including nine malware types and benign software that is accessible to the research community at¬†(www.mal-vis.org, accessed on 2 October 2025). Scripts for generating these various visualization methods are publicly available on GitHub at the link¬†(https://github.com/makkawysaleh/MalVis, accessed on 2 October 2025).Enhanced Visualization Framework: Developing an advanced MalVis framework that enhances malware visualization by incorporating an entropy encoder with an N-gram technique. This approach utilizes the three RGB channels to effectively capture a broader range of malware characteristics, including encryption, compression, packing, and¬†structural irregularities. This improves the precision of malware pattern detection in the visualizations.Enhanced Multiclass Labeling: Implementing an improved multiclass labeling approach using results from Euphony [24] and VirusTotal [25] allows precise classification and analysis of malware behavior, enhancing targeted threat identification and¬†classification.Robust Detection Model: Evaluation of the performance of the MalVis framework on several state-of-the-art visualization methods using advanced deep CNN architectures such as MobileNet-V2, DenseNet201, ResNet50, VGG16, and¬†Inception-V3, combined with several ensemble techniques, to¬†further improve detection accuracy and generalization. The¬†results showed that the MalVis framework achieved superior performance compared to others.Improved Framework Explainability and Transparency: We employ two distinct heatmap and attention mechanisms, GradCAM and GradCAM++, to¬†ensure that the MalVis Framework effectively utilizes the introduced malicious features detected by the application of our entropy and N-gram encoders to the malware representations. Additionally, identifying prevalent malicious patterns specific to each malware class in the malware representations. This research builds on our previous work on improving Android malware detection using bytecode-to-image encoding frameworks [26] to detect anomalous structural and malicious features in Android malware. Although¬†traditional detection techniques such as signature-based, static, and¬†dynamic analysis remain prevalent, visualization-based approaches have gained popularity due to their speed and ability to highlight malicious patterns. However, existing methods often rely on simplistic byte-to-color mappings based on byte location in the file, which overlook semantic features and abnormal structure traits of malware. Additionally, they struggle against obfuscation, encryption, and¬†packing. MalVis offers a richer and more interpretable representation that enhances the classification‚Äôs robustness and addresses the limitations of existing methods. This paper is organized as follows.Section 2reviews traditional detection methods and their limitations, discusses the motivation behind the visual-based detection, limitations of existing malware image-based datasets, and¬†prior grayscale and RGB visualization techniques.Section 3details our proposed MalVis framework, including the data collection and label generation process, bytecode-to-image transformation, an¬†in-depth analysis of entropy and N-gram features through two distinct visualization approaches, and¬†discusses the impact of obfuscation methods on the MalVis representations.Section 4defines the performance evaluation metrics used to assess the model‚Äôs effectiveness. InSection 5, we present experimental results across binary and multiclass classification tasks, demonstrate performance improvements through ensemble modeling, illustrate the effect of undersampling in addressing imbalance, enhance model explainability using GradCAM and GradCAM++, analyze key visualization findings, and¬†demonstrate the visualization key findings. Finally,Section 7concludes the paper and outlines future research¬†directions.",
            "2. Related¬†Works": "This section reviews state-of-the-art traditional detection methods and their limitations, highlighting the motivation for using visual representations in malware analysis. Then, it explores Android malware visualization datasets that serve as benchmarks for image-based malware detection. Finally, it explores recent visualization-based detection techniques, focusing on grayscale and RGB encoding methods for identifying malicious¬†patterns. 2.1. Signature-Based¬†AnalysisThe signature-based detection method is widely used to recognize and detect malware. A¬†software signature is a unique identifier that cannot be replicated and is typically generated using hash algorithms such as RSA, MD5, SHA1, SHA-256, and¬†SHA-512 [16,27]. The¬†detection engine generates a signature for the software and compares it with a database of known malicious signatures stored locally or remotely in the vendor‚Äôs cloud. Typically, these databases are proprietary assets of vendors, with¬†restricted access granted to licensed users. A¬†significant limitation of this method is the need for the detection engine to continuously refresh its signature database, which can lead to potential gaps in identifying new zero-day malware [28]. As¬†technology evolves, malware creators continue to find new techniques to evade detection, such as code alteration, function modification, file repacking, data encoding, or¬†null byte injection, all to generate new signatures capable of evading security defenses [17,29,30,31,32]. 2.2. Dynamic¬†AnalysisDynamic analysis is a pivotal method for malware detection, which involves observing and understanding software behavior during execution within a controlled and contained environment, such as a Sandbox or Virtual Machine (VM). This technique is effective in detecting abnormal actions, such as invoking suspicious system calls [33], examining network traffic [34], altering memory [35], and¬†detecting errors in Logcat that invoke suspicious services from the OS [36]. However, this method requires accessing or monitoring users‚Äô sensitive information, which can be impractical when managing highly confidential data [37]. Despite its promising outcomes, acquiring an extensive dataset of labeled training data for optimal performance is often both time-consuming and costly [38]. Security researchers are increasingly shifting to the visualization of malware based on static analysis images, which allows instant scanning of malware images to overcome the challenges posed by new malware [21,22,39]. Unlike dynamic analysis methods that require days or weeks to monitor suspicious behavior in an application, these image representations can be harmless, do not require manual feature engineering, and¬†resist typical obfuscation techniques employed by adversaries [40]. 2.3. Static¬†AnalysisStatic analysis is a technique used to evaluate applications without executing them or observing their execution behavior, which can often be demanding and time-consuming. By¬†not requiring execution, static analysis provides a unique assessment mode comparable to behavioral analysis techniques. One of the primary advantages of this malware detection method is its cost-effectiveness, as¬†it minimizes the need for additional hardware or extensive computational resources beyond the actual analysis tool itself [19]. Despite its advantages, this approach has notable limitations. Specifically, its effectiveness largely depends on identifying already known malware patterns, which challenges its ability to generalize and detect evolving zero-day malware. Research efforts focus on improving the detection of suspicious activities using advanced methodologies, such as machine learning (ML) and convolutional neural networks (CNNs) [26,41], to¬†mitigate this limitation. These innovations aim to enhance the adaptability and robustness of static analysis in response to evolving¬†threats. 2.4. MotivationDeep Neural Networks, especially CNNs, have shown exceptional performance across domains such as vision, biomedical [42], and¬†cybersecurity [42,43,44,45], primarily due to the availability of large, structured datasets. In¬†malware detection, transforming code into image representations allows CNNs to identify visual patterns of malicious behavior, offering a scalable, non-executable, and¬†efficient alternative to traditional analysis methods. These representations are significantly smaller than the raw executable files, reducing storage needs and execution risks, with¬†the (Figure 2) presenting the reduction ratio in percentage. Our approach further benefits from transfer learning by leveraging pretrained CNNs trained on massive image datasets such as ImageNet, enabling effective pattern recognition in malware with minimal domain-specific training. Despite these advantages, progress is hindered by limited access to large-scale, interpretable, and¬†public malware visualization datasets. Addressing this gap is essential for advancing robust, explainable, and¬†reproducible malware detection¬†research.Figure 2.Comparison of average file sizes in DEX executables vs. MalVis representations across malware categories (with size reduction ratio in percentages). 2.5. Existing Malware Image¬†DatasetsWe highlight two widely known Android malware datasets: AndroZoo [46] and Drebin [47], both commonly used in Android malware detection research. However, because¬†our MalVis approach focuses on transforming bytecode into images for visualization, we primarily compare MalVis with other existing image-based datasets in this section. Scott Freitas¬†et¬†al. [48] introduced the MalNet database, a¬†substantial contribution to the field, comprising over 1.2 million malware images spanning 47 types and 696 families. While their direct byte-to-location color mapping method is innovative within the Android application structure, our dataset, MalVis, offers enhancements in the form of over 1.3¬†million images, with¬†a particular focus on addressing malware obfuscation techniques. This focus improves the effectiveness of Android malware detection, as¬†discussed in more detail by Makkawy¬†et¬†al. [26]. Virus-MNIST, proposed by David A.¬†et¬†al. [4], is a large publicly available malware image dataset. The¬†dataset includes 51,880 grayscale images of malware, classified into nine virus classes and one benign class, all formatted as 32 √ó 32 images. The¬†dataset represents the malware classification problem, like the famous MNIST dataset used for handwriting recognition. Malware images are generated by converting the first 1024 bytes of Portable Executable (PE) files into 32 √ó 32 grayscale images. Although¬†Virus-MNIST introduces a significant step towards standardizing malware image datasets, its representation of malware using only the first 1024 bytes may result in a lack of capturing the complete characteristics of the malware [49]. L. Nataraj¬†et¬†al. [50] present the MalImg dataset, which offers a straightforward and efficient malware visualization and classification method. Using image processing techniques, this approach classifies malware samples based on their similarity to specific malware types, utilizing standard image features. MalImg achieves a notable classification accuracy of 98% on a dataset comprising 9458 samples across 25 distinct malware types. However, with¬†this limited dataset size, there is a possibility that the model is overfitting to the specific characteristics of these samples.Table 1summarizes public and private image-based malware datasets, including MalVis, MalNet, and¬†Virus-MNIST, providing details on the number of classes and dataset size. Despite existing visualization contributions, these methods face some limitations. As¬†observed by Kunwar¬†et¬†al. [51], MalNet encodes malware bytecode based on the byte location in the executable file, lacks resilience to obfuscation, and¬†fails to identify suspicious behaviors. Virus-MNIST [4] uses only the first 1024 bytes of PE files, limiting its representational scope. In¬†contrast, MalVis combines entropy and N-gram analysis to generate color-encoded RGB representations that emphasize abnormal structures, encryption, packing, and¬†compression behaviors. Our experiments show that this richer visual encoding enhances model interpretability and improves classification performance. A¬†detailed discussion of the MalVis dataset and its visualization approach is presented inSection 3.Table 1.Summary of image-based malware datasets detailing the number of classes, dataset sizes, and availability. The row highlighted in blue (MalVis) corresponds to our proposed dataset. 2.6. Visualization Strategies for Malware¬†DetectionAs image-based malware detection has become a powerful paradigm for analyzing Android applications, it bypasses manual feature engineering through automated visual pattern recognition. Current approaches primarily focus on two types of¬†representations:2.6.1. Grayscale Image¬†EncodingThe foundational work by Nataraj¬†et¬†al. [50] established grayscale conversion by mapping binary bytes to pixel values in the range (0‚Äì255), revealing structural patterns in malware families. Modern implementations include DexRay by Nadia Daoudi¬†et¬†al. [56], which converts DEX bytecode into 1D grayscale vectors (1 √ó 128 √ó 128) for CNN classification, achieving a 96% F1-score while resisting obfuscation. Despite its highest accuracy, their approach resulted in smaller-size grayscale images that could be affected by more data loss in the representations. In¬†another instance, Wang¬†et¬†al. [57] developed a novel scheme that combines static and dynamic analysis with CNN for efficient malware detection and classification. Their method integrates a Convolutional Block Attention Module (CBAM) with CNN to detect malware similarities using grayscale images from the MalImg and Microsoft datasets. However, their experiment was conducted on a relatively small dataset of approximately 20,000 samples, covering 25 types of malware, which could be susceptible to model¬†overfitting.2.6.2. RGB Image¬†EncodingAdvanced malware variants often exhibit more sophisticated patterns and behaviors that are difficult to capture using standard grayscale or single-channel representations. Additional color channels are required to encode these complex characteristics, enabling richer feature representation and enhancing the model‚Äôs ability to detect subtle malicious traits. Asim¬†et¬†al. [58] introduced a technique that transforms APK files into lightweight RGB images utilizing a predefined dictionary and an intelligent mapping mechanism. Their method converts the AndroidManifest.xml permissions into ASCII values, which are then aggregated and encoded into a single color value. While this approach facilitates the image-based representation of APK features in RGB channels, it suffers from significant information loss due to the summation of ASCII values, which flattens each permission into a single numerical value. This reduction hinders the model‚Äôs ability to capture detailed permission information, thus limiting its effectiveness in capturing nuanced malicious behaviors. Progress in this field is hindered by the scarcity of publicly available visualized malware datasets [59] and the need for robust methodologies to capture malware patterns and behaviors [48] effectively. The¬†MalVis dataset aims to tackle these challenges by providing comprehensive representations of malware, which convert abnormal operational and structural patterns in bytecode into visual forms. Additionally, it incorporates multiclass labels for precise malware classification and analysis, thereby enhancing targeted threat¬†identification.",
            "2.1. Signature-Based¬†Analysis": "The signature-based detection method is widely used to recognize and detect malware. A¬†software signature is a unique identifier that cannot be replicated and is typically generated using hash algorithms such as RSA, MD5, SHA1, SHA-256, and¬†SHA-512 [16,27]. The¬†detection engine generates a signature for the software and compares it with a database of known malicious signatures stored locally or remotely in the vendor‚Äôs cloud. Typically, these databases are proprietary assets of vendors, with¬†restricted access granted to licensed users. A¬†significant limitation of this method is the need for the detection engine to continuously refresh its signature database, which can lead to potential gaps in identifying new zero-day malware [28]. As¬†technology evolves, malware creators continue to find new techniques to evade detection, such as code alteration, function modification, file repacking, data encoding, or¬†null byte injection, all to generate new signatures capable of evading security defenses [17,29,30,31,32].",
            "2.2. Dynamic¬†Analysis": "Dynamic analysis is a pivotal method for malware detection, which involves observing and understanding software behavior during execution within a controlled and contained environment, such as a Sandbox or Virtual Machine (VM). This technique is effective in detecting abnormal actions, such as invoking suspicious system calls [33], examining network traffic [34], altering memory [35], and¬†detecting errors in Logcat that invoke suspicious services from the OS [36]. However, this method requires accessing or monitoring users‚Äô sensitive information, which can be impractical when managing highly confidential data [37]. Despite its promising outcomes, acquiring an extensive dataset of labeled training data for optimal performance is often both time-consuming and costly [38]. Security researchers are increasingly shifting to the visualization of malware based on static analysis images, which allows instant scanning of malware images to overcome the challenges posed by new malware [21,22,39]. Unlike dynamic analysis methods that require days or weeks to monitor suspicious behavior in an application, these image representations can be harmless, do not require manual feature engineering, and¬†resist typical obfuscation techniques employed by adversaries [40].",
            "2.3. Static¬†Analysis": "Static analysis is a technique used to evaluate applications without executing them or observing their execution behavior, which can often be demanding and time-consuming. By¬†not requiring execution, static analysis provides a unique assessment mode comparable to behavioral analysis techniques. One of the primary advantages of this malware detection method is its cost-effectiveness, as¬†it minimizes the need for additional hardware or extensive computational resources beyond the actual analysis tool itself [19]. Despite its advantages, this approach has notable limitations. Specifically, its effectiveness largely depends on identifying already known malware patterns, which challenges its ability to generalize and detect evolving zero-day malware. Research efforts focus on improving the detection of suspicious activities using advanced methodologies, such as machine learning (ML) and convolutional neural networks (CNNs) [26,41], to¬†mitigate this limitation. These innovations aim to enhance the adaptability and robustness of static analysis in response to evolving¬†threats.",
            "2.4. Motivation": "Deep Neural Networks, especially CNNs, have shown exceptional performance across domains such as vision, biomedical [42], and¬†cybersecurity [42,43,44,45], primarily due to the availability of large, structured datasets. In¬†malware detection, transforming code into image representations allows CNNs to identify visual patterns of malicious behavior, offering a scalable, non-executable, and¬†efficient alternative to traditional analysis methods. These representations are significantly smaller than the raw executable files, reducing storage needs and execution risks, with¬†the (Figure 2) presenting the reduction ratio in percentage. Our approach further benefits from transfer learning by leveraging pretrained CNNs trained on massive image datasets such as ImageNet, enabling effective pattern recognition in malware with minimal domain-specific training. Despite these advantages, progress is hindered by limited access to large-scale, interpretable, and¬†public malware visualization datasets. Addressing this gap is essential for advancing robust, explainable, and¬†reproducible malware detection¬†research. Figure 2.Comparison of average file sizes in DEX executables vs. MalVis representations across malware categories (with size reduction ratio in percentages).",
            "2.5. Existing Malware Image¬†Datasets": "We highlight two widely known Android malware datasets: AndroZoo [46] and Drebin [47], both commonly used in Android malware detection research. However, because¬†our MalVis approach focuses on transforming bytecode into images for visualization, we primarily compare MalVis with other existing image-based datasets in this section. Scott Freitas¬†et¬†al. [48] introduced the MalNet database, a¬†substantial contribution to the field, comprising over 1.2 million malware images spanning 47 types and 696 families. While their direct byte-to-location color mapping method is innovative within the Android application structure, our dataset, MalVis, offers enhancements in the form of over 1.3¬†million images, with¬†a particular focus on addressing malware obfuscation techniques. This focus improves the effectiveness of Android malware detection, as¬†discussed in more detail by Makkawy¬†et¬†al. [26]. Virus-MNIST, proposed by David A.¬†et¬†al. [4], is a large publicly available malware image dataset. The¬†dataset includes 51,880 grayscale images of malware, classified into nine virus classes and one benign class, all formatted as 32 √ó 32 images. The¬†dataset represents the malware classification problem, like the famous MNIST dataset used for handwriting recognition. Malware images are generated by converting the first 1024 bytes of Portable Executable (PE) files into 32 √ó 32 grayscale images. Although¬†Virus-MNIST introduces a significant step towards standardizing malware image datasets, its representation of malware using only the first 1024 bytes may result in a lack of capturing the complete characteristics of the malware [49]. L. Nataraj¬†et¬†al. [50] present the MalImg dataset, which offers a straightforward and efficient malware visualization and classification method. Using image processing techniques, this approach classifies malware samples based on their similarity to specific malware types, utilizing standard image features. MalImg achieves a notable classification accuracy of 98% on a dataset comprising 9458 samples across 25 distinct malware types. However, with¬†this limited dataset size, there is a possibility that the model is overfitting to the specific characteristics of these samples.Table 1summarizes public and private image-based malware datasets, including MalVis, MalNet, and¬†Virus-MNIST, providing details on the number of classes and dataset size. Despite existing visualization contributions, these methods face some limitations. As¬†observed by Kunwar¬†et¬†al. [51], MalNet encodes malware bytecode based on the byte location in the executable file, lacks resilience to obfuscation, and¬†fails to identify suspicious behaviors. Virus-MNIST [4] uses only the first 1024 bytes of PE files, limiting its representational scope. In¬†contrast, MalVis combines entropy and N-gram analysis to generate color-encoded RGB representations that emphasize abnormal structures, encryption, packing, and¬†compression behaviors. Our experiments show that this richer visual encoding enhances model interpretability and improves classification performance. A¬†detailed discussion of the MalVis dataset and its visualization approach is presented inSection 3. Table 1.Summary of image-based malware datasets detailing the number of classes, dataset sizes, and availability. The row highlighted in blue (MalVis) corresponds to our proposed dataset.",
            "2.6. Visualization Strategies for Malware¬†Detection": "As image-based malware detection has become a powerful paradigm for analyzing Android applications, it bypasses manual feature engineering through automated visual pattern recognition. Current approaches primarily focus on two types of¬†representations: 2.6.1. Grayscale Image¬†EncodingThe foundational work by Nataraj¬†et¬†al. [50] established grayscale conversion by mapping binary bytes to pixel values in the range (0‚Äì255), revealing structural patterns in malware families. Modern implementations include DexRay by Nadia Daoudi¬†et¬†al. [56], which converts DEX bytecode into 1D grayscale vectors (1 √ó 128 √ó 128) for CNN classification, achieving a 96% F1-score while resisting obfuscation. Despite its highest accuracy, their approach resulted in smaller-size grayscale images that could be affected by more data loss in the representations. In¬†another instance, Wang¬†et¬†al. [57] developed a novel scheme that combines static and dynamic analysis with CNN for efficient malware detection and classification. Their method integrates a Convolutional Block Attention Module (CBAM) with CNN to detect malware similarities using grayscale images from the MalImg and Microsoft datasets. However, their experiment was conducted on a relatively small dataset of approximately 20,000 samples, covering 25 types of malware, which could be susceptible to model¬†overfitting. 2.6.2. RGB Image¬†EncodingAdvanced malware variants often exhibit more sophisticated patterns and behaviors that are difficult to capture using standard grayscale or single-channel representations. Additional color channels are required to encode these complex characteristics, enabling richer feature representation and enhancing the model‚Äôs ability to detect subtle malicious traits. Asim¬†et¬†al. [58] introduced a technique that transforms APK files into lightweight RGB images utilizing a predefined dictionary and an intelligent mapping mechanism. Their method converts the AndroidManifest.xml permissions into ASCII values, which are then aggregated and encoded into a single color value. While this approach facilitates the image-based representation of APK features in RGB channels, it suffers from significant information loss due to the summation of ASCII values, which flattens each permission into a single numerical value. This reduction hinders the model‚Äôs ability to capture detailed permission information, thus limiting its effectiveness in capturing nuanced malicious behaviors. Progress in this field is hindered by the scarcity of publicly available visualized malware datasets [59] and the need for robust methodologies to capture malware patterns and behaviors [48] effectively. The¬†MalVis dataset aims to tackle these challenges by providing comprehensive representations of malware, which convert abnormal operational and structural patterns in bytecode into visual forms. Additionally, it incorporates multiclass labels for precise malware classification and analysis, thereby enhancing targeted threat¬†identification.",
            "2.6.1. Grayscale Image¬†Encoding": "The foundational work by Nataraj¬†et¬†al. [50] established grayscale conversion by mapping binary bytes to pixel values in the range (0‚Äì255), revealing structural patterns in malware families. Modern implementations include DexRay by Nadia Daoudi¬†et¬†al. [56], which converts DEX bytecode into 1D grayscale vectors (1 √ó 128 √ó 128) for CNN classification, achieving a 96% F1-score while resisting obfuscation. Despite its highest accuracy, their approach resulted in smaller-size grayscale images that could be affected by more data loss in the representations. In¬†another instance, Wang¬†et¬†al. [57] developed a novel scheme that combines static and dynamic analysis with CNN for efficient malware detection and classification. Their method integrates a Convolutional Block Attention Module (CBAM) with CNN to detect malware similarities using grayscale images from the MalImg and Microsoft datasets. However, their experiment was conducted on a relatively small dataset of approximately 20,000 samples, covering 25 types of malware, which could be susceptible to model¬†overfitting.",
            "2.6.2. RGB Image¬†Encoding": "Advanced malware variants often exhibit more sophisticated patterns and behaviors that are difficult to capture using standard grayscale or single-channel representations. Additional color channels are required to encode these complex characteristics, enabling richer feature representation and enhancing the model‚Äôs ability to detect subtle malicious traits. Asim¬†et¬†al. [58] introduced a technique that transforms APK files into lightweight RGB images utilizing a predefined dictionary and an intelligent mapping mechanism. Their method converts the AndroidManifest.xml permissions into ASCII values, which are then aggregated and encoded into a single color value. While this approach facilitates the image-based representation of APK features in RGB channels, it suffers from significant information loss due to the summation of ASCII values, which flattens each permission into a single numerical value. This reduction hinders the model‚Äôs ability to capture detailed permission information, thus limiting its effectiveness in capturing nuanced malicious behaviors. Progress in this field is hindered by the scarcity of publicly available visualized malware datasets [59] and the need for robust methodologies to capture malware patterns and behaviors [48] effectively. The¬†MalVis dataset aims to tackle these challenges by providing comprehensive representations of malware, which convert abnormal operational and structural patterns in bytecode into visual forms. Additionally, it incorporates multiclass labels for precise malware classification and analysis, thereby enhancing targeted threat¬†identification.",
            "3. Methodology": "This section presents the methodology of the MalVis framework, as¬†illustrated inFigure 3. The¬†framework operates through several interconnected stages that transform Android applications into visual representations for malware detection. First, data collection and label generation involve gathering malware and benign samples from multiple repositories, resulting in a collection of 1,300,822 APK files. Second, feature extraction utilizes reverse engineering tools to decompile APK files and extract the executable bytecode classes.dex for visualization purposes. Third, bytecode is transformed into RGB channel images using two novel MalVis encoding schemes (Classbyte and N-gram). We further analyze the impact of entropy and N-gram encoding on visualization quality, examining how these techniques capture malware obfuscation methods and structural anomalies. Finally, we describe the CNN architectures, training configurations, and¬†experimental environment setup used for the classification task. The¬†following subsections provide detailed explanations of each pipeline¬†stage. Figure 3.A schematic illustration of the proposed framework architecture is organized into four distinct rows. The first row details the data collection and labeling process. The second row focuses on feature extraction. The third row constructs and generates RGB images using entropy and N-gram methods. The bottom row describes the training process, including visualization techniques, CNN models, ensemble methods, and both binary and multiclass classifications. The circled numbers (1‚Äì9) indicate the pipline‚Äôs steps explained in detail in the corresponding sections. 3.1. Data Collection and Label¬†GenerationThe MalVis generation process utilizes a subset of the AndroZoo dataset [46], as¬†shown inFigure 3‚ûÄ, a key resource in Android research that encompasses 24,743,375 applications collected from platforms such as the Google Play Store. Our model training uses both a binary classification dataset and the MalVis multiclass classification dataset. The¬†binary dataset comprises 49,150 malware samples and 135,324 benign samples, as¬†presented in our earlier research [26]. It was primarily utilized to evaluate all proposed visualization approaches on a dataset of manageable size and to determine the optimal visualization method for Android bytecode. The¬†multiclass malware dataset utilizes Euphony [24] to categorize malware into 289 distinct categories. For¬†training purposes, we focus on the nine largest categories to enhance labeling accuracy and minimize false positives by excluding samples with multiple labels. Furthermore, we cross-verify these samples with VirusTotal [25] to ensure their reliability. The¬†refined dataset, illustrated inFigure 3‚ûÅ, includes 1,300,822 samples, comprising nine types of malware and an additional 135,324 benign samples sourced from AndroZoo.Figure 4displays the distribution and application visualizations of the nine malware types alongside the benign¬†class.Figure 4.Distribution of malware types and benign in MalVis. 3.2. MalVis Bytecode-to-Image¬†VisualizationThe MalVis bytecode-to-image visualization process begins by extracting Dalvik Executable (DEX) files from Android APKs using AndroGuard [60], a¬†well-known reverse engineering tool, presented inFigure 3‚ûÇ. This step yields the classes.dexfiles, as¬†illustrated inFigure 3‚ûÉ. These .dex files consist of byte values in the range of 0x00 to 0xFF. These values are first converted into a one-dimensional array of unsigned integers, where each value is represented by a number between 0 and 255. These integers correspond directly to the pixel color intensities. The¬†1D array is reshaped into a two-dimensional grayscale image with a fixed width and height of 256 pixels to visualize the bytecode. This transformation employs Nearest-Neighbor Interpolation (NNI) and the Pillow library in Python, ensuring consistent image dimensions while preserving the original byte sequence structure. Shannon entropy is applied to the executable dex file using a 32-byte sliding window to determine the red and blue channels as illustrated byFigure 3‚ûÑ. These channels are defined by distinct formulas motivated by [61], as¬†described in our earlier paper [26]. Each formula utilizes Shannon entropy differently to highlight regions of considerable randomness, which may indicate encryption or obfuscation. Noteùêª(ùëã)=‚àí‚àëùëñ=1ùëÅùëÉ(ùë•ùëñ)log2(ùëÉ(ùë•ùëñ)),H(X)=‚àí‚àëi=1NP(xi)log2(P(xi)),(1)whereùêª(ùëã)H(X)represents the Shannon entropy of the random variableX, which measures the uncertainty or randomness in the 32-byte sequence,ùëÉ(ùë•ùëñ)P(xi)is the probability of observing the specificùëñùë°‚Ñéithoutcome or byte valueùë•ùëñxi,Ndenotes the total number of unique outcomes for the random variableX(for a single byte,ùëÅ=256N=256),ùë•ùëñxirefers to a specific byte value in the range{0,1,2,‚Ä¶,255}{0,1,2,‚Ä¶,255}, andlog2log2is the logarithm to base two commonly used in entropy calculations. Given the varied types of malware introduced by the MalVis dataset, we have explored techniques to improve the recognition of these variations. Our analysis focuses on extending our earlier approach [26] from two color channels (red and blue) to three channels by encoding an additional feature into the green channel of RGB images using two primary encoding¬†methods:Classbyte Encoding:We adopt the Classbyte encoder introduced by Duc-Ly¬†et¬†al. [21], as¬†shown byFigure 3‚ûÖ, which maps semantic features of bytecode to varying intensities of the green channel. We selected this method due to its effectiveness and comparable performance to our previously employed entropy-based encoding for binary classification tasks.N-gram Encoding: We incorporate N-gram representations, as¬†illustrated byFigure 3‚ûÖ, derived from byte sequences to capture the malware bytecode‚Äôs underlying structural patterns and contextual dependencies. This technique, commonly used in malware detection research [62,63], enriches the green channel with statistical features that reflect code regularities and anomalies, thereby enhancing the capability to distinguish between different types of malware.The following subsections discuss the implications of these methods for advancing malware¬†visualization.3.2.1. Approach¬†ClassbyteThis approach uses the Classbyte representation, which performs similarly to the entropy encoder in binary classification. It translates the features identified by the four Classbyte colors into four distinct shades of green in the green channel, as¬†illustrated inFigure 5‚ûÄ. The¬†method highlights sections of bytecode containing both clear-text printable and non-printable ASCII characters and null byte areas, as¬†illustrated inFigure 5‚ûÅ. These distinctions assist in analyzing the bytecode to determine whether it has been encrypted or injected with null bytes to evade malware detection. The previously generated red and blue channels are combined with the newly constructed green channel, resulting in MalVis (Classbyte encoded) RGB images, as¬†shown inFigure 5‚ûÇ. Unfortunately, this approach did not yield the desired improvement in the accuracy of multiclass classification. Further results of the analysis and evaluation of this approach are presented inSection 5.Figure 5.Overview of constructing the MalVis (Classbyte) visualization method, resulting in RGB image representations using the Classbyte encoding in the green channel and encoding entropy in the red and blue channels. The circled numbers (1‚Äì3) represent the pipeline steps described in the text.3.2.2. Approach¬†N-GramThis approach utilizes the N-gram method, which has been extensively studied for malware anomaly detection. The¬†approach is particularly relevant for Android applications, which are often written in Java and Kotlin, thus inheriting the programmatic structure. Abnormalities are detected when the byte sequences differ from the typical bytecode structure using the green channel depicted inFigure 6‚ûÅ. One of the key goals of this approach is to bridge the gap between raw visualization and interpretability. Unlike prior methods that map byte values to color values without semantic linkage, our framework encodes interpretable attributes: entropy highlights encrypted or compressed code regions. At¬†the same time, N-gram transitions emphasize structural irregularities in bytecode. This mapping allows security analysts and researchers to visually associate distinct color patterns with specific malware behaviors, such as repacking functions or¬†obfuscation.Figure 6.Overview of the MalVis (N-gram) visualization pipeline, illustrating how Shannon entropy populates the red and blue channels while bi-gram (N-gram) values fill the green channel to emphasize structural and contextual bytecode patterns. The numbered circles (1‚Äì3) correspond to the pipeline steps described in the text.Figure 7,Figure 8andFigure 9further clarify this concept. For¬†example, the¬†script inFigure 7below depicts a simple Java code for a ‚Äòfor‚Äô loop before it is compiled into bytecode. The¬†bytecode often reveals specific patterns that represent the underlying syntax and structure of the program. The¬†keyword ‚Äòfor‚Äô indicates the presence of a loop followed by an initialization statement, a¬†condition, and¬†an increment surrounded by braces(‚ãØ)(‚ãØ), while curly braces{‚ãØ}{‚ãØ}denote the loop‚Äôs¬†body.Figure 7.An example of a simple for-loop written in Java.Figure 8.Translation of a Java for-loop into its equivalent JVM instructions in bytecode form after compilation.Figure 9.Representation of the employed Bi-gram approach on the Java instructions capturing the semantic transition of these instructions.Running ‚Äújavac For_Loop.java‚Äù compiles the code into a bytecode file named ‚ÄúFor_Loop.class‚Äù, which the DVM uses to execute the program, as¬†demonstrated inFigure 8.We implemented the Bi-gram method on the raw bytes using a two-byte window to recognize anomalies in the bytecode‚Äôs operational structure. Using Bi-gram on the bytecode represents the transition between instructions as listed inFigure 9.The Bi-gram method identifies obfuscated code by detecting irregular two-byte patterns within the byte sequence. While larger N-gram windows can capture more complex structural dependencies, the¬†associated feature space increases exponentially with window size. For¬†instance, the¬†use of 2-byte (Bi-gram) configuration yields2562=65,5362562=65,536possible patterns, a¬†3-byte (Tri-gram)2563‚âà16.72563‚âà16.7million, and¬†a 4-byte configuration2564‚âà4.32564‚âà4.3billion. Although¬†higher-order N-grams may reveal more intricate obfuscation techniques, they introduce substantial sparsity and computational overhead. Training several CNN models on this visualization method with the Bi-gram encoding features required two weeks of computation. Ultimately, the¬†two-byte configuration was chosen as an optimal balance between representational depth and computational efficiency. Future research could investigate the effects of utilizing larger N-gram¬†windows.The Bi-gram formula isBi-gramvalue=ùëè1√ó28+ùëè2,Bi-gramvalue=b1√ó28+b2,(2)which takes two consecutive bytes,ùëè1b1andùëè2b2, to¬†compute the Bi-gram value. The¬†multiplication of the first byteùëè1b1by28=25628=256shifts it to higher-order in the combined value, which is then added to theùëè2b2value, as¬†shown in line 16 of the Algorithm¬†1. The¬†resulting Bi-gram value represents the degree level of a green pixel and is normalized to the range[0,1][0,1]by dividing by the maximum possible value of(256√ó256)‚àí1=65,535(256√ó256)‚àí1=65,535as described¬†byùëî=Bi-gramvalue(256√ó256)‚àí1=ùëè1√ó256+ùëè265,535.g=Bi-gramvalue(256√ó256)‚àí1=b1√ó256+b265,535.(3)Finally, if¬†the byte is the last in the file, it is reset to 0, as¬†detailed in lines 18 and 19 of the Algorithm¬†1. Hence, MalVis presents a conceptually innovative visualization design that maps meaningful malware properties to distinct visual domains. Consequently, MalVis is more effective and better aligned with the objectives of explainable malware classification. This approach has demonstrated improved accuracy in the context of multiclass MalVis, and¬†both representation techniques are evaluated inSection 5. 3.3. The Impact of Entropy and N-Gram on MalVis Representations¬†ExperimentsIn this section, we further investigate the sensitivity and interpretability of MalVis visualizations. We conducted a controlled analysis by applying targeted transformations to a benign Android application, specifically WhatsApp Classes.dex. The¬†goal was to investigate and quantify the impact of encryption and unstructured operations in bytecode changes on the RGB image representations generated by our¬†framework.Algorithm 1MalVis Visualization Algorithm: generate RGB from bytecode using entropy and N-gram1:Input:Data arrayùëëùëéùë°ùëédataof bytecode, symbol mapùë†ùë¶ùëöùëèùëúùëô_ùëöùëéùëùsymbol_map, indexx2:Output:RGB values in the range [0, 255]3:ùëí‚ÜêEntropy(ùëëùëéùë°ùëé,32,ùë•,len(ùë†ùë¶ùëöùëèùëúùëô_ùëöùëéùëù))e‚ÜêEntropy(data,32,x,len(symbol_map))‚ñπCalculate entropy using a window size of 32 bytes4:functioncurve(v)5:ùëì‚Üê(4ùë£‚àí4ùë£2)4f‚Üê(4v‚àí4v2)46:ùëì‚Üêmax(ùëì,0)f‚Üêmax(f,0)7:returnf8:end function9:ifùëí>0.5e>0.5then10.ùëü‚Üêcurve(ùëí‚àí0.5)r‚Üêcurve(e‚àí0.5)‚ñπRed component is determined by the scaled entropy value11:else12:ùëü‚Üê0r‚Üê0‚ñπIf entropy is less than or equal to 0.5, set red component to 013:end if14:ùëè‚Üêùëí2b‚Üêe2‚ñπBlue component is proportional to the square of entropy15:ifùë•<len(ùëëùëéùë°ùëé)‚àí1x<len(data)‚àí1then16:ùëõ_ùëîùëüùëéùëö_ùë£ùëéùëôùë¢ùëí‚Üê(ùëëùëéùë°ùëé[ùë•]‚â™8)+ùëëùëéùë°ùëé[ùë•+1]n_gram_value‚Üê(data[x]‚â™8)+data[x+1]‚ñπCompute 2-byte n-gram value17:ùëî‚Üêùëõ_ùëîùëüùëéùëö_ùë£ùëéùëôùë¢ùëí65,535g‚Üên_gram_value65,535‚ñπNormalize n-gram value to [0, 1] for green component18:else19:ùëî‚Üê0g‚Üê0‚ñπIf at the last byte, the green component is set to 020:end if21:return[int(255¬∑ùëü),int(255¬∑ùëî),int(255¬∑ùëè)][int(255¬∑r),int(255¬∑g),int(255¬∑b)]‚ñπReturn RGB values scaled to the range [0, 255]3.3.1. Obfuscation Detection Captured by Entropy in Red and Blue¬†ChannelsIn this experiment, we applied AES-256 encryption in Electronic Codebook (ECB) mode to the initial 30% of the Classes.dex bytecode. This encryption caused a noticeable shift in entropy, particularly affecting the red and blue channels of image representations. Entropy, which quantifies randomness over 32-byte windows, increased significantly in high-entropy areas, leading to brighter pixel intensities. This effect simulates obfuscation techniques that malware creators use to evade detection. As¬†a result, the¬†red and blue channels inFigure 10display brighter pixels in the top-left region, highlighting the encrypted sections in the¬†representation.Figure 10.The impact of 30% AES-256 encryption on Classes.dex file captured by the entropy encoder in the red and blue channels of MalVis representations.3.3.2. Unstructured Bytecode Insertion Captured by N-Gram in Green¬†ChannelIn this experiment, we examined the structural sensitivity of the green channel by injecting random, unstructured operations into the initial 30% of the Classes.dex file. This action disrupted the byte sequence, causing noticeable distortions in the N-gram values, significantly impacting the green channel. MalVis, which utilizes bi-gram formulas to detect abnormal operational patterns, recorded these disturbances as increased bi-gram values, resulting in brighter pixel values within green-channel textures, as¬†depicted inFigure 11. These deviations were apparent when visualized next to an unchanged sample, highlighting the effectiveness of the green channel in detecting structural¬†anomalies.Figure 11.The impact of injecting 30% randomized unstructured operations to Classes.dex file captured by the N-gram encoder in the green channels of MalVis representations. 3.4. Model Architecture and Experiment¬†SettingsTo assess the effectiveness of our proposed approach, we utilized a selection of well-recognized CNN models, as¬†shown byFigure 3‚ûÜ, including MobileNetV2, ResNet-50, DenseNet-201, VGG-16, and¬†Inception-V3. These models were applied to our generated visualizations and baseline comparison methods. The¬†employed CNN models have proven highly effective in malware detection because they capture intricate patterns and features within image data [48,64]. To¬†ensure consistency across the CNN models, all images were resized to224√ó224224√ó224pixels using nearest neighbor interpolation to align with the input dimensions required by the models. The¬†dataset was partitioned into 80% for training, 10% for validation, and¬†10% for testing. The¬†batch size 64 was chosen based on empirical experimentation, as¬†it provided an optimal trade-off between training speed and memory consumption on our GPU setup. The¬†training was conducted over 50 epochs and carefully monitored to mitigate overfitting. This setup enabled consistent and accurate assessments of the models‚Äô performance across various visualization¬†techniques. 3.5. Environment¬†SetupMalVis visualization and model training were generated using an Ubuntu Server 22.04 LTS OS with x86 64 architecture. The¬†hardware setup consisted of a 16-core AMD Ryzen Threadripper PRO 5955WX processor, 128 GB of DDR4 RAM at 3200 MHz, and¬†an NVIDIA RTX A6000 graphics card. The¬†system was configured within a controlled environment to ensure accurate results and minimize external¬†influences.",
            "3.1. Data Collection and Label¬†Generation": "The MalVis generation process utilizes a subset of the AndroZoo dataset [46], as¬†shown inFigure 3‚ûÄ, a key resource in Android research that encompasses 24,743,375 applications collected from platforms such as the Google Play Store. Our model training uses both a binary classification dataset and the MalVis multiclass classification dataset. The¬†binary dataset comprises 49,150 malware samples and 135,324 benign samples, as¬†presented in our earlier research [26]. It was primarily utilized to evaluate all proposed visualization approaches on a dataset of manageable size and to determine the optimal visualization method for Android bytecode. The¬†multiclass malware dataset utilizes Euphony [24] to categorize malware into 289 distinct categories. For¬†training purposes, we focus on the nine largest categories to enhance labeling accuracy and minimize false positives by excluding samples with multiple labels. Furthermore, we cross-verify these samples with VirusTotal [25] to ensure their reliability. The¬†refined dataset, illustrated inFigure 3‚ûÅ, includes 1,300,822 samples, comprising nine types of malware and an additional 135,324 benign samples sourced from AndroZoo.Figure 4displays the distribution and application visualizations of the nine malware types alongside the benign¬†class. Figure 4.Distribution of malware types and benign in MalVis.",
            "3.2. MalVis Bytecode-to-Image¬†Visualization": "The MalVis bytecode-to-image visualization process begins by extracting Dalvik Executable (DEX) files from Android APKs using AndroGuard [60], a¬†well-known reverse engineering tool, presented inFigure 3‚ûÇ. This step yields the classes.dexfiles, as¬†illustrated inFigure 3‚ûÉ. These .dex files consist of byte values in the range of 0x00 to 0xFF. These values are first converted into a one-dimensional array of unsigned integers, where each value is represented by a number between 0 and 255. These integers correspond directly to the pixel color intensities. The¬†1D array is reshaped into a two-dimensional grayscale image with a fixed width and height of 256 pixels to visualize the bytecode. This transformation employs Nearest-Neighbor Interpolation (NNI) and the Pillow library in Python, ensuring consistent image dimensions while preserving the original byte sequence structure. Shannon entropy is applied to the executable dex file using a 32-byte sliding window to determine the red and blue channels as illustrated byFigure 3‚ûÑ. These channels are defined by distinct formulas motivated by [61], as¬†described in our earlier paper [26]. Each formula utilizes Shannon entropy differently to highlight regions of considerable randomness, which may indicate encryption or obfuscation. Noteùêª(ùëã)=‚àí‚àëùëñ=1ùëÅùëÉ(ùë•ùëñ)log2(ùëÉ(ùë•ùëñ)),H(X)=‚àí‚àëi=1NP(xi)log2(P(xi)),(1)whereùêª(ùëã)H(X)represents the Shannon entropy of the random variableX, which measures the uncertainty or randomness in the 32-byte sequence,ùëÉ(ùë•ùëñ)P(xi)is the probability of observing the specificùëñùë°‚Ñéithoutcome or byte valueùë•ùëñxi,Ndenotes the total number of unique outcomes for the random variableX(for a single byte,ùëÅ=256N=256),ùë•ùëñxirefers to a specific byte value in the range{0,1,2,‚Ä¶,255}{0,1,2,‚Ä¶,255}, andlog2log2is the logarithm to base two commonly used in entropy calculations. Given the varied types of malware introduced by the MalVis dataset, we have explored techniques to improve the recognition of these variations. Our analysis focuses on extending our earlier approach [26] from two color channels (red and blue) to three channels by encoding an additional feature into the green channel of RGB images using two primary encoding¬†methods: Classbyte Encoding:We adopt the Classbyte encoder introduced by Duc-Ly¬†et¬†al. [21], as¬†shown byFigure 3‚ûÖ, which maps semantic features of bytecode to varying intensities of the green channel. We selected this method due to its effectiveness and comparable performance to our previously employed entropy-based encoding for binary classification tasks.N-gram Encoding: We incorporate N-gram representations, as¬†illustrated byFigure 3‚ûÖ, derived from byte sequences to capture the malware bytecode‚Äôs underlying structural patterns and contextual dependencies. This technique, commonly used in malware detection research [62,63], enriches the green channel with statistical features that reflect code regularities and anomalies, thereby enhancing the capability to distinguish between different types of malware. The following subsections discuss the implications of these methods for advancing malware¬†visualization. 3.2.1. Approach¬†ClassbyteThis approach uses the Classbyte representation, which performs similarly to the entropy encoder in binary classification. It translates the features identified by the four Classbyte colors into four distinct shades of green in the green channel, as¬†illustrated inFigure 5‚ûÄ. The¬†method highlights sections of bytecode containing both clear-text printable and non-printable ASCII characters and null byte areas, as¬†illustrated inFigure 5‚ûÅ. These distinctions assist in analyzing the bytecode to determine whether it has been encrypted or injected with null bytes to evade malware detection. The previously generated red and blue channels are combined with the newly constructed green channel, resulting in MalVis (Classbyte encoded) RGB images, as¬†shown inFigure 5‚ûÇ. Unfortunately, this approach did not yield the desired improvement in the accuracy of multiclass classification. Further results of the analysis and evaluation of this approach are presented inSection 5.Figure 5.Overview of constructing the MalVis (Classbyte) visualization method, resulting in RGB image representations using the Classbyte encoding in the green channel and encoding entropy in the red and blue channels. The circled numbers (1‚Äì3) represent the pipeline steps described in the text. 3.2.2. Approach¬†N-GramThis approach utilizes the N-gram method, which has been extensively studied for malware anomaly detection. The¬†approach is particularly relevant for Android applications, which are often written in Java and Kotlin, thus inheriting the programmatic structure. Abnormalities are detected when the byte sequences differ from the typical bytecode structure using the green channel depicted inFigure 6‚ûÅ. One of the key goals of this approach is to bridge the gap between raw visualization and interpretability. Unlike prior methods that map byte values to color values without semantic linkage, our framework encodes interpretable attributes: entropy highlights encrypted or compressed code regions. At¬†the same time, N-gram transitions emphasize structural irregularities in bytecode. This mapping allows security analysts and researchers to visually associate distinct color patterns with specific malware behaviors, such as repacking functions or¬†obfuscation.Figure 6.Overview of the MalVis (N-gram) visualization pipeline, illustrating how Shannon entropy populates the red and blue channels while bi-gram (N-gram) values fill the green channel to emphasize structural and contextual bytecode patterns. The numbered circles (1‚Äì3) correspond to the pipeline steps described in the text.Figure 7,Figure 8andFigure 9further clarify this concept. For¬†example, the¬†script inFigure 7below depicts a simple Java code for a ‚Äòfor‚Äô loop before it is compiled into bytecode. The¬†bytecode often reveals specific patterns that represent the underlying syntax and structure of the program. The¬†keyword ‚Äòfor‚Äô indicates the presence of a loop followed by an initialization statement, a¬†condition, and¬†an increment surrounded by braces(‚ãØ)(‚ãØ), while curly braces{‚ãØ}{‚ãØ}denote the loop‚Äôs¬†body.Figure 7.An example of a simple for-loop written in Java.Figure 8.Translation of a Java for-loop into its equivalent JVM instructions in bytecode form after compilation.Figure 9.Representation of the employed Bi-gram approach on the Java instructions capturing the semantic transition of these instructions.Running ‚Äújavac For_Loop.java‚Äù compiles the code into a bytecode file named ‚ÄúFor_Loop.class‚Äù, which the DVM uses to execute the program, as¬†demonstrated inFigure 8.We implemented the Bi-gram method on the raw bytes using a two-byte window to recognize anomalies in the bytecode‚Äôs operational structure. Using Bi-gram on the bytecode represents the transition between instructions as listed inFigure 9.The Bi-gram method identifies obfuscated code by detecting irregular two-byte patterns within the byte sequence. While larger N-gram windows can capture more complex structural dependencies, the¬†associated feature space increases exponentially with window size. For¬†instance, the¬†use of 2-byte (Bi-gram) configuration yields2562=65,5362562=65,536possible patterns, a¬†3-byte (Tri-gram)2563‚âà16.72563‚âà16.7million, and¬†a 4-byte configuration2564‚âà4.32564‚âà4.3billion. Although¬†higher-order N-grams may reveal more intricate obfuscation techniques, they introduce substantial sparsity and computational overhead. Training several CNN models on this visualization method with the Bi-gram encoding features required two weeks of computation. Ultimately, the¬†two-byte configuration was chosen as an optimal balance between representational depth and computational efficiency. Future research could investigate the effects of utilizing larger N-gram¬†windows.The Bi-gram formula isBi-gramvalue=ùëè1√ó28+ùëè2,Bi-gramvalue=b1√ó28+b2,(2)which takes two consecutive bytes,ùëè1b1andùëè2b2, to¬†compute the Bi-gram value. The¬†multiplication of the first byteùëè1b1by28=25628=256shifts it to higher-order in the combined value, which is then added to theùëè2b2value, as¬†shown in line 16 of the Algorithm¬†1. The¬†resulting Bi-gram value represents the degree level of a green pixel and is normalized to the range[0,1][0,1]by dividing by the maximum possible value of(256√ó256)‚àí1=65,535(256√ó256)‚àí1=65,535as described¬†byùëî=Bi-gramvalue(256√ó256)‚àí1=ùëè1√ó256+ùëè265,535.g=Bi-gramvalue(256√ó256)‚àí1=b1√ó256+b265,535.(3)Finally, if¬†the byte is the last in the file, it is reset to 0, as¬†detailed in lines 18 and 19 of the Algorithm¬†1. Hence, MalVis presents a conceptually innovative visualization design that maps meaningful malware properties to distinct visual domains. Consequently, MalVis is more effective and better aligned with the objectives of explainable malware classification. This approach has demonstrated improved accuracy in the context of multiclass MalVis, and¬†both representation techniques are evaluated inSection 5.",
            "3.2.1. Approach¬†Classbyte": "This approach uses the Classbyte representation, which performs similarly to the entropy encoder in binary classification. It translates the features identified by the four Classbyte colors into four distinct shades of green in the green channel, as¬†illustrated inFigure 5‚ûÄ. The¬†method highlights sections of bytecode containing both clear-text printable and non-printable ASCII characters and null byte areas, as¬†illustrated inFigure 5‚ûÅ. These distinctions assist in analyzing the bytecode to determine whether it has been encrypted or injected with null bytes to evade malware detection. The previously generated red and blue channels are combined with the newly constructed green channel, resulting in MalVis (Classbyte encoded) RGB images, as¬†shown inFigure 5‚ûÇ. Unfortunately, this approach did not yield the desired improvement in the accuracy of multiclass classification. Further results of the analysis and evaluation of this approach are presented inSection 5. Figure 5.Overview of constructing the MalVis (Classbyte) visualization method, resulting in RGB image representations using the Classbyte encoding in the green channel and encoding entropy in the red and blue channels. The circled numbers (1‚Äì3) represent the pipeline steps described in the text.",
            "3.2.2. Approach¬†N-Gram": "This approach utilizes the N-gram method, which has been extensively studied for malware anomaly detection. The¬†approach is particularly relevant for Android applications, which are often written in Java and Kotlin, thus inheriting the programmatic structure. Abnormalities are detected when the byte sequences differ from the typical bytecode structure using the green channel depicted inFigure 6‚ûÅ. One of the key goals of this approach is to bridge the gap between raw visualization and interpretability. Unlike prior methods that map byte values to color values without semantic linkage, our framework encodes interpretable attributes: entropy highlights encrypted or compressed code regions. At¬†the same time, N-gram transitions emphasize structural irregularities in bytecode. This mapping allows security analysts and researchers to visually associate distinct color patterns with specific malware behaviors, such as repacking functions or¬†obfuscation. Figure 6.Overview of the MalVis (N-gram) visualization pipeline, illustrating how Shannon entropy populates the red and blue channels while bi-gram (N-gram) values fill the green channel to emphasize structural and contextual bytecode patterns. The numbered circles (1‚Äì3) correspond to the pipeline steps described in the text. Figure 7,Figure 8andFigure 9further clarify this concept. For¬†example, the¬†script inFigure 7below depicts a simple Java code for a ‚Äòfor‚Äô loop before it is compiled into bytecode. The¬†bytecode often reveals specific patterns that represent the underlying syntax and structure of the program. The¬†keyword ‚Äòfor‚Äô indicates the presence of a loop followed by an initialization statement, a¬†condition, and¬†an increment surrounded by braces(‚ãØ)(‚ãØ), while curly braces{‚ãØ}{‚ãØ}denote the loop‚Äôs¬†body. Figure 7.An example of a simple for-loop written in Java. Figure 8.Translation of a Java for-loop into its equivalent JVM instructions in bytecode form after compilation. Figure 9.Representation of the employed Bi-gram approach on the Java instructions capturing the semantic transition of these instructions. Running ‚Äújavac For_Loop.java‚Äù compiles the code into a bytecode file named ‚ÄúFor_Loop.class‚Äù, which the DVM uses to execute the program, as¬†demonstrated inFigure 8. We implemented the Bi-gram method on the raw bytes using a two-byte window to recognize anomalies in the bytecode‚Äôs operational structure. Using Bi-gram on the bytecode represents the transition between instructions as listed inFigure 9. The Bi-gram method identifies obfuscated code by detecting irregular two-byte patterns within the byte sequence. While larger N-gram windows can capture more complex structural dependencies, the¬†associated feature space increases exponentially with window size. For¬†instance, the¬†use of 2-byte (Bi-gram) configuration yields2562=65,5362562=65,536possible patterns, a¬†3-byte (Tri-gram)2563‚âà16.72563‚âà16.7million, and¬†a 4-byte configuration2564‚âà4.32564‚âà4.3billion. Although¬†higher-order N-grams may reveal more intricate obfuscation techniques, they introduce substantial sparsity and computational overhead. Training several CNN models on this visualization method with the Bi-gram encoding features required two weeks of computation. Ultimately, the¬†two-byte configuration was chosen as an optimal balance between representational depth and computational efficiency. Future research could investigate the effects of utilizing larger N-gram¬†windows. The Bi-gram formula isBi-gramvalue=ùëè1√ó28+ùëè2,Bi-gramvalue=b1√ó28+b2,(2)which takes two consecutive bytes,ùëè1b1andùëè2b2, to¬†compute the Bi-gram value. The¬†multiplication of the first byteùëè1b1by28=25628=256shifts it to higher-order in the combined value, which is then added to theùëè2b2value, as¬†shown in line 16 of the Algorithm¬†1. The¬†resulting Bi-gram value represents the degree level of a green pixel and is normalized to the range[0,1][0,1]by dividing by the maximum possible value of(256√ó256)‚àí1=65,535(256√ó256)‚àí1=65,535as described¬†byùëî=Bi-gramvalue(256√ó256)‚àí1=ùëè1√ó256+ùëè265,535.g=Bi-gramvalue(256√ó256)‚àí1=b1√ó256+b265,535.(3) Finally, if¬†the byte is the last in the file, it is reset to 0, as¬†detailed in lines 18 and 19 of the Algorithm¬†1. Hence, MalVis presents a conceptually innovative visualization design that maps meaningful malware properties to distinct visual domains. Consequently, MalVis is more effective and better aligned with the objectives of explainable malware classification. This approach has demonstrated improved accuracy in the context of multiclass MalVis, and¬†both representation techniques are evaluated inSection 5.",
            "3.3. The Impact of Entropy and N-Gram on MalVis Representations¬†Experiments": "In this section, we further investigate the sensitivity and interpretability of MalVis visualizations. We conducted a controlled analysis by applying targeted transformations to a benign Android application, specifically WhatsApp Classes.dex. The¬†goal was to investigate and quantify the impact of encryption and unstructured operations in bytecode changes on the RGB image representations generated by our¬†framework. Algorithm 1MalVis Visualization Algorithm: generate RGB from bytecode using entropy and N-gram1:Input:Data arrayùëëùëéùë°ùëédataof bytecode, symbol mapùë†ùë¶ùëöùëèùëúùëô_ùëöùëéùëùsymbol_map, indexx2:Output:RGB values in the range [0, 255]3:ùëí‚ÜêEntropy(ùëëùëéùë°ùëé,32,ùë•,len(ùë†ùë¶ùëöùëèùëúùëô_ùëöùëéùëù))e‚ÜêEntropy(data,32,x,len(symbol_map))‚ñπCalculate entropy using a window size of 32 bytes4:functioncurve(v)5:ùëì‚Üê(4ùë£‚àí4ùë£2)4f‚Üê(4v‚àí4v2)46:ùëì‚Üêmax(ùëì,0)f‚Üêmax(f,0)7:returnf8:end function9:ifùëí>0.5e>0.5then10.ùëü‚Üêcurve(ùëí‚àí0.5)r‚Üêcurve(e‚àí0.5)‚ñπRed component is determined by the scaled entropy value11:else12:ùëü‚Üê0r‚Üê0‚ñπIf entropy is less than or equal to 0.5, set red component to 013:end if14:ùëè‚Üêùëí2b‚Üêe2‚ñπBlue component is proportional to the square of entropy15:ifùë•<len(ùëëùëéùë°ùëé)‚àí1x<len(data)‚àí1then16:ùëõ_ùëîùëüùëéùëö_ùë£ùëéùëôùë¢ùëí‚Üê(ùëëùëéùë°ùëé[ùë•]‚â™8)+ùëëùëéùë°ùëé[ùë•+1]n_gram_value‚Üê(data[x]‚â™8)+data[x+1]‚ñπCompute 2-byte n-gram value17:ùëî‚Üêùëõ_ùëîùëüùëéùëö_ùë£ùëéùëôùë¢ùëí65,535g‚Üên_gram_value65,535‚ñπNormalize n-gram value to [0, 1] for green component18:else19:ùëî‚Üê0g‚Üê0‚ñπIf at the last byte, the green component is set to 020:end if21:return[int(255¬∑ùëü),int(255¬∑ùëî),int(255¬∑ùëè)][int(255¬∑r),int(255¬∑g),int(255¬∑b)]‚ñπReturn RGB values scaled to the range [0, 255] 3.3.1. Obfuscation Detection Captured by Entropy in Red and Blue¬†ChannelsIn this experiment, we applied AES-256 encryption in Electronic Codebook (ECB) mode to the initial 30% of the Classes.dex bytecode. This encryption caused a noticeable shift in entropy, particularly affecting the red and blue channels of image representations. Entropy, which quantifies randomness over 32-byte windows, increased significantly in high-entropy areas, leading to brighter pixel intensities. This effect simulates obfuscation techniques that malware creators use to evade detection. As¬†a result, the¬†red and blue channels inFigure 10display brighter pixels in the top-left region, highlighting the encrypted sections in the¬†representation.Figure 10.The impact of 30% AES-256 encryption on Classes.dex file captured by the entropy encoder in the red and blue channels of MalVis representations. 3.3.2. Unstructured Bytecode Insertion Captured by N-Gram in Green¬†ChannelIn this experiment, we examined the structural sensitivity of the green channel by injecting random, unstructured operations into the initial 30% of the Classes.dex file. This action disrupted the byte sequence, causing noticeable distortions in the N-gram values, significantly impacting the green channel. MalVis, which utilizes bi-gram formulas to detect abnormal operational patterns, recorded these disturbances as increased bi-gram values, resulting in brighter pixel values within green-channel textures, as¬†depicted inFigure 11. These deviations were apparent when visualized next to an unchanged sample, highlighting the effectiveness of the green channel in detecting structural¬†anomalies.Figure 11.The impact of injecting 30% randomized unstructured operations to Classes.dex file captured by the N-gram encoder in the green channels of MalVis representations.",
            "3.3.1. Obfuscation Detection Captured by Entropy in Red and Blue¬†Channels": "In this experiment, we applied AES-256 encryption in Electronic Codebook (ECB) mode to the initial 30% of the Classes.dex bytecode. This encryption caused a noticeable shift in entropy, particularly affecting the red and blue channels of image representations. Entropy, which quantifies randomness over 32-byte windows, increased significantly in high-entropy areas, leading to brighter pixel intensities. This effect simulates obfuscation techniques that malware creators use to evade detection. As¬†a result, the¬†red and blue channels inFigure 10display brighter pixels in the top-left region, highlighting the encrypted sections in the¬†representation. Figure 10.The impact of 30% AES-256 encryption on Classes.dex file captured by the entropy encoder in the red and blue channels of MalVis representations.",
            "3.3.2. Unstructured Bytecode Insertion Captured by N-Gram in Green¬†Channel": "In this experiment, we examined the structural sensitivity of the green channel by injecting random, unstructured operations into the initial 30% of the Classes.dex file. This action disrupted the byte sequence, causing noticeable distortions in the N-gram values, significantly impacting the green channel. MalVis, which utilizes bi-gram formulas to detect abnormal operational patterns, recorded these disturbances as increased bi-gram values, resulting in brighter pixel values within green-channel textures, as¬†depicted inFigure 11. These deviations were apparent when visualized next to an unchanged sample, highlighting the effectiveness of the green channel in detecting structural¬†anomalies. Figure 11.The impact of injecting 30% randomized unstructured operations to Classes.dex file captured by the N-gram encoder in the green channels of MalVis representations.",
            "3.4. Model Architecture and Experiment¬†Settings": "To assess the effectiveness of our proposed approach, we utilized a selection of well-recognized CNN models, as¬†shown byFigure 3‚ûÜ, including MobileNetV2, ResNet-50, DenseNet-201, VGG-16, and¬†Inception-V3. These models were applied to our generated visualizations and baseline comparison methods. The¬†employed CNN models have proven highly effective in malware detection because they capture intricate patterns and features within image data [48,64]. To¬†ensure consistency across the CNN models, all images were resized to224√ó224224√ó224pixels using nearest neighbor interpolation to align with the input dimensions required by the models. The¬†dataset was partitioned into 80% for training, 10% for validation, and¬†10% for testing. The¬†batch size 64 was chosen based on empirical experimentation, as¬†it provided an optimal trade-off between training speed and memory consumption on our GPU setup. The¬†training was conducted over 50 epochs and carefully monitored to mitigate overfitting. This setup enabled consistent and accurate assessments of the models‚Äô performance across various visualization¬†techniques.",
            "3.5. Environment¬†Setup": "MalVis visualization and model training were generated using an Ubuntu Server 22.04 LTS OS with x86 64 architecture. The¬†hardware setup consisted of a 16-core AMD Ryzen Threadripper PRO 5955WX processor, 128 GB of DDR4 RAM at 3200 MHz, and¬†an NVIDIA RTX A6000 graphics card. The¬†system was configured within a controlled environment to ensure accurate results and minimize external¬†influences.",
            "4. Performance¬†Measures": "To ensure fairness when comparing the visualization methods and evaluating our proposed approaches alongside the baseline methods presented inTable 2, we employed accuracy, precision, recall, ROC-AUC, and¬†MCC as validation metrics in the binary classification context. Similarly, the¬†same metrics were employed for consistent evaluation in multiclass classification, as¬†demonstrated inTable 3. The¬†accuracy¬†(4) indicates the percentage of instances correctly identified among the entire set of samples. The¬†F1-score¬†(5) provides a harmonic mean of the model‚Äôs precision and recall, accounting for false positives and false negatives. Precision¬†(6) refers to the proportion of true positives in relation to all positive predictions made. Recall¬†(7) denotes the fraction of actual positives correctly identified by the model. ROC-AUC measures the area under the receiver operating characteristic curve, highlighting the balance between sensitivity and specificity. The¬†MCC¬†(8) serves as a metric to assess classification performance, factoring in true and false positives and true and false negatives. Accordingly,Accuracy=ùëáùëÉ+ùëáùëÅùëáùëÉ+ùëáùëÅ+ùêπùëÉ+ùêπùëÅ,Accuracy=TP+TNTP+TN+FP+FN,(4)F1-score=2√óùëÉ√óùëÖùëÉ+ùëÖ,F1-score=2√óP√óRP+R,(5)Precision=ùëáùëÉùëáùëÉ+ùêπùëÉ,Precision=TPTP+FP,(6)Recall=ùëáùëÉùëáùëÉ+ùêπùëÅ,Recall=TPTP+FN,(7)andMCC=ùëáùëÉ√óùëáùëÅ‚àíùêπùëÉ√óùêπùëÅ(ùëáùëÉ+ùêπùëÉ)(ùëáùëÉ+ùêπùëÅ)(ùëáùëÅ+ùêπùëÉ)(ùëáùëÅ+ùêπùëÅ)‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö,MCC=TP√óTN‚àíFP√óFN(TP+FP)(TP+FN)(TN+FP)(TN+FN),(8)where TP, TN, FP, and¬†FN represent true positive, true negative, false positive, and¬†false negative, respectively. Table 2.Comparison of visualization approaches using different CNN models. Abbreviations in the table include MNv2 (MobileNet-V2), DN201 (DenseNet201), RN50 (ResNet50), and¬†INC-V3 (Inception-V3). Bold values highlight the highest score for each metric within the respective¬†model. Table 3.Performance results of different models on the MalVis imbalanced¬†dataset.",
            "5. Results": "This section provides a comparative analysis of the performance of the newly introduced visualizations encoded using Classbyte and N-gram, compared to baseline methods, namely Entropy-based, MalNet [48], and¬†Classbyte, as¬†detailed in the following¬†subsections. 5.1. Evaluation of MalVis (Classbyte Encoded) and MalVis Performance Compared to Other Methods on the Binary Classification DatasetAll methods used the same settings and were trained on identical subsets of training data to ensure a fair comparison. As¬†shown inTable 2, the¬†MalVis (Classbyte-encoded) approach, which combines Classbyte and Entropy representations, did not enhance classification performance as expected. Instead, it disrupted the entropy encoder‚Äôs ability to capture meaningful patterns, as¬†illustrated inFigure 12. Encoding the four color features from the Classbyte method into a single green channel effectively overwrote the structures previously identified by the entropy¬†encoder.Figure 12.An illustration of the disruption caused by MalVis (Classbyte encoded) in the green channel, which impacts patterns captured by the entropy encoder compared to the representations by MalVis (N-gram encoded).In contrast, the¬†proposed MalVis method, which integrates entropy and N-gram encoders, consistently achieved superior or comparable performance to other methods across most CNN architectures, with¬†only minor exceptions. For¬†instance, while DenseNet201 did not exhibit significant improvements across all metrics, it only demonstrated higher precision, indicating the model‚Äôs strong capability in correctly identifying true positives while minimizing false positives. The¬†observed shortcoming in the remaining metrics can be attributed to the highly imbalanced dataset described inSection 3.1, where adware and trojan samples dominate other classes. This imbalance motivated our efforts to mitigate class disparity in the multiclass dataset evaluation, as¬†discussed inSection 5.3. One limitation of the study is the use of a single train‚Äìtest split without implementing k-fold cross-validation. To¬†enhance the robustness and validity of the results, future research could explore the application of k-fold cross-validation. This approach would enable a more comprehensive evaluation of the model‚Äôs performance by utilizing multiple subsets of the data for training and testing, potentially yielding more reliable and generalizable¬†findings.Furthermore, we emphasize the importance of visualization techniques that deliver consistently higher detection performance across diverse CNN models.Section 5.4focuses on enhancing the MalVis framework through ensemble-based strategies to further improve its robustness and classification¬†accuracy.These experiments demonstrate that existing methods, including Classbyte and MalNet, provide limited semantic and structural variation, resulting in suboptimal performance for malware classification tasks. In¬†contrast, MalVis outperforms these approaches by integrating both entropy and N-gram patterns, producing meaningful visual representations that more effectively expose obfuscation, encryption, and¬†other malicious behaviors. Notably, our earlier method, which relied solely on entropy [26], did not achieve comparable performance, underscoring the value of combining multiple feature types. This highlights the need for enhanced visualization techniques that improve both interpretability and classification accuracy. Accordingly, MalVis was selected for the subsequent advanced multiclass classification experiments to better distinguish between diverse malware¬†types. 5.2. Evaluation of MalVis Performance on Imbalanced Multiclass¬†DatasetThe evaluation of the MalVis representation in the imbalanced multiclass malware classification task, presented inTable 3, demonstrated that the ResNet50 model achieved the highest performance. It achieved an overall accuracy of 94.03%, F1-score of 83.54%, and¬†Precision of 83.34%, surpassing the performance of state-of-the-art multiclass malware classification approaches [48]. The¬†analysis of the confusion matrix, presented inFigure 13,ùê¥Atoùê∏Ereveals significant challenges in differentiating between the majority and minority classes within the imbalanced multiclass dataset. The¬†darker column for the adware class suggests a bias due to its higher frequency in the training set, as¬†shown inFigure 4. A deeper inspection of the confusion matrix inFigure 13reveals frequent misclassification between Adware, Trojan, and¬†Spyware classes. These malware types often share similar bytecode structures and employ comparable obfuscation techniques, resulting in visually overlapping patterns in the entropy and N-gram channels. For¬†example, packed adware and spyware samples may exhibit high-entropy values with irregular n-gram sequences, which confound the classifier. These findings highlight the need for refined feature selection and possibly more semantic augmentation in future visualization efforts. This highlights the effect of class imbalance, leading to biased decision boundaries that favor the majority class at the expense of consistent performance across all classes. Various strategies can address this imbalance, such as oversampling minority classes, undersampling majority classes, applying class weighting in the loss function, and¬†using ensemble methods [65,66]. The¬†following sections cover the application of undersampling to majority classes and provide a detailed evaluation of eight different ensemble¬†methods.Figure 13.Confusion matrices of CNN models trained on the imbalanced multiclass MalVis (N-gram encoded). 5.3. Evaluation of MalVis Performance Using¬†UndersamplingWe applied undersampling to the majority classes to address the imbalanced class distribution, resulting in a more balanced dataset. Although¬†other approaches, such as class weight adjustment and the Synthetic Minority Over-sampling Technique (SMOTE), have been proposed to tackle class imbalance [67], these methods are beyond the scope of this paper. We employed undersampling due to limited computational resources and the time constraints associated with training the oversampled method.Table 4presents the evaluation results for undersampling with MalVis. The¬†confusion matrix inFigure 14, modelsùêµBtoùêπF, highlights improved differentiation between majority and minority classes. Despite a 15‚Äì20% reduction in overall performance relative to the results of the imbalanced dataset (Table 3), we discuss ensemble methods to boost model performance in the following¬†section.Figure 14.Confusion matrices for CNN models trained on a balanced multiclass MalVis dataset. The blue dashed box highlights the optimal ensemble method (Min Confidence Voting).Table 4.Performance results after undersampling approach on MalVis (N-gram encoded) dataset. 5.4. Evaluation of MalVis Performance Using Ensemble¬†ModelsTo address the performance impact caused by the undersampling approach, we explored the application of various ensemble methods. The¬†aim was to leverage the combined strengths of all CNN models, thereby enhancing both the models‚Äô performance and robustness. The¬†ensemble methods implemented and evaluated¬†include:Average Voting: Combines predictions by averaging the probabilities of all CNN¬†models.Majority Voting: Determines the final class by selecting the most predicted by individual models.Weighted Voting: Assigns different weights to CNN models based on their prediction accuracy. We preserve the ranking performance of the models and assign weights corresponding to their ranking positions.Min Confidence Voting: Only consider a model‚Äôs prediction when it meets the minimum required confidence level. In¬†our implementation, a¬†confidence threshold of 60% was selected.Soft Voting: Uses the predicted class probabilities to decide the final output.Median Voting: Determines decisions by selecting the median of predicted class probabilities.Rank-Based Voting: Ranks predictions from models and aggregates ranks to select a¬†class.Stacking Ensemble: Trains a new model to integrate the predictions of the base model and improve performance.InTable 5, the¬†Min Confidence Voting ensemble achieved the highest performance across all evaluation metrics except for ROC-AUC. These results indicate superior performance compared to those in the unbalanced dataset shown inTable 3. The¬†confusion matrix inFigure 14in boxùê¥Aillustrates that the Min Confidence Voting ensemble demonstrated enhanced performance by producing a more pronounced diagonal shape. This indicates an improved ability to accurately detect the more challenging classes compared to the CNN models shown in boxesùêµBtoùêπFafter undersampling. Moreover, the¬†Stacking ensemble achieved the highest ROC-AUC metric, attributable to its ability to integrate predictions from multiple models, thereby leveraging their strengths to improve overall performance in distinguishing different¬†classes.Table 5.Performance results of different ensemble methods on the MalVis multiclass dataset after undersampling¬†evaluation.These findings underline the effectiveness of ensemble methods, particularly Min Confidence Voting and Stacking, in¬†handling multiclass classification challenges on the MalVis¬†dataset. 5.5. Current Limitations and Future¬†WorkWhile our results demonstrate the effectiveness of the MalVis framework, several methodological limitations should be noted:Statistical Validation: Experiments were conducted using a single train‚Äìtest split without k-fold cross-validation or statistical significance testing. The¬†primary focus of this research work was to develop a bytecode visualization framework that provides competitive performance and explainable visual patterns for malware¬†analysis.Computational Constraints: Performing full cross-validation and repeated training on the 1.3 M sample multiclass dataset was computationally expensive, limiting the scope of statistical validation.Future work will include extensive cross-validation and statistical significance testing to further validate the robustness and generalizability of the proposed¬†approach.",
            "5.1. Evaluation of MalVis (Classbyte Encoded) and MalVis Performance Compared to Other Methods on the Binary Classification Dataset": "All methods used the same settings and were trained on identical subsets of training data to ensure a fair comparison. As¬†shown inTable 2, the¬†MalVis (Classbyte-encoded) approach, which combines Classbyte and Entropy representations, did not enhance classification performance as expected. Instead, it disrupted the entropy encoder‚Äôs ability to capture meaningful patterns, as¬†illustrated inFigure 12. Encoding the four color features from the Classbyte method into a single green channel effectively overwrote the structures previously identified by the entropy¬†encoder. Figure 12.An illustration of the disruption caused by MalVis (Classbyte encoded) in the green channel, which impacts patterns captured by the entropy encoder compared to the representations by MalVis (N-gram encoded). In contrast, the¬†proposed MalVis method, which integrates entropy and N-gram encoders, consistently achieved superior or comparable performance to other methods across most CNN architectures, with¬†only minor exceptions. For¬†instance, while DenseNet201 did not exhibit significant improvements across all metrics, it only demonstrated higher precision, indicating the model‚Äôs strong capability in correctly identifying true positives while minimizing false positives. The¬†observed shortcoming in the remaining metrics can be attributed to the highly imbalanced dataset described inSection 3.1, where adware and trojan samples dominate other classes. This imbalance motivated our efforts to mitigate class disparity in the multiclass dataset evaluation, as¬†discussed inSection 5.3. One limitation of the study is the use of a single train‚Äìtest split without implementing k-fold cross-validation. To¬†enhance the robustness and validity of the results, future research could explore the application of k-fold cross-validation. This approach would enable a more comprehensive evaluation of the model‚Äôs performance by utilizing multiple subsets of the data for training and testing, potentially yielding more reliable and generalizable¬†findings. Furthermore, we emphasize the importance of visualization techniques that deliver consistently higher detection performance across diverse CNN models.Section 5.4focuses on enhancing the MalVis framework through ensemble-based strategies to further improve its robustness and classification¬†accuracy. These experiments demonstrate that existing methods, including Classbyte and MalNet, provide limited semantic and structural variation, resulting in suboptimal performance for malware classification tasks. In¬†contrast, MalVis outperforms these approaches by integrating both entropy and N-gram patterns, producing meaningful visual representations that more effectively expose obfuscation, encryption, and¬†other malicious behaviors. Notably, our earlier method, which relied solely on entropy [26], did not achieve comparable performance, underscoring the value of combining multiple feature types. This highlights the need for enhanced visualization techniques that improve both interpretability and classification accuracy. Accordingly, MalVis was selected for the subsequent advanced multiclass classification experiments to better distinguish between diverse malware¬†types.",
            "5.2. Evaluation of MalVis Performance on Imbalanced Multiclass¬†Dataset": "The evaluation of the MalVis representation in the imbalanced multiclass malware classification task, presented inTable 3, demonstrated that the ResNet50 model achieved the highest performance. It achieved an overall accuracy of 94.03%, F1-score of 83.54%, and¬†Precision of 83.34%, surpassing the performance of state-of-the-art multiclass malware classification approaches [48]. The¬†analysis of the confusion matrix, presented inFigure 13,ùê¥Atoùê∏Ereveals significant challenges in differentiating between the majority and minority classes within the imbalanced multiclass dataset. The¬†darker column for the adware class suggests a bias due to its higher frequency in the training set, as¬†shown inFigure 4. A deeper inspection of the confusion matrix inFigure 13reveals frequent misclassification between Adware, Trojan, and¬†Spyware classes. These malware types often share similar bytecode structures and employ comparable obfuscation techniques, resulting in visually overlapping patterns in the entropy and N-gram channels. For¬†example, packed adware and spyware samples may exhibit high-entropy values with irregular n-gram sequences, which confound the classifier. These findings highlight the need for refined feature selection and possibly more semantic augmentation in future visualization efforts. This highlights the effect of class imbalance, leading to biased decision boundaries that favor the majority class at the expense of consistent performance across all classes. Various strategies can address this imbalance, such as oversampling minority classes, undersampling majority classes, applying class weighting in the loss function, and¬†using ensemble methods [65,66]. The¬†following sections cover the application of undersampling to majority classes and provide a detailed evaluation of eight different ensemble¬†methods. Figure 13.Confusion matrices of CNN models trained on the imbalanced multiclass MalVis (N-gram encoded).",
            "5.3. Evaluation of MalVis Performance Using¬†Undersampling": "We applied undersampling to the majority classes to address the imbalanced class distribution, resulting in a more balanced dataset. Although¬†other approaches, such as class weight adjustment and the Synthetic Minority Over-sampling Technique (SMOTE), have been proposed to tackle class imbalance [67], these methods are beyond the scope of this paper. We employed undersampling due to limited computational resources and the time constraints associated with training the oversampled method.Table 4presents the evaluation results for undersampling with MalVis. The¬†confusion matrix inFigure 14, modelsùêµBtoùêπF, highlights improved differentiation between majority and minority classes. Despite a 15‚Äì20% reduction in overall performance relative to the results of the imbalanced dataset (Table 3), we discuss ensemble methods to boost model performance in the following¬†section. Figure 14.Confusion matrices for CNN models trained on a balanced multiclass MalVis dataset. The blue dashed box highlights the optimal ensemble method (Min Confidence Voting). Table 4.Performance results after undersampling approach on MalVis (N-gram encoded) dataset.",
            "5.4. Evaluation of MalVis Performance Using Ensemble¬†Models": "To address the performance impact caused by the undersampling approach, we explored the application of various ensemble methods. The¬†aim was to leverage the combined strengths of all CNN models, thereby enhancing both the models‚Äô performance and robustness. The¬†ensemble methods implemented and evaluated¬†include: Average Voting: Combines predictions by averaging the probabilities of all CNN¬†models.Majority Voting: Determines the final class by selecting the most predicted by individual models.Weighted Voting: Assigns different weights to CNN models based on their prediction accuracy. We preserve the ranking performance of the models and assign weights corresponding to their ranking positions.Min Confidence Voting: Only consider a model‚Äôs prediction when it meets the minimum required confidence level. In¬†our implementation, a¬†confidence threshold of 60% was selected.Soft Voting: Uses the predicted class probabilities to decide the final output.Median Voting: Determines decisions by selecting the median of predicted class probabilities.Rank-Based Voting: Ranks predictions from models and aggregates ranks to select a¬†class.Stacking Ensemble: Trains a new model to integrate the predictions of the base model and improve performance. InTable 5, the¬†Min Confidence Voting ensemble achieved the highest performance across all evaluation metrics except for ROC-AUC. These results indicate superior performance compared to those in the unbalanced dataset shown inTable 3. The¬†confusion matrix inFigure 14in boxùê¥Aillustrates that the Min Confidence Voting ensemble demonstrated enhanced performance by producing a more pronounced diagonal shape. This indicates an improved ability to accurately detect the more challenging classes compared to the CNN models shown in boxesùêµBtoùêπFafter undersampling. Moreover, the¬†Stacking ensemble achieved the highest ROC-AUC metric, attributable to its ability to integrate predictions from multiple models, thereby leveraging their strengths to improve overall performance in distinguishing different¬†classes. Table 5.Performance results of different ensemble methods on the MalVis multiclass dataset after undersampling¬†evaluation. These findings underline the effectiveness of ensemble methods, particularly Min Confidence Voting and Stacking, in¬†handling multiclass classification challenges on the MalVis¬†dataset.",
            "5.5. Current Limitations and Future¬†Work": "While our results demonstrate the effectiveness of the MalVis framework, several methodological limitations should be noted: Statistical Validation: Experiments were conducted using a single train‚Äìtest split without k-fold cross-validation or statistical significance testing. The¬†primary focus of this research work was to develop a bytecode visualization framework that provides competitive performance and explainable visual patterns for malware¬†analysis.Computational Constraints: Performing full cross-validation and repeated training on the 1.3 M sample multiclass dataset was computationally expensive, limiting the scope of statistical validation. Future work will include extensive cross-validation and statistical significance testing to further validate the robustness and generalizability of the proposed¬†approach.",
            "6. Explainability Through Grad-CAM and Grad-CAM++ Visualization": "InSection 5, the¬†CNN models using MalVis with undersampling demonstrated outstanding performance in classifying different types of malware. However, their decision-making process remains unclear. To¬†enhance model interpretability and validate our model‚Äôs decision, we employed Gradient-weighted Class Activation Mapping (Grad-CAM) and its enhanced variant, Grad-CAM++, to¬†visualize the regions in MalVis images that most significantly influence classification decisions. This analysis offers crucial insights into whether our models prioritize semantically meaningful features, particularly the proposed Entropy and N-gram components within the MalVis¬†visualizations. 6.1. Grad-CAMGrad-CAM [68] generates visual explanations by utilizing gradients of the target class flowing into the final convolutional layer. The¬†technique produces localization maps highlighting important regions for predicting specific malware classes. The Grad-CAM heatmap for classcis calculated through three key steps:Step 1: compute the gradients of the class scoreùë¶ùëêycwith respect to the feature mapsùê¥ùëòAk. Where theiandjrepresent the spatial coordinates (i.e., row and column positions) within the feature map:‚àÇùë¶ùëê‚àÇùê¥ùëòùëñ,ùëó‚àÇyc‚àÇAi,jk(9)Step 2: Calculate the importance weightsùõºùëêùëòŒ±kcfor the feature mapkfor classc, employ global average pooling. WhereZhere represents the total number of spatial positions andùê¥ùëòùëñ,ùëóAi,jkis the value at spatial positionùëñ,ùëói,jwithin the feature mapk:ùõºùëêùëò=1ùëç‚àëùëñ‚àëùëó‚àÇùë¶ùëê‚àÇùê¥ùëòùëñ,ùëóŒ±kc=1Z‚àëi‚àëj‚àÇyc‚àÇAi,jk(10)Step 3: Generate the final Grad-CAM heatmap to highlight the image regions most influential in the prediction for classc. Brighter regions signify higher influence. This is achieved through a weighted combination of feature maps. Here,krepresents the index of the convolutional filter/channel. The¬†application of the ReLU function (Rectified Linear Unit) ensures that only positive contributions are retained by eliminating negative values:ùêøùëêùê∫ùëüùëéùëë‚àíùê∂ùê¥ùëÄ=ReLU‚éõ‚éù‚éú‚éú‚àëùëòùõºùëêùëòùê¥ùëò‚éû‚é†‚éü‚éüLGrad‚àíCAMc=ReLU‚àëkŒ±kcAk(11) 6.2. Grad-CAM++Grad-CAM++ [69] addresses limitations of standard Grad-CAM by providing more precise localization through enhanced weight computation. It refines the importance weight calculation to better capture pixel-wise significance:ùõºùëêùëò=‚àëùëñ‚àëùëóùë§ùëò,ùëêùëñ,ùëó¬∑ReLU‚éõ‚éù‚éú‚éú‚éú‚àÇùë¶ùëê‚àÇùê¥ùëòùëñ,ùëó‚éû‚é†‚éü‚éü‚éüŒ±kc=‚àëi‚àëjwi,jk,c¬∑ReLU‚àÇyc‚àÇAi,jk(12)ùë§ùëò,ùëêùëñ,ùëó=‚àÇ2ùë¶ùëê‚àÇ(ùê¥ùëòùëñ,ùëó)22¬∑‚àÇ2ùë¶ùëê‚àÇ(ùê¥ùëòùëñ,ùëó)2+‚àëùëé,ùëèùê¥ùëòùëé,ùëè¬∑‚àÇ3ùë¶ùëê‚àÇ(ùê¥ùëòùëé,ùëè)3wi,jk,c=‚àÇ2yc‚àÇ(Ai,jk)22¬∑‚àÇ2yc‚àÇ(Ai,jk)2+‚àëa,bAa,bk¬∑‚àÇ3yc‚àÇ(Aa,bk)3(13)In Formula¬†(13), the¬†pixel-wise weightùë§ùëò,ùëêùëñ,ùëówi,jk,cat spatial position(ùëñ,ùëó)(i,j)in feature mapkfor classcis computed as a normalized ratio. The¬†numerator computes the second-order derivative‚àÇ2ùë¶ùëê‚àÇ(ùê¥ùëòùëñ,ùëó)2‚àÇ2yc‚àÇ(Ai,jk)2, which measures how sensitively the class prediction scoreùë¶ùëêycchanges with respect to the pixel activationùê¥ùëòùëñ,ùëóAi,jk. This captures the local curvature of the prediction function at that specific pixel location. While the denominator normalizes the numerator to ensure bounded weights. This ensures that pixels exhibiting both strong gradients and consistent influence across the entire feature map receive appropriately higher weights.We utilize cosine similarity to determine the ten most similar representations for each class, resulting in a total of 100 samples, with¬†ten samples per class. Following this, we employ the two attention map techniques, Grad-CAM and Grad-CAM++. The¬†outcomes, as¬†presented inFigure 15, demonstrate the application of these methods to a single MalVis image belonging to a specific class. The¬†following section provides a comprehensive analysis and identification of key observations derived from this¬†experiment.Figure 15.Grad-CAM and Grad-CAM++ visualization results for different malware types. Each row showcases a randomly selected sample with its ground truth type, displayed across three columns: original MalVis images (left), Grad-CAM overlays (middle), and Grad-CAM++ overlays (right). In the heatmap overlays, red and orange regions indicate areas of high model attention, while blue regions represent areas of low attention. The red and blue channels of the MalVis images capture high-entropy sections of a code, whereas the green channel highlights areas with abnormal coding structures. 6.3. Key Findings from Visualization¬†AnalysisThe application of both Grad-CAM and Grad-CAM++ visualization techniques, as¬†illustrated inFigure 15, provided critical insights into the model‚Äôs decision-making process. Both methods revealed that the CNN consistently focused on semantically meaningful regions within MalVis images, with¬†Grad-CAM++ demonstrating more refined attention patterns. Notably, the¬†model concentrated on areas identified by our entropy and N-gram encoders, confirming that the CNN relied on these introduced feature patterns rather than random noise or artifacts to accurately classify¬†samples.Distinct attention patterns were observed across different malware types, validating our approach and demonstrating the model‚Äôs discriminative capabilities. For¬†example, the¬†magnified section of the Spyware sample inFigure 15shows intense focus, revealing structural regions highlighted in dark red and orange. These correspond to highly obfuscated code sections captured by the entropy encoder (red and blue channels) or abnormal code structures identified by the N-gram approach (green channel). These malicious patterns reflect obfuscation techniques used by malware creators to evade detection. In¬†contrast, the¬†magnified section of the Downloader sample exhibits distributed attention across the whole section, reflecting lower entropy levels with an indication of less randomness in the bytecode. The¬†Benign sample, notably, shows light blue with less attention overall, focusing on lower-entropy regions containing regular coding¬†patterns.This differentiation confirms that the model effectively leverages entropy and N-gram information embedded within the MalVis representations to make classification decisions, focusing on relevant malware characteristics rather than spurious patterns. The¬†interpretability offered by these attention mechanisms provides confidence in the model‚Äôs trustworthiness and establishes the reliability of its decision-making¬†process.",
            "6.1. Grad-CAM": "Grad-CAM [68] generates visual explanations by utilizing gradients of the target class flowing into the final convolutional layer. The¬†technique produces localization maps highlighting important regions for predicting specific malware classes. The Grad-CAM heatmap for classcis calculated through three key steps: Step 1: compute the gradients of the class scoreùë¶ùëêycwith respect to the feature mapsùê¥ùëòAk. Where theiandjrepresent the spatial coordinates (i.e., row and column positions) within the feature map:‚àÇùë¶ùëê‚àÇùê¥ùëòùëñ,ùëó‚àÇyc‚àÇAi,jk(9) Step 2: Calculate the importance weightsùõºùëêùëòŒ±kcfor the feature mapkfor classc, employ global average pooling. WhereZhere represents the total number of spatial positions andùê¥ùëòùëñ,ùëóAi,jkis the value at spatial positionùëñ,ùëói,jwithin the feature mapk:ùõºùëêùëò=1ùëç‚àëùëñ‚àëùëó‚àÇùë¶ùëê‚àÇùê¥ùëòùëñ,ùëóŒ±kc=1Z‚àëi‚àëj‚àÇyc‚àÇAi,jk(10) Step 3: Generate the final Grad-CAM heatmap to highlight the image regions most influential in the prediction for classc. Brighter regions signify higher influence. This is achieved through a weighted combination of feature maps. Here,krepresents the index of the convolutional filter/channel. The¬†application of the ReLU function (Rectified Linear Unit) ensures that only positive contributions are retained by eliminating negative values:ùêøùëêùê∫ùëüùëéùëë‚àíùê∂ùê¥ùëÄ=ReLU‚éõ‚éù‚éú‚éú‚àëùëòùõºùëêùëòùê¥ùëò‚éû‚é†‚éü‚éüLGrad‚àíCAMc=ReLU‚àëkŒ±kcAk(11)",
            "6.2. Grad-CAM++": "Grad-CAM++ [69] addresses limitations of standard Grad-CAM by providing more precise localization through enhanced weight computation. It refines the importance weight calculation to better capture pixel-wise significance:ùõºùëêùëò=‚àëùëñ‚àëùëóùë§ùëò,ùëêùëñ,ùëó¬∑ReLU‚éõ‚éù‚éú‚éú‚éú‚àÇùë¶ùëê‚àÇùê¥ùëòùëñ,ùëó‚éû‚é†‚éü‚éü‚éüŒ±kc=‚àëi‚àëjwi,jk,c¬∑ReLU‚àÇyc‚àÇAi,jk(12)ùë§ùëò,ùëêùëñ,ùëó=‚àÇ2ùë¶ùëê‚àÇ(ùê¥ùëòùëñ,ùëó)22¬∑‚àÇ2ùë¶ùëê‚àÇ(ùê¥ùëòùëñ,ùëó)2+‚àëùëé,ùëèùê¥ùëòùëé,ùëè¬∑‚àÇ3ùë¶ùëê‚àÇ(ùê¥ùëòùëé,ùëè)3wi,jk,c=‚àÇ2yc‚àÇ(Ai,jk)22¬∑‚àÇ2yc‚àÇ(Ai,jk)2+‚àëa,bAa,bk¬∑‚àÇ3yc‚àÇ(Aa,bk)3(13) In Formula¬†(13), the¬†pixel-wise weightùë§ùëò,ùëêùëñ,ùëówi,jk,cat spatial position(ùëñ,ùëó)(i,j)in feature mapkfor classcis computed as a normalized ratio. The¬†numerator computes the second-order derivative‚àÇ2ùë¶ùëê‚àÇ(ùê¥ùëòùëñ,ùëó)2‚àÇ2yc‚àÇ(Ai,jk)2, which measures how sensitively the class prediction scoreùë¶ùëêycchanges with respect to the pixel activationùê¥ùëòùëñ,ùëóAi,jk. This captures the local curvature of the prediction function at that specific pixel location. While the denominator normalizes the numerator to ensure bounded weights. This ensures that pixels exhibiting both strong gradients and consistent influence across the entire feature map receive appropriately higher weights. We utilize cosine similarity to determine the ten most similar representations for each class, resulting in a total of 100 samples, with¬†ten samples per class. Following this, we employ the two attention map techniques, Grad-CAM and Grad-CAM++. The¬†outcomes, as¬†presented inFigure 15, demonstrate the application of these methods to a single MalVis image belonging to a specific class. The¬†following section provides a comprehensive analysis and identification of key observations derived from this¬†experiment. Figure 15.Grad-CAM and Grad-CAM++ visualization results for different malware types. Each row showcases a randomly selected sample with its ground truth type, displayed across three columns: original MalVis images (left), Grad-CAM overlays (middle), and Grad-CAM++ overlays (right). In the heatmap overlays, red and orange regions indicate areas of high model attention, while blue regions represent areas of low attention. The red and blue channels of the MalVis images capture high-entropy sections of a code, whereas the green channel highlights areas with abnormal coding structures.",
            "6.3. Key Findings from Visualization¬†Analysis": "The application of both Grad-CAM and Grad-CAM++ visualization techniques, as¬†illustrated inFigure 15, provided critical insights into the model‚Äôs decision-making process. Both methods revealed that the CNN consistently focused on semantically meaningful regions within MalVis images, with¬†Grad-CAM++ demonstrating more refined attention patterns. Notably, the¬†model concentrated on areas identified by our entropy and N-gram encoders, confirming that the CNN relied on these introduced feature patterns rather than random noise or artifacts to accurately classify¬†samples. Distinct attention patterns were observed across different malware types, validating our approach and demonstrating the model‚Äôs discriminative capabilities. For¬†example, the¬†magnified section of the Spyware sample inFigure 15shows intense focus, revealing structural regions highlighted in dark red and orange. These correspond to highly obfuscated code sections captured by the entropy encoder (red and blue channels) or abnormal code structures identified by the N-gram approach (green channel). These malicious patterns reflect obfuscation techniques used by malware creators to evade detection. In¬†contrast, the¬†magnified section of the Downloader sample exhibits distributed attention across the whole section, reflecting lower entropy levels with an indication of less randomness in the bytecode. The¬†Benign sample, notably, shows light blue with less attention overall, focusing on lower-entropy regions containing regular coding¬†patterns. This differentiation confirms that the model effectively leverages entropy and N-gram information embedded within the MalVis representations to make classification decisions, focusing on relevant malware characteristics rather than spurious patterns. The¬†interpretability offered by these attention mechanisms provides confidence in the model‚Äôs trustworthiness and establishes the reliability of its decision-making¬†process.",
            "7. Conclusions": "This research establishes the critical importance of visualizing Android malware to safeguard user data and smartphone security. We introduced MalVis, the¬†largest publicly available image-based dataset for Android malware, containing over 1.3 million samples. To¬†complement this resource, we developed a novel visualization framework that transforms bytecode into RGB images by integrating entropy and N-gram encoding techniques. This method effectively captures the malware‚Äôs encryption, compression, structural, and¬†operational anomaly¬†patterns. Through extensive evaluation, MalVis consistently outperformed existing visualization-based detection approaches, achieving 95% accuracy, 90% F1-score, 92% precision, 89% recall, 87% Matthews Correlation Coefficient, and¬†a 98% ROC-AUC. Our interpretability analysis, using GradCAM and GradCAM++, revealed that the model focuses on semantically meaningful regions within MalVis images, particularly areas linked to entropy variations and N-gram patterns. Each malware type exhibits distinct attention patterns, validating the model‚Äôs discriminative capabilities. These visualization results confirm that classification decisions are based on relevant malware characteristics, establishing both the trustworthiness and explainability of our approach. Beyond¬†its strong performance, MalVis delivers an innovative framework that links visual representations to the semantic characteristics of malware, enhancing interpretability and classification robustness. This dataset and framework provide a valuable foundation for advancing research in malware classification and explainable threat¬†detection."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2624-800X/5/4/109",
        "scraped_at": "2025-12-05 23:56:32"
    },
    {
        "title": "Overlapping Coalition Formation for Resource Allocation in Post-Disaster Rescue UAV Swarms",
        "authors": "byWenxin Li,Yongxin Feng,Fan Zhou,Konstantin Igorevich Kostromitin,Jian WangandPeiying Zhang",
        "journal": "Drones2025,9(12), 837; https://doi.org/10.3390/drones9120837 (registering¬†DOI) - 4 Dec 2025",
        "abstract": "HighlightsWhat are the main findings?We introduce a hypergraph attention-based structured representation that captures high-order relations among tasks, UAVs, and coalitions, enabling overlapping coalition formation in heterogeneous swarms.We develop a structure-conditioned hierarchical value decomposition coupled with limited-budget Monte Carlo Tree Search with demonstration replay and distillation, achieving global optimization and policy feedback under operational constraints.What are the implications of the main findings?The combination of overlapping coalitions and structure-aware policy learning promotes cross-task resource reuse, alleviates bottlenecks under tight constraints, and strengthens global coordination for post-disaster multi-tasking.Feasible-region pruning and delay-aware search help maintain convergence efficiency and stability under latency, communication, and energy limits, pointing to a practical path toward real-world deployment.AbstractUnmanned aerial vehicle (UAV) swarms, equipped for distributed sensing and rapid response, can form coalitions to undertake complex missions such as post-disaster relief, communication support, and payload delivery. However, typical coalition formation methods assign each UAV to a single task, limiting cross-task resource sharing. To address this, we investigate overlapping coalition formation (OCF) for UAV swarms, where a single UAV is permitted to participate in multiple coalitions, enabling resource reuse and reducing idleness. We formulate OCF as a multi-objective combinatorial optimization problem that jointly balances task fulfillment ratio, coalition synchronization deviation, and operational cost, while explicitly accounting for inter-coalition resource contention and execution precedence. Specifically, we first construct a hypergraph representation of UAVs and tasks and employ a hypergraph attention network to capture their high-order interactions. Next, we propose a structure-aware hierarchical value decomposition method for policy learning, which progressively aggregates individual- and coalition-level information, models member complementarity and inter-coalition cooperative‚Äìcompetitive relations, and generates a global value estimate that is sensitive to changes in coalition structure. Furthermore, we integrate Monte Carlo Tree Search, utilizing the learned value as a heuristic to efficiently explore the feasible region, and close the loop with candidate-structure demonstration replay and policy distillation, enabling search to refine the learned policy. In multi-scale rescue simulations, the proposed approach improves task utility by up to 11.4% over the best-performing baseline and increases energy efficiency by more than 228% compared to a non-overlapping coalition variant.Keywords:unmanned aerial vehicle swarm;overlapping coalition formation;multi-objective combinatorial optimization",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "With continuing advances in avionics and wireless networking, unmanned aerial vehicle (UAV) swarms can undertake cross-scale tasks including reconnaissance, search and rescue, communication assurance, and¬†emergency power supply¬†[1], even when terrain is constrained and infrastructure is damaged¬†[2]. Nevertheless, swarm-level collaboration essentially depends on the organizational mechanisms of task allocation and resource configuration¬†[3]. Under¬†constraints on endurance, payload, and¬†sensing capacity, single-UAV assignment paradigms struggle to satisfy the joint resource and temporal requirements of complex tasks¬†[4]. To¬†enhance system-level feasibility and overall benefit, forming coalitions that aggregate capabilities across platforms¬†[5,6] becomes a necessary pathway for achieving this¬†goal. Resource scheduling, execution precedence, and¬†cost constraints at the task level jointly constitute the key difficulties of coalition coordination¬†[7]. With¬†the rise of modular payloads and multi-source sensor fusion¬†[8], a¬†single platform can be configured with diverse functional modules and can simultaneously carry multiple resource types in varying quantities to execute different kinds of tasks. For¬†example, in¬†military settings, a¬†UAV may consecutively perform target tracking and communication relay tasks¬†[9], whereas in civilian settings, a¬†rescue UAV may consecutively conduct target-detection and payload-delivery tasks. However, prevalent non-overlapping coalition formation methods typically restrict each UAV to a single coalition and a single task within the same time window. Under¬†high task load, this restriction prolongs waiting chains, and¬†under low load, it yields idle capabilities and fragmented utilization, which degrades task completion and energy efficiency. Therefore, we study overlapping coalition formation (OCF)¬†[10] for heterogeneous UAV swarms in multi-task disaster-relief scenarios, as¬†illustrated inFigure 1. Subject to feasibility constraints on energy, communication, and¬†deadlines, OCF permits a UAV to belong to multiple task coalitions within the planning horizon, enabling cross-task capability flow and resource reuse to alleviate threshold bottlenecks and improve overall resource utilization¬†efficiency. Figure 1.Illustrative coalition formation modes in a heterogeneous UAV swarm. (a) Non-overlapping coalition formation where each coalition executes one task. (b) Sequential multi-task formation where a coalition serves multiple tasks in sequence. (c) Overlapping coalition formation where UAVs can participate in multiple coalitions and jointly execute several tasks over time, enabling cross task capability flow and resource reuse. In post-disaster contexts, OCF can be cast as a multi-objective combinatorial optimization problem with explicit resource competition¬†[11]. Due to communication degradation and temporary restrictions, observation ranges shrink markedly, and¬†information becomes fragmented¬†[12]; as a result, each UAV relies primarily on local views and cannot readily grasp global dependencies across tasks and resources. Meanwhile, the¬†number and urgency of tasks fluctuate over short intervals, and¬†feasible routes and required preconditions change frequently¬†[13]. Without¬†continual updates to coalitions and resource allocations, previously feasible plans become invalid, which induces execution interruptions and response delays. Moreover, overlapping organization causes a single UAV to serve multiple tasks across time windows: consumable resources exhibit congestion effects, non-consumable capabilities are subject to occupancy and exclusivity, coalitions compete for scarce and critical capabilities, tasks display both cooperative and mutually exclusive relations, and¬†task value becomes markedly non-additive. These factors induce credit-assignment bias and global scheduling imbalance, which makes it difficult to align locally optimal choices with system-level¬†optimality. Furthermore, the¬†decision space is a joint combination of coalition structure, resource allocation, and¬†temporal scheduling, and¬†is high-dimensional, with¬†a branching explosion¬†[14]. Multiple objectives trade off against each other: higher task fulfillment often demands greater energy expenditure and tighter synchronization, whereas reducing operational cost and synchronization deviation may sacrifice fulfillment. Static weighting and local greediness are therefore often ineffective. Under¬†partial observability and highly dynamic conditions, the¬†central challenges are maintaining feasibility and responding rapidly to time-varying constraints, as¬†well as achieving a globally coordinated, multi-objective balance amid¬†competition. To address these challenges, we propose aStructure-GuidedReinforcementLearning withTreeSearch (SGRL-TS) framework. Using a hypergraph as the carrier and bidirectional attention to extract high-order relations among UAVs, task demands, and¬†coalition capabilities, the¬†framework consolidates local observations into structural semantics that enhance identifiability under incomplete information. It then employs structure-conditioned hierarchical value modeling to characterize intra-coalition complementarity and conflict while capturing inter-task interactions, thereby improving the robustness of value estimation under delayed and partial information. Building on this foundation, we adopt budgeted Monte Carlo Tree Search (MCTS) with feasible-region pruning based on energy, bandwidth, and¬†deadlines to generate a small set of high-quality candidate coalition structures and to feed them back to policy learning via demonstration replay and value distillation, achieving efficient and scalable OCF under constrained communication and time-varying feasibility. Our contributions are as follows: We formulate overlapping coalition formation on a time-varying heterogeneous hypergraph and design a masked bidirectional Hypergraph Attention Network. This network incorporates task-to-node and node-to-task attention, an¬†operator view of higher-order propagation, and¬†structural diversity regularization, yielding structure-aware node and task embeddings that encode task selectivity, contextual suitability, and¬†structural roles under evolving coalition structures.We developStructure-awareHierarchical Value-decomposItion forEfficient Multi-task CoorDination (SHIELD) that utilizes a structure-attribution encoder and agent-conditionalQfunctions, along with nested nonlinear intra-coalition composition and sparsified cross-task interaction aggregation. This approach constructs a global valueùëÑtotthat remains monotone and comparable across partial coalition structures, supporting stable credit assignment under the multi-objective reward.We integrate SHIELD with a structure-prior and a feasible-set pruned budgeted MCTS. Masked SHIELD evaluations, progressive widening, upper-bound and dominance pruning, and¬†hypervolume-based Pareto selection produce a compact set of high-quality overlapping coalition structures. These structures are injected into learning via scheduled demonstration replay and structure-consistentQ-value distillation. Experiments on heterogeneous UAV‚Äìmulti-task disaster-relief scenarios demonstrate consistent gains in task utility, energy efficiency, conditional near-linear scalability, and¬†improved ablation results compared to baselines.",
            "2. Related¬†Work": "2.1. Overlapping Coalitions: Advantages of Hypergraph Modeling over Other OCF¬†AlgorithmsIn multi-agent cooperation and resource allocation, the¬†traditional non-overlapping Coalition Formation Game (CFG) has been widely adopted. Its core assumption is that each agent can join only one coalition to complete a specific task. Guo¬†et¬†al.¬†[15] designed a fast CFG solver for urgent-task scheduling based on neighborhood search and coalition reorganization, which starts from an initial partition and performs local search to find member-replacement plans that reduce task delay. Yan¬†et¬†al.¬†[16] proposed a hierarchical nested CFG framework for large-scale heterogeneous UAV swarms. The¬†method first forms task groups via priority-based clustering at the upper level and then solves coalition partitioning within each group to optimize resource matching at the lower level. Although¬†these methods attain good local optima in specific scenarios, they rely on the non-overlapping assumption, which prevents cross-task resource reuse and yields limited adaptability of coalition partitions in heterogeneous multi-task¬†settings.To overcome these limitations, researchers introduced the Overlapping Coalition Formation Game (OCFG), which enables agents to participate in multiple coalitions, allowing for flexible resource reuse in heterogeneous multi-task environments. Qi¬†et¬†al.¬†[17] formulated task allocation as a sequential overlapping coalition formation game in which coalitions are iteratively adjusted via a bilateral mutual-benefit transfer rule that reallocates per-UAV resources across tasks until a stable overlapping partition is reached. Zhang¬†et¬†al.¬†[18] developed an OCFG for vehicular edge computing that jointly optimizes information transmission rate and energy-sharing efficiency and balances multi-coalition participation via transfer payments and a virtual-currency mechanism. Qi¬†et¬†al.¬†[19] combined OCFG with an auction mechanism for UAV data-collection task assignment, treating UAVs as buyers that can participate across coalitions and iteratively maximizing revenue through bipartite matching auctions and coalition games. Overall, OCFG outperforms traditional CFG in terms of resource efficiency and task adaptivity. However, most existing OCFG studies rely on local marginal contributions or linearly weighted utilities and lack explicit modeling of high-order interactions among tasks, agents, and¬†coalitions. As¬†a result, policies generalize poorly in dynamic multi-task environments and coalition structures tend to be trapped in local¬†optima. 2.2. Structure Modeling: Advantages over Non-Structured¬†ModelingAs the complexity of multi-agent cooperation grows, structure-aware modeling has been introduced to enhance interaction perception and policy generalization. By¬†explicitly representing inter-agent relations in graphs or geometric structures, several works have reported progress in task cooperation and system performance. HYGMA¬†[20] constructs a latent hypergraph where dynamically grouped agents are represented as hyperedges. Coalition structures emerge implicitly as the policy attends to and aggregates over these hyperedges, which approach avoids reliance on predefined or pairwise coalitions. Zhang¬†et¬†al.¬†[21] proposed a coalition formation algorithm inspired by leaf-vein geometry, which predefines vein-like branching to assign functional roles and then uses heuristic pattern matching to adjust node layouts. This structure enhances global cooperative stability while remaining rule-based and static. Liao¬†et¬†al.¬†[22] presented a two-stage cooperative anti-jamming scheme for radar networks under wideband sweep jamming. During¬†coalition formation, the¬†entire frequency-hopping range is partitioned into mutually exclusive sub-bands, each treated as a coalition unit, and¬†coalitions are updated via a coalition formation game. This realizes structured modeling and optimization of radar cooperation at the sub-band¬†level.These structured approaches strengthen cooperation awareness and deliver improved stability and generalization during resource sharing and local task switching. Nevertheless, most of them are confined to first-order adjacency and cannot capture higher-order dependencies among tasks, agents, and¬†coalitions. Consequently, UAVs cannot accurately identify their roles across multiple coalitions in multi-task settings. In¬†addition, structures are often treated as fixed input features rather than directly participating in policy¬†optimization. 2.3. Global Optimization and Policy Feedback: Integrating MCTS with Local Reinforcement¬†LearningTo move beyond local heuristics and short-sighted payoff optimization, several studies have pursued coalition generation and stabilization driven by global objectives or long-term equilibria. Taguelmimt¬†et¬†al.¬†[23] treat coalition structure generation as a centralized global optimization problem and propose SMART, which first uses offline coalition selection to precompute and prune promising coalitions, and¬†applies complementarity-based dynamic programming with gradual, integer-partition‚Äìgraph search to exhaustively derive the globally optimal coalition structure under a given utility. Chen¬†et¬†al.¬†[24] formulated a global-optimization CFG for multi-UAV cooperative defense by maximizing defense coverage. The¬†approach defines coalition utility in terms of global coverage and threat-suppression ratio, and¬†incorporates dynamic coalition reconstruction and global utility re-estimation to ensure monotonic improvement toward the global objective. It improves coverage but relies heavily on a centralized coordinator, which reduces efficiency when handling asynchronous updates and dynamic tasks. Xiong¬†et¬†al.¬†[25] proposed a global coalition formation method for radar-UAV multi-target pursuit based on multi-objective cooperative games, using evolutionary games and global Nash equilibria to determine stable structures and provide long-term optimality. Although¬†these methods improve system-wide performance through global utilities or equilibria, most rely on deterministic optimization or equilibrium derivation and lack active exploration in the combinatorial space of large-scale overlapping coalitions.To remedy these issues, some works integrate game-theoretic models with deep reinforcement learning so as to retain long-term optimality while improving the adaptability of coalition structures. Zhou¬†et¬†al.¬†[26] model coalition dynamics as a three-party repeated coalition formation game and realize coalition formation via a switch-operation‚Äìbased coalition selection process, where deep reinforcement learning optimizes each party‚Äôs continuous non-coalitional strategies while the game-theoretic analysis guarantees convergence to a stable coalition partition in each time slot. Zhang¬†et¬†al.¬†[27] designed a distributed DRL and CFG framework for coalition selection in federated learning with heterogeneous data. Periodic training utilizes global accuracy and energy efficiency as rewards, enabling nodes to adaptively adjust the probabilities of joining different coalitions across rounds, thereby improving global convergence and energy efficiency. However, the¬†process predominantly relies on experience replay and local policy convergence, which limits the adaptation speed when coalition structures undergo significant¬†changes.Overall, it is difficult for prior work to simultaneously capture high-order structural relations and explore globally optimal coalition structures. In¬†particular, for¬†overlapping coalition formation with heterogeneous UAV swarms, there is a lack of mechanisms that exploit structural semantics to actively guide policy optimization and to coordinate local learning with global search. To¬†address this gap, we propose a structure-aware and globally guided OCF method that enhances high-order interaction modeling among tasks, agents, and¬†coalitions through a hypergraph-based representation, and¬†achieves long-term optimality by coordinating global search with local policy¬†learning.",
            "2.1. Overlapping Coalitions: Advantages of Hypergraph Modeling over Other OCF¬†Algorithms": "In multi-agent cooperation and resource allocation, the¬†traditional non-overlapping Coalition Formation Game (CFG) has been widely adopted. Its core assumption is that each agent can join only one coalition to complete a specific task. Guo¬†et¬†al.¬†[15] designed a fast CFG solver for urgent-task scheduling based on neighborhood search and coalition reorganization, which starts from an initial partition and performs local search to find member-replacement plans that reduce task delay. Yan¬†et¬†al.¬†[16] proposed a hierarchical nested CFG framework for large-scale heterogeneous UAV swarms. The¬†method first forms task groups via priority-based clustering at the upper level and then solves coalition partitioning within each group to optimize resource matching at the lower level. Although¬†these methods attain good local optima in specific scenarios, they rely on the non-overlapping assumption, which prevents cross-task resource reuse and yields limited adaptability of coalition partitions in heterogeneous multi-task¬†settings. To overcome these limitations, researchers introduced the Overlapping Coalition Formation Game (OCFG), which enables agents to participate in multiple coalitions, allowing for flexible resource reuse in heterogeneous multi-task environments. Qi¬†et¬†al.¬†[17] formulated task allocation as a sequential overlapping coalition formation game in which coalitions are iteratively adjusted via a bilateral mutual-benefit transfer rule that reallocates per-UAV resources across tasks until a stable overlapping partition is reached. Zhang¬†et¬†al.¬†[18] developed an OCFG for vehicular edge computing that jointly optimizes information transmission rate and energy-sharing efficiency and balances multi-coalition participation via transfer payments and a virtual-currency mechanism. Qi¬†et¬†al.¬†[19] combined OCFG with an auction mechanism for UAV data-collection task assignment, treating UAVs as buyers that can participate across coalitions and iteratively maximizing revenue through bipartite matching auctions and coalition games. Overall, OCFG outperforms traditional CFG in terms of resource efficiency and task adaptivity. However, most existing OCFG studies rely on local marginal contributions or linearly weighted utilities and lack explicit modeling of high-order interactions among tasks, agents, and¬†coalitions. As¬†a result, policies generalize poorly in dynamic multi-task environments and coalition structures tend to be trapped in local¬†optima.",
            "2.2. Structure Modeling: Advantages over Non-Structured¬†Modeling": "As the complexity of multi-agent cooperation grows, structure-aware modeling has been introduced to enhance interaction perception and policy generalization. By¬†explicitly representing inter-agent relations in graphs or geometric structures, several works have reported progress in task cooperation and system performance. HYGMA¬†[20] constructs a latent hypergraph where dynamically grouped agents are represented as hyperedges. Coalition structures emerge implicitly as the policy attends to and aggregates over these hyperedges, which approach avoids reliance on predefined or pairwise coalitions. Zhang¬†et¬†al.¬†[21] proposed a coalition formation algorithm inspired by leaf-vein geometry, which predefines vein-like branching to assign functional roles and then uses heuristic pattern matching to adjust node layouts. This structure enhances global cooperative stability while remaining rule-based and static. Liao¬†et¬†al.¬†[22] presented a two-stage cooperative anti-jamming scheme for radar networks under wideband sweep jamming. During¬†coalition formation, the¬†entire frequency-hopping range is partitioned into mutually exclusive sub-bands, each treated as a coalition unit, and¬†coalitions are updated via a coalition formation game. This realizes structured modeling and optimization of radar cooperation at the sub-band¬†level. These structured approaches strengthen cooperation awareness and deliver improved stability and generalization during resource sharing and local task switching. Nevertheless, most of them are confined to first-order adjacency and cannot capture higher-order dependencies among tasks, agents, and¬†coalitions. Consequently, UAVs cannot accurately identify their roles across multiple coalitions in multi-task settings. In¬†addition, structures are often treated as fixed input features rather than directly participating in policy¬†optimization.",
            "2.3. Global Optimization and Policy Feedback: Integrating MCTS with Local Reinforcement¬†Learning": "To move beyond local heuristics and short-sighted payoff optimization, several studies have pursued coalition generation and stabilization driven by global objectives or long-term equilibria. Taguelmimt¬†et¬†al.¬†[23] treat coalition structure generation as a centralized global optimization problem and propose SMART, which first uses offline coalition selection to precompute and prune promising coalitions, and¬†applies complementarity-based dynamic programming with gradual, integer-partition‚Äìgraph search to exhaustively derive the globally optimal coalition structure under a given utility. Chen¬†et¬†al.¬†[24] formulated a global-optimization CFG for multi-UAV cooperative defense by maximizing defense coverage. The¬†approach defines coalition utility in terms of global coverage and threat-suppression ratio, and¬†incorporates dynamic coalition reconstruction and global utility re-estimation to ensure monotonic improvement toward the global objective. It improves coverage but relies heavily on a centralized coordinator, which reduces efficiency when handling asynchronous updates and dynamic tasks. Xiong¬†et¬†al.¬†[25] proposed a global coalition formation method for radar-UAV multi-target pursuit based on multi-objective cooperative games, using evolutionary games and global Nash equilibria to determine stable structures and provide long-term optimality. Although¬†these methods improve system-wide performance through global utilities or equilibria, most rely on deterministic optimization or equilibrium derivation and lack active exploration in the combinatorial space of large-scale overlapping coalitions. To remedy these issues, some works integrate game-theoretic models with deep reinforcement learning so as to retain long-term optimality while improving the adaptability of coalition structures. Zhou¬†et¬†al.¬†[26] model coalition dynamics as a three-party repeated coalition formation game and realize coalition formation via a switch-operation‚Äìbased coalition selection process, where deep reinforcement learning optimizes each party‚Äôs continuous non-coalitional strategies while the game-theoretic analysis guarantees convergence to a stable coalition partition in each time slot. Zhang¬†et¬†al.¬†[27] designed a distributed DRL and CFG framework for coalition selection in federated learning with heterogeneous data. Periodic training utilizes global accuracy and energy efficiency as rewards, enabling nodes to adaptively adjust the probabilities of joining different coalitions across rounds, thereby improving global convergence and energy efficiency. However, the¬†process predominantly relies on experience replay and local policy convergence, which limits the adaptation speed when coalition structures undergo significant¬†changes. Overall, it is difficult for prior work to simultaneously capture high-order structural relations and explore globally optimal coalition structures. In¬†particular, for¬†overlapping coalition formation with heterogeneous UAV swarms, there is a lack of mechanisms that exploit structural semantics to actively guide policy optimization and to coordinate local learning with global search. To¬†address this gap, we propose a structure-aware and globally guided OCF method that enhances high-order interaction modeling among tasks, agents, and¬†coalitions through a hypergraph-based representation, and¬†achieves long-term optimality by coordinating global search with local policy¬†learning.",
            "3. System Model and Problem¬†Statement": "3.1. System Architecture of Overlapping Coalitions and Hypergraph-Based¬†Representation3.1.1. Task Resource¬†TypesLet the task set beùíØ={ùëá1,ùëá2,‚Ä¶,ùëáùëÄ}. Each taskùëáùëörequires a combination of resources. We categorize resources into two types: consumable and non-consumable. Consumable resources are physical items that are depleted during execution, such as rescue kits, medicines, or¬†spraying agents. Their demand isùêã(ùëê)ùëö‚àà‚Ñùùëëùëê, where each dimension specifies the required quantity of a specific consumable. Non-consumable resources are functional capabilities that support a task without being depleted, such as communication, relay, or information acquisition. Their demand vector isùêã(ùëõùëê)ùëö‚àà‚Ñùùëëùëõùëê. The¬†overall demand for taskmisùêãùëö=[ùêã(ùëê)ùëö;ùêã(ùëõùëê)ùëö]‚àà‚Ñùùëëùëüwithùëëùëü=ùëëùëê+ùëëùëõùëê. Each task further has key attributes including spatial locationùëùùëö‚àà‚Ñù2, priority coefficientùõΩùëö‚àà[0,1], and¬†maximum waiting time (deadline tolerance)ùë°deadùëö. The¬†complete task feature vector isùê±ùëö=[ùêã(ùëê)ùëö;ùêã(ùëõùëê)ùëö;ùëùùëö;ùõΩùëö;ùë°deadùëö].3.1.2. UAV Node¬†StateLet the UAV swarm beùí∞={ùë¢1,ùë¢2,‚Ä¶,ùë¢ùëÅ}. The¬†state vector of UAVùë¢ùëõcomprises the remaining energyùëíùëõ‚àà[0,ùê∏maxùëõ], the¬†load of consumable resourcesùê´(ùëê)ùëõ‚àà‚Ñùùëëùëê, the¬†non-consumable capability vectorùê´(ùëõùëê)ùëõ‚àà‚Ñùùëëùëõùëê, the¬†current positionùëùùëõ‚àà‚Ñù2, the¬†maximum flight speedùë£maxùëõ, and¬†a representation of the recent task-history sequenceseqùëõ. Thus, the¬†full UAV state isùê±ùëõ=[ùëíùëõ;ùê´(ùëê)ùëõ;ùê´(ùëõùëê)ùëõ;ùëùùëõ;ùë£maxùëõ;seqùëõ].3.1.3. Overlapping Coalition¬†ModelingFor each taskùëáùëö, define its UAV coalition asùíûùëö‚äÜùí∞. The¬†coalition structure for the task setùíØisùíû={ùíû1,‚Ä¶,ùíûùëö,‚Ä¶,ùíûùëÄ}. For any taskùëáùëö, Equation¬†(1) specifies the set of coalition members that allocate resources to it. The¬†resource-allocation vector ofùëáùëöis given in Equation¬†(2), whereùêÄ(ùë¢ùëõ)(ùëáùëö)denotes the resource vector contributed by UAVùë¢ùëõto taskùëáùëöas defined in Equation¬†(3).Coa(ùíú(ùëáùëö))={ùë¢ùëõ‚ààùí∞‚à£ùêÄ(ùë¢ùëõ)(ùëáùëö)‚â†‚àÖ}(1)ùíú(ùëáùëö)={ùêÄ(ùë¢1)(ùëáùëö),‚Ä¶,ùêÄ(ùë¢ùëõ)(ùëáùëö),‚Ä¶,ùêÄ(ùë¢ùëÅ)(ùëáùëö)}(2)ùêÄ(ùë¢ùëõ)(ùëáùëö)={ùõº(1)ùë¢ùëõ,ùëáùëö,‚Ä¶,ùõº(ùëßùëê)ùë¢ùëõ,ùëáùëö,‚Ä¶,ùõΩ(ùëßùëõùëê)ùë¢ùëõ,ùëáùëö,‚Ä¶,ùõΩ(ùëç)ùë¢ùëõ,ùëáùëö}(3)Specifically,ùõº(ùëßùëê)ùë¢ùëõ,ùëáùëödenotes the quantity of consumable resource typeùëßùëêthat UAVùë¢ùëõallocates to taskùëáùëö, andùõΩ(ùëßùëõùëê)ùë¢ùëõ,ùëáùëödenotes the quantity of non-consumable capability typeùëßùëõùëêthat UAVùë¢ùëõallocates to taskùëáùëö. For any UAVùë¢ùëõ‚ààùí∞, the¬†set of tasks it participates in is‚Ñ≥ùëõ={ùëö‚à£ùë¢ùëõ‚ààùíûùëö}. If|‚Ñ≥ùëõ|>1, thenùë¢ùëõbelongs to multiple task coalitions within the planning horizon, and¬†the coalition structure is¬†overlapping.To capture complex overlapping relations and high-order interactions between UAVs and tasks, we formalize the coalition structure as a hypergraphùí¢=(ùí±,‚Ñ∞), where the node setùí±corresponds to the UAV setùí∞and each nodeùë£ùëõ‚ààùí±represents a UAV with featureùê±ùëõ; the hyperedge set‚Ñ∞corresponds to the task setùíØand each hyperedgeùëíùëö‚àà‚Ñ∞connects multiple nodes that participate in taskùëáùëö, with¬†initial hyperedge featureùê±ùëö. A¬†node belonging to multiple hyperedges represents overlapping coalition membership, and¬†a hyperedge connecting multiple nodes represents multi-agent cooperation within a coalition. This hypergraph provides a formal basis for employing a hypergraph attention network to capture high-order relations between UAVs and¬†tasks. 3.2. Multi-Objective¬†DesignThe core of overlapping coalition formation for UAV swarms is to evaluate the overall effectiveness when multiple UAVs execute multiple tasks, which determines whether a coalition structure is optimal. We design three objectives that quantify task fulfillment, coalition synchronization deviation, and¬†operational cost, respectively.Task execution sufficiency: To quantify the consistency between resources allocated to taskùëáùëöand its demands by first computing per-dimension satisfaction ratios. Then, perform within-class aggregation for consumable and non-consumable resources. Finally, we combine the two class-level scores into the overall task measure.For consumable resources, the¬†per-dimension satisfaction ratio is defined asùë†(ùëßùëê)c=min‚éõ‚éù‚éú‚éú‚éúùúÜ(ùëßùëê)ùëáùëöùëô(ùëßùëê)ùëáùëö,1‚éû‚é†‚éü‚éü‚éü,ùëßùëê‚ààùëáùëßùëê,ùëô(ùëßùëê)ùëáùëö>0,(4)whereùúÜ(ùëßùëê)ùëáùëö=‚àëùë¢ùëõ‚ààCoa(ùíú(ùëáùëö))ùõº(ùëßùëê)ùë¢ùëõ,ùëáùëö. Given weights{ùë§ùëßùëê}ùëßùëê‚ààùëáùëßùëêwith‚àëùëßùëê‚ààùëáùëßùëêùë§ùëßùëê=1, we adopt a shortage-sensitive generalized mean for within-class aggregationùêπc(ùëáùëö;ùúåc)=‚éõ‚éù‚éú‚éú‚éú‚àëùëßùëê‚ààùëáùëßùëêùë§ùëßùëê[ùë†(ùëßùëê)c]ùúåc‚éû‚é†‚éü‚éü‚éü1/ùúåc,ùúåc=‚àí1.(5)For non-consumable resources, the¬†per-dimension satisfaction ratio is defined asùë†(ùëßùëõùëê)n=min‚éõ‚éù‚éú‚éú‚éúùúôùëßùëõùëê(ùúÜ(ùëßùëõùëê)ùëáùëö)ùúôùëßùëõùëê(ùëô(ùëßùëõùëê)ùëáùëö),1‚éû‚é†‚éü‚éü‚éü,ùëßùëõùëê‚ààùëáùëßùëõùëê,ùëô(ùëßùëõùëê)ùëáùëö>0,(6)whereùúÜ(ùëßùëõùëê)ùëáùëödenotes the coalition-level equivalent capability on that dimension, andùúôùëßùëõùëêis a monotone mapping to form a dimensionless, larger-is-better scale. Given weights{ùë£ùëßùëõùëê}ùëßùëõùëê‚ààùëáùëßùëõùëêwith‚àëùëßùëõùëê‚ààùëáùëßùëõùëêùë£ùëßùëõùëê=1, the¬†within-class aggregation isùêπn(ùëáùëö)=‚éõ‚éù‚éú‚éú‚éú‚àëùëßùëõùëê‚ààùëáùëßùëõùëêùë£ùëßùëõùëê[ùë†(ùëßùëõùëê)n]ùúån‚éû‚é†‚éü‚éü‚éü1/ùúån,ùúån‚Üí‚àí‚àû.(7)The overall task measure is computed as the product of the two class-level scores,ùëì1=ùêπc(ùëáùëö;ùúåc)√óùêπn(ùëáùëö),ùëì1‚àà[0,1].(8)Coalition synchronization deviation: To represent arrival latency, waiting energy, and¬†deadline constraints under a unified dimension, the¬†precedence relations among tasks are modeled by a directed acyclic graphùí´; when only task priorities are available, they become a total order withinùí´. For¬†memberùë¢ùëõof coalitionùíûùëö, the¬†sequence of tasks that must be completed before executingùëáùëöis defined as:ùñ≤ùñæùóäùëõ=(Assigned(ùë¢ùëõ)‚à©Pred(ùëáùëö))‚à™{ùëáùëö}(9)Based on this sequence, two aggregate quantities are introduced for latency and energy calculation:ùêøùëõdenotes the total flight distance before reachingùëáùëö, andùëÜùëõdenotes the total processing time that must be completed before reachingùëáùëö, defined¬†as:ùêøùëõ=‚àëùëó=1ùêΩùëëùëõ,ùëó,ùëÜùëõ=‚àëùëó=1ùêΩùë†ùëõ,ùëó(10)whereùëëùëõ,ùëóis the flight distance of thej-th leg andùë†ùëõ,ùëóis the processing time that must be completed before that leg; if no processing is required, it is set to zero. Accordingly, the¬†arrival time ofùë¢ùëõcan be written as a function of speed:ùë°arrùëõ(ùë£)=ùêøùëõùë£+ùëÜùëõ(11)Under the speed boundsùë£minùëõ‚â§ùë£‚â§ùë£maxùëõ, taking the slowest path cruising at the maximum admissible speed as the baseline yields the earliest feasible synchronization¬†time:ùë°syn‚àóùëö=maxùë¢ùëõ‚ààùíûùëöùë°arrùëõ(ùë£maxùëõ)=maxùë¢ùëõ‚ààùíûùëö(ùêøùëõùë£maxùëõ+ùëÜùëõ)(12)Givenùë°syn‚àóùëö, each remaining member selects the unique target speed that achieves on-time arrival without violating the bounds:ùë£‚àóùëõ=clip(ùêøùëõùë°syn‚àóùëö‚àíùëÜùëõ,ùë£minùëõ,ùë£maxùëõ),clip(ùë•,ùëé,ùëè)=min{max{ùë•,ùëé},ùëè}(13)If the required speed falls belowùë£minùëõ, the¬†member cruises atùë£minùëõand waits upon arrival; the waiting time is:ùë°hovùëõ=max{0,ùë°syn‚àóùëö‚àíùë°arrùëõ(ùë£minùëõ)}=max{0,ùë°syn‚àóùëö‚àíùêøùëõùë£minùëõ‚àíùëÜùëõ}(14)Consequently, the¬†synchronized start time of the coalition satisfies:ùë°synùëö=ùë°syn‚àóùëö(15)To reflect the energy cost incurred during waiting within a unified metric, letùëùflyùëõ(ùë£)denote the flight power andùëùhovùëõdenote the hovering power, and¬†impose the ordering constraint:ùëùhovùëõ‚â•ùëùflyùëõ(ùë£econùëõ)(16)whereùë£econùëõis an empirical economical cruise speed. The¬†energy expenditure ofùë¢ùëõbeforeùëáùëöis then:ùê∏ùëõ=ùëùflyùëõ(ùë£‚àóùëõ)¬∑ùêøùëõùë£‚àóùëõ+ùëùhovùëõ¬∑ùë°hovùëõ(17)Further, given the deadlineùê∑ùëöof taskùëáùëö, define the lateness as:ùìÅùëö=max{0,ùë°synùëö‚àíùê∑ùëö}(18)Finally, the¬†synchronization latency, the¬†deadline deviation, and¬†the energy differential during waiting are combined via time-equivalent coefficients into a single objective:ùëì2(ùëáùëö)=ùêΩsyn(ùëáùëö)=ùë°synùëö+ùúÜlateùìÅùëö+ùúáhov‚àëùë¢ùëõ‚ààùíûùëö(ùëùhovùëõ‚àíùëùflyùëõ(ùë£‚àóùëõ))ùë°hovùëõ(19)whereùúÜlate‚â•0andùúáhov‚â•0are calibration weights that convert the energy penalty into a time-equivalent cost, allowing it to be summed withùë°synùëöandùìÅùëöunder a unified dimension.Operational cost: To assess the aggregate energy burden incurred by a coalition while completing multiple tasks, Equation¬†(20) defines the operational cost for taskùëáùëö:ùëì3=ùê∏ùëáùëö=‚àëùë¢ùëõ‚ààCoa(ùíúùëáùëö)ùëí(ùë¢ùëõ)ùëáùëö=‚àëùë¢ùëõ‚ààCoa(ùíúùëáùëö)ùê∏(ùë¢ùëõ)‚à•ùêÄ(ùë¢ùëõ)(ùëáùëö)‚à•‚àëùëáùëö‚Ä≤‚ààùíØ‚à•ùêÄ(ùë¢ùëõ)(ùëáùëö‚Ä≤)‚à•,(20)whereùê∏(ùë¢ùëõ)is the total energy consumed by UAVùë¢ùëõto complete all assigned tasks. Sinceùë¢ùëõcontributes heterogeneous resource types and quantities to multiple tasks, we use‚à•ùêÄ(ùë¢ùëõ)(ùëáùëö)‚à•as a workload proxy and proportionally split the total energy to estimate per-task cost. The¬†energy of UAVùë¢ùëõis the sum of hovering and flight energy, given¬†byùê∏ùë¢ùëõ=‚àëùëñ=1ùúÅùëÉ(0)ùë°(h)task(ùëñ)ùë¢ùëõ+‚àëùëñ=1ùúÅùëÉ(ùëâ(fly)task(ùëñ‚àí1)ùë¢ùëõ,task(ùëñ)ùë¢ùëõ)ùë°(fly)task(ùëñ‚àí1)ùë¢ùëõ,task(ùëñ)ùë¢ùëõ.(21)In Equation¬†(21),ùúÅis the length of the task schedule ofùë¢ùëõ, andùëÉ(ùëâ)is the propulsion power of a rotorcraft at constant speedV, computed asùëÉ(ùëâ)=ùëÉùëÜ‚éõ‚éù‚éú‚éú‚éú1+3ùëâ2ùëà2tip‚éû‚é†‚éü‚éü‚éü+ùëÉùëÖ‚éõ‚éù‚éú‚éú‚éú1+ùëâ44ùë£40‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö‚àíùëâ22ùë£20‚éû‚é†‚éü‚éü‚éü1/2+12ùëì0ùúåùë†0ùúÇùëâ3,(22)we instantiate the energy model with a DJI Matrice 600 Pro hexacopter equipped with 2170R 21 √ó 7 inch propellers; the parameter values used in Equation¬†(22) are listed inTable 1.Table 1.Parameterization of the propulsion power model in Equation¬†(22) for a DJI Matrice 600 Pro hexacopter used to compute flight energy consumption.To concisely characterize the aggregate benefit of taskùëáùëöon the interval[0,1], starting from the previously defined task execution sufficiencyùëì1(ùëáùëö)‚àà[0,1], we map the synchronization deviationùêΩsyn(ùëáùëö)and the energy costùê∏ùëáùëöto sufficiency scores and then couple them into a utility, as¬†follows:ùëìsync(ùëáùëö)=exp(‚àíùõºsynùêΩsyn(ùëáùëö)ùëáref(ùëáùëö)),ùëìeng(ùëáùëö)=exp(‚àíùõºengùê∏ùëáùëöùê∏ref(ùëáùëö)),(23)where by default we setùõºsyn=ùõºeng=ln2, takeùëáref(ùëáùëö)as the deadline scale ofTm, and¬†chooseEref(Tm)as the coalition energy budget or a platform-calibrated upper bound. With¬†non-negative weightsw=(wu,ws,we)satisfyingwu+ws+we=1, the¬†task utility is defined byUtask(Tm;w)=f1(Tm)wufsync(Tm)wsfeng(Tm)we.(24)Consequently,Utask(Tm)‚àà[0,1], increases withf1(Tm), and¬†decreases withJsyn(Tm)andETm. 3.3. Problem¬†StatementWe consider coalition formation with a heterogeneous UAV‚Äìmulti-task structure in which each task comprises multiple heterogeneous sub-tasks with distinct resource demands, as¬†illustrated inFigure 2. The¬†goal is to optimize overlapping coalition formation under limited resources, dynamic tasks, and¬†local communication. The¬†decision variables are the task‚ÄìUAV assignment structureCand the resource-allocation vectorsA(Tm)(un), yielding an exponentially large combinatorial search space. The¬†problem has three inherently conflicting objectivesf1,f2, andf3. For¬†example, higher task fulfillment typically increases energy consumption, while improving synchronization may conflict with preferred task orders. Hence, the¬†problem is a multi-objective combinatorial optimization with a set of non-dominated solutions. The¬†optimization seeks an overlapping coalition structure with resource allocationsBC(‚àó)={AT1(‚àó),‚Ä¶,ATm(‚àó),‚Ä¶,ATM(‚àó)}that maximizes the overall utility of the task set, as¬†in Equation¬†(25), subject to the constraints in Equation¬†(26).SC*:maxC,A‚àëTm‚ààTsATmminC,A‚àëTm‚ààTtTm(syn)minC,A‚àëTm‚ààTETm(25)subjectto‚àëun‚ààCmrn(c)‚â•Lm(c),‚àÄTm‚ààT,‚àëun‚ààCmrn(nc)‚â•Lm(nc),‚àÄTm‚ààT,tTm(syn)‚â§tTmdead,‚àÄTm‚ààT,‚àëTm‚ààMneTm(un)‚â§Enmax,‚àÄun‚ààU.(26)Figure 2.Example of resource allocation under overlapping coalitions, where heterogeneous UAV resources are assigned to multiple tasks and UAVs transfer between coalitions to reuse residual capacity.In Equation¬†(26), the¬†resource constraints ensure that the sums of consumable and non-consumable provisions from a coalition meet each task‚Äôs demand; the temporal constraint ensures that the synchronization deviation remains within the tolerated waiting time; and the energy constraint ensures that a UAV‚Äôs total energy consumption does not exceed its initial energy. Because¬†the solution space grows combinatorially with the number of tasks and UAVs, obtaining a globally optimal coalition structure is NP-hard. We therefore design a staged solving strategy with reduced computational¬†complexity.",
            "3.1. System Architecture of Overlapping Coalitions and Hypergraph-Based¬†Representation": "3.1.1. Task Resource¬†TypesLet the task set beùíØ={ùëá1,ùëá2,‚Ä¶,ùëáùëÄ}. Each taskùëáùëörequires a combination of resources. We categorize resources into two types: consumable and non-consumable. Consumable resources are physical items that are depleted during execution, such as rescue kits, medicines, or¬†spraying agents. Their demand isùêã(ùëê)ùëö‚àà‚Ñùùëëùëê, where each dimension specifies the required quantity of a specific consumable. Non-consumable resources are functional capabilities that support a task without being depleted, such as communication, relay, or information acquisition. Their demand vector isùêã(ùëõùëê)ùëö‚àà‚Ñùùëëùëõùëê. The¬†overall demand for taskmisùêãùëö=[ùêã(ùëê)ùëö;ùêã(ùëõùëê)ùëö]‚àà‚Ñùùëëùëüwithùëëùëü=ùëëùëê+ùëëùëõùëê. Each task further has key attributes including spatial locationùëùùëö‚àà‚Ñù2, priority coefficientùõΩùëö‚àà[0,1], and¬†maximum waiting time (deadline tolerance)ùë°deadùëö. The¬†complete task feature vector isùê±ùëö=[ùêã(ùëê)ùëö;ùêã(ùëõùëê)ùëö;ùëùùëö;ùõΩùëö;ùë°deadùëö]. 3.1.2. UAV Node¬†StateLet the UAV swarm beùí∞={ùë¢1,ùë¢2,‚Ä¶,ùë¢ùëÅ}. The¬†state vector of UAVùë¢ùëõcomprises the remaining energyùëíùëõ‚àà[0,ùê∏maxùëõ], the¬†load of consumable resourcesùê´(ùëê)ùëõ‚àà‚Ñùùëëùëê, the¬†non-consumable capability vectorùê´(ùëõùëê)ùëõ‚àà‚Ñùùëëùëõùëê, the¬†current positionùëùùëõ‚àà‚Ñù2, the¬†maximum flight speedùë£maxùëõ, and¬†a representation of the recent task-history sequenceseqùëõ. Thus, the¬†full UAV state isùê±ùëõ=[ùëíùëõ;ùê´(ùëê)ùëõ;ùê´(ùëõùëê)ùëõ;ùëùùëõ;ùë£maxùëõ;seqùëõ]. 3.1.3. Overlapping Coalition¬†ModelingFor each taskùëáùëö, define its UAV coalition asùíûùëö‚äÜùí∞. The¬†coalition structure for the task setùíØisùíû={ùíû1,‚Ä¶,ùíûùëö,‚Ä¶,ùíûùëÄ}. For any taskùëáùëö, Equation¬†(1) specifies the set of coalition members that allocate resources to it. The¬†resource-allocation vector ofùëáùëöis given in Equation¬†(2), whereùêÄ(ùë¢ùëõ)(ùëáùëö)denotes the resource vector contributed by UAVùë¢ùëõto taskùëáùëöas defined in Equation¬†(3).Coa(ùíú(ùëáùëö))={ùë¢ùëõ‚ààùí∞‚à£ùêÄ(ùë¢ùëõ)(ùëáùëö)‚â†‚àÖ}(1)ùíú(ùëáùëö)={ùêÄ(ùë¢1)(ùëáùëö),‚Ä¶,ùêÄ(ùë¢ùëõ)(ùëáùëö),‚Ä¶,ùêÄ(ùë¢ùëÅ)(ùëáùëö)}(2)ùêÄ(ùë¢ùëõ)(ùëáùëö)={ùõº(1)ùë¢ùëõ,ùëáùëö,‚Ä¶,ùõº(ùëßùëê)ùë¢ùëõ,ùëáùëö,‚Ä¶,ùõΩ(ùëßùëõùëê)ùë¢ùëõ,ùëáùëö,‚Ä¶,ùõΩ(ùëç)ùë¢ùëõ,ùëáùëö}(3)Specifically,ùõº(ùëßùëê)ùë¢ùëõ,ùëáùëödenotes the quantity of consumable resource typeùëßùëêthat UAVùë¢ùëõallocates to taskùëáùëö, andùõΩ(ùëßùëõùëê)ùë¢ùëõ,ùëáùëödenotes the quantity of non-consumable capability typeùëßùëõùëêthat UAVùë¢ùëõallocates to taskùëáùëö. For any UAVùë¢ùëõ‚ààùí∞, the¬†set of tasks it participates in is‚Ñ≥ùëõ={ùëö‚à£ùë¢ùëõ‚ààùíûùëö}. If|‚Ñ≥ùëõ|>1, thenùë¢ùëõbelongs to multiple task coalitions within the planning horizon, and¬†the coalition structure is¬†overlapping.To capture complex overlapping relations and high-order interactions between UAVs and tasks, we formalize the coalition structure as a hypergraphùí¢=(ùí±,‚Ñ∞), where the node setùí±corresponds to the UAV setùí∞and each nodeùë£ùëõ‚ààùí±represents a UAV with featureùê±ùëõ; the hyperedge set‚Ñ∞corresponds to the task setùíØand each hyperedgeùëíùëö‚àà‚Ñ∞connects multiple nodes that participate in taskùëáùëö, with¬†initial hyperedge featureùê±ùëö. A¬†node belonging to multiple hyperedges represents overlapping coalition membership, and¬†a hyperedge connecting multiple nodes represents multi-agent cooperation within a coalition. This hypergraph provides a formal basis for employing a hypergraph attention network to capture high-order relations between UAVs and¬†tasks.",
            "3.1.1. Task Resource¬†Types": "Let the task set beùíØ={ùëá1,ùëá2,‚Ä¶,ùëáùëÄ}. Each taskùëáùëörequires a combination of resources. We categorize resources into two types: consumable and non-consumable. Consumable resources are physical items that are depleted during execution, such as rescue kits, medicines, or¬†spraying agents. Their demand isùêã(ùëê)ùëö‚àà‚Ñùùëëùëê, where each dimension specifies the required quantity of a specific consumable. Non-consumable resources are functional capabilities that support a task without being depleted, such as communication, relay, or information acquisition. Their demand vector isùêã(ùëõùëê)ùëö‚àà‚Ñùùëëùëõùëê. The¬†overall demand for taskmisùêãùëö=[ùêã(ùëê)ùëö;ùêã(ùëõùëê)ùëö]‚àà‚Ñùùëëùëüwithùëëùëü=ùëëùëê+ùëëùëõùëê. Each task further has key attributes including spatial locationùëùùëö‚àà‚Ñù2, priority coefficientùõΩùëö‚àà[0,1], and¬†maximum waiting time (deadline tolerance)ùë°deadùëö. The¬†complete task feature vector isùê±ùëö=[ùêã(ùëê)ùëö;ùêã(ùëõùëê)ùëö;ùëùùëö;ùõΩùëö;ùë°deadùëö].",
            "3.1.2. UAV Node¬†State": "Let the UAV swarm beùí∞={ùë¢1,ùë¢2,‚Ä¶,ùë¢ùëÅ}. The¬†state vector of UAVùë¢ùëõcomprises the remaining energyùëíùëõ‚àà[0,ùê∏maxùëõ], the¬†load of consumable resourcesùê´(ùëê)ùëõ‚àà‚Ñùùëëùëê, the¬†non-consumable capability vectorùê´(ùëõùëê)ùëõ‚àà‚Ñùùëëùëõùëê, the¬†current positionùëùùëõ‚àà‚Ñù2, the¬†maximum flight speedùë£maxùëõ, and¬†a representation of the recent task-history sequenceseqùëõ. Thus, the¬†full UAV state isùê±ùëõ=[ùëíùëõ;ùê´(ùëê)ùëõ;ùê´(ùëõùëê)ùëõ;ùëùùëõ;ùë£maxùëõ;seqùëõ].",
            "3.1.3. Overlapping Coalition¬†Modeling": "For each taskùëáùëö, define its UAV coalition asùíûùëö‚äÜùí∞. The¬†coalition structure for the task setùíØisùíû={ùíû1,‚Ä¶,ùíûùëö,‚Ä¶,ùíûùëÄ}. For any taskùëáùëö, Equation¬†(1) specifies the set of coalition members that allocate resources to it. The¬†resource-allocation vector ofùëáùëöis given in Equation¬†(2), whereùêÄ(ùë¢ùëõ)(ùëáùëö)denotes the resource vector contributed by UAVùë¢ùëõto taskùëáùëöas defined in Equation¬†(3).Coa(ùíú(ùëáùëö))={ùë¢ùëõ‚ààùí∞‚à£ùêÄ(ùë¢ùëõ)(ùëáùëö)‚â†‚àÖ}(1)ùíú(ùëáùëö)={ùêÄ(ùë¢1)(ùëáùëö),‚Ä¶,ùêÄ(ùë¢ùëõ)(ùëáùëö),‚Ä¶,ùêÄ(ùë¢ùëÅ)(ùëáùëö)}(2)ùêÄ(ùë¢ùëõ)(ùëáùëö)={ùõº(1)ùë¢ùëõ,ùëáùëö,‚Ä¶,ùõº(ùëßùëê)ùë¢ùëõ,ùëáùëö,‚Ä¶,ùõΩ(ùëßùëõùëê)ùë¢ùëõ,ùëáùëö,‚Ä¶,ùõΩ(ùëç)ùë¢ùëõ,ùëáùëö}(3) Specifically,ùõº(ùëßùëê)ùë¢ùëõ,ùëáùëödenotes the quantity of consumable resource typeùëßùëêthat UAVùë¢ùëõallocates to taskùëáùëö, andùõΩ(ùëßùëõùëê)ùë¢ùëõ,ùëáùëödenotes the quantity of non-consumable capability typeùëßùëõùëêthat UAVùë¢ùëõallocates to taskùëáùëö. For any UAVùë¢ùëõ‚ààùí∞, the¬†set of tasks it participates in is‚Ñ≥ùëõ={ùëö‚à£ùë¢ùëõ‚ààùíûùëö}. If|‚Ñ≥ùëõ|>1, thenùë¢ùëõbelongs to multiple task coalitions within the planning horizon, and¬†the coalition structure is¬†overlapping. To capture complex overlapping relations and high-order interactions between UAVs and tasks, we formalize the coalition structure as a hypergraphùí¢=(ùí±,‚Ñ∞), where the node setùí±corresponds to the UAV setùí∞and each nodeùë£ùëõ‚ààùí±represents a UAV with featureùê±ùëõ; the hyperedge set‚Ñ∞corresponds to the task setùíØand each hyperedgeùëíùëö‚àà‚Ñ∞connects multiple nodes that participate in taskùëáùëö, with¬†initial hyperedge featureùê±ùëö. A¬†node belonging to multiple hyperedges represents overlapping coalition membership, and¬†a hyperedge connecting multiple nodes represents multi-agent cooperation within a coalition. This hypergraph provides a formal basis for employing a hypergraph attention network to capture high-order relations between UAVs and¬†tasks.",
            "3.2. Multi-Objective¬†Design": "The core of overlapping coalition formation for UAV swarms is to evaluate the overall effectiveness when multiple UAVs execute multiple tasks, which determines whether a coalition structure is optimal. We design three objectives that quantify task fulfillment, coalition synchronization deviation, and¬†operational cost, respectively. Task execution sufficiency: To quantify the consistency between resources allocated to taskùëáùëöand its demands by first computing per-dimension satisfaction ratios. Then, perform within-class aggregation for consumable and non-consumable resources. Finally, we combine the two class-level scores into the overall task measure.For consumable resources, the¬†per-dimension satisfaction ratio is defined asùë†(ùëßùëê)c=min‚éõ‚éù‚éú‚éú‚éúùúÜ(ùëßùëê)ùëáùëöùëô(ùëßùëê)ùëáùëö,1‚éû‚é†‚éü‚éü‚éü,ùëßùëê‚ààùëáùëßùëê,ùëô(ùëßùëê)ùëáùëö>0,(4)whereùúÜ(ùëßùëê)ùëáùëö=‚àëùë¢ùëõ‚ààCoa(ùíú(ùëáùëö))ùõº(ùëßùëê)ùë¢ùëõ,ùëáùëö. Given weights{ùë§ùëßùëê}ùëßùëê‚ààùëáùëßùëêwith‚àëùëßùëê‚ààùëáùëßùëêùë§ùëßùëê=1, we adopt a shortage-sensitive generalized mean for within-class aggregationùêπc(ùëáùëö;ùúåc)=‚éõ‚éù‚éú‚éú‚éú‚àëùëßùëê‚ààùëáùëßùëêùë§ùëßùëê[ùë†(ùëßùëê)c]ùúåc‚éû‚é†‚éü‚éü‚éü1/ùúåc,ùúåc=‚àí1.(5)For non-consumable resources, the¬†per-dimension satisfaction ratio is defined asùë†(ùëßùëõùëê)n=min‚éõ‚éù‚éú‚éú‚éúùúôùëßùëõùëê(ùúÜ(ùëßùëõùëê)ùëáùëö)ùúôùëßùëõùëê(ùëô(ùëßùëõùëê)ùëáùëö),1‚éû‚é†‚éü‚éü‚éü,ùëßùëõùëê‚ààùëáùëßùëõùëê,ùëô(ùëßùëõùëê)ùëáùëö>0,(6)whereùúÜ(ùëßùëõùëê)ùëáùëödenotes the coalition-level equivalent capability on that dimension, andùúôùëßùëõùëêis a monotone mapping to form a dimensionless, larger-is-better scale. Given weights{ùë£ùëßùëõùëê}ùëßùëõùëê‚ààùëáùëßùëõùëêwith‚àëùëßùëõùëê‚ààùëáùëßùëõùëêùë£ùëßùëõùëê=1, the¬†within-class aggregation isùêπn(ùëáùëö)=‚éõ‚éù‚éú‚éú‚éú‚àëùëßùëõùëê‚ààùëáùëßùëõùëêùë£ùëßùëõùëê[ùë†(ùëßùëõùëê)n]ùúån‚éû‚é†‚éü‚éü‚éü1/ùúån,ùúån‚Üí‚àí‚àû.(7)The overall task measure is computed as the product of the two class-level scores,ùëì1=ùêπc(ùëáùëö;ùúåc)√óùêπn(ùëáùëö),ùëì1‚àà[0,1].(8)Coalition synchronization deviation: To represent arrival latency, waiting energy, and¬†deadline constraints under a unified dimension, the¬†precedence relations among tasks are modeled by a directed acyclic graphùí´; when only task priorities are available, they become a total order withinùí´. For¬†memberùë¢ùëõof coalitionùíûùëö, the¬†sequence of tasks that must be completed before executingùëáùëöis defined as:ùñ≤ùñæùóäùëõ=(Assigned(ùë¢ùëõ)‚à©Pred(ùëáùëö))‚à™{ùëáùëö}(9)Based on this sequence, two aggregate quantities are introduced for latency and energy calculation:ùêøùëõdenotes the total flight distance before reachingùëáùëö, andùëÜùëõdenotes the total processing time that must be completed before reachingùëáùëö, defined¬†as:ùêøùëõ=‚àëùëó=1ùêΩùëëùëõ,ùëó,ùëÜùëõ=‚àëùëó=1ùêΩùë†ùëõ,ùëó(10)whereùëëùëõ,ùëóis the flight distance of thej-th leg andùë†ùëõ,ùëóis the processing time that must be completed before that leg; if no processing is required, it is set to zero. Accordingly, the¬†arrival time ofùë¢ùëõcan be written as a function of speed:ùë°arrùëõ(ùë£)=ùêøùëõùë£+ùëÜùëõ(11)Under the speed boundsùë£minùëõ‚â§ùë£‚â§ùë£maxùëõ, taking the slowest path cruising at the maximum admissible speed as the baseline yields the earliest feasible synchronization¬†time:ùë°syn‚àóùëö=maxùë¢ùëõ‚ààùíûùëöùë°arrùëõ(ùë£maxùëõ)=maxùë¢ùëõ‚ààùíûùëö(ùêøùëõùë£maxùëõ+ùëÜùëõ)(12)Givenùë°syn‚àóùëö, each remaining member selects the unique target speed that achieves on-time arrival without violating the bounds:ùë£‚àóùëõ=clip(ùêøùëõùë°syn‚àóùëö‚àíùëÜùëõ,ùë£minùëõ,ùë£maxùëõ),clip(ùë•,ùëé,ùëè)=min{max{ùë•,ùëé},ùëè}(13)If the required speed falls belowùë£minùëõ, the¬†member cruises atùë£minùëõand waits upon arrival; the waiting time is:ùë°hovùëõ=max{0,ùë°syn‚àóùëö‚àíùë°arrùëõ(ùë£minùëõ)}=max{0,ùë°syn‚àóùëö‚àíùêøùëõùë£minùëõ‚àíùëÜùëõ}(14)Consequently, the¬†synchronized start time of the coalition satisfies:ùë°synùëö=ùë°syn‚àóùëö(15)To reflect the energy cost incurred during waiting within a unified metric, letùëùflyùëõ(ùë£)denote the flight power andùëùhovùëõdenote the hovering power, and¬†impose the ordering constraint:ùëùhovùëõ‚â•ùëùflyùëõ(ùë£econùëõ)(16)whereùë£econùëõis an empirical economical cruise speed. The¬†energy expenditure ofùë¢ùëõbeforeùëáùëöis then:ùê∏ùëõ=ùëùflyùëõ(ùë£‚àóùëõ)¬∑ùêøùëõùë£‚àóùëõ+ùëùhovùëõ¬∑ùë°hovùëõ(17)Further, given the deadlineùê∑ùëöof taskùëáùëö, define the lateness as:ùìÅùëö=max{0,ùë°synùëö‚àíùê∑ùëö}(18)Finally, the¬†synchronization latency, the¬†deadline deviation, and¬†the energy differential during waiting are combined via time-equivalent coefficients into a single objective:ùëì2(ùëáùëö)=ùêΩsyn(ùëáùëö)=ùë°synùëö+ùúÜlateùìÅùëö+ùúáhov‚àëùë¢ùëõ‚ààùíûùëö(ùëùhovùëõ‚àíùëùflyùëõ(ùë£‚àóùëõ))ùë°hovùëõ(19)whereùúÜlate‚â•0andùúáhov‚â•0are calibration weights that convert the energy penalty into a time-equivalent cost, allowing it to be summed withùë°synùëöandùìÅùëöunder a unified dimension.Operational cost: To assess the aggregate energy burden incurred by a coalition while completing multiple tasks, Equation¬†(20) defines the operational cost for taskùëáùëö:ùëì3=ùê∏ùëáùëö=‚àëùë¢ùëõ‚ààCoa(ùíúùëáùëö)ùëí(ùë¢ùëõ)ùëáùëö=‚àëùë¢ùëõ‚ààCoa(ùíúùëáùëö)ùê∏(ùë¢ùëõ)‚à•ùêÄ(ùë¢ùëõ)(ùëáùëö)‚à•‚àëùëáùëö‚Ä≤‚ààùíØ‚à•ùêÄ(ùë¢ùëõ)(ùëáùëö‚Ä≤)‚à•,(20)whereùê∏(ùë¢ùëõ)is the total energy consumed by UAVùë¢ùëõto complete all assigned tasks. Sinceùë¢ùëõcontributes heterogeneous resource types and quantities to multiple tasks, we use‚à•ùêÄ(ùë¢ùëõ)(ùëáùëö)‚à•as a workload proxy and proportionally split the total energy to estimate per-task cost. The¬†energy of UAVùë¢ùëõis the sum of hovering and flight energy, given¬†byùê∏ùë¢ùëõ=‚àëùëñ=1ùúÅùëÉ(0)ùë°(h)task(ùëñ)ùë¢ùëõ+‚àëùëñ=1ùúÅùëÉ(ùëâ(fly)task(ùëñ‚àí1)ùë¢ùëõ,task(ùëñ)ùë¢ùëõ)ùë°(fly)task(ùëñ‚àí1)ùë¢ùëõ,task(ùëñ)ùë¢ùëõ.(21)In Equation¬†(21),ùúÅis the length of the task schedule ofùë¢ùëõ, andùëÉ(ùëâ)is the propulsion power of a rotorcraft at constant speedV, computed asùëÉ(ùëâ)=ùëÉùëÜ‚éõ‚éù‚éú‚éú‚éú1+3ùëâ2ùëà2tip‚éû‚é†‚éü‚éü‚éü+ùëÉùëÖ‚éõ‚éù‚éú‚éú‚éú1+ùëâ44ùë£40‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àö‚àíùëâ22ùë£20‚éû‚é†‚éü‚éü‚éü1/2+12ùëì0ùúåùë†0ùúÇùëâ3,(22)we instantiate the energy model with a DJI Matrice 600 Pro hexacopter equipped with 2170R 21 √ó 7 inch propellers; the parameter values used in Equation¬†(22) are listed inTable 1. Table 1.Parameterization of the propulsion power model in Equation¬†(22) for a DJI Matrice 600 Pro hexacopter used to compute flight energy consumption. To concisely characterize the aggregate benefit of taskùëáùëöon the interval[0,1], starting from the previously defined task execution sufficiencyùëì1(ùëáùëö)‚àà[0,1], we map the synchronization deviationùêΩsyn(ùëáùëö)and the energy costùê∏ùëáùëöto sufficiency scores and then couple them into a utility, as¬†follows:ùëìsync(ùëáùëö)=exp(‚àíùõºsynùêΩsyn(ùëáùëö)ùëáref(ùëáùëö)),ùëìeng(ùëáùëö)=exp(‚àíùõºengùê∏ùëáùëöùê∏ref(ùëáùëö)),(23)where by default we setùõºsyn=ùõºeng=ln2, takeùëáref(ùëáùëö)as the deadline scale ofTm, and¬†chooseEref(Tm)as the coalition energy budget or a platform-calibrated upper bound. With¬†non-negative weightsw=(wu,ws,we)satisfyingwu+ws+we=1, the¬†task utility is defined byUtask(Tm;w)=f1(Tm)wufsync(Tm)wsfeng(Tm)we.(24)Consequently,Utask(Tm)‚àà[0,1], increases withf1(Tm), and¬†decreases withJsyn(Tm)andETm.",
            "3.3. Problem¬†Statement": "We consider coalition formation with a heterogeneous UAV‚Äìmulti-task structure in which each task comprises multiple heterogeneous sub-tasks with distinct resource demands, as¬†illustrated inFigure 2. The¬†goal is to optimize overlapping coalition formation under limited resources, dynamic tasks, and¬†local communication. The¬†decision variables are the task‚ÄìUAV assignment structureCand the resource-allocation vectorsA(Tm)(un), yielding an exponentially large combinatorial search space. The¬†problem has three inherently conflicting objectivesf1,f2, andf3. For¬†example, higher task fulfillment typically increases energy consumption, while improving synchronization may conflict with preferred task orders. Hence, the¬†problem is a multi-objective combinatorial optimization with a set of non-dominated solutions. The¬†optimization seeks an overlapping coalition structure with resource allocationsBC(‚àó)={AT1(‚àó),‚Ä¶,ATm(‚àó),‚Ä¶,ATM(‚àó)}that maximizes the overall utility of the task set, as¬†in Equation¬†(25), subject to the constraints in Equation¬†(26).SC*:maxC,A‚àëTm‚ààTsATmminC,A‚àëTm‚ààTtTm(syn)minC,A‚àëTm‚ààTETm(25)subjectto‚àëun‚ààCmrn(c)‚â•Lm(c),‚àÄTm‚ààT,‚àëun‚ààCmrn(nc)‚â•Lm(nc),‚àÄTm‚ààT,tTm(syn)‚â§tTmdead,‚àÄTm‚ààT,‚àëTm‚ààMneTm(un)‚â§Enmax,‚àÄun‚ààU.(26) Figure 2.Example of resource allocation under overlapping coalitions, where heterogeneous UAV resources are assigned to multiple tasks and UAVs transfer between coalitions to reuse residual capacity. In Equation¬†(26), the¬†resource constraints ensure that the sums of consumable and non-consumable provisions from a coalition meet each task‚Äôs demand; the temporal constraint ensures that the synchronization deviation remains within the tolerated waiting time; and the energy constraint ensures that a UAV‚Äôs total energy consumption does not exceed its initial energy. Because¬†the solution space grows combinatorially with the number of tasks and UAVs, obtaining a globally optimal coalition structure is NP-hard. We therefore design a staged solving strategy with reduced computational¬†complexity.",
            "4. Overlapping Coalition Formation¬†Algorithm": "Overlapping coalition formation involves coupled relations among multiple agents and multiple tasks. It must perceive high-order structure, avoid local optima, and¬†control global exploration cost under feasibility constraints. This chapter proposes a coordinated algorithmic framework SGRL-TS. First, we extract the structural semantics of task‚ÄìUAV‚Äìcoalition relations using a bidirectional HAN and utilize structure embeddings to modulate policy parameters, thereby improving generalization. Next, we develop a structure-aware and efficient multi-task cooperative hierarchical value-decomposition algorithm called SHIELD. SHIELD captures member complementarity or conflict via nested nonlinear aggregation within a coalition and models inter-task cooperation or competition via second-order interaction terms across coalitions, thereby yielding the global valueQtot. Finally, we conduct a global search with MCTS guided byQtotand feed high-quality structures back into training through demonstration replay, Q-value distillation, and¬†structure-neighborhood¬†exploration. 4.1. High-Order Structural Modeling and State¬†RepresentationIn overlapping coalition formation for heterogeneous UAVs across multiple tasks, the¬†local observation of a single agent is insufficient to capture the high-order interactions between tasks and UAVs. Decisions are influenced by the task-demand context and by the role that each agent plays within multi-task collaborative structures. To¬†improve policy generalization and coalition organization efficiency, we adopt a bidirectional hypergraph attention network that combines structural awareness with context-induced modulation. The¬†core objective is to disentangle an agent‚Äôs intrinsic capability from the external task context at the representation¬†level.To accommodate structural evolution during planning, we construct a time-varying heterogeneous hypergraphGt=(V,Et)at each decision stept. The¬†node setVdenotes UAVs. The¬†hyperedge setEtdenotes the tasks within the planning scope at stept. A¬†taskTmappears as a hyperedgeem,twith member setCm,t‚äÜV. A¬†node in multiple hyperedge models represents overlapping coalition membership. This structure specifies connectivity, propagation paths, and¬†normalization domains of structural information during policy learning, as¬†illustrated inFigure 3.Figure 3.High -order structural modeling with the bidirectional hypergraph attention network, which encodes UAV nodes and task hyperedges and propagates features along membership and communication links. Here,em(1)denotes the task structural embedding produced by task-to-node aggregation, andhn*denotes the structure-aware node embedding produced by node-to-task aggregation.To restrict attention normalization to valid candidate sets, we introduce three masks for membership, task activation, and¬†UAV availability:Mnmt=1[un‚ààCm,t],Œºmt=1[Tmisinthecurrentplanningscope],Œ∑nt=1[unisschedulable](27)Given the initial node and hyperedge embeddingshn,t(0)andem,t(0), we define a differentiable¬†scoresnmt=LeakyReLUa‚ä§Wvhn,t(0)‚ÄñWeem,t(0)(28)whereWvandWeare learnable mapping matrices,ais a learnable vector, and¬†‚Äñ denotes concatenation. Two complementary attentions are then normalized over the mask-restricted¬†domains.Task-to-node aggregation.This aggregation builds the task representation from candidate members and expressestask selectivity. The attention coefficientsŒ±nmtare normalized over the candidate-member domain:Œ±nmt=exp(snmt)ŒºmtMnmt‚àën‚Ä≤:Œ∑n‚Ä≤tMn‚Ä≤mt=1exp(sn‚Ä≤mt)(29)The task structural embedding is then computed asem,t(1)=ReLU‚àën‚ààCm,tŒ±nmtWvhn,t(0)(30)To make task selectivity explicit, we pose an entropy-regularized matching objective over the simplexŒî(Cm,t):maxŒ±m‚ààŒî(Cm,t)‚àën‚ààCm,tsnmtŒ±nmt‚àíœÑŒ±‚àën‚ààCm,tŒ±nmtlogŒ±nmt(31)whose closed-form solution is the masked softmaxŒ±nmt=expsnmt/œÑŒ±ŒºmtMnmt‚àën‚Ä≤:Œ∑n‚Ä≤tMn‚Ä≤mt=1expsn‚Ä≤mt/œÑŒ±(32)Under a linearization, Equation¬†(30) becomes the convex combinationem,t(1)‚âà‚àën‚ààCm,tŒ±nmtWvhn,t(0)(33)showing that the task embedding aggregates member features with weights proportional to the marginal contribution scores.Node-to-task aggregation.This aggregation fuses information over a node‚Äôs candidate tasks and expressescontextual suitability. The¬†attention coefficientsŒ≤mntare normalized over the candidate-task domain:Œ≤mnt=exp(snmt)Œ∑ntMnmt‚àëm‚Ä≤:Œºm‚Ä≤tMnm‚Ä≤t=1exp(snm‚Ä≤t)(34)The structure-aware node representation ishn,t*=œÉ‚àëm:Mnmt=1Œ≤mntWe‚Ä≤em,t(1)(35)To formalize contextual suitability, we optimize an entropy-regularized utility over the simplexŒî({m:Mnmt=1})using a context utilityun‚Üímt:maxŒ≤n‚ààŒî({m:Mnmt=1})‚àëm:Mnmt=1un‚ÜímtŒ≤mnt‚àíœÑŒ≤‚àëm:Mnmt=1Œ≤mntlogŒ≤mnt(36)whose solution isŒ≤mnt=expun‚Üímt/œÑŒ≤Œ∑ntMnmt‚àëm‚Ä≤:Œºm‚Ä≤tMnm‚Ä≤t=1expun‚Üím‚Ä≤t/œÑŒ≤(37)WithœÑŒ≤=1and a parameterizationun‚Üímt=snmt, Equation¬†(37) reduces to Equation¬†(34). Under¬†the same linearization, Equation¬†(35) becomeshn,t*‚âà‚àëm:Mnmt=1Œ≤mntWe‚Ä≤em,t(1)(38)so that the node embedding is a convex combination of task embeddings weighted by contextual suitability.Operator view of higher-order relations.Substituting Equation¬†(33) into Equation¬†(38) yields, after¬†one bidirectional pass,hn,t*‚âà‚àëm‚àën‚Ä≤Œ≤mntŒ±n‚Ä≤mt‚èücoupledweightWe‚Ä≤Wvhn‚Ä≤,t(0)(39)which explicitly encodes the second-order node‚Äìtask‚Äìnode relation via the multiplicative couplingŒ≤mntŒ±n‚Ä≤mt. LetHt‚àà{0,1}|V|√ó|Et|be the incidence matrix. One bidirectional pass admits the operator approximationh*‚âàBtHtAt‚ä§Ht‚ä§Wh(0)(40)whereAt=diag{Œ±m}mandBt=diag{Œ≤n}nembed masked, within-domain normalizations into the feasible sets, andWabsorbs linear mappings. Hence, bidirectional attention realizes a learnable second-order polynomial operator inHt; stackingLlayers induces up to order2Lpolynomials, which approximate a broad class of higher-order hypergraph convolution kernels and subset functions, thereby capturing UAV‚Äìtask‚Äìcoalition higher-order relations with shallow propagation.To improve discriminability and prevent representation collapse, we introduce a structural diversity regularizer:Ldiv=‚àëm<m‚Ä≤em(1)¬∑em‚Ä≤(1)‚à•em(1)‚à•‚à•em‚Ä≤(1)‚à•2(41)Considering the continual evolution of the structure over time, we employ residual in-step updates to maintain adaptability of the features:hÀún,t=‚àëmŒ±nmtWeem,t,hn,t+1=Normhn,t+œÉ(hÀún,t)(42)eÀúm,t=‚àënŒ≤mntWvhn,t,em,t+1=Normem,t+œÉ(eÀúm,t)(43)whereNorm(¬∑)denotes a normalization operator. Tasks not in the planning scope and UAVs that are not schedulable are automatically excluded from normalization and aggregation through the masks in¬†(27), which stabilizes computation and keeps the cost controlled. For¬†brevity, time subscripts are omitted in the sequel when no ambiguity arises.The pseudocode for HAN forward computation with diversity regularization is presented in Algorithm¬†1.Algorithm 1Bidirectional Hypergraph Attention with Diversity Regularization1:Input:HypergraphG=(V,E); memberships{Cm}and{Mn}; initial features{hn(0)=xn}and{em(0)=xm}; trainable parameters (consistent withSection 4.1)2:Output:Task embeddings{em(1)}; node embeddings{hn*}; diversity regularizerLdiv3:Task-driven aggregation4:form=1toMdo5:Compute{Œ±nm}n‚ààCmby Equation¬†(29)6:Obtainem(1)by Equation¬†(30)7:end for8:Task-to-node context feedback9:forn=1toNdo10:Compute{Œ≤mn}m‚ààMnby Equation¬†(34)11:Updatehn*by Equation¬†(35)12:end for13:Structural diversity regularization14:ComputeLdivby Equation¬†(41)15:Apply membership masking and renormalize within valid sets16:return{em(1)},{hn*},Ldiv 4.2. Joint Policy Learning and Local¬†OptimizationTo address structural role modeling, policy coordination, and¬†global value optimization for heterogeneous UAV swarms with multi-task overlapping coalitions, we propose a structure-aware hierarchical value decomposition algorithm,SHIELD. The¬†algorithm takes as inputs each UAV‚Äôs local observation and the structure-aware node and task embeddings fromSection 4.1, namely the node embeddinghn,t*and the task embeddingem,t(1), and¬†performs encoding and aggregation under the unified masksMnmt(membership),Œºmt(task activation), andŒ∑nt(UAV availability). This design ensures that task selectivity and contextual suitability propagate from the representation layer to the value-decomposition layer. The global valueQtotremains well-defined even with only a partial coalition structure. An¬†overview is shown inFigure 4.Figure 4.SHIELD architecture: A hypergraph encoder, role attribution, and nested coalition and global mixers jointly implement structure-aware hierarchical value decomposition for multi-task coordination.To disentangle node embedding, structural semantics are injected into a role representation. This makes each UAV‚Äôs role and functional positioning explicit within specific task coalitions. Givenhn,t*, the¬†role gate and the role vector are computed aszn,t=œÉMhn,t*,rn,t=tanhUhn,t*+V(hn,t*‚äôzn,t),(44)whereU,V, andMare learnable matrices;œÉ(¬∑)is the sigmoid;tanh(¬∑)is the hyperbolic tangent; and ‚äô denotes the Hadamard product. To¬†transmit task selectivityŒ±nmtand contextual suitabilityŒ≤mntinto downstream aggregation, we convert both into a non-negative member-to-task gate effective only on the feasible domain,Œ∫n‚Üím,t=softpluswŒ±Œ±nmt+wŒ≤Œ≤mnt¬∑Mnmt¬∑Œ∑nt,(45)and renormalize it within each coalition to remove scale effects:Œ∫¬Øn‚Üím,t=Œ∫n‚Üím,t‚àën‚Ä≤:Mn‚Ä≤mt=1Œ∫n‚Ä≤‚Üím,t+Œµ,(46)wherewŒ±andwŒ≤are learnable scalars andŒµis a small constant.Agent-Conditional Value Function.An individual policy must adapt to structural roles and local observations. We define an agent-conditionalQfunction. Here, the¬†role vector modulates both the temporal encoder and the action embedding:Qn,tœÑn,t,an,t;rn,t=MLPGRUœÑn,t;Œ≥(rn,t)‚äïœÜan,t,rn,t,(47)whereœÑn,tis the local history,an,tis the current action,Œ≥(rn,t)generates GRU parameters conditioned on the role,œÜ(an,t,rn,t)encodes actions jointly with roles, and¬†‚äï denotes concatenation.Intra-Coalition Value Decomposer.To capture complementarities and conflicts among coalition members, we adopt a nested nonlinear composition. The¬†role-conditioned contribution of a member isœïQn,t,em,t(1),rn,t=ReLUW2ReLUW1[Qn,t‚Äñem,t(1)‚Äñrn,t]+b1+b2,(48)with learnable parametersW1,W2,b1, andb2, and¬†concatenation[¬∑‚Äñ¬∑]. The¬†gated, role-modulated fusion within coalitionmisŒ®m,t=tanh‚àën:Mnmt=1Œ∫¬Øn‚Üím,trn,t‚äôœïQn,t,em,t(1),rn,t,(49)and the coalition value readsQm,t=fdecem,t(1),Œ®m,t¬∑Œºmt,(50)wherefdec(¬∑)is a small MLP with normalization and bounded activation, andŒºmtmasks inactive tasks.Cross-Task Interaction Aggregator.Tasks exhibit cooperation and competition that cannot be captured by a simple linear mixture. We therefore define a structure-aware interaction aggregator with quadratic and cross terms, restricted to active tasks. The¬†global value isQtot(st,at)=faggst,Œìt,(51)wherestis a global state summary andfagg(¬∑)is a multilayer mapping. The¬†interaction module isŒìt=œÉ‚àëmŒºmtWm(st,em,t(1))Qm,t+Um(st,em,t(1))Qm,t2+‚àëm‚Ä≤‚â†mŒºm‚Ä≤tVm,m‚Ä≤(st)Qm,tQm‚Ä≤,t,(52)with task-wise base weightsWm, quadratic weightsUm, pairwise interaction weightsVm,m‚Ä≤, and¬†the sigmoidœÉ(¬∑).Stable Encoding under Partial Structures.When only a subset of tasks is active, we preserve tensor shapes and statistical stability by using a softened placeholder for inactive¬†tasks:em,t(0)=(1‚àíŒºmt)evac+Œºmte^m,t(0),(53)whereevacis a learnable placeholder ande^m,t(0)is the standard encoder output. Because¬†attention normalizations and gated aggregations are masked, inactive tasks neither appear in denominators nor contribute to value sums.Training Objectives and Optimization.We normalize three performance indicators‚Äîtask fulfillment, coalition synchronization bias, and¬†action cost‚Äîasf^i(t)=fi(t)‚àífiminfimax‚àífimin,(54)and define the reward with a balance-promoting term:R(t)=œâ1f^1(t)Œ±1+œâ2f^2(t)Œ±2+œâ3f^3(t)Œ±3+Œª1‚àístd(f^1(t),f^2(t),f^3(t)),(55)whereœâi‚â•0with‚àëiœâi=1, exponentsŒ±icontrol sensitivity, andŒªweights the balance term. The¬†temporal-difference loss over a mini-batchBisLTD=1|B|‚àë(s,œÑ,a,s‚Ä≤)‚ààBQtot(s,a)‚àíR+Œ≥maxa‚Ä≤Qtot(s‚Ä≤,a‚Ä≤)2,(56)and the attention‚Äìgate alignment regularizer isLcons=‚àëmKLsoftmaxnŒ±nmt1[Mnmt=1]‚à•softmaxnŒ∫¬Øn‚Üím,t,(57)so that the total loss becomesL=LTD+Œ∑divLdiv+Œ∑consLcons,(58)whereŒ∑div,Œ∑cons‚â•0andLdivis defined in¬†(41).We now present the single-step training procedure of SHIELD, which integrates structure-attribution encoding, hierarchical value decomposition, and¬†the multi-objective reward into one parameter update, as¬†shown in Algorithm¬†2.Algorithm 2SHIELD single-step training with structural regularization and distillation1:Input:mini-batchB={(s,œÑ,a,s‚Ä≤)}; HAN outputs{hn*},{em(1)}; discountŒ≥; learning rateŒ∑; regularization weightsŒ∑div,Œ∑cons; distillation weight scheduleŒªdist(t); soft-update factorœÑ2:Output:updated SHIELD parameters and target networks3:foreach sample(s,œÑ,a,s‚Ä≤)‚ààBdo4:Compute role vectorsrnvia Equation¬†(44)5:Compute agent-conditional valuesQn(œÑn,an;rn)via Equation¬†(47)6:foreach coalition (task)mdo7:Compute coalition valueQmusing Equations¬†(48)‚Äì(50)8:end for9:Fuse to global valueQtot(s,a)via Equations¬†(51) and (52)10:Computef1,f2,f3via Equations¬†(8)‚Äì(20); normalize with Equation¬†(54); form rewardRvia Equation¬†(55)11:TD target:y‚ÜêR+Œ≥maxa‚Ä≤Qtot(s‚Ä≤,a‚Ä≤;target)12:Per-sample TD loss:LTD(i)‚ÜêQtot(s,a)‚àíy213:end for14:Average TD loss:LTD‚Üê1|B|‚àëiLTD(i)15:Compute attention‚Äìgating consistency lossLconsvia Equation¬†(57) onB16:ifHAN diversity regularizer availablethen17:ObtainLdivvia Equation¬†(41)18:else19:Ldiv‚Üê020:end if21:ifMCTS distillation targets availablethen22:ComputeLdistvia Equation¬†(70)23:else24:Ldist‚Üê025:end if26:Total loss:L‚ÜêLTD+Œ∑consLcons+Œªdist(t)Ldist+Œ∑divLdiv27:Update SHIELD parameters by gradient descent with learning rateŒ∑28:Soft update targets:target‚ÜêœÑcurrent+(1‚àíœÑ)target29:returnupdated SHIELD parameters and targets 4.3. Global Optimization and Policy¬†FeedbackTo overcome the combinatorial explosion of overlapping coalition structures and the tendency of local search to get trapped in suboptimal regions, we augment the structure-aware hierarchical value decomposition in the previous section with MCTS for global structure optimization, and¬†feed the discovered high-quality structural evidence back to policy learning, thereby forming a closed loop of structure prior, feasible-set pruning, global evaluation, and¬†policy feedback. The¬†key idea is to use the masked encoding of a time-varying hypergraph as a bridge so that partially assigned structures receive stable and comparable value estimates within the bidirectional hypergraph attention and hierarchical value decomposition networks; structure priors then guide the tree policy, incremental constraint checks maintain real-time feasibility, and¬†Pareto-nondominated solutions are transformed into demonstrations and distillation signals to jointly accelerate policy convergence and improve global quality. The¬†workflow is shown inFigure 5.Figure 5.Pipeline of the MCTS-based global structure optimizer, where SHIELD values guide feasible coalition expansions and Pareto optimal structures are fed back as demonstrations and distillation targets.4.3.1. Structure-Prior and Feasible-Set‚ÄìConstrained Global¬†OptimizerIn MCTS, each nodesrepresents a partially realized resource allocationC1:m; that is, the¬†coalitionsCmand their resource assignmentsATmfor the firstmtasks. The¬†root is ‚àÖ, indicating no task has been assigned; expanding tochild(s,a)selects a coalitionCm+1and resource planATm+1for taskm+ 1, and¬†the full depth isM, whose leaves encode a complete overlapping coalitionC={C1,‚Ä¶,CM}. To¬†uniformly encode partial structures, we reuse the masked mechanism inSection 4.1: inactive tasks use the learnable placeholder embedding in (53), while attention and gating normalize only over mask-restricted domains per (27) and (46); thus, anyC1:mcan be stably fed into SHIELD to evaluateQtot.During node selection, we adopt PUCT with a structure prior,PUCT(s,a)=Q¬Ø(s,a)+cpuctPstr(s,a)N(s)1+N(s,a),(59)whereQ¬Ø(s,a)is the running mean of backed-up action values,N(s)andN(s,a)are visit counts, andcpuctcontrols exploration. The¬†priorPstr(s,a)aggregates the gating strengths of the candidate coalition for taskm+1and is softmax-normalized. Letg(Cm+1)=‚àën‚ààCm+1Œ∫¬Øn‚Üí(m+1),twithŒ∫¬Øn‚Üí(m+1),tdefined by (46); thenPstr(s,a)=expg(Cm+1)/œÑp‚àëCm+1‚Ä≤expg(Cm+1‚Ä≤)/œÑp,(60)whereœÑpis a temperature. To¬†prevent branch explosion, we use progressive widening to limit the branching factor,|A(s)|‚â§cpwN(s)Œ∂,0<Œ∂<1,(61)with growth tuned bycpwandŒ∂; if the cap is not reached, high-prior candidates are expanded first, otherwise children are chosen by (59).One-step expansion modifies only a small subset of resources and links related to the new task, so we perform incremental checks rather than full recomputation. For¬†each UAVun, we maintain residual resources and accumulated costs, and¬†update them incrementally when expanding toCm+1. Energy feasibility uses a ‚Äúmaneuver + workload‚Äù decomposition withŒîEn=ŒîEnfly+ŒîEntask, imposingEnused+ŒîEn‚â§(1‚àíŒ¥E)Enmax,(62)whereŒ¥E‚àà[0.05,0.15]is a safety margin. Bandwidth feasibility uses an optimistic expectation of available throughput: if taskTmrequiresBmandE[Bmavail]=‚àë(i,j)‚ààLmqij,tRij,t, thenE[Bmavail]‚â•(1+Œ¥B)Bm,(63)whereqij,tis the link success probability andRij,tthe achievable rate, andŒ¥Bis a redundancy factor. End-to-end latency is approximated by computation plus communication delayLm=Lmcomp+LmcommwithLmcomm‚âàDm/E[Bmavail]for data sizeDm, yieldingLm‚â§(1‚àíŒ¥L)Lmmax.(64)Beyond early rejection by (62)‚Äì(64), we define a layerwise optimistic upper bound for unassigned tasks at nodes,UB(s)=QtotSHIELD(s)+‚àëm‚Ä≤>mŒºm‚Ä≤+Q^m‚Ä≤ub,(65)whereQtotSHIELD(s)is the current estimate for the partial structure,Q^m‚Ä≤ubis obtained by a conflict-ignorant greedy assignment, andŒºm‚Ä≤+flags tasks still within scope. IfUB(s)is below the incumbent best, the¬†branch is cut; at the same depth andm, candidates dominated in both residual resources andQtotSHIELDare removed, realizing a triple pruning of feasibility, upper bound, and¬†dominance.Upon reaching a leaf or a cutoff depth, we greedily complete remaining tasks with a lightweight policy and use SHIELD‚Äôs joint value as a rollout proxy; visit counts are then updated andQ¬Ø(s,a)is backed up by a moving average. This proxy is scale-consistent with (65), maintaining evaluation coherence under limited budgets.4.3.2. Policy Feedback: Demonstration Replay and Structure-Consistent Q-Value¬†DistillationMCTS solutions are not used as one-shot offline references; instead, they are injected into the learning loop under a controlled cadence so that global optimization continuously feeds local policy updates. Let an MCTS run be triggered whenever the interaction steps reachKmcts(t), with¬†a search budget ofBmcts(t)simulations; after search, we performUmcts(t)parameter updates where each mini-batch mixes demonstration and environment samples with ratioœÅdemo(t). To¬†rely more on structural evidence early and anneal toward autonomous improvement later, we use the scheduleKmcts(t)=K01+t/TK,Bmcts(t)=B01+t/TB,œÅdemo(t)=œÅ0e‚àít/TœÅ,Œªdist(t)=Œª0(1+t/TŒª)‚àí1,(66)where(K0,B0,œÅ0,Œª0)are initial values and(TK,TB,TœÅ,TŒª)are time scales. This guarantees that triggers and budgets gradually increase as the policy matures, whereas the demonstration ratio and distillation weight decay to avoid long-term reliance on external supervision.The update count per trigger scales with budget asUmcts(t)=U0+Œ∫Bmcts(t),Œ∫‚àà[0.1,0.3],(67)whereU0is a base count andŒ∫controls coupling between search and learning. With¬†mini-batch sizeB, we sample from demonstration and environment buffers with a fixed mix,|Bdemo|=œÅdemo(t)B,|Benv|=1‚àíœÅdemo(t)B.(68)Demonstrations come from the union of current and past nondominated solutions, each mapped to a scalar return via the hypervolume of multi-objective scores and decoded into joint actions and transition tuples for storage. To¬†improve sample efficiency, we use prioritized sampling that balances TD error and structural prior: if sampleihas TD errorŒ¥iand a priorœÄimcts‚àà[0,1]derived from structural gating and hypervolume rank, thenpi=Œ±per|Œ¥i|+(1‚àíŒ±per)œÄimcts,Œ±per‚àà[0,1],(69)and sampling is drawn proportionally to the normalizedpiacross both buffers.Beyond the TD loss and the attention‚Äìgating consistency regularizer, we introduce a structure-consistent distillation term to align marginal contributions in the value decomposition with structural evidence from search. Let the coalition value for taskTmbeQm,tand its leave-one-out counterfactual beQm,t‚àñn; define the estimated marginal contributionŒî^n,m=Qm,t‚àíQm,t‚àñn, and¬†letŒîn,mmctsbe the normalized contribution returned by search on the same scale. The¬†distillation loss isLdist=Qtot‚àíymcts2+ŒªŒî‚àëm‚àën‚ààCmŒî^n,m‚àíŒîn,mmcts2,(70)whereymctsis an unbiased backup target andŒªŒîis a weight. The¬†per-step objective becomesL=LTD+Œ∑consLcons+Œªdist(t)Ldist,(71)whereLTDis the joint-value TD loss,Lconsaligns attention distributions with downstream gating to preserve the structural semantics of task selectivity and contextual suitability, andŒªdist(t)follows (66) to emphasize structural supervision early and gradually release policy self-organization later. The¬†pseudocode of this MCTS-guided policy feedback loop is given in Algorithm¬†3.Algorithm 3MCTS-guided policy feedback with scheduled demonstration and distillation1:Input:current interaction stept; environment bufferDenv; demonstration bufferDdemo; HAN‚ÄìSHIELD networks; hyperparametersK0,B0,œÅ0,Œª0,U0,Œ∫,TK,TB,TœÅ,TŒª,Œ±per; mini-batch sizeB2:Output:updated replay buffers and SHIELD parameters3:ComputeKmcts(t),Bmcts(t),œÅdemo(t),Œªdist(t)via Equation¬†(66)4:ComputeUmcts(t)via Equation¬†(67)5:iftis a multiple ofKmcts(t)then6:Run budgeted MCTS withBmcts(t)simulations using SHIELD values and feasibility pruning; obtain nondominated solutionsSMCTS7:foreach solution(C(i),A(i))‚ààSMCTSdo8:Decode into trajectories(su(i),au(i),Ru(i),su‚Ä≤(i))9:For each stepu, compute multi-objective scores and hypervolume-based returnyumcts, and structural contributions{Œîn,m,umcts}10:Store(su(i),au(i),Ru(i),su‚Ä≤(i),yumcts,{Œîn,m,umcts})inDdemo11:end for12:foru=1toUmcts(t)do13:Set|Bdemo|=œÅdemo(t)Band|Benv|=(1‚àíœÅdemo(t))Bby Equation¬†(68)14:For each sampleiinDenv‚à™Ddemo, compute TD errorŒ¥iand structural priorœÄimcts; set prioritypivia Equation¬†(69)15:SampleBdemofromDdemoandBenvfromDenvproportionally topi; form mini-batchB=Bdemo‚à™Benv16:OnB, computeLTD,Lcons, andLdivas inSection 4.217:For samples with MCTS labels, computeLdistvia Equation¬†(70); setLdist=0otherwise18:Form total lossLby Equation¬†(71) and update SHIELD parameters and targets with learning rateŒ∑19:end for20:end if21:returnDenv,Ddemo, and updated networks 4.4. Complexity and Real-Time¬†FeasibilityTo render the overall framework deployable, we formalize the end-to-end latency, derive module-wise asymptotic costs, and state the resulting real-time criterion and scalability conditions.LetN=|V|be the number of UAVs,Mact‚â§Mthe number of active tasks in the current planning scope,S=‚àëm|Cm|the total member count across active task coalitions,dthe embedding dimension,Lthe number of HAN layers,Cmaxa per-task coalition-size cap, andkthe sparsity level for cross-task interactions.4.4.1. End-to-End Latency and Real-Time¬†CriterionWe decompose a single decision period asTe2e=THAN+TSHIELD+TMCTS+TI/O,TMCTS‚âàBTeval+Tcheck,(72)whereBis the search budget,Tevalis the time to evaluate SHIELD on a partial structure, andTcheckis the incremental feasibility checking time (energy, bandwidth, latency updated only for affected UAVs and links). Given a control periodTc, real-time feasibility requiresTe2e‚â§Tc‚áíB‚â§Tc‚àíTHAN+TSHIELD+TI/OTeval+Tcheck.(73)This bound links real-time feasibility to tunable parameters(B,k,Cmax)for pre-deployment selection and online adaptive pruning.4.4.2. Asymptotic Costs by¬†ModuleHAN.Bidirectional hypergraph attention uses masked normalizations restricted to candidate sets. With oneL-layer pass, the cost isTHAN=OLNd2+Mactd2+Sd,(74)whereNd2andMactd2arise from affine mappings for nodes and hyperedges, andSdfrom member‚Äìtask scoring plus masked softmax. Masking confines normalization to valid candidates and avoids unnecessary global normalization.SHIELD.Role attribution and agent-conditionalQincurO(Nd2); the nested intra-coalition composition costsO(Sd2); cross-task interactions, when sparsified to at mostkneighbors per task, reduce the na√ØveO(Mact2d2)toTSHIELD=ONd2+Sd2+kMactd2,S=O(MactCmax)ifCmaxisbounded.(75)Budgeted MCTS.With progressive widening|A(s)|‚â§cpwN(s)Œ∂for0<Œ∂<1and layer-wise upper-bound pruning, the per-decision search time isTMCTS=OBTSHIELDpartial+Tcheck,(76)whereTSHIELDpartialhas the same order as (75) but is evaluated on a partial structure withMpart‚â§Mact; masking stabilizes tensor shape and scale during partial evaluations.4.4.3. Conditional Near-Linear¬†ScalabilityWhend,L,Cmax,kare engineering constants and masking ensuresMact‚â™M, we obtainTHAN+TSHIELD=ON+Mact.(77)Combining (73) with (77) yields end-to-end latency that scales near-linearly with(N,Mact), whileB,k,andCmaxserve as explicit knobs to trade search effort for throughput under the hard constraintTc.",
            "4.1. High-Order Structural Modeling and State¬†Representation": "In overlapping coalition formation for heterogeneous UAVs across multiple tasks, the¬†local observation of a single agent is insufficient to capture the high-order interactions between tasks and UAVs. Decisions are influenced by the task-demand context and by the role that each agent plays within multi-task collaborative structures. To¬†improve policy generalization and coalition organization efficiency, we adopt a bidirectional hypergraph attention network that combines structural awareness with context-induced modulation. The¬†core objective is to disentangle an agent‚Äôs intrinsic capability from the external task context at the representation¬†level. To accommodate structural evolution during planning, we construct a time-varying heterogeneous hypergraphGt=(V,Et)at each decision stept. The¬†node setVdenotes UAVs. The¬†hyperedge setEtdenotes the tasks within the planning scope at stept. A¬†taskTmappears as a hyperedgeem,twith member setCm,t‚äÜV. A¬†node in multiple hyperedge models represents overlapping coalition membership. This structure specifies connectivity, propagation paths, and¬†normalization domains of structural information during policy learning, as¬†illustrated inFigure 3. Figure 3.High -order structural modeling with the bidirectional hypergraph attention network, which encodes UAV nodes and task hyperedges and propagates features along membership and communication links. Here,em(1)denotes the task structural embedding produced by task-to-node aggregation, andhn*denotes the structure-aware node embedding produced by node-to-task aggregation. To restrict attention normalization to valid candidate sets, we introduce three masks for membership, task activation, and¬†UAV availability:Mnmt=1[un‚ààCm,t],Œºmt=1[Tmisinthecurrentplanningscope],Œ∑nt=1[unisschedulable](27) Given the initial node and hyperedge embeddingshn,t(0)andem,t(0), we define a differentiable¬†scoresnmt=LeakyReLUa‚ä§Wvhn,t(0)‚ÄñWeem,t(0)(28)whereWvandWeare learnable mapping matrices,ais a learnable vector, and¬†‚Äñ denotes concatenation. Two complementary attentions are then normalized over the mask-restricted¬†domains. Task-to-node aggregation.This aggregation builds the task representation from candidate members and expressestask selectivity. The attention coefficientsŒ±nmtare normalized over the candidate-member domain:Œ±nmt=exp(snmt)ŒºmtMnmt‚àën‚Ä≤:Œ∑n‚Ä≤tMn‚Ä≤mt=1exp(sn‚Ä≤mt)(29)The task structural embedding is then computed asem,t(1)=ReLU‚àën‚ààCm,tŒ±nmtWvhn,t(0)(30)To make task selectivity explicit, we pose an entropy-regularized matching objective over the simplexŒî(Cm,t):maxŒ±m‚ààŒî(Cm,t)‚àën‚ààCm,tsnmtŒ±nmt‚àíœÑŒ±‚àën‚ààCm,tŒ±nmtlogŒ±nmt(31)whose closed-form solution is the masked softmaxŒ±nmt=expsnmt/œÑŒ±ŒºmtMnmt‚àën‚Ä≤:Œ∑n‚Ä≤tMn‚Ä≤mt=1expsn‚Ä≤mt/œÑŒ±(32)Under a linearization, Equation¬†(30) becomes the convex combinationem,t(1)‚âà‚àën‚ààCm,tŒ±nmtWvhn,t(0)(33)showing that the task embedding aggregates member features with weights proportional to the marginal contribution scores.Node-to-task aggregation.This aggregation fuses information over a node‚Äôs candidate tasks and expressescontextual suitability. The¬†attention coefficientsŒ≤mntare normalized over the candidate-task domain:Œ≤mnt=exp(snmt)Œ∑ntMnmt‚àëm‚Ä≤:Œºm‚Ä≤tMnm‚Ä≤t=1exp(snm‚Ä≤t)(34)The structure-aware node representation ishn,t*=œÉ‚àëm:Mnmt=1Œ≤mntWe‚Ä≤em,t(1)(35)To formalize contextual suitability, we optimize an entropy-regularized utility over the simplexŒî({m:Mnmt=1})using a context utilityun‚Üímt:maxŒ≤n‚ààŒî({m:Mnmt=1})‚àëm:Mnmt=1un‚ÜímtŒ≤mnt‚àíœÑŒ≤‚àëm:Mnmt=1Œ≤mntlogŒ≤mnt(36)whose solution isŒ≤mnt=expun‚Üímt/œÑŒ≤Œ∑ntMnmt‚àëm‚Ä≤:Œºm‚Ä≤tMnm‚Ä≤t=1expun‚Üím‚Ä≤t/œÑŒ≤(37)WithœÑŒ≤=1and a parameterizationun‚Üímt=snmt, Equation¬†(37) reduces to Equation¬†(34). Under¬†the same linearization, Equation¬†(35) becomeshn,t*‚âà‚àëm:Mnmt=1Œ≤mntWe‚Ä≤em,t(1)(38)so that the node embedding is a convex combination of task embeddings weighted by contextual suitability. Operator view of higher-order relations.Substituting Equation¬†(33) into Equation¬†(38) yields, after¬†one bidirectional pass,hn,t*‚âà‚àëm‚àën‚Ä≤Œ≤mntŒ±n‚Ä≤mt‚èücoupledweightWe‚Ä≤Wvhn‚Ä≤,t(0)(39)which explicitly encodes the second-order node‚Äìtask‚Äìnode relation via the multiplicative couplingŒ≤mntŒ±n‚Ä≤mt. LetHt‚àà{0,1}|V|√ó|Et|be the incidence matrix. One bidirectional pass admits the operator approximationh*‚âàBtHtAt‚ä§Ht‚ä§Wh(0)(40)whereAt=diag{Œ±m}mandBt=diag{Œ≤n}nembed masked, within-domain normalizations into the feasible sets, andWabsorbs linear mappings. Hence, bidirectional attention realizes a learnable second-order polynomial operator inHt; stackingLlayers induces up to order2Lpolynomials, which approximate a broad class of higher-order hypergraph convolution kernels and subset functions, thereby capturing UAV‚Äìtask‚Äìcoalition higher-order relations with shallow propagation. To improve discriminability and prevent representation collapse, we introduce a structural diversity regularizer:Ldiv=‚àëm<m‚Ä≤em(1)¬∑em‚Ä≤(1)‚à•em(1)‚à•‚à•em‚Ä≤(1)‚à•2(41) Considering the continual evolution of the structure over time, we employ residual in-step updates to maintain adaptability of the features:hÀún,t=‚àëmŒ±nmtWeem,t,hn,t+1=Normhn,t+œÉ(hÀún,t)(42)eÀúm,t=‚àënŒ≤mntWvhn,t,em,t+1=Normem,t+œÉ(eÀúm,t)(43)whereNorm(¬∑)denotes a normalization operator. Tasks not in the planning scope and UAVs that are not schedulable are automatically excluded from normalization and aggregation through the masks in¬†(27), which stabilizes computation and keeps the cost controlled. For¬†brevity, time subscripts are omitted in the sequel when no ambiguity arises. The pseudocode for HAN forward computation with diversity regularization is presented in Algorithm¬†1.Algorithm 1Bidirectional Hypergraph Attention with Diversity Regularization1:Input:HypergraphG=(V,E); memberships{Cm}and{Mn}; initial features{hn(0)=xn}and{em(0)=xm}; trainable parameters (consistent withSection 4.1)2:Output:Task embeddings{em(1)}; node embeddings{hn*}; diversity regularizerLdiv3:Task-driven aggregation4:form=1toMdo5:Compute{Œ±nm}n‚ààCmby Equation¬†(29)6:Obtainem(1)by Equation¬†(30)7:end for8:Task-to-node context feedback9:forn=1toNdo10:Compute{Œ≤mn}m‚ààMnby Equation¬†(34)11:Updatehn*by Equation¬†(35)12:end for13:Structural diversity regularization14:ComputeLdivby Equation¬†(41)15:Apply membership masking and renormalize within valid sets16:return{em(1)},{hn*},Ldiv",
            "4.2. Joint Policy Learning and Local¬†Optimization": "To address structural role modeling, policy coordination, and¬†global value optimization for heterogeneous UAV swarms with multi-task overlapping coalitions, we propose a structure-aware hierarchical value decomposition algorithm,SHIELD. The¬†algorithm takes as inputs each UAV‚Äôs local observation and the structure-aware node and task embeddings fromSection 4.1, namely the node embeddinghn,t*and the task embeddingem,t(1), and¬†performs encoding and aggregation under the unified masksMnmt(membership),Œºmt(task activation), andŒ∑nt(UAV availability). This design ensures that task selectivity and contextual suitability propagate from the representation layer to the value-decomposition layer. The global valueQtotremains well-defined even with only a partial coalition structure. An¬†overview is shown inFigure 4. Figure 4.SHIELD architecture: A hypergraph encoder, role attribution, and nested coalition and global mixers jointly implement structure-aware hierarchical value decomposition for multi-task coordination. To disentangle node embedding, structural semantics are injected into a role representation. This makes each UAV‚Äôs role and functional positioning explicit within specific task coalitions. Givenhn,t*, the¬†role gate and the role vector are computed aszn,t=œÉMhn,t*,rn,t=tanhUhn,t*+V(hn,t*‚äôzn,t),(44)whereU,V, andMare learnable matrices;œÉ(¬∑)is the sigmoid;tanh(¬∑)is the hyperbolic tangent; and ‚äô denotes the Hadamard product. To¬†transmit task selectivityŒ±nmtand contextual suitabilityŒ≤mntinto downstream aggregation, we convert both into a non-negative member-to-task gate effective only on the feasible domain,Œ∫n‚Üím,t=softpluswŒ±Œ±nmt+wŒ≤Œ≤mnt¬∑Mnmt¬∑Œ∑nt,(45)and renormalize it within each coalition to remove scale effects:Œ∫¬Øn‚Üím,t=Œ∫n‚Üím,t‚àën‚Ä≤:Mn‚Ä≤mt=1Œ∫n‚Ä≤‚Üím,t+Œµ,(46)wherewŒ±andwŒ≤are learnable scalars andŒµis a small constant. Agent-Conditional Value Function.An individual policy must adapt to structural roles and local observations. We define an agent-conditionalQfunction. Here, the¬†role vector modulates both the temporal encoder and the action embedding:Qn,tœÑn,t,an,t;rn,t=MLPGRUœÑn,t;Œ≥(rn,t)‚äïœÜan,t,rn,t,(47)whereœÑn,tis the local history,an,tis the current action,Œ≥(rn,t)generates GRU parameters conditioned on the role,œÜ(an,t,rn,t)encodes actions jointly with roles, and¬†‚äï denotes concatenation.Intra-Coalition Value Decomposer.To capture complementarities and conflicts among coalition members, we adopt a nested nonlinear composition. The¬†role-conditioned contribution of a member isœïQn,t,em,t(1),rn,t=ReLUW2ReLUW1[Qn,t‚Äñem,t(1)‚Äñrn,t]+b1+b2,(48)with learnable parametersW1,W2,b1, andb2, and¬†concatenation[¬∑‚Äñ¬∑]. The¬†gated, role-modulated fusion within coalitionmisŒ®m,t=tanh‚àën:Mnmt=1Œ∫¬Øn‚Üím,trn,t‚äôœïQn,t,em,t(1),rn,t,(49)and the coalition value readsQm,t=fdecem,t(1),Œ®m,t¬∑Œºmt,(50)wherefdec(¬∑)is a small MLP with normalization and bounded activation, andŒºmtmasks inactive tasks.Cross-Task Interaction Aggregator.Tasks exhibit cooperation and competition that cannot be captured by a simple linear mixture. We therefore define a structure-aware interaction aggregator with quadratic and cross terms, restricted to active tasks. The¬†global value isQtot(st,at)=faggst,Œìt,(51)wherestis a global state summary andfagg(¬∑)is a multilayer mapping. The¬†interaction module isŒìt=œÉ‚àëmŒºmtWm(st,em,t(1))Qm,t+Um(st,em,t(1))Qm,t2+‚àëm‚Ä≤‚â†mŒºm‚Ä≤tVm,m‚Ä≤(st)Qm,tQm‚Ä≤,t,(52)with task-wise base weightsWm, quadratic weightsUm, pairwise interaction weightsVm,m‚Ä≤, and¬†the sigmoidœÉ(¬∑).Stable Encoding under Partial Structures.When only a subset of tasks is active, we preserve tensor shapes and statistical stability by using a softened placeholder for inactive¬†tasks:em,t(0)=(1‚àíŒºmt)evac+Œºmte^m,t(0),(53)whereevacis a learnable placeholder ande^m,t(0)is the standard encoder output. Because¬†attention normalizations and gated aggregations are masked, inactive tasks neither appear in denominators nor contribute to value sums.Training Objectives and Optimization.We normalize three performance indicators‚Äîtask fulfillment, coalition synchronization bias, and¬†action cost‚Äîasf^i(t)=fi(t)‚àífiminfimax‚àífimin,(54)and define the reward with a balance-promoting term:R(t)=œâ1f^1(t)Œ±1+œâ2f^2(t)Œ±2+œâ3f^3(t)Œ±3+Œª1‚àístd(f^1(t),f^2(t),f^3(t)),(55)whereœâi‚â•0with‚àëiœâi=1, exponentsŒ±icontrol sensitivity, andŒªweights the balance term. The¬†temporal-difference loss over a mini-batchBisLTD=1|B|‚àë(s,œÑ,a,s‚Ä≤)‚ààBQtot(s,a)‚àíR+Œ≥maxa‚Ä≤Qtot(s‚Ä≤,a‚Ä≤)2,(56)and the attention‚Äìgate alignment regularizer isLcons=‚àëmKLsoftmaxnŒ±nmt1[Mnmt=1]‚à•softmaxnŒ∫¬Øn‚Üím,t,(57)so that the total loss becomesL=LTD+Œ∑divLdiv+Œ∑consLcons,(58)whereŒ∑div,Œ∑cons‚â•0andLdivis defined in¬†(41). We now present the single-step training procedure of SHIELD, which integrates structure-attribution encoding, hierarchical value decomposition, and¬†the multi-objective reward into one parameter update, as¬†shown in Algorithm¬†2.Algorithm 2SHIELD single-step training with structural regularization and distillation1:Input:mini-batchB={(s,œÑ,a,s‚Ä≤)}; HAN outputs{hn*},{em(1)}; discountŒ≥; learning rateŒ∑; regularization weightsŒ∑div,Œ∑cons; distillation weight scheduleŒªdist(t); soft-update factorœÑ2:Output:updated SHIELD parameters and target networks3:foreach sample(s,œÑ,a,s‚Ä≤)‚ààBdo4:Compute role vectorsrnvia Equation¬†(44)5:Compute agent-conditional valuesQn(œÑn,an;rn)via Equation¬†(47)6:foreach coalition (task)mdo7:Compute coalition valueQmusing Equations¬†(48)‚Äì(50)8:end for9:Fuse to global valueQtot(s,a)via Equations¬†(51) and (52)10:Computef1,f2,f3via Equations¬†(8)‚Äì(20); normalize with Equation¬†(54); form rewardRvia Equation¬†(55)11:TD target:y‚ÜêR+Œ≥maxa‚Ä≤Qtot(s‚Ä≤,a‚Ä≤;target)12:Per-sample TD loss:LTD(i)‚ÜêQtot(s,a)‚àíy213:end for14:Average TD loss:LTD‚Üê1|B|‚àëiLTD(i)15:Compute attention‚Äìgating consistency lossLconsvia Equation¬†(57) onB16:ifHAN diversity regularizer availablethen17:ObtainLdivvia Equation¬†(41)18:else19:Ldiv‚Üê020:end if21:ifMCTS distillation targets availablethen22:ComputeLdistvia Equation¬†(70)23:else24:Ldist‚Üê025:end if26:Total loss:L‚ÜêLTD+Œ∑consLcons+Œªdist(t)Ldist+Œ∑divLdiv27:Update SHIELD parameters by gradient descent with learning rateŒ∑28:Soft update targets:target‚ÜêœÑcurrent+(1‚àíœÑ)target29:returnupdated SHIELD parameters and targets",
            "4.3. Global Optimization and Policy¬†Feedback": "To overcome the combinatorial explosion of overlapping coalition structures and the tendency of local search to get trapped in suboptimal regions, we augment the structure-aware hierarchical value decomposition in the previous section with MCTS for global structure optimization, and¬†feed the discovered high-quality structural evidence back to policy learning, thereby forming a closed loop of structure prior, feasible-set pruning, global evaluation, and¬†policy feedback. The¬†key idea is to use the masked encoding of a time-varying hypergraph as a bridge so that partially assigned structures receive stable and comparable value estimates within the bidirectional hypergraph attention and hierarchical value decomposition networks; structure priors then guide the tree policy, incremental constraint checks maintain real-time feasibility, and¬†Pareto-nondominated solutions are transformed into demonstrations and distillation signals to jointly accelerate policy convergence and improve global quality. The¬†workflow is shown inFigure 5. Figure 5.Pipeline of the MCTS-based global structure optimizer, where SHIELD values guide feasible coalition expansions and Pareto optimal structures are fed back as demonstrations and distillation targets. 4.3.1. Structure-Prior and Feasible-Set‚ÄìConstrained Global¬†OptimizerIn MCTS, each nodesrepresents a partially realized resource allocationC1:m; that is, the¬†coalitionsCmand their resource assignmentsATmfor the firstmtasks. The¬†root is ‚àÖ, indicating no task has been assigned; expanding tochild(s,a)selects a coalitionCm+1and resource planATm+1for taskm+ 1, and¬†the full depth isM, whose leaves encode a complete overlapping coalitionC={C1,‚Ä¶,CM}. To¬†uniformly encode partial structures, we reuse the masked mechanism inSection 4.1: inactive tasks use the learnable placeholder embedding in (53), while attention and gating normalize only over mask-restricted domains per (27) and (46); thus, anyC1:mcan be stably fed into SHIELD to evaluateQtot.During node selection, we adopt PUCT with a structure prior,PUCT(s,a)=Q¬Ø(s,a)+cpuctPstr(s,a)N(s)1+N(s,a),(59)whereQ¬Ø(s,a)is the running mean of backed-up action values,N(s)andN(s,a)are visit counts, andcpuctcontrols exploration. The¬†priorPstr(s,a)aggregates the gating strengths of the candidate coalition for taskm+1and is softmax-normalized. Letg(Cm+1)=‚àën‚ààCm+1Œ∫¬Øn‚Üí(m+1),twithŒ∫¬Øn‚Üí(m+1),tdefined by (46); thenPstr(s,a)=expg(Cm+1)/œÑp‚àëCm+1‚Ä≤expg(Cm+1‚Ä≤)/œÑp,(60)whereœÑpis a temperature. To¬†prevent branch explosion, we use progressive widening to limit the branching factor,|A(s)|‚â§cpwN(s)Œ∂,0<Œ∂<1,(61)with growth tuned bycpwandŒ∂; if the cap is not reached, high-prior candidates are expanded first, otherwise children are chosen by (59).One-step expansion modifies only a small subset of resources and links related to the new task, so we perform incremental checks rather than full recomputation. For¬†each UAVun, we maintain residual resources and accumulated costs, and¬†update them incrementally when expanding toCm+1. Energy feasibility uses a ‚Äúmaneuver + workload‚Äù decomposition withŒîEn=ŒîEnfly+ŒîEntask, imposingEnused+ŒîEn‚â§(1‚àíŒ¥E)Enmax,(62)whereŒ¥E‚àà[0.05,0.15]is a safety margin. Bandwidth feasibility uses an optimistic expectation of available throughput: if taskTmrequiresBmandE[Bmavail]=‚àë(i,j)‚ààLmqij,tRij,t, thenE[Bmavail]‚â•(1+Œ¥B)Bm,(63)whereqij,tis the link success probability andRij,tthe achievable rate, andŒ¥Bis a redundancy factor. End-to-end latency is approximated by computation plus communication delayLm=Lmcomp+LmcommwithLmcomm‚âàDm/E[Bmavail]for data sizeDm, yieldingLm‚â§(1‚àíŒ¥L)Lmmax.(64)Beyond early rejection by (62)‚Äì(64), we define a layerwise optimistic upper bound for unassigned tasks at nodes,UB(s)=QtotSHIELD(s)+‚àëm‚Ä≤>mŒºm‚Ä≤+Q^m‚Ä≤ub,(65)whereQtotSHIELD(s)is the current estimate for the partial structure,Q^m‚Ä≤ubis obtained by a conflict-ignorant greedy assignment, andŒºm‚Ä≤+flags tasks still within scope. IfUB(s)is below the incumbent best, the¬†branch is cut; at the same depth andm, candidates dominated in both residual resources andQtotSHIELDare removed, realizing a triple pruning of feasibility, upper bound, and¬†dominance.Upon reaching a leaf or a cutoff depth, we greedily complete remaining tasks with a lightweight policy and use SHIELD‚Äôs joint value as a rollout proxy; visit counts are then updated andQ¬Ø(s,a)is backed up by a moving average. This proxy is scale-consistent with (65), maintaining evaluation coherence under limited budgets. 4.3.2. Policy Feedback: Demonstration Replay and Structure-Consistent Q-Value¬†DistillationMCTS solutions are not used as one-shot offline references; instead, they are injected into the learning loop under a controlled cadence so that global optimization continuously feeds local policy updates. Let an MCTS run be triggered whenever the interaction steps reachKmcts(t), with¬†a search budget ofBmcts(t)simulations; after search, we performUmcts(t)parameter updates where each mini-batch mixes demonstration and environment samples with ratioœÅdemo(t). To¬†rely more on structural evidence early and anneal toward autonomous improvement later, we use the scheduleKmcts(t)=K01+t/TK,Bmcts(t)=B01+t/TB,œÅdemo(t)=œÅ0e‚àít/TœÅ,Œªdist(t)=Œª0(1+t/TŒª)‚àí1,(66)where(K0,B0,œÅ0,Œª0)are initial values and(TK,TB,TœÅ,TŒª)are time scales. This guarantees that triggers and budgets gradually increase as the policy matures, whereas the demonstration ratio and distillation weight decay to avoid long-term reliance on external supervision.The update count per trigger scales with budget asUmcts(t)=U0+Œ∫Bmcts(t),Œ∫‚àà[0.1,0.3],(67)whereU0is a base count andŒ∫controls coupling between search and learning. With¬†mini-batch sizeB, we sample from demonstration and environment buffers with a fixed mix,|Bdemo|=œÅdemo(t)B,|Benv|=1‚àíœÅdemo(t)B.(68)Demonstrations come from the union of current and past nondominated solutions, each mapped to a scalar return via the hypervolume of multi-objective scores and decoded into joint actions and transition tuples for storage. To¬†improve sample efficiency, we use prioritized sampling that balances TD error and structural prior: if sampleihas TD errorŒ¥iand a priorœÄimcts‚àà[0,1]derived from structural gating and hypervolume rank, thenpi=Œ±per|Œ¥i|+(1‚àíŒ±per)œÄimcts,Œ±per‚àà[0,1],(69)and sampling is drawn proportionally to the normalizedpiacross both buffers.Beyond the TD loss and the attention‚Äìgating consistency regularizer, we introduce a structure-consistent distillation term to align marginal contributions in the value decomposition with structural evidence from search. Let the coalition value for taskTmbeQm,tand its leave-one-out counterfactual beQm,t‚àñn; define the estimated marginal contributionŒî^n,m=Qm,t‚àíQm,t‚àñn, and¬†letŒîn,mmctsbe the normalized contribution returned by search on the same scale. The¬†distillation loss isLdist=Qtot‚àíymcts2+ŒªŒî‚àëm‚àën‚ààCmŒî^n,m‚àíŒîn,mmcts2,(70)whereymctsis an unbiased backup target andŒªŒîis a weight. The¬†per-step objective becomesL=LTD+Œ∑consLcons+Œªdist(t)Ldist,(71)whereLTDis the joint-value TD loss,Lconsaligns attention distributions with downstream gating to preserve the structural semantics of task selectivity and contextual suitability, andŒªdist(t)follows (66) to emphasize structural supervision early and gradually release policy self-organization later. The¬†pseudocode of this MCTS-guided policy feedback loop is given in Algorithm¬†3.Algorithm 3MCTS-guided policy feedback with scheduled demonstration and distillation1:Input:current interaction stept; environment bufferDenv; demonstration bufferDdemo; HAN‚ÄìSHIELD networks; hyperparametersK0,B0,œÅ0,Œª0,U0,Œ∫,TK,TB,TœÅ,TŒª,Œ±per; mini-batch sizeB2:Output:updated replay buffers and SHIELD parameters3:ComputeKmcts(t),Bmcts(t),œÅdemo(t),Œªdist(t)via Equation¬†(66)4:ComputeUmcts(t)via Equation¬†(67)5:iftis a multiple ofKmcts(t)then6:Run budgeted MCTS withBmcts(t)simulations using SHIELD values and feasibility pruning; obtain nondominated solutionsSMCTS7:foreach solution(C(i),A(i))‚ààSMCTSdo8:Decode into trajectories(su(i),au(i),Ru(i),su‚Ä≤(i))9:For each stepu, compute multi-objective scores and hypervolume-based returnyumcts, and structural contributions{Œîn,m,umcts}10:Store(su(i),au(i),Ru(i),su‚Ä≤(i),yumcts,{Œîn,m,umcts})inDdemo11:end for12:foru=1toUmcts(t)do13:Set|Bdemo|=œÅdemo(t)Band|Benv|=(1‚àíœÅdemo(t))Bby Equation¬†(68)14:For each sampleiinDenv‚à™Ddemo, compute TD errorŒ¥iand structural priorœÄimcts; set prioritypivia Equation¬†(69)15:SampleBdemofromDdemoandBenvfromDenvproportionally topi; form mini-batchB=Bdemo‚à™Benv16:OnB, computeLTD,Lcons, andLdivas inSection 4.217:For samples with MCTS labels, computeLdistvia Equation¬†(70); setLdist=0otherwise18:Form total lossLby Equation¬†(71) and update SHIELD parameters and targets with learning rateŒ∑19:end for20:end if21:returnDenv,Ddemo, and updated networks",
            "4.3.1. Structure-Prior and Feasible-Set‚ÄìConstrained Global¬†Optimizer": "In MCTS, each nodesrepresents a partially realized resource allocationC1:m; that is, the¬†coalitionsCmand their resource assignmentsATmfor the firstmtasks. The¬†root is ‚àÖ, indicating no task has been assigned; expanding tochild(s,a)selects a coalitionCm+1and resource planATm+1for taskm+ 1, and¬†the full depth isM, whose leaves encode a complete overlapping coalitionC={C1,‚Ä¶,CM}. To¬†uniformly encode partial structures, we reuse the masked mechanism inSection 4.1: inactive tasks use the learnable placeholder embedding in (53), while attention and gating normalize only over mask-restricted domains per (27) and (46); thus, anyC1:mcan be stably fed into SHIELD to evaluateQtot. During node selection, we adopt PUCT with a structure prior,PUCT(s,a)=Q¬Ø(s,a)+cpuctPstr(s,a)N(s)1+N(s,a),(59)whereQ¬Ø(s,a)is the running mean of backed-up action values,N(s)andN(s,a)are visit counts, andcpuctcontrols exploration. The¬†priorPstr(s,a)aggregates the gating strengths of the candidate coalition for taskm+1and is softmax-normalized. Letg(Cm+1)=‚àën‚ààCm+1Œ∫¬Øn‚Üí(m+1),twithŒ∫¬Øn‚Üí(m+1),tdefined by (46); thenPstr(s,a)=expg(Cm+1)/œÑp‚àëCm+1‚Ä≤expg(Cm+1‚Ä≤)/œÑp,(60)whereœÑpis a temperature. To¬†prevent branch explosion, we use progressive widening to limit the branching factor,|A(s)|‚â§cpwN(s)Œ∂,0<Œ∂<1,(61)with growth tuned bycpwandŒ∂; if the cap is not reached, high-prior candidates are expanded first, otherwise children are chosen by (59). One-step expansion modifies only a small subset of resources and links related to the new task, so we perform incremental checks rather than full recomputation. For¬†each UAVun, we maintain residual resources and accumulated costs, and¬†update them incrementally when expanding toCm+1. Energy feasibility uses a ‚Äúmaneuver + workload‚Äù decomposition withŒîEn=ŒîEnfly+ŒîEntask, imposingEnused+ŒîEn‚â§(1‚àíŒ¥E)Enmax,(62)whereŒ¥E‚àà[0.05,0.15]is a safety margin. Bandwidth feasibility uses an optimistic expectation of available throughput: if taskTmrequiresBmandE[Bmavail]=‚àë(i,j)‚ààLmqij,tRij,t, thenE[Bmavail]‚â•(1+Œ¥B)Bm,(63)whereqij,tis the link success probability andRij,tthe achievable rate, andŒ¥Bis a redundancy factor. End-to-end latency is approximated by computation plus communication delayLm=Lmcomp+LmcommwithLmcomm‚âàDm/E[Bmavail]for data sizeDm, yieldingLm‚â§(1‚àíŒ¥L)Lmmax.(64) Beyond early rejection by (62)‚Äì(64), we define a layerwise optimistic upper bound for unassigned tasks at nodes,UB(s)=QtotSHIELD(s)+‚àëm‚Ä≤>mŒºm‚Ä≤+Q^m‚Ä≤ub,(65)whereQtotSHIELD(s)is the current estimate for the partial structure,Q^m‚Ä≤ubis obtained by a conflict-ignorant greedy assignment, andŒºm‚Ä≤+flags tasks still within scope. IfUB(s)is below the incumbent best, the¬†branch is cut; at the same depth andm, candidates dominated in both residual resources andQtotSHIELDare removed, realizing a triple pruning of feasibility, upper bound, and¬†dominance. Upon reaching a leaf or a cutoff depth, we greedily complete remaining tasks with a lightweight policy and use SHIELD‚Äôs joint value as a rollout proxy; visit counts are then updated andQ¬Ø(s,a)is backed up by a moving average. This proxy is scale-consistent with (65), maintaining evaluation coherence under limited budgets.",
            "4.3.2. Policy Feedback: Demonstration Replay and Structure-Consistent Q-Value¬†Distillation": "MCTS solutions are not used as one-shot offline references; instead, they are injected into the learning loop under a controlled cadence so that global optimization continuously feeds local policy updates. Let an MCTS run be triggered whenever the interaction steps reachKmcts(t), with¬†a search budget ofBmcts(t)simulations; after search, we performUmcts(t)parameter updates where each mini-batch mixes demonstration and environment samples with ratioœÅdemo(t). To¬†rely more on structural evidence early and anneal toward autonomous improvement later, we use the scheduleKmcts(t)=K01+t/TK,Bmcts(t)=B01+t/TB,œÅdemo(t)=œÅ0e‚àít/TœÅ,Œªdist(t)=Œª0(1+t/TŒª)‚àí1,(66)where(K0,B0,œÅ0,Œª0)are initial values and(TK,TB,TœÅ,TŒª)are time scales. This guarantees that triggers and budgets gradually increase as the policy matures, whereas the demonstration ratio and distillation weight decay to avoid long-term reliance on external supervision. The update count per trigger scales with budget asUmcts(t)=U0+Œ∫Bmcts(t),Œ∫‚àà[0.1,0.3],(67)whereU0is a base count andŒ∫controls coupling between search and learning. With¬†mini-batch sizeB, we sample from demonstration and environment buffers with a fixed mix,|Bdemo|=œÅdemo(t)B,|Benv|=1‚àíœÅdemo(t)B.(68) Demonstrations come from the union of current and past nondominated solutions, each mapped to a scalar return via the hypervolume of multi-objective scores and decoded into joint actions and transition tuples for storage. To¬†improve sample efficiency, we use prioritized sampling that balances TD error and structural prior: if sampleihas TD errorŒ¥iand a priorœÄimcts‚àà[0,1]derived from structural gating and hypervolume rank, thenpi=Œ±per|Œ¥i|+(1‚àíŒ±per)œÄimcts,Œ±per‚àà[0,1],(69)and sampling is drawn proportionally to the normalizedpiacross both buffers. Beyond the TD loss and the attention‚Äìgating consistency regularizer, we introduce a structure-consistent distillation term to align marginal contributions in the value decomposition with structural evidence from search. Let the coalition value for taskTmbeQm,tand its leave-one-out counterfactual beQm,t‚àñn; define the estimated marginal contributionŒî^n,m=Qm,t‚àíQm,t‚àñn, and¬†letŒîn,mmctsbe the normalized contribution returned by search on the same scale. The¬†distillation loss isLdist=Qtot‚àíymcts2+ŒªŒî‚àëm‚àën‚ààCmŒî^n,m‚àíŒîn,mmcts2,(70)whereymctsis an unbiased backup target andŒªŒîis a weight. The¬†per-step objective becomesL=LTD+Œ∑consLcons+Œªdist(t)Ldist,(71)whereLTDis the joint-value TD loss,Lconsaligns attention distributions with downstream gating to preserve the structural semantics of task selectivity and contextual suitability, andŒªdist(t)follows (66) to emphasize structural supervision early and gradually release policy self-organization later. The¬†pseudocode of this MCTS-guided policy feedback loop is given in Algorithm¬†3.Algorithm 3MCTS-guided policy feedback with scheduled demonstration and distillation1:Input:current interaction stept; environment bufferDenv; demonstration bufferDdemo; HAN‚ÄìSHIELD networks; hyperparametersK0,B0,œÅ0,Œª0,U0,Œ∫,TK,TB,TœÅ,TŒª,Œ±per; mini-batch sizeB2:Output:updated replay buffers and SHIELD parameters3:ComputeKmcts(t),Bmcts(t),œÅdemo(t),Œªdist(t)via Equation¬†(66)4:ComputeUmcts(t)via Equation¬†(67)5:iftis a multiple ofKmcts(t)then6:Run budgeted MCTS withBmcts(t)simulations using SHIELD values and feasibility pruning; obtain nondominated solutionsSMCTS7:foreach solution(C(i),A(i))‚ààSMCTSdo8:Decode into trajectories(su(i),au(i),Ru(i),su‚Ä≤(i))9:For each stepu, compute multi-objective scores and hypervolume-based returnyumcts, and structural contributions{Œîn,m,umcts}10:Store(su(i),au(i),Ru(i),su‚Ä≤(i),yumcts,{Œîn,m,umcts})inDdemo11:end for12:foru=1toUmcts(t)do13:Set|Bdemo|=œÅdemo(t)Band|Benv|=(1‚àíœÅdemo(t))Bby Equation¬†(68)14:For each sampleiinDenv‚à™Ddemo, compute TD errorŒ¥iand structural priorœÄimcts; set prioritypivia Equation¬†(69)15:SampleBdemofromDdemoandBenvfromDenvproportionally topi; form mini-batchB=Bdemo‚à™Benv16:OnB, computeLTD,Lcons, andLdivas inSection 4.217:For samples with MCTS labels, computeLdistvia Equation¬†(70); setLdist=0otherwise18:Form total lossLby Equation¬†(71) and update SHIELD parameters and targets with learning rateŒ∑19:end for20:end if21:returnDenv,Ddemo, and updated networks",
            "4.4. Complexity and Real-Time¬†Feasibility": "To render the overall framework deployable, we formalize the end-to-end latency, derive module-wise asymptotic costs, and state the resulting real-time criterion and scalability conditions. LetN=|V|be the number of UAVs,Mact‚â§Mthe number of active tasks in the current planning scope,S=‚àëm|Cm|the total member count across active task coalitions,dthe embedding dimension,Lthe number of HAN layers,Cmaxa per-task coalition-size cap, andkthe sparsity level for cross-task interactions. 4.4.1. End-to-End Latency and Real-Time¬†CriterionWe decompose a single decision period asTe2e=THAN+TSHIELD+TMCTS+TI/O,TMCTS‚âàBTeval+Tcheck,(72)whereBis the search budget,Tevalis the time to evaluate SHIELD on a partial structure, andTcheckis the incremental feasibility checking time (energy, bandwidth, latency updated only for affected UAVs and links). Given a control periodTc, real-time feasibility requiresTe2e‚â§Tc‚áíB‚â§Tc‚àíTHAN+TSHIELD+TI/OTeval+Tcheck.(73)This bound links real-time feasibility to tunable parameters(B,k,Cmax)for pre-deployment selection and online adaptive pruning. 4.4.2. Asymptotic Costs by¬†ModuleHAN.Bidirectional hypergraph attention uses masked normalizations restricted to candidate sets. With oneL-layer pass, the cost isTHAN=OLNd2+Mactd2+Sd,(74)whereNd2andMactd2arise from affine mappings for nodes and hyperedges, andSdfrom member‚Äìtask scoring plus masked softmax. Masking confines normalization to valid candidates and avoids unnecessary global normalization.SHIELD.Role attribution and agent-conditionalQincurO(Nd2); the nested intra-coalition composition costsO(Sd2); cross-task interactions, when sparsified to at mostkneighbors per task, reduce the na√ØveO(Mact2d2)toTSHIELD=ONd2+Sd2+kMactd2,S=O(MactCmax)ifCmaxisbounded.(75)Budgeted MCTS.With progressive widening|A(s)|‚â§cpwN(s)Œ∂for0<Œ∂<1and layer-wise upper-bound pruning, the per-decision search time isTMCTS=OBTSHIELDpartial+Tcheck,(76)whereTSHIELDpartialhas the same order as (75) but is evaluated on a partial structure withMpart‚â§Mact; masking stabilizes tensor shape and scale during partial evaluations. 4.4.3. Conditional Near-Linear¬†ScalabilityWhend,L,Cmax,kare engineering constants and masking ensuresMact‚â™M, we obtainTHAN+TSHIELD=ON+Mact.(77)Combining (73) with (77) yields end-to-end latency that scales near-linearly with(N,Mact), whileB,k,andCmaxserve as explicit knobs to trade search effort for throughput under the hard constraintTc.",
            "4.4.1. End-to-End Latency and Real-Time¬†Criterion": "We decompose a single decision period asTe2e=THAN+TSHIELD+TMCTS+TI/O,TMCTS‚âàBTeval+Tcheck,(72)whereBis the search budget,Tevalis the time to evaluate SHIELD on a partial structure, andTcheckis the incremental feasibility checking time (energy, bandwidth, latency updated only for affected UAVs and links). Given a control periodTc, real-time feasibility requiresTe2e‚â§Tc‚áíB‚â§Tc‚àíTHAN+TSHIELD+TI/OTeval+Tcheck.(73)This bound links real-time feasibility to tunable parameters(B,k,Cmax)for pre-deployment selection and online adaptive pruning.",
            "4.4.2. Asymptotic Costs by¬†Module": "HAN.Bidirectional hypergraph attention uses masked normalizations restricted to candidate sets. With oneL-layer pass, the cost isTHAN=OLNd2+Mactd2+Sd,(74)whereNd2andMactd2arise from affine mappings for nodes and hyperedges, andSdfrom member‚Äìtask scoring plus masked softmax. Masking confines normalization to valid candidates and avoids unnecessary global normalization.SHIELD.Role attribution and agent-conditionalQincurO(Nd2); the nested intra-coalition composition costsO(Sd2); cross-task interactions, when sparsified to at mostkneighbors per task, reduce the na√ØveO(Mact2d2)toTSHIELD=ONd2+Sd2+kMactd2,S=O(MactCmax)ifCmaxisbounded.(75)Budgeted MCTS.With progressive widening|A(s)|‚â§cpwN(s)Œ∂for0<Œ∂<1and layer-wise upper-bound pruning, the per-decision search time isTMCTS=OBTSHIELDpartial+Tcheck,(76)whereTSHIELDpartialhas the same order as (75) but is evaluated on a partial structure withMpart‚â§Mact; masking stabilizes tensor shape and scale during partial evaluations.",
            "4.4.3. Conditional Near-Linear¬†Scalability": "Whend,L,Cmax,kare engineering constants and masking ensuresMact‚â™M, we obtainTHAN+TSHIELD=ON+Mact.(77) Combining (73) with (77) yields end-to-end latency that scales near-linearly with(N,Mact), whileB,k,andCmaxserve as explicit knobs to trade search effort for throughput under the hard constraintTc.",
            "5. Experimental Results and¬†Analysis": "To verify the effectiveness and adaptability of the proposed overlapping coalition formation method for heterogeneous UAV swarms, we design multiple simulation scenarios of different scales that cover a range of task densities, resource tightness, and UAV configurations. All experiments are conducted on a Windows 11 system, equipped with an Intel Core i9-13900K processor (3.00 GHz base frequency), two NVIDIA GeForce RTX 4090 GPUs with 24 GB of VRAM each, and 128.0 GB of RAM. 5.1. Simulation¬†SetupTo evaluate SGRL-TS, we build a simulator with heterogeneous tasks and multiple UAV platform types. We configure five representative tasks and specify their resource requirements inTable 2. To meet these demands, we design four UAV platform types, with their capabilities summarized inTable 3. The platforms emphasize support, delivery, firefighting, and communication relay. To ensure realism and reproducibility, we specify the environment and communication parameters, including area size, communication radius, energy model, and weights for the multi-objective reward, as listed inTable 4.Table 2.Resource requirements of representative tasks.Table 3.Capabilities of heterogeneous UAV platform types.Table 4.Simulation environment parameters.To highlight the advantages of SGRL-TS, we compare it against the following methods:PGG-TS-OCF[17] establishes preference relations between UAV supply and task demand, guides tabu search via preference gravity, and achieves overlapping allocation and scheduling with a stable overlapping coalition.LocalSearch-CF[15] greedily seeds UAV‚Äìtask coalitions and iteratively swaps or reassigns UAVs between neighboring coalitions to minimize mission completion time.HYGMA[20] dynamically clusters nearby UAVs based on state and task affinity, treating each cluster as a coalition and learning joint actions through hypergraph attention.SMART[23] enables each UAV to repeatedly switch its serving task coalition according to a locally evaluated payoff, yielding a distributed best response coalition formation.RCFG-DRL[26] models UAV‚Äìtask assignment as a repeated coalition game, where UAVs learn join or leave decisions via deep RL until coalitions reach equilibrium. 5.2. Performance¬†EvaluationFigure 6reports convergence and final utilities under four resource conditions. SGRL-TS consistently outperforms the strongest baseline, PGG-TS-OCF, by 3.19%, 4.49%, 6.25%, and 9.68% in the abundant, balanced, constrained, and scarce settings, respectively. It enters the efficient ascent earlier and shows a smoother plateau. The gains arise from heterogeneous hypergraph attention, which captures high-order couplings among UAVs, tasks, and coalitions; a structure-conditioned hierarchical value decomposition that yields globally comparable, monotone scores and suppresses merge-split oscillations; and budgeted MCTS under feasibility masks, which focuses expansions on high-value structures and reduces wasted search.Figure 6.Convergence of SGRL-TS and baselines across four resource regimes: (a) 18 UAVs‚Äì5 tasks (resource-abundant); (b) 20 UAVs‚Äì10 tasks (resource-balanced); (c) 15 UAVs‚Äì15 tasks (resource-constrained); (d) 10 UAVs‚Äì20 tasks (resource-scarce).Baseline behavior clarifies the gaps. PGG-TS-OCF employs a parallel population search that identifies feasible overlaps early on; however, it stalls at suboptimal mixes under budget constraints and lacks a stable cross-level yardstick. HYGMA strengthens interaction modelling, but long value propagation under multiple constraints slows ascent. RCFG-DRL introduces adversarial robustness; however, nonstationarity induces mid-horizon oscillations, diverting budget from structural improvements. LocalSearch-CF and SMART use anytime stepwise moves bounded by small neighborhoods, which promote local optima and capped utility. Non-overlapping SGRL-TS forbids resource reuse, creating capacity bottlenecks, synchronization penalties, and heightening sensitivity to task density and temporal perturbations.We next examine resource utilization efficiency, task adaptability, and robustness under overload and scarcity by varying the number of UAVs from 4 to 20 under different task loads, as shown inFigure 7. Across the four task scales, the curve of SGRL-TS remains at the top and reaches a higher peak near moderate swarm sizes; the standard deviation bars show markedly lower variability compared to all baselines. Relative to the strongest competitor, PGG-TS-OCF, SGRL-TS achieves higher peak utilities by approximately 2.27%, 3.00%, 6.36%, and 9.76% at task numbers 5, 10, 15, and 20, respectively. The advantage increases as resource constraints tighten, indicating stronger resource scheduling and parallel coordination under crowded and scarce conditions.Figure 7.Average task utility versus the number of UAVs under different task loads: (a)T=5; (b)T=10; (c)T=15; (d)T=20.This performance stems from a careful consideration of the benefit‚Äìcost balance. SGRL-TS estimates timing and energy constraints online and maps them onto a unified utility scale, which steers the search toward ranges where adding UAVs yields net gains while suppressing ineffective parallelism and communication congestion as scale grows, thus avoiding high-scale regression. Temperature-controlled sampling and reuse of candidate structures broaden exploration, and as early and later convergence occurs, they converge to low-conflict configurations. Combined with penalties and pruning for repeated assignment and resource contention, these mechanisms reduce structural oscillation and tail-phase jitter. The result is a better compromise between task completion and coordination cost, leading to higher and more stable final utility.We further test adaptability under sparse and dense tasks, reuse efficiency, and robustness to task pressure by fixingN‚àà{8,12,16}and increasing the number of tasks from 3 to 12. Results are shown inFigure 8. SGRL-TS stays on top across the three UAV scales and reaches a higher peak near the midrange of task counts, while the tail declines more gently and the variability remains smaller. Compared with the strongest baseline PGG-TS-OCF, the average utility over the full range improves by about 3.86% at 16 UAVs, 2.97% at 12 UAVs, and 3.53% at 8 UAVs.Figure 8.Average task utility versus the number of tasks under different swarm sizes: (a)N=16UAVs; (b)N=12UAVs; (c)N=8UAVs.This advantage and stability arise because bidirectional hypergraph attention normalizes task selectivity and contextual suitability within mask-constrained candidate sets, enabling precise member screening and task assignment as the number of tasks increases, which suppresses ineffective overlaps and resource contention. The SHIELD nested nonlinear aggregation with cross-task interaction terms provides a monotonic and comparable global value for cooperation and competition across tasks, making diminishing marginal returns detectable as the task load grows. This concentrates resources on actions with positive net gain, producing a higher midrange peak and slower performance decay.To assess task completion under scaling, we vary the tasks from 4 to 12 withN=15,10,5. Results are inFigure 9. Across the three experimental settings, the SGRL-TS curve remains closely aligned with the upper bound provided by the Task-completion OCF baseline, which optimizes only task fulfillment. As the number of tasks increases from 4 to 12, it improves the average task execution sufficiency over the entire range by approximately 2.42%, 2.63%, and 10.94% relative to PGG-TS-OCF. Moreover, when other methods exhibit pronounced degradation at higher task counts, SGRL-TS shows a significantly slower decline and can maintain a larger fraction of tasks close to complete execution, even under severely constrained resources. This advantage primarily stems from the balance term in the reward, which discourages extreme solutions that sacrifice a subset of tasks, thereby driving the policy to maintain medium to high completion levels across more tasks as the task load increases.Figure 9.Average task completion versus the number of tasks for three UAV swarm sizes: (a) UAV=15; (b) UAV=10; (c) UAV=5.Figure 10a‚Äìc report the coalition-level temporal coordination performance of all methods. The evaluation metric is the coalition synchronization sufficiency, defined as the normalized scorefsync‚àà[0,1]obtained from the coalition arrival-time deviation costJsynaccording to (23); larger values indicate more synchronized coalition arrivals under the given reference time scale. As the number of tasks increases, the synchronization sufficiency of all methods decreases overall, indicating that higher task congestion makes it harder for coalitions to achieve good temporal coordination; moreover, when the number of UAVs is reduced from 15 to 5, the overall degradation in synchronization performance becomes more pronounced. The advantage of SGRL-TS in synchronization sufficiency is most pronounced in configurations with more tasks and tighter resources, suggesting that structure-guided overlapping coalition formation combined with joint value decomposition can effectively suppress the dispersion in coalition arrival times. In contrast, non-overlapping SGRL-TS and Task-completion OCF, which only focus on task completion rate, exhibit significantly lower synchronization sufficiency under high-load scenarios, indicating that ignoring overlapping structures or lacking explicit synchronization modeling leads to markedly degraded temporal coordination among coalitions.Figure 10.Average coalition synchronization sufficiency versus the number of tasks under three UAV swarm sizes: (a) UAV=15; (b) UAV=10; (c) UAV=5. The metricfsync‚àà[0,1]is obtained by normalizing the coalition arrival-time deviation cost according to (23), with larger values indicating more synchronized coalition arrivals.Previous experiments on task completion and utility have shown that SGRL-TS achieves returns close to the Task-completion OCF upper bound and outperforms PGG-TS-OCF. This section further examines its cost side from the perspective of energy utilization. As shown inFigure 11, under four task scalesT=5,10,15,20, the energy-efficiency curves of SGRL-TS lie consistently above those of all baselines. Compared with PGG-TS-OCF, the average energy efficiency over the entire UAV range improves by approximately 8.09%, 8.02%, 3.05%, and 12.85%, respectively. Moreover, relative to the Task-completion OCF scheme, which optimizes only task completion, SGRL-TS achieves comparable completion levels while attaining between two and five times higher energy efficiency, thereby providing a considerably more economical way of sustaining overlapping coalition structures from the energy consumption viewpoint. Overall, this advantage primarily stems from incorporating energy safety margins and residual-resource-driven feasible-set pruning into the MCTS search guided by SHIELD evaluations, which reduces the expansion of high-cost, overlapping structures at the search level, and thus markedly improves global energy utilization without sacrificing task completion.Figure 11.Energy efficiency versus the number of UAVs under different task loads: (a)T=5; (b)T=10; (c)T=15; (d)T=20.To examine the sensitivity of average task utility and method ranking to multi-objective weight settings, task-priority scenarios, and time/energy normalization scales, we conducted comparative experiments in a scenario with 10 tasks and 20 UAVs, as shown inFigure 12. Under fixed network parameters and training configurations, we change only the weight vectorœâ(k)=(wcomp,wsyn,weng)at evaluation time to assess the weight sensitivity of the multi-objective design; inFigure 12b, under the default settingœâ(1)=(0.4,0.3,0.3), we keep the training process unchanged and modify only the task-priority directed acyclic graph to construct three scenarios,Balanced priorities,Rescue priority, andCommunication priority, with the horizontal axis corresponding to these three priority configurations, respectively; inFigure 12c, we scale the normalization ranges of the synchronization reference timeTrefand energy reference valueErefand, in turn, investigate the impact of five combinations on the final task utility, where the horizontal axis corresponds to these five(T,E)configurations.Figure 12.Sensitivity of average task utility to (a) multi-objective weight vectorsœâ(k)=(wcomp,wsyn,weng), (b) task-priority scenarios, and (c) normalization scales(T,E)of synchronization and energy costs in the 10-task, 20-UAV resource-neutral setting. Each curve reports the mean utility with standard-deviation error bars for SGRL-TS, HYGMA, PGG-TS-OCF, and RDFG-DRL.Across all configurations of the three sensitivity tests, SGRL-TS consistently attains the highest average task utility, with a performance gap of approximately 2‚Äì3 percentage points relative to the best-performing baseline PGG-TS-OCF, and exhibits substantially more minor variance, indicating overall robustness to perturbations in weights, priority settings, and normalization scales. Specifically, inFigure 12a, the completion-emphasized weight vectorœâ(2)increases the utilities of all four methods, whereas the synchronization- and energy-emphasized weight vectorsœâ(3)andœâ(4)reduce the overall utilities, with a particularly pronounced impact on RDFG-DRL, while the curve of SGRL-TS exhibits only mild fluctuations;Figure 12b shows that different task-priority topologies induce only slight changes in the utilities of all methods, and SGRL-TS can still better coordinate resources and synchronization constraints in the rescue-priority scenario, maintaining a stable performance lead; inFigure 12c, scalingTreforErefmainly changes the absolute level of utility, and more stringent normalization (such asT=0.5orE=0.5) has a more pronounced negative impact on RCFG-TS, whereas both the performance and variance of SGRL-TS vary only moderately.We analyze the per-decision computational overhead of SGRL-TS under different swarm scales and search budgets, as shown inTable 5. We reuse the SGRL-TS policy trained in the previous experiments and perform online evaluation on six task configurations in inference mode without enabling backpropagation. For each configuration, we record the wall-clock time of the HAN encodingTHAN, the SHIELD mixingTSHIELD, the MCTS planningTMCTS, and the end-to-end decision latencyTe2eover104consecutive decision steps, and we collect the average number of node expansions of MCTS under feasible-region pruning (Avg. exp.).Table 5.Per-decision runtime profiling of SGRL-TS under different swarm scales and search budgets (N: number of UAVs, M: number of tasks). All results are averaged over104decision steps on the hardware platform described inSection 5.As shown inTable 5, in S4the end-to-end decision latencyTe2eis about 5.54 ms, whereas the lower bound under the no-search configuration (S2,B=0) is only 2.10 ms, indicating that even with structured MCTS enabled the overall overhead remains significantly below the typical UAV control period on the order of tens of milliseconds and thus satisfies real-time application requirements. As the task scale increases,THANandTSHIELDslowly increase from about 0.82/0.64 ms in S1to about 1.05/0.82 ms in S4, exhibiting an approximately linear growth trend that is consistent with theO(N+Mact)complexity result given inSection 4.4, which indicates that high-order structural modeling and structure-conditioned value decomposition themselves do not become the main bottlenecks. For the same swarm scale (S2), when the search budget is increased fromB=0to 32 and 64, the average number of expanded nodes grows from 0 to about 21.37 and 41.96, and the correspondingTMCTSincreases from 0 to 1.63 ms and 3.01 ms. At the same time, Avg. exp. consistently remains clearly below the budgetB, which confirms that feasible-region pruning effectively suppresses the size of the search tree and makes the MCTS computational cost approximately linearly controllable with respect to the budget. Taken together, these results show that SGRL-TS achieves both low per-step latency and good scalability within the swarm sizes and search budgets considered in this work.InFigure 13, we evaluate all algorithms in a scenario with 10 tasks and 15 heterogeneous UAVs. Multiple independent runs are conducted under different search budgets and random seeds to sample a set of feasible overlapping coalition-structure solutions. For each solution, we first compute the task execution sufficiencyf1(Tm)‚àà[0,1]and the synchronization deviation costJsyn(Tm)according to (23), and then take the average over all tasks to obtain the overall task execution sufficiency and synchronization deviation; we also record the total energy costETincurred to complete all tasks. The normalized task shortfall is then defined asJÀú1=1‚àí1M‚àëm=1Mf1(Tm). Furthermore, synchronization deviation and energy cost are min‚Äìmax normalized over the union of all methods and sampled solutions to obtain the normalized synchronization deviationJÀúsynand normalized energy costEÀúT. Consequently, all three quantities are scaled to the interval[0,1]with smaller values being better, which facilitates multi-objective Pareto analysis in a unified cost space.Figure 13.Multi-objective trade-off among normalized task shortfall, synchronization deviation, and energy cost: (a) normalized task shortfallJÀú1versus normalized synchronization deviationJÀúsyn; (b) normalized task shortfallJÀú1versus normalized energy costEÀúT; (c) normalized synchronization deviationJÀúsynversus normalized energy costEÀúT. All three objectives are min‚Äìmax normalized over the collected solutions so that lower values are better.FromFigure 13a, the scatter of SGRL-TS is more concentrated in the lower-left region of the(JÀú1,JÀúsyn)plane, yielding fewer dominated solutions with either a small task shortfall but significant synchronization deviation, or good synchronization at the price of a significantly increased task shortfall, compared with the baseline methods. InFigure 13b, SGRL-TS maintains a smallerJÀú1under lower energy costEÀúT, whereas other methods typically require higher energy to achieve a similar level of task completion or suffer a larger task shortfall at comparable energy, indicating a more favorable energy‚Äìefficiency trade-off.Figure 13c further shows that, in the(JÀúsyn,EÀúT)plane, the SGRL-TS samples overall lie closer to the lower-left Pareto boundary and form a more compact ‚Äúknee‚Äù region around low energy and low synchronization deviation. In contrast, the samples of the baseline methods more frequently fall outside this frontier. Taken together, these results demonstrate that SGRL-TS achieves superior Pareto performance in the joint objective space of task completion, coalition-time synchronization, and energy consumption. 5.3. Ablation¬†StudiesTable 6presents a systematic ablation of the encoder, value decomposition, and global search modules under resource-neutral and resource-tight configurations. Here, Viol. denotes the constraint violation rate, AUC is the normalized area under the curve of average task utility Util versus training iterations, and Iter@95% is defined as the training iteration at which the Util curve first reaches 95% of its steady-state mean, where the steady-state mean is computed from a moving average over the final training window. Comparing SGRL-TS with typical graph-based DRL, under the resource-tight regime Full-SGRL-TS improves Compl, Util, and Eff over GAT-QMIX by about 23.9%, 22.2%, and 26.5%, respectively, while reducing Viol by 45.9%, increasing AUC by 48.3%, and shortening Iter@95% by 36.9%. These gains do not come from merely swapping in a GAT encoder and a QMIX mixer, but from jointly exploiting three structural mechanisms: HAN explicitly encodes the high-order UAV‚Äìtask‚Äìcoalition hypergraph so that structural semantics enter value estimation and search rather than being limited to one-hop graph attention; SHIELD injects structure-conditioned terms within and across coalitions, enabling finer modeling of cooperative gains and resource competition and yielding a more monotone and comparable global valueQtot; and the structured MCTS uses these signals for feasibility-set pruning and structural heuristics, so that coalition configurations are optimized globally around structural priors instead of relying on local GNN outputs.Table 6.Ablation and replacement study of SGRL-TS under resource-neutral and resource-tight regimes.Module-level ablations further support this view. With SHIELD-full and structured MCTS fixed, replacing the encoder with a standard GAT (A1) or HyperGCN (A2) shows that, relative to A2, the full HAN still improves Util and Eff in the resource-neutral regime by about 2.2% and 4.6%, further reduces Viol by 17.9%, increases AUC by 6.9%, and shortens Iter@95% by 11.4%; under the resource-tight regime it maintains roughly 6.0% and 6.7% gains in Util and Eff and a 20.7% reduction in violation rate, confirming that heterogeneous node types and high-order hyperedges provide additional structural information beyond conventional graph encoders. For the value decomposition, when VDN or QMIX mixing is used on top of HAN, SHIELD-lite already improves Util and Eff over QMIX-mix by about 2.2% and 4.7%, reduces Viol by 11.7%, increases AUC by 6.0%, and shortens Iter@95% by 5.7%; enabling full SHIELD further raises Util and Eff relative to QMIX-mix by 3.4% and 6.9%, decreases Viol by 22.0%, increases AUC by 8.4%, and reduces Iter@95% by 15.3%, with similar relative improvements in the resource-tight regime, indicating that the structure-conditioned nested mixer improves decomposability and credit assignment beyond monotonic mixing networks. In the global search module, removing search causes Util and Eff to drop by about 4.5% and 6.1%, Viol to increase by 34.4%, AUC to decrease by 11.3%, and Iter@95% to be extended by 24.6% compared with Full-SGRL-TS; greedy search or plain MCTS partially reduce this gap but still underperform structured MCTS in AUC and Iter@95%, showing that high-order structure‚Äìguided feasibility pruning and budget allocation are likewise crucial to achieving high AUC and fast, low-violation convergence. 5.4. Future Research Directions and¬†DeployabilityEvaluation is currently confined to simulation. Subsequent work will progress along a simulation‚Äìhardware-in-the-loop‚Äìfield-trial path. First, we will build a reproducible benchmark that includes a library of parameterized scenarios, communication traces for synthetic generation and replay, and standardized evaluation scripts, accompanied by anonymized logs to strengthen comparability and reproducibility. Second, we will integrate the flight controller and onboard inference compute within a hardware-in-the-loop platform to quantify control-cycle stability, end-to-end latency, per-task energy consumption, and throughput, and we will replay post-disaster communication traces to assess robustness under burst losses, delay jitter, and bandwidth limitations. Third, we will conduct small-scale outdoor experiments with heterogeneous multi-UAV formations and calibrate the simulation models using field measurements of task completion rate, link quality, energy budget, and coalition-reconfiguration delay. To narrow the sim-to-real gap, we will systematically employ dynamics and payload domain randomization, bursty-channel perturbations based on the Gilbert‚ÄìElliott model, delay-aware action buffering with safety shielding, and online-estimate-driven feasibility-set tightening and parameter adaptation.",
            "5.1. Simulation¬†Setup": "To evaluate SGRL-TS, we build a simulator with heterogeneous tasks and multiple UAV platform types. We configure five representative tasks and specify their resource requirements inTable 2. To meet these demands, we design four UAV platform types, with their capabilities summarized inTable 3. The platforms emphasize support, delivery, firefighting, and communication relay. To ensure realism and reproducibility, we specify the environment and communication parameters, including area size, communication radius, energy model, and weights for the multi-objective reward, as listed inTable 4. Table 2.Resource requirements of representative tasks. Table 3.Capabilities of heterogeneous UAV platform types. Table 4.Simulation environment parameters. To highlight the advantages of SGRL-TS, we compare it against the following methods: PGG-TS-OCF[17] establishes preference relations between UAV supply and task demand, guides tabu search via preference gravity, and achieves overlapping allocation and scheduling with a stable overlapping coalition.LocalSearch-CF[15] greedily seeds UAV‚Äìtask coalitions and iteratively swaps or reassigns UAVs between neighboring coalitions to minimize mission completion time.HYGMA[20] dynamically clusters nearby UAVs based on state and task affinity, treating each cluster as a coalition and learning joint actions through hypergraph attention.SMART[23] enables each UAV to repeatedly switch its serving task coalition according to a locally evaluated payoff, yielding a distributed best response coalition formation.RCFG-DRL[26] models UAV‚Äìtask assignment as a repeated coalition game, where UAVs learn join or leave decisions via deep RL until coalitions reach equilibrium.",
            "5.2. Performance¬†Evaluation": "Figure 6reports convergence and final utilities under four resource conditions. SGRL-TS consistently outperforms the strongest baseline, PGG-TS-OCF, by 3.19%, 4.49%, 6.25%, and 9.68% in the abundant, balanced, constrained, and scarce settings, respectively. It enters the efficient ascent earlier and shows a smoother plateau. The gains arise from heterogeneous hypergraph attention, which captures high-order couplings among UAVs, tasks, and coalitions; a structure-conditioned hierarchical value decomposition that yields globally comparable, monotone scores and suppresses merge-split oscillations; and budgeted MCTS under feasibility masks, which focuses expansions on high-value structures and reduces wasted search. Figure 6.Convergence of SGRL-TS and baselines across four resource regimes: (a) 18 UAVs‚Äì5 tasks (resource-abundant); (b) 20 UAVs‚Äì10 tasks (resource-balanced); (c) 15 UAVs‚Äì15 tasks (resource-constrained); (d) 10 UAVs‚Äì20 tasks (resource-scarce). Baseline behavior clarifies the gaps. PGG-TS-OCF employs a parallel population search that identifies feasible overlaps early on; however, it stalls at suboptimal mixes under budget constraints and lacks a stable cross-level yardstick. HYGMA strengthens interaction modelling, but long value propagation under multiple constraints slows ascent. RCFG-DRL introduces adversarial robustness; however, nonstationarity induces mid-horizon oscillations, diverting budget from structural improvements. LocalSearch-CF and SMART use anytime stepwise moves bounded by small neighborhoods, which promote local optima and capped utility. Non-overlapping SGRL-TS forbids resource reuse, creating capacity bottlenecks, synchronization penalties, and heightening sensitivity to task density and temporal perturbations. We next examine resource utilization efficiency, task adaptability, and robustness under overload and scarcity by varying the number of UAVs from 4 to 20 under different task loads, as shown inFigure 7. Across the four task scales, the curve of SGRL-TS remains at the top and reaches a higher peak near moderate swarm sizes; the standard deviation bars show markedly lower variability compared to all baselines. Relative to the strongest competitor, PGG-TS-OCF, SGRL-TS achieves higher peak utilities by approximately 2.27%, 3.00%, 6.36%, and 9.76% at task numbers 5, 10, 15, and 20, respectively. The advantage increases as resource constraints tighten, indicating stronger resource scheduling and parallel coordination under crowded and scarce conditions. Figure 7.Average task utility versus the number of UAVs under different task loads: (a)T=5; (b)T=10; (c)T=15; (d)T=20. This performance stems from a careful consideration of the benefit‚Äìcost balance. SGRL-TS estimates timing and energy constraints online and maps them onto a unified utility scale, which steers the search toward ranges where adding UAVs yields net gains while suppressing ineffective parallelism and communication congestion as scale grows, thus avoiding high-scale regression. Temperature-controlled sampling and reuse of candidate structures broaden exploration, and as early and later convergence occurs, they converge to low-conflict configurations. Combined with penalties and pruning for repeated assignment and resource contention, these mechanisms reduce structural oscillation and tail-phase jitter. The result is a better compromise between task completion and coordination cost, leading to higher and more stable final utility. We further test adaptability under sparse and dense tasks, reuse efficiency, and robustness to task pressure by fixingN‚àà{8,12,16}and increasing the number of tasks from 3 to 12. Results are shown inFigure 8. SGRL-TS stays on top across the three UAV scales and reaches a higher peak near the midrange of task counts, while the tail declines more gently and the variability remains smaller. Compared with the strongest baseline PGG-TS-OCF, the average utility over the full range improves by about 3.86% at 16 UAVs, 2.97% at 12 UAVs, and 3.53% at 8 UAVs. Figure 8.Average task utility versus the number of tasks under different swarm sizes: (a)N=16UAVs; (b)N=12UAVs; (c)N=8UAVs. This advantage and stability arise because bidirectional hypergraph attention normalizes task selectivity and contextual suitability within mask-constrained candidate sets, enabling precise member screening and task assignment as the number of tasks increases, which suppresses ineffective overlaps and resource contention. The SHIELD nested nonlinear aggregation with cross-task interaction terms provides a monotonic and comparable global value for cooperation and competition across tasks, making diminishing marginal returns detectable as the task load grows. This concentrates resources on actions with positive net gain, producing a higher midrange peak and slower performance decay. To assess task completion under scaling, we vary the tasks from 4 to 12 withN=15,10,5. Results are inFigure 9. Across the three experimental settings, the SGRL-TS curve remains closely aligned with the upper bound provided by the Task-completion OCF baseline, which optimizes only task fulfillment. As the number of tasks increases from 4 to 12, it improves the average task execution sufficiency over the entire range by approximately 2.42%, 2.63%, and 10.94% relative to PGG-TS-OCF. Moreover, when other methods exhibit pronounced degradation at higher task counts, SGRL-TS shows a significantly slower decline and can maintain a larger fraction of tasks close to complete execution, even under severely constrained resources. This advantage primarily stems from the balance term in the reward, which discourages extreme solutions that sacrifice a subset of tasks, thereby driving the policy to maintain medium to high completion levels across more tasks as the task load increases. Figure 9.Average task completion versus the number of tasks for three UAV swarm sizes: (a) UAV=15; (b) UAV=10; (c) UAV=5. Figure 10a‚Äìc report the coalition-level temporal coordination performance of all methods. The evaluation metric is the coalition synchronization sufficiency, defined as the normalized scorefsync‚àà[0,1]obtained from the coalition arrival-time deviation costJsynaccording to (23); larger values indicate more synchronized coalition arrivals under the given reference time scale. As the number of tasks increases, the synchronization sufficiency of all methods decreases overall, indicating that higher task congestion makes it harder for coalitions to achieve good temporal coordination; moreover, when the number of UAVs is reduced from 15 to 5, the overall degradation in synchronization performance becomes more pronounced. The advantage of SGRL-TS in synchronization sufficiency is most pronounced in configurations with more tasks and tighter resources, suggesting that structure-guided overlapping coalition formation combined with joint value decomposition can effectively suppress the dispersion in coalition arrival times. In contrast, non-overlapping SGRL-TS and Task-completion OCF, which only focus on task completion rate, exhibit significantly lower synchronization sufficiency under high-load scenarios, indicating that ignoring overlapping structures or lacking explicit synchronization modeling leads to markedly degraded temporal coordination among coalitions. Figure 10.Average coalition synchronization sufficiency versus the number of tasks under three UAV swarm sizes: (a) UAV=15; (b) UAV=10; (c) UAV=5. The metricfsync‚àà[0,1]is obtained by normalizing the coalition arrival-time deviation cost according to (23), with larger values indicating more synchronized coalition arrivals. Previous experiments on task completion and utility have shown that SGRL-TS achieves returns close to the Task-completion OCF upper bound and outperforms PGG-TS-OCF. This section further examines its cost side from the perspective of energy utilization. As shown inFigure 11, under four task scalesT=5,10,15,20, the energy-efficiency curves of SGRL-TS lie consistently above those of all baselines. Compared with PGG-TS-OCF, the average energy efficiency over the entire UAV range improves by approximately 8.09%, 8.02%, 3.05%, and 12.85%, respectively. Moreover, relative to the Task-completion OCF scheme, which optimizes only task completion, SGRL-TS achieves comparable completion levels while attaining between two and five times higher energy efficiency, thereby providing a considerably more economical way of sustaining overlapping coalition structures from the energy consumption viewpoint. Overall, this advantage primarily stems from incorporating energy safety margins and residual-resource-driven feasible-set pruning into the MCTS search guided by SHIELD evaluations, which reduces the expansion of high-cost, overlapping structures at the search level, and thus markedly improves global energy utilization without sacrificing task completion. Figure 11.Energy efficiency versus the number of UAVs under different task loads: (a)T=5; (b)T=10; (c)T=15; (d)T=20. To examine the sensitivity of average task utility and method ranking to multi-objective weight settings, task-priority scenarios, and time/energy normalization scales, we conducted comparative experiments in a scenario with 10 tasks and 20 UAVs, as shown inFigure 12. Under fixed network parameters and training configurations, we change only the weight vectorœâ(k)=(wcomp,wsyn,weng)at evaluation time to assess the weight sensitivity of the multi-objective design; inFigure 12b, under the default settingœâ(1)=(0.4,0.3,0.3), we keep the training process unchanged and modify only the task-priority directed acyclic graph to construct three scenarios,Balanced priorities,Rescue priority, andCommunication priority, with the horizontal axis corresponding to these three priority configurations, respectively; inFigure 12c, we scale the normalization ranges of the synchronization reference timeTrefand energy reference valueErefand, in turn, investigate the impact of five combinations on the final task utility, where the horizontal axis corresponds to these five(T,E)configurations. Figure 12.Sensitivity of average task utility to (a) multi-objective weight vectorsœâ(k)=(wcomp,wsyn,weng), (b) task-priority scenarios, and (c) normalization scales(T,E)of synchronization and energy costs in the 10-task, 20-UAV resource-neutral setting. Each curve reports the mean utility with standard-deviation error bars for SGRL-TS, HYGMA, PGG-TS-OCF, and RDFG-DRL. Across all configurations of the three sensitivity tests, SGRL-TS consistently attains the highest average task utility, with a performance gap of approximately 2‚Äì3 percentage points relative to the best-performing baseline PGG-TS-OCF, and exhibits substantially more minor variance, indicating overall robustness to perturbations in weights, priority settings, and normalization scales. Specifically, inFigure 12a, the completion-emphasized weight vectorœâ(2)increases the utilities of all four methods, whereas the synchronization- and energy-emphasized weight vectorsœâ(3)andœâ(4)reduce the overall utilities, with a particularly pronounced impact on RDFG-DRL, while the curve of SGRL-TS exhibits only mild fluctuations;Figure 12b shows that different task-priority topologies induce only slight changes in the utilities of all methods, and SGRL-TS can still better coordinate resources and synchronization constraints in the rescue-priority scenario, maintaining a stable performance lead; inFigure 12c, scalingTreforErefmainly changes the absolute level of utility, and more stringent normalization (such asT=0.5orE=0.5) has a more pronounced negative impact on RCFG-TS, whereas both the performance and variance of SGRL-TS vary only moderately. We analyze the per-decision computational overhead of SGRL-TS under different swarm scales and search budgets, as shown inTable 5. We reuse the SGRL-TS policy trained in the previous experiments and perform online evaluation on six task configurations in inference mode without enabling backpropagation. For each configuration, we record the wall-clock time of the HAN encodingTHAN, the SHIELD mixingTSHIELD, the MCTS planningTMCTS, and the end-to-end decision latencyTe2eover104consecutive decision steps, and we collect the average number of node expansions of MCTS under feasible-region pruning (Avg. exp.). Table 5.Per-decision runtime profiling of SGRL-TS under different swarm scales and search budgets (N: number of UAVs, M: number of tasks). All results are averaged over104decision steps on the hardware platform described inSection 5. As shown inTable 5, in S4the end-to-end decision latencyTe2eis about 5.54 ms, whereas the lower bound under the no-search configuration (S2,B=0) is only 2.10 ms, indicating that even with structured MCTS enabled the overall overhead remains significantly below the typical UAV control period on the order of tens of milliseconds and thus satisfies real-time application requirements. As the task scale increases,THANandTSHIELDslowly increase from about 0.82/0.64 ms in S1to about 1.05/0.82 ms in S4, exhibiting an approximately linear growth trend that is consistent with theO(N+Mact)complexity result given inSection 4.4, which indicates that high-order structural modeling and structure-conditioned value decomposition themselves do not become the main bottlenecks. For the same swarm scale (S2), when the search budget is increased fromB=0to 32 and 64, the average number of expanded nodes grows from 0 to about 21.37 and 41.96, and the correspondingTMCTSincreases from 0 to 1.63 ms and 3.01 ms. At the same time, Avg. exp. consistently remains clearly below the budgetB, which confirms that feasible-region pruning effectively suppresses the size of the search tree and makes the MCTS computational cost approximately linearly controllable with respect to the budget. Taken together, these results show that SGRL-TS achieves both low per-step latency and good scalability within the swarm sizes and search budgets considered in this work. InFigure 13, we evaluate all algorithms in a scenario with 10 tasks and 15 heterogeneous UAVs. Multiple independent runs are conducted under different search budgets and random seeds to sample a set of feasible overlapping coalition-structure solutions. For each solution, we first compute the task execution sufficiencyf1(Tm)‚àà[0,1]and the synchronization deviation costJsyn(Tm)according to (23), and then take the average over all tasks to obtain the overall task execution sufficiency and synchronization deviation; we also record the total energy costETincurred to complete all tasks. The normalized task shortfall is then defined asJÀú1=1‚àí1M‚àëm=1Mf1(Tm). Furthermore, synchronization deviation and energy cost are min‚Äìmax normalized over the union of all methods and sampled solutions to obtain the normalized synchronization deviationJÀúsynand normalized energy costEÀúT. Consequently, all three quantities are scaled to the interval[0,1]with smaller values being better, which facilitates multi-objective Pareto analysis in a unified cost space. Figure 13.Multi-objective trade-off among normalized task shortfall, synchronization deviation, and energy cost: (a) normalized task shortfallJÀú1versus normalized synchronization deviationJÀúsyn; (b) normalized task shortfallJÀú1versus normalized energy costEÀúT; (c) normalized synchronization deviationJÀúsynversus normalized energy costEÀúT. All three objectives are min‚Äìmax normalized over the collected solutions so that lower values are better. FromFigure 13a, the scatter of SGRL-TS is more concentrated in the lower-left region of the(JÀú1,JÀúsyn)plane, yielding fewer dominated solutions with either a small task shortfall but significant synchronization deviation, or good synchronization at the price of a significantly increased task shortfall, compared with the baseline methods. InFigure 13b, SGRL-TS maintains a smallerJÀú1under lower energy costEÀúT, whereas other methods typically require higher energy to achieve a similar level of task completion or suffer a larger task shortfall at comparable energy, indicating a more favorable energy‚Äìefficiency trade-off.Figure 13c further shows that, in the(JÀúsyn,EÀúT)plane, the SGRL-TS samples overall lie closer to the lower-left Pareto boundary and form a more compact ‚Äúknee‚Äù region around low energy and low synchronization deviation. In contrast, the samples of the baseline methods more frequently fall outside this frontier. Taken together, these results demonstrate that SGRL-TS achieves superior Pareto performance in the joint objective space of task completion, coalition-time synchronization, and energy consumption.",
            "5.3. Ablation¬†Studies": "Table 6presents a systematic ablation of the encoder, value decomposition, and global search modules under resource-neutral and resource-tight configurations. Here, Viol. denotes the constraint violation rate, AUC is the normalized area under the curve of average task utility Util versus training iterations, and Iter@95% is defined as the training iteration at which the Util curve first reaches 95% of its steady-state mean, where the steady-state mean is computed from a moving average over the final training window. Comparing SGRL-TS with typical graph-based DRL, under the resource-tight regime Full-SGRL-TS improves Compl, Util, and Eff over GAT-QMIX by about 23.9%, 22.2%, and 26.5%, respectively, while reducing Viol by 45.9%, increasing AUC by 48.3%, and shortening Iter@95% by 36.9%. These gains do not come from merely swapping in a GAT encoder and a QMIX mixer, but from jointly exploiting three structural mechanisms: HAN explicitly encodes the high-order UAV‚Äìtask‚Äìcoalition hypergraph so that structural semantics enter value estimation and search rather than being limited to one-hop graph attention; SHIELD injects structure-conditioned terms within and across coalitions, enabling finer modeling of cooperative gains and resource competition and yielding a more monotone and comparable global valueQtot; and the structured MCTS uses these signals for feasibility-set pruning and structural heuristics, so that coalition configurations are optimized globally around structural priors instead of relying on local GNN outputs. Table 6.Ablation and replacement study of SGRL-TS under resource-neutral and resource-tight regimes. Module-level ablations further support this view. With SHIELD-full and structured MCTS fixed, replacing the encoder with a standard GAT (A1) or HyperGCN (A2) shows that, relative to A2, the full HAN still improves Util and Eff in the resource-neutral regime by about 2.2% and 4.6%, further reduces Viol by 17.9%, increases AUC by 6.9%, and shortens Iter@95% by 11.4%; under the resource-tight regime it maintains roughly 6.0% and 6.7% gains in Util and Eff and a 20.7% reduction in violation rate, confirming that heterogeneous node types and high-order hyperedges provide additional structural information beyond conventional graph encoders. For the value decomposition, when VDN or QMIX mixing is used on top of HAN, SHIELD-lite already improves Util and Eff over QMIX-mix by about 2.2% and 4.7%, reduces Viol by 11.7%, increases AUC by 6.0%, and shortens Iter@95% by 5.7%; enabling full SHIELD further raises Util and Eff relative to QMIX-mix by 3.4% and 6.9%, decreases Viol by 22.0%, increases AUC by 8.4%, and reduces Iter@95% by 15.3%, with similar relative improvements in the resource-tight regime, indicating that the structure-conditioned nested mixer improves decomposability and credit assignment beyond monotonic mixing networks. In the global search module, removing search causes Util and Eff to drop by about 4.5% and 6.1%, Viol to increase by 34.4%, AUC to decrease by 11.3%, and Iter@95% to be extended by 24.6% compared with Full-SGRL-TS; greedy search or plain MCTS partially reduce this gap but still underperform structured MCTS in AUC and Iter@95%, showing that high-order structure‚Äìguided feasibility pruning and budget allocation are likewise crucial to achieving high AUC and fast, low-violation convergence.",
            "5.4. Future Research Directions and¬†Deployability": "Evaluation is currently confined to simulation. Subsequent work will progress along a simulation‚Äìhardware-in-the-loop‚Äìfield-trial path. First, we will build a reproducible benchmark that includes a library of parameterized scenarios, communication traces for synthetic generation and replay, and standardized evaluation scripts, accompanied by anonymized logs to strengthen comparability and reproducibility. Second, we will integrate the flight controller and onboard inference compute within a hardware-in-the-loop platform to quantify control-cycle stability, end-to-end latency, per-task energy consumption, and throughput, and we will replay post-disaster communication traces to assess robustness under burst losses, delay jitter, and bandwidth limitations. Third, we will conduct small-scale outdoor experiments with heterogeneous multi-UAV formations and calibrate the simulation models using field measurements of task completion rate, link quality, energy budget, and coalition-reconfiguration delay. To narrow the sim-to-real gap, we will systematically employ dynamics and payload domain randomization, bursty-channel perturbations based on the Gilbert‚ÄìElliott model, delay-aware action buffering with safety shielding, and online-estimate-driven feasibility-set tightening and parameter adaptation.",
            "6. Conclusions": "Starting from an overlapping-coalition paradigm that better reflects real rescue requirements, this work proposes a unified framework grounded in high-order structural modeling, centered on hierarchical value decomposition, and driven by global search with policy feedback. We jointly embed tasks and UAVs into a hypergraph, allowing for the capture of resource reuse, member complementarity, and inter-coalition competition within a common semantic space. Based on this representation, the hierarchical value decomposition stably aggregates information across the agent and coalition levels, providing reliable value signals for subsequent global structure search. The global search efficiently identifies high-potential structures within the feasible region. The resulting candidates are then fed back to the policy via demonstration replay and distillation, which forms a closed loop between learning and search. This loop consistently yields solutions with higher quality, lower energy consumption, and more stable coordination under complex feasibility constraints. In extensive post-disaster rescue simulations of varying scales, the proposed method improves task utility by up to 11.4% over the strongest baseline and surpasses the non-overlapping variant in energy efficiency by more than 228%. These advantages are more pronounced under resource-limited settings, indicating that overlapping coalitions combined with structure-aware policy learning can substantially enhance resource coordination under tight constraints while keeping coalition synchronization deviation and operational cost within a more reasonable range."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2504-446X/9/12/837",
        "scraped_at": "2025-12-05 23:56:42"
    },
    {
        "title": "Assessing the Readability of Russian Textbooks Using Large Language Models",
        "authors": "byAndrei Paraschiv,Mihai DascaluandMarina Solnyshkina",
        "journal": "Information2025,16(12), 1071;https://doi.org/10.3390/info16121071- 4 Dec 2025",
        "abstract": "This study aims to assess the capability of Large Language Models (LLMs), particularly GPT-4o, to evaluate and modify the complexity level of Russian school textbooks. We lay the groundwork for developing scalable, context-aware methods for readability assessment and text simplification in Russian educational materials, areas where traditional formulas often fall short. Using a corpus of 154 textbooks covering various subjects and grade levels, we evaluate the extent to which LLMs accurately predict the appropriate comprehension level of a text and how well they simplify texts by targeted grade reduction. Our evaluation framework employs GPT-4o as a multi-role agent in three distinct experiments. First, we prompt the model to estimate the target comprehension age for each segment and identify five key linguistic or conceptual features underpinning its assessment. Second, we simulate student comprehension by instructing the model to reason step-by-step through whether the text is understandable for a hypothetical student of the given grade. Third, we examine the model‚Äôs ability to simplify selected fragments by reducing their complexity by three grade levels. We further measure model perplexity and output token probabilities to probe the prediction confidence and coherence. Results indicate that while LLMs show considerable potential in complexity assessment (i.e., MAE of 1 grade level), they tend to overestimate text difficulty and face challenges in achieving precise simplification levels. Ease of understanding assessments generally align with human expectations, although texts with abstract, technical, or poetic content (e.g., Physics, History, and Literary Russian) pose challenges. Our study concludes that LLMs can substantially complement traditional readability metrics and assist teachers in developing suitable Russian educational materials.Keywords:readability assessment;Large Language Models;Russian educational textbooks;text simplification",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Textbooks serve as a cornerstone to educational systems by providing structure for learning at both early and later stages of schooling. As such, ensuring that these materials match the learners‚Äô cognitive and linguistic levels is essential. Consequently, researchers have long sought objective methods for assessing the difficulty of textbooks [1,2,3]. Developing automated, standardized measures would significantly benefit curriculum designers and educators by providing scalable evaluation tools. However, accurately determining the difficulty of a textbook‚Äôs content is a complex task that requires considerable expertise, as it involves not only linguistic and lexical analysis but also an evaluation of the underlying informational and conceptual density [4]. Automatically determining whether a textbook‚Äôs content is appropriate for a specific educational level remains challenging. Although traditional readability metrics such as the Flesch‚ÄìKincaid Reading Ease score [5] have been widely used, they rely on simplified formulae based primarily on surface-level linguistic features, including sentence length, word length, and vocabulary frequency [6]. While useful, these measures capture only a narrow dimension of textual complexity and fail to account for the informational and conceptual demands embedded within educational content. Large Language Models (LLMs) can address a broad spectrum of tasks that involve both linguistic and content-related challenges. They achieved high accuracy in explaining complex concepts and answering domain-specific questions [7,8]. The rapid advancement of LLMs has significantly accelerated research activity, with new applications emerging at an unprecedented pace. While their integration into education is still in its early stages, existing studies have explored diverse use cases, including automated essay evaluation [9], personalized content generation [10], and the deployment of conversational teaching assistants [11]. This paper explores and evaluates the potential of Large Language Models (LLMs) in assessing the complexity of Russian-language textbooks. Rather than replacing traditional static readability metrics, these models are proposed as complementary tools that leverage their broad linguistic capabilities to offer deeper, more context-aware insights. While our primary focus is on Russian educational materials, the findings are broadly applicable given the multilingual architecture of current LLMs. Our study centers on OpenAI‚Äôs GPT-4o, although the methodologies presented are model-agnostic and can be adapted to other LLMs with comparable capabilities. To the best of our knowledge, this study is the first to systematically apply large-scale decoder LLMs rather than encoder models to assess the complexity and readability of Russian-language textbooks. Research ObjectivesUnderstanding whether LLMs can reliably assess the complexity of certain texts is crucial for their potential use in automating text selection or evaluation. This cannot be achieved without identifying the key influential features that lead the LLM to one conclusion or another. Evaluating its biases or shortcomings might reveal whether its assessment aligns with established linguistic or pedagogical principles of text complexity.This study aims to conduct a comprehensive evaluation of LLMs‚Äô capability to evaluate the complexity and cognitive accessibility of Russian educational texts. Our analysis is structured around the following three key research questions:RQ1: To what extent do LLMs correctly assess the complexity of a Russian-language text? What are the key features that influence the decision?RQ2: Can LLMs be used as effective judges (or proxies) of the ease of understanding of Russian schoolbooks, while simulating how a student might understand the material? What are the key features that influence the LLM‚Äôs decision regarding the ease of understanding?RQ3: Can LLMs successfully reduce the complexity of a schoolbook text to a certain level of comprehension (in our specific case, by 3 school years)? Can this be achieved across different school subjects?Addressing these research questions provides insights into the capabilities and limitations of LLMs in analyzing and modifying Russian educational texts. The results offer valuable guidance to educators, researchers, and developers regarding the practical applications and constraints of these models in generating and assessing appropriately leveled learning materials within the Russian education framework.",
            "Research Objectives": "Understanding whether LLMs can reliably assess the complexity of certain texts is crucial for their potential use in automating text selection or evaluation. This cannot be achieved without identifying the key influential features that lead the LLM to one conclusion or another. Evaluating its biases or shortcomings might reveal whether its assessment aligns with established linguistic or pedagogical principles of text complexity. This study aims to conduct a comprehensive evaluation of LLMs‚Äô capability to evaluate the complexity and cognitive accessibility of Russian educational texts. Our analysis is structured around the following three key research questions: RQ1: To what extent do LLMs correctly assess the complexity of a Russian-language text? What are the key features that influence the decision?RQ2: Can LLMs be used as effective judges (or proxies) of the ease of understanding of Russian schoolbooks, while simulating how a student might understand the material? What are the key features that influence the LLM‚Äôs decision regarding the ease of understanding?RQ3: Can LLMs successfully reduce the complexity of a schoolbook text to a certain level of comprehension (in our specific case, by 3 school years)? Can this be achieved across different school subjects? Addressing these research questions provides insights into the capabilities and limitations of LLMs in analyzing and modifying Russian educational texts. The results offer valuable guidance to educators, researchers, and developers regarding the practical applications and constraints of these models in generating and assessing appropriately leveled learning materials within the Russian education framework.",
            "2. Related Work": "There has been a long line of research aimed at identifying and validating predictors of text complexity. Each of the five paradigms of discourse complexology, i.e., formative, classical, closed tests, constructive-cognitive, and Natural Language Processing, experimented with a new approach expanding the earlier models [12] In their systematic review, AlKhuzaey et al. [13] observed that traditional methods for assessing text difficulty predominantly rely on syntactic features, which are usually extracted using NLP tools‚Äîsuch as counts of complex words [14], sentence length [15], and overall word count [16]. In contrast, recent research has increasingly turned to semantic-level analysis, enabled by advances in semantically annotated data structures such as domain ontologies and the emergence of neural language models. Among the earliest and most influential surface-level approaches are the Flesch Reading Ease [5] and Flesch‚ÄìKincaid readability score [17], which estimate readability based on surface indicators such as orthographic (e.g., number of letters), syntactic (e.g., number of words), and phonological (e.g., number of syllables) features. In this framework, a higher score reflects greater difficulty. However, Yaneva et al. [18] highlighted the limitations of these traditional metrics, arguing that Flesch-based scores are weak predictors of actual difficulty, as they fail to distinguish between easy and difficult content based on lexical features alone. The emergence of neural language models has transformed the landscape of automatic text complexity assessment. Trained on large-scale corpora, these models can capture not only syntactic and semantic patterns but also elements of world knowledge, allowing for deeper interpretation of textual content. Modern readability estimation methods now frequently use Transformer-based architectures such as BERT and RoBERTa, which have superseded earlier neural models, like recurrent networks. For instance, Imperial [19] proposed a hybrid model combining BERT embeddings with traditional linguistic features, achieving a notable 12.4% improvement in F1 score over classic readability metrics for English texts, and achieving robust performance on a low-resource language (i.e., Filipino). Similarly, Paraschiv et al. [20] applied a BERT-based hybrid model within the ReaderBench framework to classify the difficulty of Russian-language textbooks by grade level, achieving an F1 score of 54.06%. The use of multilingual transformer models such as mBERT, XLM-R, and multilingual T5 has further expanded the applicability of these methods across languages. The ReadMe++ benchmark introduced by Naous et al. [21] includes nearly 10,000 human-rated sentences across five languages‚ÄîEnglish, Arabic, French, Hindi, and Russian‚Äîspanning various domains. Their findings reveal that fine-tuned LLMs exhibit stronger correlations with human judgments than traditional scores such as Flesch‚ÄìKincaid and unsupervised LM-based baselines. With the emergence of very large generative models like GPT-3 and GPT-4, another convenient method is to prompt the model to evaluate a text‚Äôs readability or generate a difficulty rating. In this approach, we consider the LLM as an expert evaluator: we feed it a passage and ask (in natural language) for an analysis of how easy or hard that passage would be for a certain reader group [22,23]. In addition, LLMs have also been employed for generating or rewriting texts at specified reading levels, mostly in English‚Äîan inverse but closely related task to readability evaluation [24,25]. The capacity to simplify a complex passage requires an underlying model of readability, as it requires the LLM to recognize what makes a text difficult and how to reduce that difficulty while preserving meaning. Huang et al. [26] introduced the task of leveled-text generation, in which a model is given a source text and a target readability level (e.g., converting a 12th-grade passage to an 8th-grade level) and asked to rewrite the content accordingly. Evaluations of models such as GPT-3.5 and LLaMA-2 on a dataset of 100 educational texts, measured using metrics like the Lexile score, revealed that while the models could generally approximate the target level, human inspection identified notable issues, including occasional misinformation and inconsistent simplification across passages. In parallel, Rooein et al. [27] introduced PROMPT-BASED metrics, an evaluation method that uses targeted yes/no questions to probe a model‚Äôs judgment of text difficulty. When integrated with traditional static metrics, this approach improved overall classification performance. Further, Gobara et al. [28] argued that instruction-tuned models show stronger alignment between user-specified difficulty levels and generated outputs, having a stronger influence than the model size. In contrast, Imperial and Tayyar Madabushi [29] highlighted the limitations of proprietary models like ChatGPT (gpt-3.5-turbo and gpt-4-32k) for text simplification, showing that, without carefully crafted prompts, their outputs lagged behind open-source alternatives such as BLOOMZ and FlanT5 in both consistency and effectiveness.",
            "3. Method": "3.1. Russian Schoolbook CorpusWe base our analysis on a corpus of 154 Russian textbooks curated by linguistic experts at the Multidisciplinary Investigations of Texts Laboratory, Institute of Philology and Intercultural Communication, Kazan, in 2023 [3]. This corpus spans 13 key subjects and covers grade levels 2 through 11. However, due to the structure of the Russian educational curriculum and the availability of digitized textbooks, certain subjects are not represented at all grade levels. For example, subjects like Mathematics, Science, and Technology are absent in this corpus beyond the 4th grade. In contrast, others (i.e., Biology, Geography, History, and Physics) are introduced only in higher grades. The textbooks selected for this study are approved by the Ministry of Education and Science of the Russian Federation and are part of the standard curriculum for secondary and high schools.Table 1provides a detailed breakdown of the topics and the number of textbooks available for each grade level. In it, we can observe the uneven distribution of subjects across textbooks. Notably, subjects such as Arts and Music are only offered in lower grades, while subjects such as Biology and Physics are predominantly offered in higher grades. In particular, Biology for grades 10 and 11 presents a unique case, as all three textbooks used in these grades were evenly distributed between both levels. Since a clear separation between the textbooks for these two grades was not possible, we grouped all three under 10th grade.Table 1.Class distribution for subjects after splitting the documents using the two approaches (# denotes counts per subject).Furthermore,Table 2provides a detailed analysis of the distribution of paragraphs, phrases, and words across textbooks by grade level. Phrase segmentation was determined using punctuation markers, while words were separated by either whitespace or punctuation. This data reveals the increasing complexity and length of the texts as students progress through the grades. For example, while the average number of paragraphs remains relatively consistent throughout the dataset, the number of phrases and words shows a noticeable progression, in step with the increasing linguistic and conceptual demands placed on students as they advance in their education.Table 2.Average number of paragraphs, phrases, and words per textbook and grade (# denotes counts per grade level). 3.2. Extractive Summarizing of TextbooksWith more than 15,000 paragraphs across the entire corpus, compute time and costs can quickly become prohibitive. To mitigate this issue, we applied extractive summarization, selecting the 10 most representative segments from each textbook. Unlike abstractive methods, the extractive approach preserves the original text complexity within the chosen segments.The summarization process involves an initial pass through the textbook, segmenting the content into discrete paragraphs. Each paragraph was subsequently embedded using thecointegrated/LaBSE-en-ru(https://huggingface.co/cointegrated/LaBSE-en-ru, accessed on 28 November 2025) model, which is specifically fine-tuned for sentence embeddings in the Russian language. To address potential disruptions in text cohesion caused by simple paragraph-based splitting, we apply thesplit_optimalmethod from the textsplit library [30]. This approach ensured summarization coherence by grouping adjacent text fragments with high cosine similarity. Following this re-segmentation, we exclude segments with fewer than 100 tokens and further split segments exceeding 512 tokens.After the segmentation phase, centrality scores for each segment were computed using the LexRank algorithm [31], a graph-based extractive summarization algorithm that relies on PageRank [32]. In extractive summarization, the most important content is often defined by centrality, reflecting the intuition that the sentences or segments most similar to the rest of the document embody its core meaning [33]. In particular, LexRank pinpoints the salient sentences that collectively represent the original text [34].Using cosine similarity as edge weights, the model ranks all fragments based on their connections. A highly connected segment (similar to many other segments or to highly ranked segments) accumulates a high centrality score. In effect, the PageRank-like computation will elevate segments by how representative they are of the overall text. Segments that cover content common to multiple parts of the document naturally rise to the top, as they are strongly connected to the rest of the document through content similarity. After processing each document, the ten segments with the highest centrality for each textbook were chosen as the summary.The selected key segments serve as reliable proxies for the textbook‚Äôs overall complexity level. By using these central text fragments, our approach ensures that complexity assessments remain both representative of the document and computationally efficient. This enabled us to significantly reduce the volume of tokens required for further processing by LLMs, resulting in lower computational overhead and reduced operational costs.To deliver a qualitative dimension to our analysis, we visualized the key segments within the embedding space (seeFigure 1). We can observe that the selected key phrases are well distributed across the embedding space, capturing a broad range of thematic and semantic aspects of the textbook. This supports the claim that our methodology effectively identifies central passages, ensuring that the summary accurately reflects the text‚Äôs complexity and thematic focus.Figure 1.The graph displays the distribution of text segments in the embedding space for the 4th Grade Math schoolbook. Light blue points represent general segments, and dark blue points indicate the key segments selected through extractive summarization. 3.3. Assessing the Readability with LLM PromptingLLMs have increasingly been employed to assess text readability and evaluate reading comprehension [27,29]. Unlike traditional readability metrics, such as Flesch‚ÄìKincaid Reading Ease score [17], LLMs can exhibit intrinsic linguistic understanding and can offer more nuanced assessments. These methodologies can broadly be categorized into three distinct approaches: LLMs as Readability Evaluators [21], Prompt-Based Comprehension Testing [35,36], and Automated Response Grading and Summarization [9].The first approach involves using LLMs as evaluators to determine the reading level or complexity of a passage. For example, an LLM can be prompted to assess the complexity of a text, allowing for a nuanced evaluation of whether a topic or prior knowledge requirements might increase the difficulty, areas where traditional metrics may fall short [27].In the second approach, LLMs are prompted to answer questions based on the text, simulating how a student‚Äôs comprehension would be tested. This method uses the LLM as a stand-in for a student [37,38].The final approach employs LLMs to evaluate short-answer or essay responses, assessing both correctness and comprehension. In this role, the LLM acts as a proxy for a teacher, providing a detailed evaluation in natural language that goes beyond assigning a simple numerical score. It can replicate the qualitative judgment a teacher might apply, checking that important information is not missing or distorted [9,39,40].In this study, we employ a hybrid methodology that leverages all three strategies, first by prompting the LLM to determine the target grade level for each text fragment, producing a numeric prediction between grades 2 and 11. To gain a deeper understanding of the LLM‚Äôs decision-making process, we request the extraction of 5 key phrases that highlight the text‚Äôs complexity. Additionally, we simulate a comprehension assessment by prompting the LLM to adopt the perspective of an n-th grade student and evaluate whether the fragment would be accessible to students at that educational level. Finally, we instruct the model to simplify texts designed for grades 5 through 11 to assess its capability to reduce text complexity by 3 grade levels.In addition to these methods, we also measure the model‚Äôs perplexity during response generation. Perplexity is a quantitative measure of the model‚Äôs capability to process long-form texts. Generally, a lower perplexity score indicates better language modeling performance, suggesting the model generates coherent, contextually appropriate responses. This metric has proven effective in evaluating LLMs‚Äô capability to manage and interpret lengthy text passages [41]. 3.4. Experimental SetupTo address our research questions, we designed a series of experiments that investigate the effectiveness of LLMs in evaluating and understanding the complexity of Russian schoolbook texts. By leveraging the previously mentioned key sequences extracted from the textbook corpus, we devised 3 experiments to highlight the model‚Äôs abilities. These experiments were conducted using GPT-4o with a temperature setting of 0.5, ensuring a balanced approach to creativity and response consistency. We accessed GPT-4o via the official OpenAI API endpoint, programmatically submitting prompts and retrieving completions using a predefined template for each experiment. This template ensured that the model consistently adhered to structured output expectations, delivering answers in a predefined JSON format.The first experiment aimed to evaluate the LLM‚Äôs capability to accurately assess the complexity of Russian educational texts. We prompted the model with the following query: ‚ÄúFor what age would this Russian-language school text be suitable, assuming that the reader is a native Russian speaker? Additionally, extract a list of five key phrases that best pinpoint the complexity level of the text. Write your answer as a JSON with the following structure: {‚Äôage‚Äô: 0, ‚Äôphrases‚Äô: [‚Äô..‚Äô]}.‚Äù This task not only required the LLM to predict the grade level, ranging from 2 to 11, but also to provide insights into its decision-making process by highlighting the specific linguistic or thematic elements that influenced its judgment. By examining the extracted key phrases, we aim to reveal the features the model uses to assess text complexity and to uncover how closely the LLM‚Äôs assessments align with established educational standards.In the second experiment, we shifted the focus to the evaluation of text cognitive accessibility from a student‚Äôs perspective. The LLM was prompted with the following instruction: ‚ÄúThis text is in your [Subject] class schoolbook. Is this text comprehensible for you? Answer in English. Let‚Äôs think step by step. It‚Äôs crucial that the final sentence contains your conclusion in the form ‚ÄòAnswer: yes/no‚Äô.‚Äù Here, the model was instructed to simulate the thought process of a student, critically analyzing the text‚Äôs clarity and accessibility. By instructing the LLM to adopt a step-by-step approach, we encouraged it to articulate the rationale behind its conclusions [42], enabling us to assess not only the final answer but also the reasoning path leading to it. This approach aimed to validate whether the model could effectively serve as a proxy for human judgment in educational settings, identifying specific features that contribute to or hinder comprehension.The third experiment extended our analysis to the domain of text simplification. We explored whether the LLM could actively reduce the complexity of schoolbook texts to match a lower grade level, aiming to reduce the complexity by approximately three school years. The prompt provided to the model was:‚ÄúRephrase this Russian text in a way that would be comprehensible for a Russian student in the 6th grade (12 years old). Reply in Russian. No other comments.‚ÄùThis experiment required the model to transform the original text while preserving essential information, making sure that its content remained valuable yet accessible to younger students. In this experiment, we specifically targeted texts from Russian Language, Social Studies, and History, since these subjects rely more on lexical complexity, unlike subjects such as Mathematics or Informatics, where structural and formulaic complexity play a more dominant role. Additionally, these subjects span a broader range of grade levels than most other disciplines. By analyzing simplified outputs, we aimed to determine whether LLMs could be used to develop simplification strategies across different subjects while still maintaining the intended educational outcomes.In conjunction, these experiments offer a multifaceted evaluation of LLMs in the context of Russian-language textbook evaluation and facilitate the analysis of LLMs‚Äô immediate effectiveness and the exploration of their broader potential to support teachers and educators in developing age-appropriate and pedagogically sound materials.",
            "3.1. Russian Schoolbook Corpus": "We base our analysis on a corpus of 154 Russian textbooks curated by linguistic experts at the Multidisciplinary Investigations of Texts Laboratory, Institute of Philology and Intercultural Communication, Kazan, in 2023 [3]. This corpus spans 13 key subjects and covers grade levels 2 through 11. However, due to the structure of the Russian educational curriculum and the availability of digitized textbooks, certain subjects are not represented at all grade levels. For example, subjects like Mathematics, Science, and Technology are absent in this corpus beyond the 4th grade. In contrast, others (i.e., Biology, Geography, History, and Physics) are introduced only in higher grades. The textbooks selected for this study are approved by the Ministry of Education and Science of the Russian Federation and are part of the standard curriculum for secondary and high schools. Table 1provides a detailed breakdown of the topics and the number of textbooks available for each grade level. In it, we can observe the uneven distribution of subjects across textbooks. Notably, subjects such as Arts and Music are only offered in lower grades, while subjects such as Biology and Physics are predominantly offered in higher grades. In particular, Biology for grades 10 and 11 presents a unique case, as all three textbooks used in these grades were evenly distributed between both levels. Since a clear separation between the textbooks for these two grades was not possible, we grouped all three under 10th grade. Table 1.Class distribution for subjects after splitting the documents using the two approaches (# denotes counts per subject). Furthermore,Table 2provides a detailed analysis of the distribution of paragraphs, phrases, and words across textbooks by grade level. Phrase segmentation was determined using punctuation markers, while words were separated by either whitespace or punctuation. This data reveals the increasing complexity and length of the texts as students progress through the grades. For example, while the average number of paragraphs remains relatively consistent throughout the dataset, the number of phrases and words shows a noticeable progression, in step with the increasing linguistic and conceptual demands placed on students as they advance in their education. Table 2.Average number of paragraphs, phrases, and words per textbook and grade (# denotes counts per grade level).",
            "3.2. Extractive Summarizing of Textbooks": "With more than 15,000 paragraphs across the entire corpus, compute time and costs can quickly become prohibitive. To mitigate this issue, we applied extractive summarization, selecting the 10 most representative segments from each textbook. Unlike abstractive methods, the extractive approach preserves the original text complexity within the chosen segments. The summarization process involves an initial pass through the textbook, segmenting the content into discrete paragraphs. Each paragraph was subsequently embedded using thecointegrated/LaBSE-en-ru(https://huggingface.co/cointegrated/LaBSE-en-ru, accessed on 28 November 2025) model, which is specifically fine-tuned for sentence embeddings in the Russian language. To address potential disruptions in text cohesion caused by simple paragraph-based splitting, we apply thesplit_optimalmethod from the textsplit library [30]. This approach ensured summarization coherence by grouping adjacent text fragments with high cosine similarity. Following this re-segmentation, we exclude segments with fewer than 100 tokens and further split segments exceeding 512 tokens. After the segmentation phase, centrality scores for each segment were computed using the LexRank algorithm [31], a graph-based extractive summarization algorithm that relies on PageRank [32]. In extractive summarization, the most important content is often defined by centrality, reflecting the intuition that the sentences or segments most similar to the rest of the document embody its core meaning [33]. In particular, LexRank pinpoints the salient sentences that collectively represent the original text [34]. Using cosine similarity as edge weights, the model ranks all fragments based on their connections. A highly connected segment (similar to many other segments or to highly ranked segments) accumulates a high centrality score. In effect, the PageRank-like computation will elevate segments by how representative they are of the overall text. Segments that cover content common to multiple parts of the document naturally rise to the top, as they are strongly connected to the rest of the document through content similarity. After processing each document, the ten segments with the highest centrality for each textbook were chosen as the summary. The selected key segments serve as reliable proxies for the textbook‚Äôs overall complexity level. By using these central text fragments, our approach ensures that complexity assessments remain both representative of the document and computationally efficient. This enabled us to significantly reduce the volume of tokens required for further processing by LLMs, resulting in lower computational overhead and reduced operational costs. To deliver a qualitative dimension to our analysis, we visualized the key segments within the embedding space (seeFigure 1). We can observe that the selected key phrases are well distributed across the embedding space, capturing a broad range of thematic and semantic aspects of the textbook. This supports the claim that our methodology effectively identifies central passages, ensuring that the summary accurately reflects the text‚Äôs complexity and thematic focus. Figure 1.The graph displays the distribution of text segments in the embedding space for the 4th Grade Math schoolbook. Light blue points represent general segments, and dark blue points indicate the key segments selected through extractive summarization.",
            "3.3. Assessing the Readability with LLM Prompting": "LLMs have increasingly been employed to assess text readability and evaluate reading comprehension [27,29]. Unlike traditional readability metrics, such as Flesch‚ÄìKincaid Reading Ease score [17], LLMs can exhibit intrinsic linguistic understanding and can offer more nuanced assessments. These methodologies can broadly be categorized into three distinct approaches: LLMs as Readability Evaluators [21], Prompt-Based Comprehension Testing [35,36], and Automated Response Grading and Summarization [9]. The first approach involves using LLMs as evaluators to determine the reading level or complexity of a passage. For example, an LLM can be prompted to assess the complexity of a text, allowing for a nuanced evaluation of whether a topic or prior knowledge requirements might increase the difficulty, areas where traditional metrics may fall short [27]. In the second approach, LLMs are prompted to answer questions based on the text, simulating how a student‚Äôs comprehension would be tested. This method uses the LLM as a stand-in for a student [37,38]. The final approach employs LLMs to evaluate short-answer or essay responses, assessing both correctness and comprehension. In this role, the LLM acts as a proxy for a teacher, providing a detailed evaluation in natural language that goes beyond assigning a simple numerical score. It can replicate the qualitative judgment a teacher might apply, checking that important information is not missing or distorted [9,39,40]. In this study, we employ a hybrid methodology that leverages all three strategies, first by prompting the LLM to determine the target grade level for each text fragment, producing a numeric prediction between grades 2 and 11. To gain a deeper understanding of the LLM‚Äôs decision-making process, we request the extraction of 5 key phrases that highlight the text‚Äôs complexity. Additionally, we simulate a comprehension assessment by prompting the LLM to adopt the perspective of an n-th grade student and evaluate whether the fragment would be accessible to students at that educational level. Finally, we instruct the model to simplify texts designed for grades 5 through 11 to assess its capability to reduce text complexity by 3 grade levels. In addition to these methods, we also measure the model‚Äôs perplexity during response generation. Perplexity is a quantitative measure of the model‚Äôs capability to process long-form texts. Generally, a lower perplexity score indicates better language modeling performance, suggesting the model generates coherent, contextually appropriate responses. This metric has proven effective in evaluating LLMs‚Äô capability to manage and interpret lengthy text passages [41].",
            "3.4. Experimental Setup": "To address our research questions, we designed a series of experiments that investigate the effectiveness of LLMs in evaluating and understanding the complexity of Russian schoolbook texts. By leveraging the previously mentioned key sequences extracted from the textbook corpus, we devised 3 experiments to highlight the model‚Äôs abilities. These experiments were conducted using GPT-4o with a temperature setting of 0.5, ensuring a balanced approach to creativity and response consistency. We accessed GPT-4o via the official OpenAI API endpoint, programmatically submitting prompts and retrieving completions using a predefined template for each experiment. This template ensured that the model consistently adhered to structured output expectations, delivering answers in a predefined JSON format. The first experiment aimed to evaluate the LLM‚Äôs capability to accurately assess the complexity of Russian educational texts. We prompted the model with the following query: ‚ÄúFor what age would this Russian-language school text be suitable, assuming that the reader is a native Russian speaker? Additionally, extract a list of five key phrases that best pinpoint the complexity level of the text. Write your answer as a JSON with the following structure: {‚Äôage‚Äô: 0, ‚Äôphrases‚Äô: [‚Äô..‚Äô]}.‚Äù This task not only required the LLM to predict the grade level, ranging from 2 to 11, but also to provide insights into its decision-making process by highlighting the specific linguistic or thematic elements that influenced its judgment. By examining the extracted key phrases, we aim to reveal the features the model uses to assess text complexity and to uncover how closely the LLM‚Äôs assessments align with established educational standards. In the second experiment, we shifted the focus to the evaluation of text cognitive accessibility from a student‚Äôs perspective. The LLM was prompted with the following instruction: ‚ÄúThis text is in your [Subject] class schoolbook. Is this text comprehensible for you? Answer in English. Let‚Äôs think step by step. It‚Äôs crucial that the final sentence contains your conclusion in the form ‚ÄòAnswer: yes/no‚Äô.‚Äù Here, the model was instructed to simulate the thought process of a student, critically analyzing the text‚Äôs clarity and accessibility. By instructing the LLM to adopt a step-by-step approach, we encouraged it to articulate the rationale behind its conclusions [42], enabling us to assess not only the final answer but also the reasoning path leading to it. This approach aimed to validate whether the model could effectively serve as a proxy for human judgment in educational settings, identifying specific features that contribute to or hinder comprehension. The third experiment extended our analysis to the domain of text simplification. We explored whether the LLM could actively reduce the complexity of schoolbook texts to match a lower grade level, aiming to reduce the complexity by approximately three school years. The prompt provided to the model was:‚ÄúRephrase this Russian text in a way that would be comprehensible for a Russian student in the 6th grade (12 years old). Reply in Russian. No other comments.‚ÄùThis experiment required the model to transform the original text while preserving essential information, making sure that its content remained valuable yet accessible to younger students. In this experiment, we specifically targeted texts from Russian Language, Social Studies, and History, since these subjects rely more on lexical complexity, unlike subjects such as Mathematics or Informatics, where structural and formulaic complexity play a more dominant role. Additionally, these subjects span a broader range of grade levels than most other disciplines. By analyzing simplified outputs, we aimed to determine whether LLMs could be used to develop simplification strategies across different subjects while still maintaining the intended educational outcomes. In conjunction, these experiments offer a multifaceted evaluation of LLMs in the context of Russian-language textbook evaluation and facilitate the analysis of LLMs‚Äô immediate effectiveness and the exploration of their broader potential to support teachers and educators in developing age-appropriate and pedagogically sound materials.",
            "4. Results": "In this section, we present a comprehensive analysis of the LLM‚Äôs capability to assess the reading complexity of Russian educational texts across multiple dimensions. Our evaluation is structured around three core experiments designed to probe different facets of model performance: age prediction accuracy, subjective cognitive accessibility, and the ability to simplify a text. The aim is not only to evaluate the model‚Äôs raw prediction accuracy, but also to explore the underlying cognitive and linguistic factors that shape these predictions‚Äîparticularly in the context of how LLMs perceive and assess educational difficulty. 4.1. Experiment 1‚ÄîComprehension Age PredictionIn the first experiment, we evaluate the large language model‚Äôs ability to predict the target comprehension age for Russian schoolbook texts across various academic subjects. This assessment serves as a baseline for understanding how well the model aligns with age-appropriate educational standards in the Russian curriculum.To this end, we evaluate the performance of the LLM in predicting the age of comprehension for Russian schoolbook texts, based on their lexical complexity and information content (Table 3).Table 3.Performance Metrics by Subject over the whole dataset.InTable 3, we evaluate the performance of the LLM in predicting the age of comprehension for Russian schoolbook texts, based on their lexical complexity and information content. Overall, the LLM reasonably estimated the target age group for the presented texts, with predictions generally aligning with the intended grade levels. Our findings indicate that the mean absolute error (MAE) across most subjects is approximately one year. Considering the natural overlap of educational content between adjacent grade levels, this degree of error is within a reasonable and acceptable range.Figure 2illustrates the relationship between the predicted and actual comprehension ages for each text segment, with marker size proportional to the frequency of that (actual, predicted) pair. The red LOESS smoothing curve depicts the overall trend, while the dashed diagonal line (y = x) represents perfect predictions. The plot shows that GPT-4o‚Äôs predictions broadly follow the expected grade levels but exhibit a systematic bias. It is overestimating the difficulty of texts for younger students and underestimating the target age for more complex fragments, consistent with the quantitative results reported above.Figure 2.Relationship between predicted and actual comprehension ages across Russian school textbook fragments. The marker size is proportional to the frequency of the ‚Äúactual age‚Äù‚Äì‚Äúpredicted age‚Äù pair.We analyzed the log probabilities of predicted output tokens to quantify the model‚Äôs confidence, specifically examining the probability associated with the token that encodes the predicted age and the cumulative joint probability of the preceding tokens. The joint probability was computed as the exponential of the sum of all log probabilities from the beginning of the generated sequence up to and including the age token, effectively capturing the total likelihood of the model producing that specific prediction path. This measure provides a more expressive indicator of confidence, as it reflects not only the certainty of the final numeric token for the age but also the probability associated with the reasoning chain leading to it. The mean probability derived from this analysis reflects the LLM‚Äôs confidence in its predictions. Interestingly, the table shows that while the Music subject shows an MAE approaching 2 years, the model maintains a high degree of confidence in its predictions; nevertheless, these textbooks have the shortest history and tradition and, as such, lack consistency. Moreover, this observation suggests that the model‚Äôs confidence does not always align with actual prediction accuracy, particularly in subjects with less structured content.The relatively high error rate in Informatics is understandable, given the advanced technical concepts that may not align well with the cognitive level of younger students. However, the significantly higher error rates for Art and Music are less intuitive and require further investigation. This phenomenon may be partially explained by Solovyev et al. [3], Solnyshkina et al. [43], who noted that 5th-grade textbooks sometimes contain a higher density of specialized terms than those for higher grades. Future research could examine whether this discrepancy arises from difficulties in evaluating creative, subjective content or from biases in the training data.One notable trend was the LLM‚Äôs systematic overestimation of text complexity. On average, errors were predominantly skewed towards assigning a higher age group than expected when the model‚Äôs predictions had an MAE over 1 (seeFigure 3). This tendency suggests a conservative bias in the model‚Äôs complexity assessments, potentially reflecting a cautious interpretation of challenging vocabulary or abstract concepts within the texts. In the case of higher error subjects, such as Art, Informatics, Music, and Physics, this skew is more pronounced (seeFigure 4).Figure 3.Overall distribution of prediction errors for comprehension ages over all school textbooks.Figure 4.Error distributions of comprehension age predictions for (a) Art and (b) Music textbooks.Focusing on the relationship between the model‚Äôs confidence levels and its prediction accuracy, we extract only the segments with an MAE of 2 years or higher inTable 4. Here, we also notice no strong correlation between confidence and correctness. Even when the model produced erroneous age predictions, its confidence scores remained comparable to or even higher than the average. Physics was the only subject in which a notable drop in confidence was consistently observed for incorrect predictions, suggesting a degree of uncertainty in the model‚Äôs processing of physics-related material that is less evident in other subjects.Table 4.Mean Probability for erroneous predictions with an age difference higher than 2 years.The mean perplexity scores were relatively uniform across all subjects. However, an increase in perplexity was observed alongside erroneous predictions in Physics texts. This elevated perplexity suggests that the model found these texts particularly challenging to process, potentially contributing to its reduced accuracy and confidence in this subject area.Table 5analyzes extreme outliers identified within the prediction dataset. Observations indicate that these outliers predominantly belong to the subjects of Russian, Physics, and Informatics. Notably, the elevated Mean Absolute Error (MAE) values for Physics (1.32) and Informatics (1.73) suggest difficulties in accurately assessing the appropriate comprehension age for these subjects. Contrastingly, Russian exhibits a lower MAE, despite its frequent appearance among the outliers.Table 5.Prediction results for different subjects, including predicted age, probability scores, perplexity, and error.To better understand the underlying problem, we focus on the 3 Russian-language outliers (seeFigure 5). All 3 texts are Literary and, in isolation, might appear of much lower complexity than the intended age target. The first example is a narrative excerpt featuring a dialogue between a forester and the narrator about spring. The vocabulary is mostly accessible, and the dialogue structure aids comprehension. The second text is a famous example of grammatically structured nonsense, created by the writer Ludmila Petrushevskaya to illustrate linguistic principles. The vocabulary is almost entirely composed of nonsense words, but the syntax follows standard Russian patterns. Due to its playfulness, the text might be regarded as targeting a lower age than the textbook was designed for. We also observe a similar case for the third text, a lullaby poem by Vasily Lebedev-Kumach. If we take it by itself, without the literary analysis in the textbook, the poem is indeed aimed at a much younger audience. These instances underscore a limitation inherent in text segmentation approaches: the partial removal of surrounding context, particularly the pedagogical framework (e.g., analytical tasks), can lead to inaccurate complexity assessments and produce such outliers.Figure 5.Outliers in the Russian-language subject with an error in prediction of over 4 years.To gain deeper insights into the linguistic features that influence the model‚Äôs decisions, we analyzed the key phrases the LLM extracted during its assessments. As Viswanathan et al. [44] and Tarekegn [45] show, LLMs are highly effective at producing contextually relevant keyphrase expansions, which can enhance document clustering. To structure and interpret these key phrases, we applied the BERTopic framework [46], which clusters short texts based on semantic similarity using transformer-based embeddings. This method yielded approximately 39 distinct clusters (seeFigure 6), revealing patterns in the model‚Äôs interpretive framework. Each cluster was automatically labeled by BERTopic using the most representative features‚Äîtypically high-frequency or contextually significant n-grams within the cluster‚Äîproviding interpretable topic names. Notably, many top clusters were closely tied to specific subjects, indicating that the model associates particular linguistic or conceptual motifs with complexity in those domains. Prominent examples of these clusters include ‚ÄúInterconnected Aspects of Life, Nature, and Human Culture‚Äù, ‚ÄúHistorical and Cultural Dynamics of Russia and Europe‚Äù, ‚ÄúIntersections of Nature, Communication, and Society‚Äù, ‚ÄúChallenges of Warfare, Disease, Substance Abuse, and Social Conflict‚Äù, ‚ÄúQuantitative Descriptions and Numerical Comparisons Across Various Contexts‚Äù.Figure 6.Key phrases clustering that underlines the complexity of the fragments, as extracted by the LLM. Each color represents a detected cluster.These findings suggest that the model‚Äôs complexity judgments are not based solely on surface-level metrics such as sentence length or vocabulary difficulty, but also involve a deeper analysis of thematic and contextual elements. The identified clusters indicate that the themes, topics, and underlying narrative influence the LLM‚Äôs decision. 4.2. Experiment 2‚ÄîEase of Understanding ClassificationIn our second experiment, we prompted the LLM to assess the ease of understanding the selected fragments.Table 6shows that, for most subjects, the fragments are evaluated as being comprehensible. This finding suggests that the model is generally aligned with a student‚Äôs perspective when assessing the clarity and accessibility of educational content.Table 6.Ease of understanding Assessment by Subject. High incomprehensible rates ( > 20%) marked bold.However, there were notable exceptions to this trend. Texts related to Russian, Informatics, Biology, and History exhibited higher comprehension difficulty scores. These subjects pose unique challenges, possibly due to more advanced language requirements or technical terminology that students at lower grade levels may find difficult to grasp.Interestingly, the relationship between the model‚Äôs prediction errors in Experiment 1 and the ease of understanding assessments in Experiment 2 was not straightforward. Not all subjects with high age prediction errors (e.g., Art, Music) showed similarly low intelligibility. This discrepancy indicates that while the model might overestimate the complexity of certain texts, it does not necessarily perceive them as incomprehensible, suggesting the presence of some biases in its training data.To gain deeper insights into the model‚Äôs decision-making process, we specifically analyzed the conclusion the LLM reached in its chain of thought for cases in which texts were deemed incomprehensible. By applying clustering techniques to these concluding statements, we identified seven distinct clusters, each reflecting specific factors that might hinder comprehension:Challenging Texts for Young Students: Complexity in Language, Concepts, and Contexts;Challenges in Comprehending Complex Instructions and Concepts: Across Educational Levels;Challenges of Understanding Complex Language and Concepts: At Elementary and Middle School Levels;Complexity of Educational Texts for Early Grade Students: Challenges in Comprehension and Vocabulary;Challenges of Complex Language and Concepts for Young Russian StudentsComplex Topics and Vocabulary: Beyond the Understanding of Young Children;Comprehension with Guidance and Instructional Support.Similar to the first experiment, these clusters indicate that the model‚Äôs judgments are influenced not only by linguistic difficulty but also by the concepts and contextual demands presented by the documents. Particularly, the clusters emphasize challenges related to abstract language, complex vocabulary, complex instructions, and the need for guidance and support to facilitate understanding. By identifying these themes, we provide valuable guidance for future research, particularly in developing educational tools to support students at different cognitive and developmental stages. 4.3. Experiment 3‚ÄîAcademic Text Simplification with LLMsUsing a previously trained BERT classification model [20], we assessed the generated texts to determine whether the targeted complexity reduction was achieved, namely a shift of 3 school years. The confusion matrix for the model‚Äôs prediction (seeFigure 7) indicates a significant performance gap, with the classification model achieving very low accuracy on the generated set. The overall accuracy of the BERT model on the generated texts was 14.88%, reflecting the difficulty in aligning the generated text complexity with the targeted grade levels. The very low Recall in almost all target grades further highlights the significant discrepancy between the intended and achieved complexity levels.Figure 7.Confusion matrix of the BERT-based predictions on the simplified texts (higher color intensity for higher agreement). As the simplification aimed to reduce complexity by three grade levels, no true labels exist for grades 9‚Äì11. Nevertheless, BERT assigns a number of samples to these higher-grade categories.One particular observation is that most prediction errors aligned precisely with a three-year gap (seeFigure 8), the exact reduction target. This pattern implies that the LLM largely failed to effectively lower the complexity of the texts. Instead of adapting the material to the intended, simpler level, the model maintained complexity closer to the original grade level, suggesting a potential limitation in its simplification capabilities or in its interpretation of the prompt.Figure 8.Error distribution for the Pushkin 100 score for the generated, simplified texts, BERT predicted grade for the simplified texts, compared to the error for the original texts.To further validate these findings, we employ the Textometr backend as described by [47], with added use of the internalformula_pushkin_100metric3‚Äîa composite measure of grammatical complexity (structure_complex), lexical coverage (lexical_complex), andnarrativity. While the 2021 paper explains the ingredients of these features (average sentence length, lexical list coverage, Flesch-style readability, passive/participle counts), it does not formally publish the aggregation formula above. We nevertheless use this formula, as it maps to the well-known Flesch‚ÄìKincaid readability score (ranging from 0 to 100), has a straightforward open-source implementation, and serves as the foundation of the Russian Text Complexity Analyzer developed by the Pushkin State Russian Language Institute (https://textometr.ru, accessed on 28 November 2025). Interestingly, according to this metric, the LLM performed bette, with an error distribution similar to that when applied to the original texts (seeFigure 8). When examining the score distributions, we observed a notable leftward shift in the mean scores from 60.52 to 37.19, indicating that the model did achieve a reduction in complexity, albeit not to the full extent initially targeted (seeFigure 9). The Wilcoxon signed-rank test revealed a statistically significant downward shift in the distribution of scores for the newly generated texts relative to the Pushkin 100 scores from the original text samples (p< 0.001). This shift suggests that while the model may not have precisely met the specified grade-level adjustment, it did contribute to a general simplification of the texts.Figure 9.Distributions of Pushkin 100 scores (generated versus original texts). Mean value marked with a red vertical line and the standard deviation represented in green. (a) Distribution of the Pushkin 100 score for the generated, simplified texts. (b) Distribution of the Pushkin 100 score for the original texts.A particularly surprising outcome was the variation in complexity reduction across subjects, as measured by this metric (Table 7). While all subjects exhibited a clear decrease in complexity, the Russian-language subject showed the smallest reduction, from a mean score of 38.34 to one of 25.45. This finding may reflect the inherent challenges of simplifying texts in the literature domain, where maintaining grammatical structure, stylistic nuances, and educational appropriateness adds complexity to the simplification process. For example, theoretical texts often contain specialized terminology and dense explanations, instructional materials [48] that prioritize clarity and procedural accuracy, while literary texts emphasize narrative style and expressive language. Keeping this style intact while maintaining grammatical integrity and educational value increases the task‚Äôs complexity.Table 7.Comparison of original and modified average metrics and predicted grades or ages across subjects.Further, we analyze the outliers in these newly generated texts. namely, text where the complexity score increased through generation, or texts where the complexity scores remained unchanged. InFigure 10, we observe the two samples where the complexity score increased after the generation based on Pushkin 100 scores. Both textbook samples that showed an increase in complexity are for the 5th grade. The first text from the Russian subject textbook is a fill-in-the-blanks task with adjectives. In the generated text, the LLM filled in the blanks, thereby increasing the vocabulary and complexity of the resulting text. The other is from a social studies book. Here we can see the LLM trying to simplify the texts by shortening the phrases and using more colloquial language. Since the original score was relatively low, we can assume that small changes could increase it by 4‚Äì5 points. Overall, these outliers are exceptions, and we can infer that the LLM generally reduces the complexity of the processed texts.Figure 10.Samples of generations with increased complexity.",
            "4.1. Experiment 1‚ÄîComprehension Age Prediction": "In the first experiment, we evaluate the large language model‚Äôs ability to predict the target comprehension age for Russian schoolbook texts across various academic subjects. This assessment serves as a baseline for understanding how well the model aligns with age-appropriate educational standards in the Russian curriculum. To this end, we evaluate the performance of the LLM in predicting the age of comprehension for Russian schoolbook texts, based on their lexical complexity and information content (Table 3). Table 3.Performance Metrics by Subject over the whole dataset. InTable 3, we evaluate the performance of the LLM in predicting the age of comprehension for Russian schoolbook texts, based on their lexical complexity and information content. Overall, the LLM reasonably estimated the target age group for the presented texts, with predictions generally aligning with the intended grade levels. Our findings indicate that the mean absolute error (MAE) across most subjects is approximately one year. Considering the natural overlap of educational content between adjacent grade levels, this degree of error is within a reasonable and acceptable range. Figure 2illustrates the relationship between the predicted and actual comprehension ages for each text segment, with marker size proportional to the frequency of that (actual, predicted) pair. The red LOESS smoothing curve depicts the overall trend, while the dashed diagonal line (y = x) represents perfect predictions. The plot shows that GPT-4o‚Äôs predictions broadly follow the expected grade levels but exhibit a systematic bias. It is overestimating the difficulty of texts for younger students and underestimating the target age for more complex fragments, consistent with the quantitative results reported above. Figure 2.Relationship between predicted and actual comprehension ages across Russian school textbook fragments. The marker size is proportional to the frequency of the ‚Äúactual age‚Äù‚Äì‚Äúpredicted age‚Äù pair. We analyzed the log probabilities of predicted output tokens to quantify the model‚Äôs confidence, specifically examining the probability associated with the token that encodes the predicted age and the cumulative joint probability of the preceding tokens. The joint probability was computed as the exponential of the sum of all log probabilities from the beginning of the generated sequence up to and including the age token, effectively capturing the total likelihood of the model producing that specific prediction path. This measure provides a more expressive indicator of confidence, as it reflects not only the certainty of the final numeric token for the age but also the probability associated with the reasoning chain leading to it. The mean probability derived from this analysis reflects the LLM‚Äôs confidence in its predictions. Interestingly, the table shows that while the Music subject shows an MAE approaching 2 years, the model maintains a high degree of confidence in its predictions; nevertheless, these textbooks have the shortest history and tradition and, as such, lack consistency. Moreover, this observation suggests that the model‚Äôs confidence does not always align with actual prediction accuracy, particularly in subjects with less structured content. The relatively high error rate in Informatics is understandable, given the advanced technical concepts that may not align well with the cognitive level of younger students. However, the significantly higher error rates for Art and Music are less intuitive and require further investigation. This phenomenon may be partially explained by Solovyev et al. [3], Solnyshkina et al. [43], who noted that 5th-grade textbooks sometimes contain a higher density of specialized terms than those for higher grades. Future research could examine whether this discrepancy arises from difficulties in evaluating creative, subjective content or from biases in the training data. One notable trend was the LLM‚Äôs systematic overestimation of text complexity. On average, errors were predominantly skewed towards assigning a higher age group than expected when the model‚Äôs predictions had an MAE over 1 (seeFigure 3). This tendency suggests a conservative bias in the model‚Äôs complexity assessments, potentially reflecting a cautious interpretation of challenging vocabulary or abstract concepts within the texts. In the case of higher error subjects, such as Art, Informatics, Music, and Physics, this skew is more pronounced (seeFigure 4). Figure 3.Overall distribution of prediction errors for comprehension ages over all school textbooks. Figure 4.Error distributions of comprehension age predictions for (a) Art and (b) Music textbooks. Focusing on the relationship between the model‚Äôs confidence levels and its prediction accuracy, we extract only the segments with an MAE of 2 years or higher inTable 4. Here, we also notice no strong correlation between confidence and correctness. Even when the model produced erroneous age predictions, its confidence scores remained comparable to or even higher than the average. Physics was the only subject in which a notable drop in confidence was consistently observed for incorrect predictions, suggesting a degree of uncertainty in the model‚Äôs processing of physics-related material that is less evident in other subjects. Table 4.Mean Probability for erroneous predictions with an age difference higher than 2 years. The mean perplexity scores were relatively uniform across all subjects. However, an increase in perplexity was observed alongside erroneous predictions in Physics texts. This elevated perplexity suggests that the model found these texts particularly challenging to process, potentially contributing to its reduced accuracy and confidence in this subject area. Table 5analyzes extreme outliers identified within the prediction dataset. Observations indicate that these outliers predominantly belong to the subjects of Russian, Physics, and Informatics. Notably, the elevated Mean Absolute Error (MAE) values for Physics (1.32) and Informatics (1.73) suggest difficulties in accurately assessing the appropriate comprehension age for these subjects. Contrastingly, Russian exhibits a lower MAE, despite its frequent appearance among the outliers. Table 5.Prediction results for different subjects, including predicted age, probability scores, perplexity, and error. To better understand the underlying problem, we focus on the 3 Russian-language outliers (seeFigure 5). All 3 texts are Literary and, in isolation, might appear of much lower complexity than the intended age target. The first example is a narrative excerpt featuring a dialogue between a forester and the narrator about spring. The vocabulary is mostly accessible, and the dialogue structure aids comprehension. The second text is a famous example of grammatically structured nonsense, created by the writer Ludmila Petrushevskaya to illustrate linguistic principles. The vocabulary is almost entirely composed of nonsense words, but the syntax follows standard Russian patterns. Due to its playfulness, the text might be regarded as targeting a lower age than the textbook was designed for. We also observe a similar case for the third text, a lullaby poem by Vasily Lebedev-Kumach. If we take it by itself, without the literary analysis in the textbook, the poem is indeed aimed at a much younger audience. These instances underscore a limitation inherent in text segmentation approaches: the partial removal of surrounding context, particularly the pedagogical framework (e.g., analytical tasks), can lead to inaccurate complexity assessments and produce such outliers. Figure 5.Outliers in the Russian-language subject with an error in prediction of over 4 years. To gain deeper insights into the linguistic features that influence the model‚Äôs decisions, we analyzed the key phrases the LLM extracted during its assessments. As Viswanathan et al. [44] and Tarekegn [45] show, LLMs are highly effective at producing contextually relevant keyphrase expansions, which can enhance document clustering. To structure and interpret these key phrases, we applied the BERTopic framework [46], which clusters short texts based on semantic similarity using transformer-based embeddings. This method yielded approximately 39 distinct clusters (seeFigure 6), revealing patterns in the model‚Äôs interpretive framework. Each cluster was automatically labeled by BERTopic using the most representative features‚Äîtypically high-frequency or contextually significant n-grams within the cluster‚Äîproviding interpretable topic names. Notably, many top clusters were closely tied to specific subjects, indicating that the model associates particular linguistic or conceptual motifs with complexity in those domains. Prominent examples of these clusters include ‚ÄúInterconnected Aspects of Life, Nature, and Human Culture‚Äù, ‚ÄúHistorical and Cultural Dynamics of Russia and Europe‚Äù, ‚ÄúIntersections of Nature, Communication, and Society‚Äù, ‚ÄúChallenges of Warfare, Disease, Substance Abuse, and Social Conflict‚Äù, ‚ÄúQuantitative Descriptions and Numerical Comparisons Across Various Contexts‚Äù. Figure 6.Key phrases clustering that underlines the complexity of the fragments, as extracted by the LLM. Each color represents a detected cluster. These findings suggest that the model‚Äôs complexity judgments are not based solely on surface-level metrics such as sentence length or vocabulary difficulty, but also involve a deeper analysis of thematic and contextual elements. The identified clusters indicate that the themes, topics, and underlying narrative influence the LLM‚Äôs decision.",
            "4.2. Experiment 2‚ÄîEase of Understanding Classification": "In our second experiment, we prompted the LLM to assess the ease of understanding the selected fragments.Table 6shows that, for most subjects, the fragments are evaluated as being comprehensible. This finding suggests that the model is generally aligned with a student‚Äôs perspective when assessing the clarity and accessibility of educational content. Table 6.Ease of understanding Assessment by Subject. High incomprehensible rates ( > 20%) marked bold. However, there were notable exceptions to this trend. Texts related to Russian, Informatics, Biology, and History exhibited higher comprehension difficulty scores. These subjects pose unique challenges, possibly due to more advanced language requirements or technical terminology that students at lower grade levels may find difficult to grasp. Interestingly, the relationship between the model‚Äôs prediction errors in Experiment 1 and the ease of understanding assessments in Experiment 2 was not straightforward. Not all subjects with high age prediction errors (e.g., Art, Music) showed similarly low intelligibility. This discrepancy indicates that while the model might overestimate the complexity of certain texts, it does not necessarily perceive them as incomprehensible, suggesting the presence of some biases in its training data. To gain deeper insights into the model‚Äôs decision-making process, we specifically analyzed the conclusion the LLM reached in its chain of thought for cases in which texts were deemed incomprehensible. By applying clustering techniques to these concluding statements, we identified seven distinct clusters, each reflecting specific factors that might hinder comprehension: Challenging Texts for Young Students: Complexity in Language, Concepts, and Contexts;Challenges in Comprehending Complex Instructions and Concepts: Across Educational Levels;Challenges of Understanding Complex Language and Concepts: At Elementary and Middle School Levels;Complexity of Educational Texts for Early Grade Students: Challenges in Comprehension and Vocabulary;Challenges of Complex Language and Concepts for Young Russian StudentsComplex Topics and Vocabulary: Beyond the Understanding of Young Children;Comprehension with Guidance and Instructional Support. Similar to the first experiment, these clusters indicate that the model‚Äôs judgments are influenced not only by linguistic difficulty but also by the concepts and contextual demands presented by the documents. Particularly, the clusters emphasize challenges related to abstract language, complex vocabulary, complex instructions, and the need for guidance and support to facilitate understanding. By identifying these themes, we provide valuable guidance for future research, particularly in developing educational tools to support students at different cognitive and developmental stages.",
            "4.3. Experiment 3‚ÄîAcademic Text Simplification with LLMs": "Using a previously trained BERT classification model [20], we assessed the generated texts to determine whether the targeted complexity reduction was achieved, namely a shift of 3 school years. The confusion matrix for the model‚Äôs prediction (seeFigure 7) indicates a significant performance gap, with the classification model achieving very low accuracy on the generated set. The overall accuracy of the BERT model on the generated texts was 14.88%, reflecting the difficulty in aligning the generated text complexity with the targeted grade levels. The very low Recall in almost all target grades further highlights the significant discrepancy between the intended and achieved complexity levels. Figure 7.Confusion matrix of the BERT-based predictions on the simplified texts (higher color intensity for higher agreement). As the simplification aimed to reduce complexity by three grade levels, no true labels exist for grades 9‚Äì11. Nevertheless, BERT assigns a number of samples to these higher-grade categories. One particular observation is that most prediction errors aligned precisely with a three-year gap (seeFigure 8), the exact reduction target. This pattern implies that the LLM largely failed to effectively lower the complexity of the texts. Instead of adapting the material to the intended, simpler level, the model maintained complexity closer to the original grade level, suggesting a potential limitation in its simplification capabilities or in its interpretation of the prompt. Figure 8.Error distribution for the Pushkin 100 score for the generated, simplified texts, BERT predicted grade for the simplified texts, compared to the error for the original texts. To further validate these findings, we employ the Textometr backend as described by [47], with added use of the internalformula_pushkin_100metric3‚Äîa composite measure of grammatical complexity (structure_complex), lexical coverage (lexical_complex), andnarrativity. While the 2021 paper explains the ingredients of these features (average sentence length, lexical list coverage, Flesch-style readability, passive/participle counts), it does not formally publish the aggregation formula above. We nevertheless use this formula, as it maps to the well-known Flesch‚ÄìKincaid readability score (ranging from 0 to 100), has a straightforward open-source implementation, and serves as the foundation of the Russian Text Complexity Analyzer developed by the Pushkin State Russian Language Institute (https://textometr.ru, accessed on 28 November 2025). Interestingly, according to this metric, the LLM performed bette, with an error distribution similar to that when applied to the original texts (seeFigure 8). When examining the score distributions, we observed a notable leftward shift in the mean scores from 60.52 to 37.19, indicating that the model did achieve a reduction in complexity, albeit not to the full extent initially targeted (seeFigure 9). The Wilcoxon signed-rank test revealed a statistically significant downward shift in the distribution of scores for the newly generated texts relative to the Pushkin 100 scores from the original text samples (p< 0.001). This shift suggests that while the model may not have precisely met the specified grade-level adjustment, it did contribute to a general simplification of the texts. Figure 9.Distributions of Pushkin 100 scores (generated versus original texts). Mean value marked with a red vertical line and the standard deviation represented in green. (a) Distribution of the Pushkin 100 score for the generated, simplified texts. (b) Distribution of the Pushkin 100 score for the original texts. A particularly surprising outcome was the variation in complexity reduction across subjects, as measured by this metric (Table 7). While all subjects exhibited a clear decrease in complexity, the Russian-language subject showed the smallest reduction, from a mean score of 38.34 to one of 25.45. This finding may reflect the inherent challenges of simplifying texts in the literature domain, where maintaining grammatical structure, stylistic nuances, and educational appropriateness adds complexity to the simplification process. For example, theoretical texts often contain specialized terminology and dense explanations, instructional materials [48] that prioritize clarity and procedural accuracy, while literary texts emphasize narrative style and expressive language. Keeping this style intact while maintaining grammatical integrity and educational value increases the task‚Äôs complexity. Table 7.Comparison of original and modified average metrics and predicted grades or ages across subjects. Further, we analyze the outliers in these newly generated texts. namely, text where the complexity score increased through generation, or texts where the complexity scores remained unchanged. InFigure 10, we observe the two samples where the complexity score increased after the generation based on Pushkin 100 scores. Both textbook samples that showed an increase in complexity are for the 5th grade. The first text from the Russian subject textbook is a fill-in-the-blanks task with adjectives. In the generated text, the LLM filled in the blanks, thereby increasing the vocabulary and complexity of the resulting text. The other is from a social studies book. Here we can see the LLM trying to simplify the texts by shortening the phrases and using more colloquial language. Since the original score was relatively low, we can assume that small changes could increase it by 4‚Äì5 points. Overall, these outliers are exceptions, and we can infer that the LLM generally reduces the complexity of the processed texts. Figure 10.Samples of generations with increased complexity.",
            "5. Discussion": "This study aimed to provide an exploration and evaluation of a state-of-the-art LLM, specifically GPT-4o, in assessing the complexity, judging the ease of understanding, and performing targeted simplification of Russian educational textbook texts. By analyzing the LLM‚Äôs performance across three distinct experimental setups using a curated corpus of Russian schoolbooks, we deepen the understanding of its potential utility and limitations within the Russian educational context. To accomplish these objectives, we examined three core research questions, each targeting a distinct aspect of LLM performance. Our first research question investigated whether LLMs can correctly assess the complexity of Russian-language texts and, during this experiment, identify the key features that have influenced the decisions. The results from Experiment 1 (seeTable 3) show that the LLM adequately estimated the target age group, with a Mean Absolute Error (MAE) generally within 1 school year for most subjects. This suggests a fairly good capability to evaluate the difficulty of a textbook for the average Russian student. However, several important observations emerged. Firstly, we can notice a consistent trend across specific subjects, particularly Art, Informatics, Music, and Physics (seeFigure 4). For these textbooks, the LLM tends to overestimate complexity, predicting a higher age level than the intended one. This bias might stem from the model overestimating factors such as challenging vocabulary, abstract concepts, or specific technical terminology encountered in these subjects. Analysis through the clustering of key phrases extracted by the LLM (seeFigure 6) supports this, indicating that the model‚Äôs decisions are influenced not merely by surface-level metrics (like sentence length or vocabulary used) but also by the addressed topics and domain-specific concepts that are discussed in the evaluated fragments. In addition, the analysis of extreme outliers, especially within the Russian-language subject (Figure 5), highlighted an important limitation: the lack of context when evaluating a textbook fragment. Literary excerpts, when assessed in isolation as text segments, were often judged by the LLM to be far simpler than their intended target age. This occurred because the surrounding pedagogical context within the textbook (such as accompanying literary analysis tasks that significantly increased the effective cognitive load) was necessarily lost during the segmentation process required for analysis. This finding underscores that LLM assessments based on isolated text chunks may not accurately reflect the complexity a student encounters when interacting with the full material. Furthermore, the model‚Äôs confidence, as measured by output token probabilities (seeTable 4andTable 8), did not always correlate with the accuracy. Even in cases of significant error (MAE > 2 years), the model often maintained high confidence, except notably in Geography, History, and Physics, where incorrect predictions were associated with lower confidence. Table 8.Mean errors for predictions with a high confidence. To conclude RQ1, while LLMs show considerable potential in assessing text complexity, their evaluations often display a bias toward overestimation. Notably, the model‚Äôs judgments appear to depend not only on linguistic features but also on the alignment of content themes with the cognitive and developmental expectations of the target age group. This thematic sensitivity, if properly understood and controlled, could serve as a valuable complement to traditional readability metrics. LLMs thus hold promise as valuable tools in augmenting classical methods of comprehension assessment. A promising direction for future research would be the development of techniques to disentangle purely linguistic signals from content- and theme-aware influences in the model‚Äôs predictions. The second research question explored whether LLMs could serve as an effective proxy for student comprehension, simulating the extent to which the text is understandable. Results from Experiment 2 (seeTable 6) suggest a general alignment, with the LLM deeming the majority of fragments across most subjects as ‚Äúcomprehensible.‚Äù This indicates that the model can broadly distinguish between texts that are likely accessible and those that pose significant challenges. However, subjects with lower comprehension ease included Russian, Informatics, Biology, and History. Analysis of the LLM‚Äôs rationale, by clustering its chain-of-thought conclusions, indicated that judgments of lower comprehension ease were often linked to a high terminological density, as well as challenges with complex language, abstract concepts, technical vocabulary, intricate instructions, or the perceived need for external guidance‚Äîfactors that are highly relevant to actual student comprehension difficulties. Interestingly, there was no direct one-to-one mapping between the age-prediction errors observed in Experiment 1 and the cognitive accessibility judgments in Experiment 2, suggesting that the LLM uses distinct pathways for these two tasks. Subjects in which the LLM significantly overestimated age (e.g., Art and Music) did not necessarily show lower comprehension ease. This suggests that the LLM might employ different internal weighting or criteria when assessing abstract ‚Äúcomplexity‚Äù versus simulating direct ‚Äúcomprehension ease,‚Äù potentially separating linguistic difficulty from conceptual accessibility in its judgment process. Our third research question assessed the LLM‚Äôs capability to successfully reduce text complexity to a specific target level, i.e., 3 school years lower across different subjects. The findings here present a mixed picture depending on the evaluation metric used. When evaluated using a BERT-based grade-level classifier (seeTable 9), the LLM achieved very poor performance (14.88% accuracy), largely failing to align the generated text with the target grade. Many prediction errors matched the intended three-year gap exactly, indicating that the model often maintained complexity close to the original level rather than sufficiently simplifying the text. Table 9.Classification Performance Metrics of BERT Model by Grade Level. Conversely, a statistically significant overall reduction in complexity was observed when evaluated using the Pushkin 100 readability score (seeFigure 9) (p< 0.001), evidenced by a clear leftward shift in the mean score distribution, from 60.52 to 37.19 (seeFigure 9). This reflects an average simplification of 23.33 points, corresponding to a decrease in estimated reading age from 14 to 10 years. Given that the Pushkin 100 score predicts comprehension across 2-year age intervals, the observed reduction closely aligns with our intended 3-grade simplification target. Performance varied significantly across subjects, with Russian-language texts showing the least reduction according to the Pushkin 100 metric, potentially reflecting the inherent difficulty of simplifying literary texts while preserving essential meaning and style. Analysis of outliers where complexity increased (seeFigure 5andFigure 10) revealed these were often artifacts of the original task‚Äîe.g., the task of filling blanks in the Russian subject example steers the LLM to perform this task rather than simplifying it, or possibly metric sensitivity to minor changes in already simple texts (e.g., social studies). These outliers were deemed exceptions to the general trend of simplification. Therefore, as a conclusion to RQ3, LLMs can contribute to text simplification, but achieving precise, controlled reduction to a specific pedagogical level (especially one defined by grade levels) remains a challenge that researchers can address. In addition, subject-specific consistent simplification outcomes remain an open research question that merits further exploration. Practical and Ethical ImplicationsLLMs inherit and may amplify biases present in their training data, raising fairness concerns when applied to educational content assessment. These models have been shown to encode both explicit and implicit stereotypes‚Äîfor example, prompts using male-associated names can provide more confident evaluations than those using female-associated names [49]. Cultural and linguistic biases are also a concern. Many LLMs display Anglocentric biases that privilege Western or dominant-curriculum perspectives, which can marginalize content rooted in other languages, cultures, or educational systems [50,51,52]. Such misalignment risks producing unfair assessments of materials that deviate from these norms, potentially disadvantaging learners in diverse cultural contexts.Without deliberate safeguards, these biases can be perpetuated or even intensified by LLM-based tools. Notably, the findings of the present study reflect these broader concerns: even within a specific national context (e.g., Russian-language textbook analysis), the model exhibited some interpretations of textual content that align with known patterns of systemic bias.Beyond these biases, LLMs face key technical limitations when used to evaluate or simplify academic content. Their assessments often rely on surface-level features‚Äîsuch as sentence length or vocabulary‚Äîwithout understanding the deeper conceptual or pedagogical context, leading to over- or underestimations of text complexity [50]. Moreover, LLM-generated feedback lacks the intentionality of human instruction. Unlike educators who adapt explanations based on curriculum goals or student needs, LLMs tend to produce generic simplifications that may overlook important learning objectives or do not align with instructional intent [53]. Thus, over-reliance on LLM-generated simplifications can diminish valuable learning opportunities [54].To realize the benefits of LLMs in education while minimizing risks, their use must be guided by responsible oversight. Educators should treat AI-generated outputs, such as readability ratings or summaries, as starting points, not final judgments, always adapting them to learner needs. Policymakers can support equitable adoption by promoting culturally inclusive AI tools and funding models trained on diverse curricula. As this study and others show, LLMs offer both promise and pitfalls, making ongoing research, human oversight, and strong ethical safeguards essential to ensuring their alignment with sound educational practice.",
            "Practical and Ethical Implications": "LLMs inherit and may amplify biases present in their training data, raising fairness concerns when applied to educational content assessment. These models have been shown to encode both explicit and implicit stereotypes‚Äîfor example, prompts using male-associated names can provide more confident evaluations than those using female-associated names [49]. Cultural and linguistic biases are also a concern. Many LLMs display Anglocentric biases that privilege Western or dominant-curriculum perspectives, which can marginalize content rooted in other languages, cultures, or educational systems [50,51,52]. Such misalignment risks producing unfair assessments of materials that deviate from these norms, potentially disadvantaging learners in diverse cultural contexts. Without deliberate safeguards, these biases can be perpetuated or even intensified by LLM-based tools. Notably, the findings of the present study reflect these broader concerns: even within a specific national context (e.g., Russian-language textbook analysis), the model exhibited some interpretations of textual content that align with known patterns of systemic bias. Beyond these biases, LLMs face key technical limitations when used to evaluate or simplify academic content. Their assessments often rely on surface-level features‚Äîsuch as sentence length or vocabulary‚Äîwithout understanding the deeper conceptual or pedagogical context, leading to over- or underestimations of text complexity [50]. Moreover, LLM-generated feedback lacks the intentionality of human instruction. Unlike educators who adapt explanations based on curriculum goals or student needs, LLMs tend to produce generic simplifications that may overlook important learning objectives or do not align with instructional intent [53]. Thus, over-reliance on LLM-generated simplifications can diminish valuable learning opportunities [54]. To realize the benefits of LLMs in education while minimizing risks, their use must be guided by responsible oversight. Educators should treat AI-generated outputs, such as readability ratings or summaries, as starting points, not final judgments, always adapting them to learner needs. Policymakers can support equitable adoption by promoting culturally inclusive AI tools and funding models trained on diverse curricula. As this study and others show, LLMs offer both promise and pitfalls, making ongoing research, human oversight, and strong ethical safeguards essential to ensuring their alignment with sound educational practice.",
            "6. Conclusions and Future Work": "Our study conducted a comprehensive evaluation of a state-of-the-art LLM, namely GPT-4o, in the context of assessing Russian educational materials to investigate its ability to gauge the complexity and readability of school textbooks. Through a series of experiments using a curated corpus of Russian school textbooks, we identified several insights into the strengths and limitations of current LLM models in this domain. LLMs adequately estimated text complexity and served as a good proxy for student comprehension. However, their complexity assessments frequently overestimated, predicting a higher text complexity. We showed that the model‚Äôs judgments do not rely solely on linguistic features, such as sentence length or vocabulary; they also reflect thematic alignment with students‚Äô age and the conceptual density of the text. Nevertheless, evaluating text segments in isolation, without pedagogical scaffolding or contextual cues (e.g., accompanying tasks or full-unit structure), reduced the reliability of these complexity assessments. In addition to refining the use of GPT-4o, future work will evaluate the performance of alternative LLMs, both open-source and via API, to benchmark their capabilities across educational domains and linguistic contexts. Comparative model analysis could reveal important differences in accuracy, bias, or simplification strategies. Furthermore, to better assess the quality and pedagogical utility of LLM-generated simplifications, we recommend conducting blind human evaluations in which educators rate simplified outputs without knowing whether they were AI- or human-authored. Such studies would offer critical insights into the real-world applicability of LLMs in instructional settings. Overall, our findings indicate that large-scale LLMs, such as GPT-4o, have substantial potential as assistive tools in educational contexts, particularly for Russian-language materials. They can complement traditional complexity-scoring systems and support preliminary assessments and adaptations. However, despite acceptable performance, these models are not yet sufficiently reliable to replace expert human judgment. Future research aimed at aligning LLM outputs with established educational standards, such as through fine-tuning on domain-specific corpora or employing reinforcement learning with readability- and pedagogy-oriented objectives, could offer more reliable results. Nevertheless, current capabilities lay a solid foundation for enhancing conventional readability frameworks and fostering more responsive, personalized educational content."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2078-2489/16/12/1071",
        "scraped_at": "2025-12-05 23:56:51"
    },
    {
        "title": "OTSU-UCAN: An OTSU-Based Integrated Satellite‚ÄìTerrestrial Information System for 6G in Vehicle Navigation",
        "authors": "byYawei Li,Kui Lu,Gang Cao,Shuyu Fan,Mingyue Zhang,Bohan LiandTao Li",
        "journal": "Information2025,16(12), 1072;https://doi.org/10.3390/info16121072- 4 Dec 2025",
        "abstract": "One of the bottlenecks hindering the applications (e.g., vehicle navigation) of blockchain‚ÄìUCAN is privacy. A sharded blockchain can protect vehicle data to a certain extent. However, unbalanced shard loads lead to low throughput and poor feature extraction in blockchain‚ÄìUCAN. This paper proposes an optimal image binarization method (OTSU-GK) to enhance image features and reduce the amount of uploaded data, thereby improving throughput. Specifically, OTSU-GK uses a Gaussian kernel method where the parameters are optimized using grid search to improve the calculation of the threshold. Additionally, we have designed a Node Load Score (NLS)-based sharding blockchain, which considers the number of historical transactions, transaction types, transaction frequency, and other metrics to balance the sharding loads and further improve throughput. The experimental results show that OTSU-GK improves by 74.3%, 58.7%, and 83% in SSIM, RMSE/MAE/AER, and throughput. In addition, it reduces IL by 70.3% compared to other methods.Keywords:UCAN;OTSU;Gaussian kernel;sharding blockchain",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "UCAN (User-centric Access Network) is a de-cellular network with a universal architecture for unified management and flexible organization of diverse APs on various platforms to provide on-demand and consistent services for users Chen et al. [1]. UCAN is fit for 6G scenarios to provide user-centric services by integrating the UCAN architecture into a wide-area network. For example, the satellite-based base stations in UCAN provide backhaul services to moving-based stations, e.g., vehicles. Unfortunately, the privacy of vehicle data (e.g., track information) is undermined since all data is placed on one public blockchain, which is accessible to all nodes on the chain. To solve this problem, a sharding blockchain divides the entire blockchain into multiple shards so that nodes can only access the data of the shard they are on. Thus, data privacy is protected to some extent. However, the image feature extraction in navigation system and the throughput problem of the sharding blockchain, in turn, reduce the efficiency of the whole navigation system. In a navigation system, images collected from cameras are embedded images. There are many challenges in processing embedded images, such as increasing resolution and frame rates while operating at low power Bailey [2]. Image binarization is a fundamental technique that simplifies images to two colours, reducing data complexity, highlighting image features, and enhancing processing efficiency. Recently, researchers have proposed a variety of binarization methods. However, these methods fails in the scenario of blockchain‚ÄìUCAN due to the computational limitations and resource constraints. Therefore, a new UCAN framework is needed to satisfy the requirements of both feature extraction and a sharding blockchain. In this paper, we propose an OTSU-UCAN (ref.Figure 1) by combining two novel OTSU-GK and NLS-Chain techniques. Specifically, color figures collected from the vehicles are binarized by OTSU-GK, where a Gaussian kernel is used to enhance the feature extraction. Grey images are then uploaded to access points (APs) to save storage space. Conversely, AP data is evaluated using the Node Load Score, which takes into account historical transactions, transaction types, and frequency. Finally, the nodes are assigned to different shards according to their targets. The main contributions of this paper are summarized as follows: Figure 1.Framework of the proposed OTSU-GK method. OTSU-GK: The OTSU-GK is a new adaptive thresholding approach, which integrates a Gaussian kernel. The parameters of the Gaussian kernel are optimized through grid search to enhance threshold calculation. OTSU-GK provides a new solution in the field of image binarization.Node Load Score-based (NLS) sharding blockchain: We propose a blockchain segmentation method based on node load scoring to segment blockchain nodes by predicting the number of transactions of each node in the next epoch and calculating the corresponding transaction load scoring, which reflects the node‚Äôs transaction load, with the aim of achieving load balancing between segments.Significant Performance Improvement: Compared to the traditional OTSU method, OTSU-GK shows approximately 50% improvement in SSIM (Structural Similarity), RMSE (root mean square error), and IL (information loss). This indicates that OTSU-GK performs better in image processing, thereby supporting the advancement of embedded image processing.Effectiveness of Parameter Optimization: Ablation experiments confirm the effectiveness of parameter optimization through grid search. When compared to methods with non-optimized parameters, OTSU-GK demonstrates a 14.3% increase in the SSIM metric and a 13% reduction in the IL metric on the KITTI dataset, which indicates that the process of optimizing parameters significantly enhances the performance of OTSU-GK.",
            "2. Related Work": "2.1. Image BinarizationCurrently, image binarization methods include fixed threshold methods, adaptive threshold methods, and deep learning approaches. The fixed threshold method¬†[3] involves setting a fixed threshold to categorize image pixels into foreground and background. This method relies on a fixed threshold for binarization, which requires manual intervention for threshold selection and exhibits limited adaptability.Adaptive thresholding methods are proposed to dynamically calculate the threshold by considering local image characteristics. Yang et al. [4] optimized the OTSU method by utilizing the relationship between pixel grayscale values and cumulative pixel count changes, which further enhanced the performance of the OTSU method. Zhang et al. [5] proposed a new adaptive threshold segmentation method based on image sharpness evaluation, which aims to accurately find a reasonable threshold for improving image segmentation performance. Lin et al. [6] introduced an improved decision-based adaptive threshold median filter (IDBATMF) for denoising salt-and-pepper noise in fingerprint images. Pan et al. [7] proposed a method for single image dehazing and segmentation based on the dark channel prior (DCP) and adaptive thresholding, which improved target recognition efficiency. Rehman et al. [8] developed a new image enhancement method by combining adaptive thresholding methods.The Gaussian kernel assigns higher weights to the central regions of the image by applying a weighted average to the threshold matrix after image segmentation. This effectively adapts to the uneven illumination and local contrast variations commonly found in in-vehicle images. This processing enhances the structural features of the image, thereby directly optimizing binarization performance, manifested as improved SSIM and reduced IL. Kim et al. [9] applied Gaussian weighting to Transformers, which was intended to enhance their performance. Chen et al. [10] proposed a Gaussian mixture model, which was designed to fully exploit point cloud information. Suryowati et al. [11] employed fixed and adaptive Gaussian kernel weighting functions for geographic weighted regression modeling, which was used to analyze maternal mortality rates. Basteri & Trevisan [12] initialized neural networks with random Gaussian parameters, which aimed to investigate how the sizes of hidden and output layers affect the Gaussian behavior of the network. 2.2. Optimization Methods of HyperparametersHyperparameter optimization holds significant importance in the field of computer image processing, with its primary aim being the enhancement of model performance through the adjustment of model hyperparameters. Indeed, the configuration of hyperparameters directly influences the performance of the model¬†[13], making it an essential step in the training process. Common methods for hyperparameter optimization include Bayesian optimization, genetic algorithms, and grid search. In short, Bayesian optimization¬†[14] employs probabilistic models to guide the selection of hyperparameters, continuously updating the model to identify the optimal hyperparameters, which demonstrates notable efficiency and accuracy. Conversely, genetic algorithms¬†[15] simulate the process of natural selection, incorporating operations such as crossover and mutation to interactively improve parameter configurations, thereby achieving hyperparameter optimization. Each of these methods has distinct characteristics, which can be chosen based on specific needs to attain the ultimate model performance.Furthermore, grid search provides significant advantages due to its systematic and comprehensive nature, allowing it to explore all possible parameter combinations and thereby ensure the identification of a global optimum. Consequently, for embedded image processing, employing grid search to determine the optimal parameters for fitting Gaussian kernel parameters in the training set represents a highly effective approach. 2.3. Sharding BlockchainElastico [16] is proposed to divide the blockchain into different shards, each with an independent consensus mechanism to process part of the transactions separately. However, since Elastico adopts the PBFT (Practical Byzantine Fault Tolerance) consensus mechanism, it suffers from the problem of slow transaction validation when processing transactions. Omniledger¬†[17] adopts the ByzCoin consensus mechanism as an improved version of PBFT but still faces the problem of performance limitation when processing large-scale transactions. Pyramid¬†[18] proposes a hierarchical sharding blockchain system that can complete the verification of cross-shard transactions within one round of consensus, which improves the speed of the system in processing large-scale transactions. However, Pyramid uses random sharding and does not consider load balancing among nodes. LB-Chain¬†[19] proposes a sharding method with shard load balancing, which migrates accounts from a high-load shard to a low-load shard through an account migration algorithm; however, LB-Chain does not consider the security of the system.",
            "2.1. Image Binarization": "Currently, image binarization methods include fixed threshold methods, adaptive threshold methods, and deep learning approaches. The fixed threshold method¬†[3] involves setting a fixed threshold to categorize image pixels into foreground and background. This method relies on a fixed threshold for binarization, which requires manual intervention for threshold selection and exhibits limited adaptability. Adaptive thresholding methods are proposed to dynamically calculate the threshold by considering local image characteristics. Yang et al. [4] optimized the OTSU method by utilizing the relationship between pixel grayscale values and cumulative pixel count changes, which further enhanced the performance of the OTSU method. Zhang et al. [5] proposed a new adaptive threshold segmentation method based on image sharpness evaluation, which aims to accurately find a reasonable threshold for improving image segmentation performance. Lin et al. [6] introduced an improved decision-based adaptive threshold median filter (IDBATMF) for denoising salt-and-pepper noise in fingerprint images. Pan et al. [7] proposed a method for single image dehazing and segmentation based on the dark channel prior (DCP) and adaptive thresholding, which improved target recognition efficiency. Rehman et al. [8] developed a new image enhancement method by combining adaptive thresholding methods. The Gaussian kernel assigns higher weights to the central regions of the image by applying a weighted average to the threshold matrix after image segmentation. This effectively adapts to the uneven illumination and local contrast variations commonly found in in-vehicle images. This processing enhances the structural features of the image, thereby directly optimizing binarization performance, manifested as improved SSIM and reduced IL. Kim et al. [9] applied Gaussian weighting to Transformers, which was intended to enhance their performance. Chen et al. [10] proposed a Gaussian mixture model, which was designed to fully exploit point cloud information. Suryowati et al. [11] employed fixed and adaptive Gaussian kernel weighting functions for geographic weighted regression modeling, which was used to analyze maternal mortality rates. Basteri & Trevisan [12] initialized neural networks with random Gaussian parameters, which aimed to investigate how the sizes of hidden and output layers affect the Gaussian behavior of the network.",
            "2.2. Optimization Methods of Hyperparameters": "Hyperparameter optimization holds significant importance in the field of computer image processing, with its primary aim being the enhancement of model performance through the adjustment of model hyperparameters. Indeed, the configuration of hyperparameters directly influences the performance of the model¬†[13], making it an essential step in the training process. Common methods for hyperparameter optimization include Bayesian optimization, genetic algorithms, and grid search. In short, Bayesian optimization¬†[14] employs probabilistic models to guide the selection of hyperparameters, continuously updating the model to identify the optimal hyperparameters, which demonstrates notable efficiency and accuracy. Conversely, genetic algorithms¬†[15] simulate the process of natural selection, incorporating operations such as crossover and mutation to interactively improve parameter configurations, thereby achieving hyperparameter optimization. Each of these methods has distinct characteristics, which can be chosen based on specific needs to attain the ultimate model performance. Furthermore, grid search provides significant advantages due to its systematic and comprehensive nature, allowing it to explore all possible parameter combinations and thereby ensure the identification of a global optimum. Consequently, for embedded image processing, employing grid search to determine the optimal parameters for fitting Gaussian kernel parameters in the training set represents a highly effective approach.",
            "2.3. Sharding Blockchain": "Elastico [16] is proposed to divide the blockchain into different shards, each with an independent consensus mechanism to process part of the transactions separately. However, since Elastico adopts the PBFT (Practical Byzantine Fault Tolerance) consensus mechanism, it suffers from the problem of slow transaction validation when processing transactions. Omniledger¬†[17] adopts the ByzCoin consensus mechanism as an improved version of PBFT but still faces the problem of performance limitation when processing large-scale transactions. Pyramid¬†[18] proposes a hierarchical sharding blockchain system that can complete the verification of cross-shard transactions within one round of consensus, which improves the speed of the system in processing large-scale transactions. However, Pyramid uses random sharding and does not consider load balancing among nodes. LB-Chain¬†[19] proposes a sharding method with shard load balancing, which migrates accounts from a high-load shard to a low-load shard through an account migration algorithm; however, LB-Chain does not consider the security of the system.",
            "3. Method": "3.1. Framework of OTSU-GKThe framework of OTSU-GK is illustrated inFigure 1and consists of three core modules: Grid Search, Cut Images, and Gaussian Kernel Average.Grid Search: We employ grid search for hyperparameter tuning, specifically to optimize the two hyperparameters of the Gaussian kernel, sigma (ùúéœÉ) and size, which determine the number of cut images (seeSection 3.2for details).Cut Images: We select original images from the dataset and perform proportional slicing from left to right. OTSU thresholding is then applied to each sliced image, resulting in a corresponding threshold matrix (seeSection 3.3for details).Gaussian Kernel Average: The Gaussian kernel performs matrix multiplication with the threshold matrices generated from the cut images. The resulting matrix is summed to determine the final threshold value for the original image, which is then used for binarization (seeSection 3.4for details). 3.2. Grid SearchGrid search ensures the identification of the optimal model configuration by systematically exploring all possible hyperparameter combinations. In this work, it is used to optimize the Gaussian kernel parameters: sigma (ùúéœÉ) and size.The algorithm defines a set of predefined ranges and steps for each hyperparameter, evaluates each combination using a scoring function, and selects the one with the best performance. This exhaustive search guarantees that the most suitable parameter set is identified within the given range. The pseudocode of the grid search algorithm for OTSU-GK is presented in Algorithm¬†1.Algorithm 1Grid search for hyperparameter tuning1:procedureGridSearch(ùúéœÉ-range,ùë†ùëñùëßùëísize-range)2:ùëèùëíùë†ùë°_ùë†ùëêùëúùëüùëí‚Üê0best_score‚Üê03:ùëèùëíùë†ùë°_ùëùùëéùëüùëéùëöùë†‚Üê(None,None)best_params‚Üê(None,None)4:forùúéœÉinùúéœÉ-rangedo5:forùë†ùëñùëßùëísizeinùë†ùëñùëßùëísize-rangedo6:ùë†ùëêùëúùëüùëí‚Üêevaluate_params(ùúé,ùë†ùëñùëßùëí)score‚Üêevaluate_params(œÉ,size)7:ifùë†ùëêùëúùëüùëí>ùëèùëíùë†ùë°_ùë†ùëêùëúùëüùëíscore>best_scorethen8:ùëèùëíùë†ùë°_ùë†ùëêùëúùëüùëí‚Üêùë†ùëêùëúùëüùëíbest_score‚Üêscore9:ùëèùëíùë†ùë°_ùëùùëéùëüùëéùëöùë†‚Üê(ùúé,ùë†ùëñùëßùëí)best_params‚Üê(œÉ,size)10:end if11:end for12:end for13:returnùëèùëíùë†ùë°_ùëùùëéùëüùëéùëöùë†best_params,ùëèùëíùë†ùë°_ùë†ùëêùëúùëüùëíbest_score14:end procedureDuring the experiment, we randomly select 300 images from the dataset for testing, and 20% of them are used for parameter evaluation. The range of sigma is set from 0.5 to 6.0, divided into 10 equal intervals. The size of the Gaussian kernel ranges from 3 to 7, considering only odd values (i.e., 3, 5, and 7) to maintain symmetry. For the abnormal images (entirely black or entirely white images) that appear after image segmentation, we do not apply OTSU thresholding. Instead, we set their threshold to a default value (such as the global image threshold or the average of adjacent slices) or directly exclude the slice from the threshold matrix. This ensures the threshold matrix does not contain extreme¬†outliers.In the evaluate_params function, we normalize the evaluation metrics, including SSIM, MAE, and information loss (IL). For metrics where higher values indicate better performance (e.g., SSIM), we directly aggregate their scores. For those where lower values are better (e.g., MAE and IL), we subtract their normalized scores. The final score is flexibly adjusted based on experimental objectives to ensure optimal Gaussian kernel parameter selection. 3.3. Cut-Image GenerationOnce the Gaussian kernel hyperparameters(ùúé,ùë†ùëñùëßùëí)(œÉ,size)have been obtained by grid search (Algorithm¬†1), OTSU-GK segments the original image intoùë†ùëñùëßùëí√óùë†ùëñùëßùëísize√ósizeequal-width (or equal-height) tiles starting from the left-most column. Each tile is processed independently by classical OTSU thresholding, yielding one threshold per tile. The resulting thresholds are then laid out row-wise to form aùë†ùëñùëßùëí√óùë†ùëñùëßùëísize√ósizethreshold matrixùêìTthat will later be convolved with the Gaussian kernel.Figure 2illustrates the cutting procedure forùë†ùëñùëßùëí=3size=3; the original image is partitioned into nine non-overlapping sub-images, The asterisks denote matrix multiplication and the corresponding threshold matrix isùêì=‚é°‚é£‚é¢‚é¢ùë°11ùë°21ùë°31ùë°12ùë°22ùë°32ùë°13ùë°23ùë°33‚é§‚é¶‚é•‚é•,ùë°ùëñùëó=ùôæùöÉùöÇùöÑ(tileùëñùëó).T=t11t12t13t21t22t23t31t32t33,tij=OTSU(tileij).Figure 2.Cut-image generation whenùë†ùëñùëßùëí=3size=3. 3.4. Experimental Settings3.4.1. Cut ImagesThe Gaussian kernel is selected since it smooths noise and enhances image features through distance-weighted processing, which is particularly crucial for in-vehicle image processing. In OTSU-GK, the Gaussian kernel is applied to the thresholding matrix, effectively integrating local features across different image regions. This avoids detail loss caused by a single threshold, thereby improving binarization performance (as evidenced by enhancements in SSIM and IL metrics). After obtaining the Gaussian kernel hyperparameters (ùúé,ùë†ùëñùëßùëíœÉ,size) through grid search, OTSU-GK segments the original image based on the value ofùë†ùëñùëßùëísize. Specifically, the image is segmented with equal-width or equal-height cuts starting from the left side of the original image, resulting inùë†ùëñùëßùëí√óùë†ùëñùëßùëísize√ósizecut images. Subsequently, each cut image is processed individually using OTSU to generate thresholds. These thresholds are then arranged sequentially to form a matrix ofùë†ùëñùëßùëí√óùë†ùëñùëßùëísize√ósizethresholds. For example, withùë†ùëñùëßùëí=3size=3, the process of generating cut images is illustrated inFigure 3.Figure 3.Percentage of relative performance improvement on DOTA.3.4.2. Gaussian Kernel AverageGaussian weighted average is a method that assigns weights to data points based on their distance from a central point, with closer points receiving higher weights. This technique is commonly used in signal processing, image processing, and data analysis to smooth out noise and emphasize important features. The Gaussian weighted average algorithm first defines a Gaussian function of the form. The formula for a Gaussian function is as follows, whereùúáŒºrepresents the mean of the Gaussian function andùúéœÉrepresents the standard deviation, which controls the center point of the function and the width of the¬†function:ùëì(ùë•)=12ùúã‚àí‚àí‚àí‚àöùúéùëí‚àí(ùë•‚àíùúá)22ùúé2f(x)=12œÄœÉe‚àí(x‚àíŒº)22œÉ2(1)When using the Gaussian function, it is necessary to input the parametersùúéœÉandùë†ùëñùëßùëísizeinto the function. To better understand the distribution of the Gaussian kernel, we set different values forùúéœÉandùë†ùëñùëßùëísizeand illustrated the characteristics of the Gaussian kernel through heatmaps, as shown inFigure 3.In OTSU-GK, the threshold matrix generated from cut images is combined with the Gaussian kernel matrix, which is used for matrix multiplication. This operation, which performs the smoothing and adjustment of the threshold values, ultimately yields the threshold of the original image.",
            "3.1. Framework of OTSU-GK": "The framework of OTSU-GK is illustrated inFigure 1and consists of three core modules: Grid Search, Cut Images, and Gaussian Kernel Average. Grid Search: We employ grid search for hyperparameter tuning, specifically to optimize the two hyperparameters of the Gaussian kernel, sigma (ùúéœÉ) and size, which determine the number of cut images (seeSection 3.2for details).Cut Images: We select original images from the dataset and perform proportional slicing from left to right. OTSU thresholding is then applied to each sliced image, resulting in a corresponding threshold matrix (seeSection 3.3for details).Gaussian Kernel Average: The Gaussian kernel performs matrix multiplication with the threshold matrices generated from the cut images. The resulting matrix is summed to determine the final threshold value for the original image, which is then used for binarization (seeSection 3.4for details).",
            "3.2. Grid Search": "Grid search ensures the identification of the optimal model configuration by systematically exploring all possible hyperparameter combinations. In this work, it is used to optimize the Gaussian kernel parameters: sigma (ùúéœÉ) and size. The algorithm defines a set of predefined ranges and steps for each hyperparameter, evaluates each combination using a scoring function, and selects the one with the best performance. This exhaustive search guarantees that the most suitable parameter set is identified within the given range. The pseudocode of the grid search algorithm for OTSU-GK is presented in Algorithm¬†1.Algorithm 1Grid search for hyperparameter tuning1:procedureGridSearch(ùúéœÉ-range,ùë†ùëñùëßùëísize-range)2:ùëèùëíùë†ùë°_ùë†ùëêùëúùëüùëí‚Üê0best_score‚Üê03:ùëèùëíùë†ùë°_ùëùùëéùëüùëéùëöùë†‚Üê(None,None)best_params‚Üê(None,None)4:forùúéœÉinùúéœÉ-rangedo5:forùë†ùëñùëßùëísizeinùë†ùëñùëßùëísize-rangedo6:ùë†ùëêùëúùëüùëí‚Üêevaluate_params(ùúé,ùë†ùëñùëßùëí)score‚Üêevaluate_params(œÉ,size)7:ifùë†ùëêùëúùëüùëí>ùëèùëíùë†ùë°_ùë†ùëêùëúùëüùëíscore>best_scorethen8:ùëèùëíùë†ùë°_ùë†ùëêùëúùëüùëí‚Üêùë†ùëêùëúùëüùëíbest_score‚Üêscore9:ùëèùëíùë†ùë°_ùëùùëéùëüùëéùëöùë†‚Üê(ùúé,ùë†ùëñùëßùëí)best_params‚Üê(œÉ,size)10:end if11:end for12:end for13:returnùëèùëíùë†ùë°_ùëùùëéùëüùëéùëöùë†best_params,ùëèùëíùë†ùë°_ùë†ùëêùëúùëüùëíbest_score14:end procedure During the experiment, we randomly select 300 images from the dataset for testing, and 20% of them are used for parameter evaluation. The range of sigma is set from 0.5 to 6.0, divided into 10 equal intervals. The size of the Gaussian kernel ranges from 3 to 7, considering only odd values (i.e., 3, 5, and 7) to maintain symmetry. For the abnormal images (entirely black or entirely white images) that appear after image segmentation, we do not apply OTSU thresholding. Instead, we set their threshold to a default value (such as the global image threshold or the average of adjacent slices) or directly exclude the slice from the threshold matrix. This ensures the threshold matrix does not contain extreme¬†outliers. In the evaluate_params function, we normalize the evaluation metrics, including SSIM, MAE, and information loss (IL). For metrics where higher values indicate better performance (e.g., SSIM), we directly aggregate their scores. For those where lower values are better (e.g., MAE and IL), we subtract their normalized scores. The final score is flexibly adjusted based on experimental objectives to ensure optimal Gaussian kernel parameter selection.",
            "3.3. Cut-Image Generation": "Once the Gaussian kernel hyperparameters(ùúé,ùë†ùëñùëßùëí)(œÉ,size)have been obtained by grid search (Algorithm¬†1), OTSU-GK segments the original image intoùë†ùëñùëßùëí√óùë†ùëñùëßùëísize√ósizeequal-width (or equal-height) tiles starting from the left-most column. Each tile is processed independently by classical OTSU thresholding, yielding one threshold per tile. The resulting thresholds are then laid out row-wise to form aùë†ùëñùëßùëí√óùë†ùëñùëßùëísize√ósizethreshold matrixùêìTthat will later be convolved with the Gaussian kernel. Figure 2illustrates the cutting procedure forùë†ùëñùëßùëí=3size=3; the original image is partitioned into nine non-overlapping sub-images, The asterisks denote matrix multiplication and the corresponding threshold matrix isùêì=‚é°‚é£‚é¢‚é¢ùë°11ùë°21ùë°31ùë°12ùë°22ùë°32ùë°13ùë°23ùë°33‚é§‚é¶‚é•‚é•,ùë°ùëñùëó=ùôæùöÉùöÇùöÑ(tileùëñùëó).T=t11t12t13t21t22t23t31t32t33,tij=OTSU(tileij). Figure 2.Cut-image generation whenùë†ùëñùëßùëí=3size=3.",
            "3.4. Experimental Settings": "3.4.1. Cut ImagesThe Gaussian kernel is selected since it smooths noise and enhances image features through distance-weighted processing, which is particularly crucial for in-vehicle image processing. In OTSU-GK, the Gaussian kernel is applied to the thresholding matrix, effectively integrating local features across different image regions. This avoids detail loss caused by a single threshold, thereby improving binarization performance (as evidenced by enhancements in SSIM and IL metrics). After obtaining the Gaussian kernel hyperparameters (ùúé,ùë†ùëñùëßùëíœÉ,size) through grid search, OTSU-GK segments the original image based on the value ofùë†ùëñùëßùëísize. Specifically, the image is segmented with equal-width or equal-height cuts starting from the left side of the original image, resulting inùë†ùëñùëßùëí√óùë†ùëñùëßùëísize√ósizecut images. Subsequently, each cut image is processed individually using OTSU to generate thresholds. These thresholds are then arranged sequentially to form a matrix ofùë†ùëñùëßùëí√óùë†ùëñùëßùëísize√ósizethresholds. For example, withùë†ùëñùëßùëí=3size=3, the process of generating cut images is illustrated inFigure 3.Figure 3.Percentage of relative performance improvement on DOTA. 3.4.2. Gaussian Kernel AverageGaussian weighted average is a method that assigns weights to data points based on their distance from a central point, with closer points receiving higher weights. This technique is commonly used in signal processing, image processing, and data analysis to smooth out noise and emphasize important features. The Gaussian weighted average algorithm first defines a Gaussian function of the form. The formula for a Gaussian function is as follows, whereùúáŒºrepresents the mean of the Gaussian function andùúéœÉrepresents the standard deviation, which controls the center point of the function and the width of the¬†function:ùëì(ùë•)=12ùúã‚àí‚àí‚àí‚àöùúéùëí‚àí(ùë•‚àíùúá)22ùúé2f(x)=12œÄœÉe‚àí(x‚àíŒº)22œÉ2(1)When using the Gaussian function, it is necessary to input the parametersùúéœÉandùë†ùëñùëßùëísizeinto the function. To better understand the distribution of the Gaussian kernel, we set different values forùúéœÉandùë†ùëñùëßùëísizeand illustrated the characteristics of the Gaussian kernel through heatmaps, as shown inFigure 3.In OTSU-GK, the threshold matrix generated from cut images is combined with the Gaussian kernel matrix, which is used for matrix multiplication. This operation, which performs the smoothing and adjustment of the threshold values, ultimately yields the threshold of the original image.",
            "3.4.1. Cut Images": "The Gaussian kernel is selected since it smooths noise and enhances image features through distance-weighted processing, which is particularly crucial for in-vehicle image processing. In OTSU-GK, the Gaussian kernel is applied to the thresholding matrix, effectively integrating local features across different image regions. This avoids detail loss caused by a single threshold, thereby improving binarization performance (as evidenced by enhancements in SSIM and IL metrics). After obtaining the Gaussian kernel hyperparameters (ùúé,ùë†ùëñùëßùëíœÉ,size) through grid search, OTSU-GK segments the original image based on the value ofùë†ùëñùëßùëísize. Specifically, the image is segmented with equal-width or equal-height cuts starting from the left side of the original image, resulting inùë†ùëñùëßùëí√óùë†ùëñùëßùëísize√ósizecut images. Subsequently, each cut image is processed individually using OTSU to generate thresholds. These thresholds are then arranged sequentially to form a matrix ofùë†ùëñùëßùëí√óùë†ùëñùëßùëísize√ósizethresholds. For example, withùë†ùëñùëßùëí=3size=3, the process of generating cut images is illustrated inFigure 3. Figure 3.Percentage of relative performance improvement on DOTA.",
            "3.4.2. Gaussian Kernel Average": "Gaussian weighted average is a method that assigns weights to data points based on their distance from a central point, with closer points receiving higher weights. This technique is commonly used in signal processing, image processing, and data analysis to smooth out noise and emphasize important features. The Gaussian weighted average algorithm first defines a Gaussian function of the form. The formula for a Gaussian function is as follows, whereùúáŒºrepresents the mean of the Gaussian function andùúéœÉrepresents the standard deviation, which controls the center point of the function and the width of the¬†function:ùëì(ùë•)=12ùúã‚àí‚àí‚àí‚àöùúéùëí‚àí(ùë•‚àíùúá)22ùúé2f(x)=12œÄœÉe‚àí(x‚àíŒº)22œÉ2(1) When using the Gaussian function, it is necessary to input the parametersùúéœÉandùë†ùëñùëßùëísizeinto the function. To better understand the distribution of the Gaussian kernel, we set different values forùúéœÉandùë†ùëñùëßùëísizeand illustrated the characteristics of the Gaussian kernel through heatmaps, as shown inFigure 3. In OTSU-GK, the threshold matrix generated from cut images is combined with the Gaussian kernel matrix, which is used for matrix multiplication. This operation, which performs the smoothing and adjustment of the threshold values, ultimately yields the threshold of the original image.",
            "4. NLS-Chain: NLS Sharding Blockchain": "We construct a high-TPS sharding blockchain system named NLS-Chain to address the low throughput of traditional sharded blockchain systems. NLS-Chain predicts the number of transactions for each node in the upcoming epoch and computes a Node Load Score (NLS) based on this prediction. By sharding nodes according to their NLS, the system achieves inter-shard load balance and significantly improves overall TPS. The framework of NLS-Chain is illustrated inFigure 4. Figure 4.Process of generating cut images. 4.1. Prediction of the Number of Node TransactionsIn the transaction prediction phase, we construct a GRU-based sequence prediction method¬†[20] to forecast the number of transactions for each node in the next epoch. The prediction result is used as the basis for calculating the NLS. In fact, GRU contains one input layer, one output layer, and five hidden layers. The length of the input layer depends on the length of the transaction sequence. Algorithm 2 details the generation of the transaction sequence. First, input the raw data and step size. Then create two empty lists to store the input sequence and output labels. Use a for loop to control the data length and prevent out-of-bounds access, adding data to the empty lists one item at a time. Finally, output the lists as the transformed data format. Use the output of Algorithm 2 as input for the GRU prediction model. Train the model using 2/3 of the data as the training set, and use the remaining 1/3 as the test set to evaluate the model‚Äôs accuracy. Five hidden layers are connected sequentially to process time series data, with their units controlled by gates to regulate information flow. The output layer is typically a fully connected layer that processes and extracts features from the GRU layer‚Äôs outputs, mapping them to the output space via activation functions to produce the final prediction results (e.g., transaction¬†sequences).Transaction sequences generated by GRU are subsequently fed into a large language model (LLM) for further processing, with the Transformer serving as the core component of the LLM. The Transformer employs an attention mechanism to enhance the model‚Äôs ability to capture relationships between different parts of a transaction sequence, enabling it to better understand contextual information and improve its processing capabilities.Algorithm 2The generation of a transaction sequence1:procedureGeneration(ùëëùëéùë°ùëédata,ùë†ùëíùëû_ùëôùëíùëõùëîùë°‚Ñéseq_length)2:X,Y=[], []3:foriinùë†ùë°ùëíùëùstepùëôùëíùëõùëîùë°‚Ñélengthdo4:ùëã.ùëéùëùùëùùëíùëõùëë(ùëëùëéùë°ùëé[ùëñ+ùë†ùëíùëû_ùëôùëíùëõùëîùë°‚Ñé])X.append(data[i+seq_length])5:ùëå.ùëéùëùùëùùëíùëõùëë(ùëëùëéùë°ùëé[ùëñ+ùë†ùëíùëû_ùëôùëíùëõùëîùë°‚Ñé])Y.append(data[i+seq_length])6:end for7:returnùëõùëù.ùëéùëüùëüùëéùë¶(ùëã)np.array(X),ùëõùëù.ùëéùëüùëüùëéùë¶(ùëå)np.array(Y)8:end procedureWe first convert the raw transaction data into sequence format using a create_sequence function. This function splits the data into fixed-length input sequences and corresponding target values. The process is controlled by a loop to avoid out-of-bounds access, and the resulting sequences are stored in lists for model input.The converted data is then fed into the GRU model, which is trained using the Adam optimizer. Adam calculates both the first-order moment (mean) and the second-order moment (uncentered variance) of the gradients and applies bias correction to prevent small initial updates. After training, the model is evaluated using Mean Squared Error (MSE), which measures the average squared difference between predicted and actual values. A lower MSE indicates better model fitting. We train the model for 30 epochs and validate its performance on both training and validation datasets. 4.2. Sharding Method Based on NLSWe propose a Node Load Score (NLS)-based sharding method to achieve transaction load balancing across shards. At the beginning of each epoch, nodes are reassigned to shards based on their NLS. A higher NLS indicates stronger transaction processing capability and a better resource configuration, while a lower NLS suggests limited capacity. Our method considers both the NLS and its volatility to determine the optimal shard assignment for each node.The NLS algorithm is designed for a sharded blockchain system based on node reputation to enhance blockchain scalability. The specific process is as follows: First, a node reputation assessment method is designed. This method combines the number of node transactions predicted by a transaction prediction model with the node‚Äôs historical transaction behavior to form a node reputation score. Initial sharding is performed based on node reputation scores before each iteration period. Next, to address load balancing among nodes, a polling mechanism dynamically adjusts node assignments to minimize load disparities between shards, adapting to dynamic node changes. Note that although NLS handles dynamic changes between nodes, these changes refer specifically to shifts in on-chain nodes across different shards. The current work does not account for sudden alterations in node behavior patterns; future research will explore scenarios where dynamic shifts in node behavior patterns impact reputation.NLS CalculationWe use a weighted scoring method to calculate the NLS for each node. The score is derived from three main factors: predicted transaction volume, node stability, and historical performance. The formula is given in Equation (2):NLSùëñ=Weightùëù¬∑ùëÑùëù+Weightùë†¬∑ùëÑùë†+Weight‚Ñé¬∑ùëÑ‚ÑéNLSi=Weightp¬∑Qp+Weights¬∑Qs+Weighth¬∑Qh(2)Here,ùëÑùëùQp: Quantized predicted transaction volume (0‚Äì1), output from the GRU model.ùëÑùë†Qs: Quantized node stability, including online time, response time, sync status, error rate, and resource availability.ùëÑ‚ÑéQh: Quantized historical performance, including success rate, transaction efficiency, verification accuracy, participation, and violation records.Weightùëù+Weightùë†+Weight‚Ñé=1Weightp+Weights+Weighth=1: Weight coefficients balancing the contribution of each factor.ùëäùëíùëñùëî‚Ñéùë°ùëùWeightp,ùëäùëíùëñùëî‚Ñéùë°ùë†Weights, andùëäùëíùëñùëî‚Ñéùë°‚ÑéWeighthrepresent the weighting coefficients for predicted transaction volume, node stability, and historical performance, respectively, withùëäùëíùëñùëî‚Ñéùë°ùëù+ùëäùëíùëñùëî‚Ñéùë°ùë†+ùëäùëíùëñùëî‚Ñéùë°‚ÑéWeightp+Weights+Weighth= 1. Specifically,ùëäùëíùëñùëî‚Ñéùë°ùëùWeightpdenotes predicted transaction volume, indicating the node‚Äôs projected transaction count over the next epoch.ùëäùëíùëñùëî‚Ñéùë°ùë†Weightsmeasures node stability, encompassing uptime, response time, synchronization status, error rate, and resource availability.ùëäùëíùëñùëî‚Ñéùë°‚ÑéWeighthreflects historical performance, including success rate, transaction efficiency, validation accuracy, node participation, and past violation records. Each factorùëÑ‚àóQ‚àó, where ‚àó denotesp,s, andh, respectively, is computed as a weighted average of its sub-indicators, as shown in Equation (3):ùëÑ‚àó=‚àëùëò=1ùêæùë§ùëò¬∑ùë•ùëò,with‚àëùëò=1ùêæùë§ùëò=1Q‚àó=‚àëk=1Kwk¬∑xk,with‚àëk=1Kwk=1(3)ùë•ùëòxkis the normalized value of thek-th sub-indicator andùë§ùëòwkis its corresponding weight. All sub-indicators are normalized to the range [0, 1] to ensure consistency and comparability.After calculating each node‚Äôs NLS, nodes must be periodically re-sharded based on this score. Additionally, to guarantee high throughput and low latency across epochs, nodes are periodically re-sharded according to their freshly computed NLS. The sharding problem is formulated as minimizing the sum of squared deviations between each shard‚Äôs aggregated NLS and the global average NLS. The objective is to minimize the squared difference between each shard‚Äôs total node transaction load and the shard‚Äôs average transaction load. The squared term ensures that load discrepancies exert a greater influence on the inter-shard load distribution, thereby incentivizing the optimization process to reduce these differences. 4.3. NLS-Chain: Load-Balanced Sharding via NLSTo guarantee high throughput and low latency across epochs, nodes are periodically re-sharded according to their freshly computed Node Load Score (NLS). The sharding problem is cast as the minimization of the sum of squared deviations between each shard‚Äôs aggregated NLS and the global average NLS. The quadratic term penalizes large imbalances more severely than a linear criterion, thus encouraging a tight load distribution.4.3.1. Optimization ObjectiveLetùíÆ={1,‚Ä¶,ùëÜ}S={1,‚Ä¶,S}be the set of shards,ùí©={1,‚Ä¶,ùëÅ}N={1,‚Ä¶,N}be the set of nodes,ùúÜùë°ùëó‚àà[0,1]Œªjt‚àà[0,1]be the NLS of nodejat epocht(computed via Equation (2)), andùë•ùë°ùëñùëó‚àà{0,1}xijt‚àà{0,1}indicate whether nodeiis assigned to shardjat epocht. The total NLS of shardjisùëÜùë¢ùëöùë°ùëóSumjt(ref. Equation (4)) and the ideal average NLS per shard isùê¥ùë£ùëîùë°Avgt(ref. Equation (5)).Sumùë°ùëó=‚àëùëñ‚ààùí©ùúÜùë°ùëñùë•ùë°ùëñùëó,Sumjt=‚àëi‚ààNŒªitxijt,(4)Avgùë°=1ùëÜ‚àëùëó‚ààùíÆSumùë°ùëó=1ùëÜ‚àëùëñ‚ààùí©ùúÜùë°ùëñ.Avgt=1S‚àëj‚ààSSumjt=1S‚àëi‚ààNŒªit.(5)min{ùë•ùë°ùëñùëó}‚àëùëó‚ààùíÆ(Sumùë°ùëó‚àíAvgùë°)2subjectto‚àëùëó‚ààùíÆùë•ùë°ùëñùëó=1‚àÄùëñ‚ààùí©min{xijt}‚àëj‚ààSSumjt‚àíAvgt2subjectto‚àëj‚ààSxijt=1‚àÄi‚ààN(6)Equation (6) is a convex quadratic assignment problem and an exact solution is NP-hard for largeN. NLS-Chain therefore employs a fast greedy heuristic:Sort nodes by descendingùúÜùë°ùëñŒªit;Round-robin assignment to obtain equal-sized shards;Iteratively migrate the node that yields the largest marginal reduction in (6) until no improvement is possible.The heuristic converges within 5‚Äì10 iterations and scales linearly withN.4.3.2. Throughput EvaluationTable 1summarizes the average TPS achieved under increasing shard counts. NLS-Chain consistently outperforms both random sharding and LB-Chain [19], delivering up to2.2√ó2.2√óhigher throughput whenùëÜ=32S=32.Table 1.Comparison of average TPS.",
            "4.1. Prediction of the Number of Node Transactions": "In the transaction prediction phase, we construct a GRU-based sequence prediction method¬†[20] to forecast the number of transactions for each node in the next epoch. The prediction result is used as the basis for calculating the NLS. In fact, GRU contains one input layer, one output layer, and five hidden layers. The length of the input layer depends on the length of the transaction sequence. Algorithm 2 details the generation of the transaction sequence. First, input the raw data and step size. Then create two empty lists to store the input sequence and output labels. Use a for loop to control the data length and prevent out-of-bounds access, adding data to the empty lists one item at a time. Finally, output the lists as the transformed data format. Use the output of Algorithm 2 as input for the GRU prediction model. Train the model using 2/3 of the data as the training set, and use the remaining 1/3 as the test set to evaluate the model‚Äôs accuracy. Five hidden layers are connected sequentially to process time series data, with their units controlled by gates to regulate information flow. The output layer is typically a fully connected layer that processes and extracts features from the GRU layer‚Äôs outputs, mapping them to the output space via activation functions to produce the final prediction results (e.g., transaction¬†sequences). Transaction sequences generated by GRU are subsequently fed into a large language model (LLM) for further processing, with the Transformer serving as the core component of the LLM. The Transformer employs an attention mechanism to enhance the model‚Äôs ability to capture relationships between different parts of a transaction sequence, enabling it to better understand contextual information and improve its processing capabilities.Algorithm 2The generation of a transaction sequence1:procedureGeneration(ùëëùëéùë°ùëédata,ùë†ùëíùëû_ùëôùëíùëõùëîùë°‚Ñéseq_length)2:X,Y=[], []3:foriinùë†ùë°ùëíùëùstepùëôùëíùëõùëîùë°‚Ñélengthdo4:ùëã.ùëéùëùùëùùëíùëõùëë(ùëëùëéùë°ùëé[ùëñ+ùë†ùëíùëû_ùëôùëíùëõùëîùë°‚Ñé])X.append(data[i+seq_length])5:ùëå.ùëéùëùùëùùëíùëõùëë(ùëëùëéùë°ùëé[ùëñ+ùë†ùëíùëû_ùëôùëíùëõùëîùë°‚Ñé])Y.append(data[i+seq_length])6:end for7:returnùëõùëù.ùëéùëüùëüùëéùë¶(ùëã)np.array(X),ùëõùëù.ùëéùëüùëüùëéùë¶(ùëå)np.array(Y)8:end procedure We first convert the raw transaction data into sequence format using a create_sequence function. This function splits the data into fixed-length input sequences and corresponding target values. The process is controlled by a loop to avoid out-of-bounds access, and the resulting sequences are stored in lists for model input. The converted data is then fed into the GRU model, which is trained using the Adam optimizer. Adam calculates both the first-order moment (mean) and the second-order moment (uncentered variance) of the gradients and applies bias correction to prevent small initial updates. After training, the model is evaluated using Mean Squared Error (MSE), which measures the average squared difference between predicted and actual values. A lower MSE indicates better model fitting. We train the model for 30 epochs and validate its performance on both training and validation datasets.",
            "4.2. Sharding Method Based on NLS": "We propose a Node Load Score (NLS)-based sharding method to achieve transaction load balancing across shards. At the beginning of each epoch, nodes are reassigned to shards based on their NLS. A higher NLS indicates stronger transaction processing capability and a better resource configuration, while a lower NLS suggests limited capacity. Our method considers both the NLS and its volatility to determine the optimal shard assignment for each node. The NLS algorithm is designed for a sharded blockchain system based on node reputation to enhance blockchain scalability. The specific process is as follows: First, a node reputation assessment method is designed. This method combines the number of node transactions predicted by a transaction prediction model with the node‚Äôs historical transaction behavior to form a node reputation score. Initial sharding is performed based on node reputation scores before each iteration period. Next, to address load balancing among nodes, a polling mechanism dynamically adjusts node assignments to minimize load disparities between shards, adapting to dynamic node changes. Note that although NLS handles dynamic changes between nodes, these changes refer specifically to shifts in on-chain nodes across different shards. The current work does not account for sudden alterations in node behavior patterns; future research will explore scenarios where dynamic shifts in node behavior patterns impact reputation. NLS CalculationWe use a weighted scoring method to calculate the NLS for each node. The score is derived from three main factors: predicted transaction volume, node stability, and historical performance. The formula is given in Equation (2):NLSùëñ=Weightùëù¬∑ùëÑùëù+Weightùë†¬∑ùëÑùë†+Weight‚Ñé¬∑ùëÑ‚ÑéNLSi=Weightp¬∑Qp+Weights¬∑Qs+Weighth¬∑Qh(2)Here,ùëÑùëùQp: Quantized predicted transaction volume (0‚Äì1), output from the GRU model.ùëÑùë†Qs: Quantized node stability, including online time, response time, sync status, error rate, and resource availability.ùëÑ‚ÑéQh: Quantized historical performance, including success rate, transaction efficiency, verification accuracy, participation, and violation records.Weightùëù+Weightùë†+Weight‚Ñé=1Weightp+Weights+Weighth=1: Weight coefficients balancing the contribution of each factor.ùëäùëíùëñùëî‚Ñéùë°ùëùWeightp,ùëäùëíùëñùëî‚Ñéùë°ùë†Weights, andùëäùëíùëñùëî‚Ñéùë°‚ÑéWeighthrepresent the weighting coefficients for predicted transaction volume, node stability, and historical performance, respectively, withùëäùëíùëñùëî‚Ñéùë°ùëù+ùëäùëíùëñùëî‚Ñéùë°ùë†+ùëäùëíùëñùëî‚Ñéùë°‚ÑéWeightp+Weights+Weighth= 1. Specifically,ùëäùëíùëñùëî‚Ñéùë°ùëùWeightpdenotes predicted transaction volume, indicating the node‚Äôs projected transaction count over the next epoch.ùëäùëíùëñùëî‚Ñéùë°ùë†Weightsmeasures node stability, encompassing uptime, response time, synchronization status, error rate, and resource availability.ùëäùëíùëñùëî‚Ñéùë°‚ÑéWeighthreflects historical performance, including success rate, transaction efficiency, validation accuracy, node participation, and past violation records. Each factorùëÑ‚àóQ‚àó, where ‚àó denotesp,s, andh, respectively, is computed as a weighted average of its sub-indicators, as shown in Equation (3):ùëÑ‚àó=‚àëùëò=1ùêæùë§ùëò¬∑ùë•ùëò,with‚àëùëò=1ùêæùë§ùëò=1Q‚àó=‚àëk=1Kwk¬∑xk,with‚àëk=1Kwk=1(3)ùë•ùëòxkis the normalized value of thek-th sub-indicator andùë§ùëòwkis its corresponding weight. All sub-indicators are normalized to the range [0, 1] to ensure consistency and comparability.After calculating each node‚Äôs NLS, nodes must be periodically re-sharded based on this score. Additionally, to guarantee high throughput and low latency across epochs, nodes are periodically re-sharded according to their freshly computed NLS. The sharding problem is formulated as minimizing the sum of squared deviations between each shard‚Äôs aggregated NLS and the global average NLS. The objective is to minimize the squared difference between each shard‚Äôs total node transaction load and the shard‚Äôs average transaction load. The squared term ensures that load discrepancies exert a greater influence on the inter-shard load distribution, thereby incentivizing the optimization process to reduce these differences.",
            "NLS Calculation": "We use a weighted scoring method to calculate the NLS for each node. The score is derived from three main factors: predicted transaction volume, node stability, and historical performance. The formula is given in Equation (2):NLSùëñ=Weightùëù¬∑ùëÑùëù+Weightùë†¬∑ùëÑùë†+Weight‚Ñé¬∑ùëÑ‚ÑéNLSi=Weightp¬∑Qp+Weights¬∑Qs+Weighth¬∑Qh(2) Here,ùëÑùëùQp: Quantized predicted transaction volume (0‚Äì1), output from the GRU model.ùëÑùë†Qs: Quantized node stability, including online time, response time, sync status, error rate, and resource availability.ùëÑ‚ÑéQh: Quantized historical performance, including success rate, transaction efficiency, verification accuracy, participation, and violation records.Weightùëù+Weightùë†+Weight‚Ñé=1Weightp+Weights+Weighth=1: Weight coefficients balancing the contribution of each factor. ùëäùëíùëñùëî‚Ñéùë°ùëùWeightp,ùëäùëíùëñùëî‚Ñéùë°ùë†Weights, andùëäùëíùëñùëî‚Ñéùë°‚ÑéWeighthrepresent the weighting coefficients for predicted transaction volume, node stability, and historical performance, respectively, withùëäùëíùëñùëî‚Ñéùë°ùëù+ùëäùëíùëñùëî‚Ñéùë°ùë†+ùëäùëíùëñùëî‚Ñéùë°‚ÑéWeightp+Weights+Weighth= 1. Specifically,ùëäùëíùëñùëî‚Ñéùë°ùëùWeightpdenotes predicted transaction volume, indicating the node‚Äôs projected transaction count over the next epoch.ùëäùëíùëñùëî‚Ñéùë°ùë†Weightsmeasures node stability, encompassing uptime, response time, synchronization status, error rate, and resource availability.ùëäùëíùëñùëî‚Ñéùë°‚ÑéWeighthreflects historical performance, including success rate, transaction efficiency, validation accuracy, node participation, and past violation records. Each factorùëÑ‚àóQ‚àó, where ‚àó denotesp,s, andh, respectively, is computed as a weighted average of its sub-indicators, as shown in Equation (3):ùëÑ‚àó=‚àëùëò=1ùêæùë§ùëò¬∑ùë•ùëò,with‚àëùëò=1ùêæùë§ùëò=1Q‚àó=‚àëk=1Kwk¬∑xk,with‚àëk=1Kwk=1(3) ùë•ùëòxkis the normalized value of thek-th sub-indicator andùë§ùëòwkis its corresponding weight. All sub-indicators are normalized to the range [0, 1] to ensure consistency and comparability. After calculating each node‚Äôs NLS, nodes must be periodically re-sharded based on this score. Additionally, to guarantee high throughput and low latency across epochs, nodes are periodically re-sharded according to their freshly computed NLS. The sharding problem is formulated as minimizing the sum of squared deviations between each shard‚Äôs aggregated NLS and the global average NLS. The objective is to minimize the squared difference between each shard‚Äôs total node transaction load and the shard‚Äôs average transaction load. The squared term ensures that load discrepancies exert a greater influence on the inter-shard load distribution, thereby incentivizing the optimization process to reduce these differences.",
            "4.3. NLS-Chain: Load-Balanced Sharding via NLS": "To guarantee high throughput and low latency across epochs, nodes are periodically re-sharded according to their freshly computed Node Load Score (NLS). The sharding problem is cast as the minimization of the sum of squared deviations between each shard‚Äôs aggregated NLS and the global average NLS. The quadratic term penalizes large imbalances more severely than a linear criterion, thus encouraging a tight load distribution. 4.3.1. Optimization ObjectiveLetùíÆ={1,‚Ä¶,ùëÜ}S={1,‚Ä¶,S}be the set of shards,ùí©={1,‚Ä¶,ùëÅ}N={1,‚Ä¶,N}be the set of nodes,ùúÜùë°ùëó‚àà[0,1]Œªjt‚àà[0,1]be the NLS of nodejat epocht(computed via Equation (2)), andùë•ùë°ùëñùëó‚àà{0,1}xijt‚àà{0,1}indicate whether nodeiis assigned to shardjat epocht. The total NLS of shardjisùëÜùë¢ùëöùë°ùëóSumjt(ref. Equation (4)) and the ideal average NLS per shard isùê¥ùë£ùëîùë°Avgt(ref. Equation (5)).Sumùë°ùëó=‚àëùëñ‚ààùí©ùúÜùë°ùëñùë•ùë°ùëñùëó,Sumjt=‚àëi‚ààNŒªitxijt,(4)Avgùë°=1ùëÜ‚àëùëó‚ààùíÆSumùë°ùëó=1ùëÜ‚àëùëñ‚ààùí©ùúÜùë°ùëñ.Avgt=1S‚àëj‚ààSSumjt=1S‚àëi‚ààNŒªit.(5)min{ùë•ùë°ùëñùëó}‚àëùëó‚ààùíÆ(Sumùë°ùëó‚àíAvgùë°)2subjectto‚àëùëó‚ààùíÆùë•ùë°ùëñùëó=1‚àÄùëñ‚ààùí©min{xijt}‚àëj‚ààSSumjt‚àíAvgt2subjectto‚àëj‚ààSxijt=1‚àÄi‚ààN(6)Equation (6) is a convex quadratic assignment problem and an exact solution is NP-hard for largeN. NLS-Chain therefore employs a fast greedy heuristic:Sort nodes by descendingùúÜùë°ùëñŒªit;Round-robin assignment to obtain equal-sized shards;Iteratively migrate the node that yields the largest marginal reduction in (6) until no improvement is possible.The heuristic converges within 5‚Äì10 iterations and scales linearly withN. 4.3.2. Throughput EvaluationTable 1summarizes the average TPS achieved under increasing shard counts. NLS-Chain consistently outperforms both random sharding and LB-Chain [19], delivering up to2.2√ó2.2√óhigher throughput whenùëÜ=32S=32.Table 1.Comparison of average TPS.",
            "4.3.1. Optimization Objective": "LetùíÆ={1,‚Ä¶,ùëÜ}S={1,‚Ä¶,S}be the set of shards,ùí©={1,‚Ä¶,ùëÅ}N={1,‚Ä¶,N}be the set of nodes,ùúÜùë°ùëó‚àà[0,1]Œªjt‚àà[0,1]be the NLS of nodejat epocht(computed via Equation (2)), andùë•ùë°ùëñùëó‚àà{0,1}xijt‚àà{0,1}indicate whether nodeiis assigned to shardjat epocht. The total NLS of shardjisùëÜùë¢ùëöùë°ùëóSumjt(ref. Equation (4)) and the ideal average NLS per shard isùê¥ùë£ùëîùë°Avgt(ref. Equation (5)).Sumùë°ùëó=‚àëùëñ‚ààùí©ùúÜùë°ùëñùë•ùë°ùëñùëó,Sumjt=‚àëi‚ààNŒªitxijt,(4)Avgùë°=1ùëÜ‚àëùëó‚ààùíÆSumùë°ùëó=1ùëÜ‚àëùëñ‚ààùí©ùúÜùë°ùëñ.Avgt=1S‚àëj‚ààSSumjt=1S‚àëi‚ààNŒªit.(5)min{ùë•ùë°ùëñùëó}‚àëùëó‚ààùíÆ(Sumùë°ùëó‚àíAvgùë°)2subjectto‚àëùëó‚ààùíÆùë•ùë°ùëñùëó=1‚àÄùëñ‚ààùí©min{xijt}‚àëj‚ààSSumjt‚àíAvgt2subjectto‚àëj‚ààSxijt=1‚àÄi‚ààN(6) Equation (6) is a convex quadratic assignment problem and an exact solution is NP-hard for largeN. NLS-Chain therefore employs a fast greedy heuristic: Sort nodes by descendingùúÜùë°ùëñŒªit;Round-robin assignment to obtain equal-sized shards;Iteratively migrate the node that yields the largest marginal reduction in (6) until no improvement is possible. The heuristic converges within 5‚Äì10 iterations and scales linearly withN.",
            "4.3.2. Throughput Evaluation": "Table 1summarizes the average TPS achieved under increasing shard counts. NLS-Chain consistently outperforms both random sharding and LB-Chain [19], delivering up to2.2√ó2.2√óhigher throughput whenùëÜ=32S=32. Table 1.Comparison of average TPS.",
            "5. Experiment": "We conduct experiments on three public datasets: Set12, KITTI, and DOTA [21]. Set12 is a standard benchmark for image denoising and restoration; KITTI is widely used in autonomous driving and robotics; and DOTA provides richly annotated aerial images suitable for binarization tasks. In the experiment, all datasets from SET12 (12 grayscale images, including Lena, Cameraman, House, Pepper, Fishstar, Monarch, Airplane, Parrot, Barbara, Ship, Man, and Couple) and the left color images from the Road/Lane Detection Evaluation 2013 dataset in KITTI are selected as algorithm application data. The data is collected from various real-world environments such as urban areas, rural landscapes, and highways. From the DOTA-V1.0 dataset, 300 images are randomly selected by category as algorithm application data, primarily including baseball diamond, tennis court, basketball court, track field, harbor, bridge, large vehicle, small vehicle, helicopter, roundabout, soccer field, and swimming pool. Five metrics are adopted to quantify similarity and discrepancy: SSIM, RMSE, MAE, AER, and IL. Note that, in this paper, OTSU-GK is primarily employed for image binarization to enhance feature extraction, rather than for pixel-level classification. Therefore, metrics such as SSIM and RMSE, instead of precision and recall, are used to measure the similarity between the binarized image and either the original image or the expected image. 5.1. Results of OTSU-GKTo justify the optimization of OTSU in OTSU-GK, we first compare three adaptive thresholding strategies on Set12.Table 2shows that plain OTSU obtains the highest SSIM and the lowest RMSE, MAE, AER, and IL. It is therefore selected as the baseline for subsequent Gaussian kernel enhancement.Table 2.Adaptive thresholding results on Set12. * Higher is better; all others prefer lower values.We next randomly sample 300 images from KITTI and DOTA to evaluate OTSU-GK. KITTI: SSIM improves by 74.3% and RMSE/MAE/AER by 58.7%, and IL is reduced by 70.3% (Figure 5andFigure 6). DOTA: SSIM improves by 61.3% and RMSE/MAE/AER by 43.3%, and IL is reduced by 52.3% (Figure 7andFigure 8). The consistent gains demonstrate that the Gaussian kernel average preserves structural details while suppressing noise. Test results on the KITTI and DOTA datasets show improvements in the Structural Similarity Index (SSIM) of 60% and 61.3%, respectively, indicating enhanced preservation of structural information in processed images and visual quality closer to the original images. Root mean square error (RMSE)/Mean Absolute Error (MAE)/Average Error Rate (AER) improved by 63.7% and 43.3%, respectively, reflecting enhanced pixel-level reconstruction accuracy and significantly reduced discrepancies between algorithm outputs and target images. The information loss metric improved by 57.3% and 52.3%, proving that the optimized algorithm more effectively preserves the original information and detailed features of the image. The synergistic improvement in these metrics collectively validates the effectiveness of the proposed algorithmic enhancements across three dimensions: perceptual quality, numerical accuracy, and information integrity.Figure 5.Metric trends on KITTI.Figure 6.Relative improvement on KITTI.Figure 7.Metric trends on DOTA.Figure 8.Relative improvement on DOTA. 5.2. Ablation StudyWe further verify the contribution of each component on KITTI. OTSU-GK denotes the basic Gaussian kernel enhancement; OTSU-GK* denotes the version whose kernel parameters (ùúéœÉ, size) are optimized by grid search. The asterisks inTable 3represent the algorithms with parameters optimized through grid search. FromTable 3, grid search brings an extra 14.3% SSIM boost and 13% IL reduction, confirming the necessity of systematic hyperparameter tuning. It should be noted that the row for the OTSU method inTable 3is blank: the optimization rates of the other two methods for the column attribute indicators are calculated relative to OTSU (with OTSU as the baseline), so no numerical values are provided for the OTSU row. The step size of the grid search in the experiment ensures global exploration while keeping computational overhead manageable, with a low risk of discretization. This is because finer step sizes (e.g.,ùúéœÉstep size of 0.1) did not yield significant performance improvements in preliminary experiments. Note that the smoothing property of the Gaussian kernel reduces abrupt changes at image boundaries through a weighted average threshold matrix, thereby preserving edge continuity. As weights decay with distance, adjacent image thresholds are fused, minimizing boundary artifacts.Table 3.Ablation results on KITTI. 5.3. Throughput of NLS-ChainWe implement a prototype of NLS-Chain in Python 3.9 under PyCharm (https://www.jetbrains.com/pycharm/). The simulation comprises 500 geographically distributed nodes that communicate via the Gossip protocol; transaction generation and block validation are driven by SimPy. The test-bed is an Intel i7-8550U machine with 8 GB RAM; inter-node bandwidth is set to 35 Mbps with full-mesh connectivity.We measure steady-state TPS over 50 epochs while varying the number of shardsùëÜ‚àà{4,8,16,32}S‚àà{4,8,16,32}.Figure 9reports both average TPS and 95th-percentile TPS (after discarding the top/bottom 5% outliers). NLS-Chain consistently surpasses the two baselines:Figure 9.TPS comparison among Random, LB-Chain, and NLS-Chain.Random: Nodes and transactions are randomly assigned to shards.LB-Chain: It predicts node traffic via LSTM and re-shards through account migration.As summarized inTable 4, NLS-Chain delivers up to 83% higher average TPS than LB-Chain whenùëÜ=32S=32, while maintaining linear scalability with the number of shards.Table 4.Average TPS vs. shard count (500 nodes).",
            "5.1. Results of OTSU-GK": "To justify the optimization of OTSU in OTSU-GK, we first compare three adaptive thresholding strategies on Set12.Table 2shows that plain OTSU obtains the highest SSIM and the lowest RMSE, MAE, AER, and IL. It is therefore selected as the baseline for subsequent Gaussian kernel enhancement. Table 2.Adaptive thresholding results on Set12. * Higher is better; all others prefer lower values. We next randomly sample 300 images from KITTI and DOTA to evaluate OTSU-GK. KITTI: SSIM improves by 74.3% and RMSE/MAE/AER by 58.7%, and IL is reduced by 70.3% (Figure 5andFigure 6). DOTA: SSIM improves by 61.3% and RMSE/MAE/AER by 43.3%, and IL is reduced by 52.3% (Figure 7andFigure 8). The consistent gains demonstrate that the Gaussian kernel average preserves structural details while suppressing noise. Test results on the KITTI and DOTA datasets show improvements in the Structural Similarity Index (SSIM) of 60% and 61.3%, respectively, indicating enhanced preservation of structural information in processed images and visual quality closer to the original images. Root mean square error (RMSE)/Mean Absolute Error (MAE)/Average Error Rate (AER) improved by 63.7% and 43.3%, respectively, reflecting enhanced pixel-level reconstruction accuracy and significantly reduced discrepancies between algorithm outputs and target images. The information loss metric improved by 57.3% and 52.3%, proving that the optimized algorithm more effectively preserves the original information and detailed features of the image. The synergistic improvement in these metrics collectively validates the effectiveness of the proposed algorithmic enhancements across three dimensions: perceptual quality, numerical accuracy, and information integrity. Figure 5.Metric trends on KITTI. Figure 6.Relative improvement on KITTI. Figure 7.Metric trends on DOTA. Figure 8.Relative improvement on DOTA.",
            "5.2. Ablation Study": "We further verify the contribution of each component on KITTI. OTSU-GK denotes the basic Gaussian kernel enhancement; OTSU-GK* denotes the version whose kernel parameters (ùúéœÉ, size) are optimized by grid search. The asterisks inTable 3represent the algorithms with parameters optimized through grid search. FromTable 3, grid search brings an extra 14.3% SSIM boost and 13% IL reduction, confirming the necessity of systematic hyperparameter tuning. It should be noted that the row for the OTSU method inTable 3is blank: the optimization rates of the other two methods for the column attribute indicators are calculated relative to OTSU (with OTSU as the baseline), so no numerical values are provided for the OTSU row. The step size of the grid search in the experiment ensures global exploration while keeping computational overhead manageable, with a low risk of discretization. This is because finer step sizes (e.g.,ùúéœÉstep size of 0.1) did not yield significant performance improvements in preliminary experiments. Note that the smoothing property of the Gaussian kernel reduces abrupt changes at image boundaries through a weighted average threshold matrix, thereby preserving edge continuity. As weights decay with distance, adjacent image thresholds are fused, minimizing boundary artifacts. Table 3.Ablation results on KITTI.",
            "5.3. Throughput of NLS-Chain": "We implement a prototype of NLS-Chain in Python 3.9 under PyCharm (https://www.jetbrains.com/pycharm/). The simulation comprises 500 geographically distributed nodes that communicate via the Gossip protocol; transaction generation and block validation are driven by SimPy. The test-bed is an Intel i7-8550U machine with 8 GB RAM; inter-node bandwidth is set to 35 Mbps with full-mesh connectivity. We measure steady-state TPS over 50 epochs while varying the number of shardsùëÜ‚àà{4,8,16,32}S‚àà{4,8,16,32}.Figure 9reports both average TPS and 95th-percentile TPS (after discarding the top/bottom 5% outliers). NLS-Chain consistently surpasses the two baselines: Figure 9.TPS comparison among Random, LB-Chain, and NLS-Chain. Random: Nodes and transactions are randomly assigned to shards.LB-Chain: It predicts node traffic via LSTM and re-shards through account migration. As summarized inTable 4, NLS-Chain delivers up to 83% higher average TPS than LB-Chain whenùëÜ=32S=32, while maintaining linear scalability with the number of shards. Table 4.Average TPS vs. shard count (500 nodes).",
            "6. Conclusions": "We present OTSU-UCAN, a unified framework for secure and efficient vehicle navigation in 6G satellite‚Äìterrestrial networks. OTSU-GK enhances embedded image binarization via a Gaussian kernel weighted threshold, while NLS-Chain guarantees privacy-preserving, load-balanced sharding with up to 83% TPS gain. Future work will explore adaptive kernel selection and dynamic re-sharding intervals to further boost performance. In addition, future works will also include exploring adaptive step size strategies to optimize search efficiency."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2078-2489/16/12/1072",
        "scraped_at": "2025-12-05 23:57:00"
    },
    {
        "title": "Comparing Explainable AI Models: SHAP, LIME, and Their Role in Electric Field Strength Prediction over Urban Areas",
        "authors": "byIoannis Givisis,Dimitris Kalatzis,Christos ChristakisandYiannis Kiouvrekis",
        "journal": "Electronics2025,14(23), 4766;https://doi.org/10.3390/electronics14234766- 4 Dec 2025",
        "abstract": "This study presents a comparative evaluation of state-of-the-art Machine Learning (ML) and Explainable Artificial Intelligence (XAI) methods, specifically SHAP and LIME, for predicting electromagnetic field (EMF) strength in urban environments. Using more than 19,000 in situ EMF measurements across Catalonia, Spain, combined with high-resolution geospatial features such as building height, built-up volume, and population density, six ML algorithms were trained and assessed over 50 randomized train‚Äìtest splits. The k-Nearest Neighbors (kNN) model achieved the highest predictive accuracy (RMSE = 0.623), followed by XGBoost (RMSE = 0.711) and LightGBM (RMSE = 0.717). Explainability analysis showed that SHAP consistently identified built-up volume, building height, degree of urbanization, and population density as the dominant global predictors of EMF strength, whereas LIME revealed that degree of urbanization, population density, and building height were the most influential at the local (micro-scale) level. The results demonstrate that integrating interpretable ML frameworks with enriched geospatial datasets improves both predictive performance and transparency in EMF exposure modeling, supporting data-driven urban planning and public health assessment.Keywords:explainable artificial intelligence (XAI);SHAP;LIME;machine learning;electromagnetic field (EMF);urban environment;geospatial modeling;model interpretability;public health;electric field strength prediction",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "The rapid proliferation of advanced technologies and their integration into nearly all aspects of personal, professional and industrial activity have transformed modern societies. Central to this transformation is the communication infrastructure, which has evolved to meet the increasing demands for high-speed, low-latency, and reliable connectivity [1,2]. Fifth-generation (5G) networks are now widely deployed, and the transition to sixth-generation (6G) technologies is already underway [3,4], enabling an unprecedented degree of interconnection among devices, systems, and users. However, this technological expansion has led to the continuous and widespread exposure of human populations to electromagnetic fields (EMFs) [5,6] emitted by wireless communication systems. The dominant contribution arises from dense deployments of transmitting antennas, designed to meet growing coverage and capacity requirements [7]. Although international regulatory frameworks provide safety guidelines for exposure [8,9,10], the long-term health effects of chronic exposure to EMF remain an active area of scientific inquiry [11,12,13], fueling both societal concern and the need for rigorous monitoring [14,15,16,17]. In this context, two critical challenges emerge: first, ensuring that EMF levels remain within established safety thresholds to safeguard public health [18], and second, optimizing the distribution of transmitting antennas to simultaneously satisfy service quality and minimize population exposure [19,20]. Addressing these challenges requires an accurate prediction of EMF levels in heterogeneous urban environments [21,22,23], where variations in building density, topography, and user demand complicate the propagation patterns [24,25]. Traditional physics-based propagation models provide a partial solution, but are often limited by their simplifying assumptions and high computational cost when scaled to complex real-world environments [26]. Machine Learning (ML) methods [27,28,29], supported by Explainable Artificial Intelligence (XAI) frameworks [30,31], offer a promising alternative by enabling data-driven EMF prediction and also providing interpretability regarding the factors influencing model output. Such approaches not only enhance predictive accuracy, but also contribute to transparency in decision-making, a critical requirement for regulatory acceptance and public trust [32]. Although recent studies have improved EMF prediction using geospatial [33] and machine learning techniques [7,34], model interpretability remains insufficiently explored [35]. Prior work commonly applies SHAP [36], but rarely compares it directly with LIME for EMF strength prediction, and most analyses focus on a single urban area, limiting generalizability. This study addresses these gaps by systematically evaluating SHAP and LIME across multiple ML models and diverse urban environments. Overall, the aim is to compare state-of-the-art ML and XAI methods for EMF prediction while identifying the key spatial and environmental factors influencing EMF variability. By jointly analyzing SHAP (global explanations) and LIME (local explanations), this work provides a comprehensive and novel interpretability framework for EMF modeling, enhancing both predictive accuracy and understanding of EMF propagation in urban settings.",
            "2. Related Work": "In the era of rapidly advancing machine learning, numerous studies have been conducted within IoT frameworks to enhance prediction [37], automation, and intelligent environmental monitoring [27,38,39,40,41,42,43]. Research into electromagnetic field (EMF) strength is advancing rapidly, thanks to the integration of multidimensional datasets and cutting-edge methods like mathematical geospatial modeling, machine learning (ML), and explainable artificial intelligence (XAI). XAI, in particular, addresses the growing need to make scientific findings more interpretable [44]. These innovations greatly improve the depth and scope of research results. 2.1. Geospatial Methods for EMF and Environmental MonitoringIn a widely referenced practical study, Phillips et al. [45] explore the use of geostatistical methods for radio environment mapping (REM). They evaluated kriging and geostatistical techniques for mapping radio environments from sparse measurements, demonstrating that geostatistics offers accurate interpolation and explicit uncertainty estimates. The study also suggests hybrid GP/physics approaches for handling complex urban environments, making it a valuable resource for REM construction and sensor placement. Tesfay and Clavier [46] propose a Gaussian Process regression approach for EMF spatial reconstruction from sparse sensor data. The GP gives both mean field estimates and spatial uncertainty; with an informative mean and properly chosen kernel it outperforms zero-mean GP and naive interpolation in their experiments. The study stresses the usefulness of GP uncertainty maps to guide sensor placement.In a more complex approach as part of consecutive novel studies, Kiouvrekis et al. [47] evaluated five geospatial modeling methods to create national-level EMF exposure maps, using 3621 measurements over a 9251 km2area. Gaussian Process Regression models, also known as Kriging models, outperformed others in accuracy; however, when outliers were removed, the classical nearest neighbor method showed comparable performance. In general, the study highlights the importance of model selection and data quality in accurately assessing EMF exposure for public health considerations. In a relevant study, Panagiotakopoulos et al. [48] investigated geospatial interpolation modeling techniques of electromagnetic field (EMF) exposure by also using Gaussian Process Regression (Kriging) and nearest-neighbor interpolation for large-scale EMF mapping. Using an enriched feature dataset of 3632 measurements collected in Paris, their study demonstrated that Kriging, particularly with the exponential covariance model, achieved superior predictive accuracy by effectively capturing spatial autocorrelation in the EMF data, while outlier removal further improved model performance by reducing both mean square error (MSE) and variability.Jorge Guillen-Pina et al. [24] in their study presented an innovative multi-method ensemble algorithm for optimization (PCRO-SL) to create EMF exposure maps using multi-method evolutionary ensembles, specifically genetic algorithms, to optimize measurement point selection. This approach allows for efficient exploration of the solution space to identify optimal configurations of measurement points and so significantly reduces the number of required measurements while maintaining the accuracy of the map, offering a resource-efficient solution for urban EMF monitoring. 2.2. ML Models Used in EMF Prediction and Environmental MonitoringArtificial Intelligence, through innovative Machine Learning techniques, seeks to enhance the accuracy, adaptability, and efficiency of well-established mathematical geospatial methods in EMF predictions. In this spirit, Yarui Zhang et al. [28] proposed ExposNet, a deep learning framework that predicts the levels of the electric field in urban environments using real-world measurements and data from the base station. By encoding environmental features as multidimensional tensors for input to CNNs, the model captures complex spatial patterns and achieves improved prediction accuracy, demonstrating the potential of deep learning in environmental monitoring. In another study, Xinwei Song et al. [49] compared six predictive models for forecasting RF-EMF exposure in urban settings using two years of monitoring data from Novi Sad, Serbia. Among the models evaluated, PLS, CNN and Transformer models outperforming the others but with the Partial Least Squares Regression (PLS) demonstrated superior accuracy and efficiency above all, suggesting its suitability for real-time EMF exposure monitoring and public health protection initiatives.Wang and Wiart [50] developed a hybrid artificial neural network model designed to efficiently process various input data, including spatial and temporal variations in exposure to EMF to map urban exposure to EMF by integrating sensor data, drive tests and public BSA information. Applied to Paris‚Äôs 14th district, the model outperformed traditional methods, effectively capturing complex spatial variations in EMF exposure. The study offers a cost-effective and accurate approach to urban EMF monitoring. Mallik et al. [51] proposed a generative method, GLIP (Generative Local Image Prior), to reconstruct electromagnetic field (EMF) exposure maps from sparse sensor data. Instead of training a full GAN, GLIP employs a pretrained generator as an image prior, optimizing its latent space to match sparse measurements while preserving realistic spatial structures. Evaluated on simulated and real EMF datasets, GLIP outperformed traditional interpolation techniques (e.g., Kriging, nearest-neighbor) in accuracy and spatial coherence, demonstrating its effectiveness in data-sparse EMF mapping scenarios.Bilson et al. [52] proposed a physics-informed machine learning (PIML) framework to model RF-EMF exposure from 5G massive multiple-input-multiple-output (mMIMO) base stations. By integrating wave propagation physics and antenna radiation patterns into neural network training through physics-based loss terms, the approach enforces physically consistent predictions and improves extrapolation in sparse data regions. Evaluations in simulated and real scenarios show that the PIML model achieves lower RMSE and greater robustness than purely data-driven or physics-only methods, offering an efficient and physically grounded solution for large-scale EMF mapping. Shahid et al. [53] proposed ReVeal, a physics-informed neural network (PINN) for radio environment mapping (REM) that embeds a path-loss PDE into the learning objective to enforce physical consistency while fitting sparse RF measurements. By combining data-driven learning with propagation constraints, ReVeal achieves high-fidelity RSS maps with improved RMSE, robustness, and data efficiency compared to standard ML and classical models. Evaluations of real-world ARA testbed data demonstrate substantial accuracy gains and reduced sample requirements, highlighting ReVeal‚Äôs suitability for large-scale and resource-constrained EMF mapping. 2.3. Use of XAI in Spatial and Environmental PredictionsAlthough the wide application of machine learning techniques in the prediction of EMF and the implementation of environmental research has achieved remarkable results, the need for interpretability and comprehensive, human-understandable documentation of the results has become more evident than ever. In this spirit, the study of Abekoon et al. [54] developed a deep neural network (DNN) to predict soil nitrogen (N), phosphorus (P) and potassium (K) levels in cabbage cultivation based on plant growth metrics such as height, leaf count and leaf area. The study was applied over an 85-day period in Sri Lanka‚Äôs central highlands, with the model aimed to support soil fertility assessment without extensive chemical tests. To improve transparency and interpretability, the study used explainable AI (XAI) techniques, namely SHAP and LIME, offering clear insights into the contribution of the features and enhancing the relevance of the model for precision agriculture and sustainable soil management. Junhao Zhang‚Äôs study [55] applies the LightGBM model to predict water potability using various water quality indicators. To interpret the model‚Äôs predictions, the research compares SHAP and LIME techniques. SHAP offers a global view, identifying sulfate and pH as key features, while LIME provides local explanations for individual predictions through a local linear approximation. The study concludes that SHAP is suitable for understanding the overall behavior of the model, whereas LIME is more effective for interpreting specific instances.Kiouvrekis et al. [36] developed an explainable machine learning framework to map the strength of the electric field in Paris using an enriched dataset incorporating geographical features such as building volume and population density. Among 410 models evaluated, the k-nearest neighbors (kNN) algorithm demonstrated the most consistent performance, generating the lowest RMSE and standard deviation. To improve interpretability, SHapley Additive exPlanations (SHAP) were applied, revealing that the building volume around each antenna was the most influential predictor of EMF levels, followed by population density. The resulting dynamic maps offer valuable insights for urban planning and public health. Xianlin Ma et al. [56] developed interpretable machine learning models to predict tight gas well productivity using data from the Eastern Sulige field in China. They developed machine learning models, including Support Vector Machines (SVMs), Gradient Boosting Decision Trees (GBDTs), and Random Forest (RF), to forecast well productivity. Among these, the GBDT model demonstrated the most superior predictive performance. By applying SHAP and LIME, the study provided global and local explanations of model predictions, enhancing the understanding of key factors that affect productivity and helping in strategic decision-making. Nasir and Li [57] applied advanced machine learning models such as ANN, GBM, RF, XGBoost, and a hybrid RF-GBM, to predict key wastewater treatment plant (WWTP) variables such as BOD, TSS,ùëÅùêª3NH3, and P using multi-year operational data. To enhance interpretability, SHAP and LIME were employed, revealing the most influential features driving the model predictions. The results demonstrated high predictive accuracy(ùëÖ2‚âà0.98)(R2‚âà0.98)and strong generalization, while the integration of XAI improved transparency and informed data-driven optimization of WWTP operations.In another novel study, Kiouvrekis et al. [31] evaluated 566 machine learning models to predict electric field strength in eight French cities by integrating sensor-based EMF data with urban features such as population density, urbanization, and building characteristics. Among the six machine learning approaches tested, the ensemble methods, in particular Random Forest and XGBoost, achieved the highest predictive accuracy, especially when including geographic features as predictive variables. To enhance interpretability, SHAP was employed to interpret model outputs, revealing notable differences in feature importance rankings between tree-based and other machine learning models. A study by Kalatzis et al. [30], which conducted in Cyprus applied Explainable Artificial Intelligence (XAI) techniques to analyze electromagnetic field (EMF) exposure across multiple frequency bands (30 MHz‚Äì6 GHz) from mobile, radio, and television sources. Six machine learning models were compared, with ensemble methods, particularly Random Forests and LightGBM, achieving the highest predictive accuracy. Using SHAP, the study identified key spatial and demographic factors influencing EMF intensity, supporting transparent, data-driven exposure mapping for urban planning and public health assessment.However, despite the significant progress demonstrated in the reviewed studies, several limitations remain evident in the current body of research. Most existing work has focused on improving predictive accuracy through advanced machine learning or on enhancing spatial interpolation via geospatial models, yet few have systematically addressed the interpretability of EMF prediction models. Although SHAP has been increasingly adopted for global explainability, there is a notable absence of studies that directly compare SHAP and LIME in the context of EMF strength prediction, particularly when combined with heterogeneous geospatial and environmental datasets. Furthermore, existing explainable approaches often concentrate on a single urban area or frequency band, limiting the generalizability and scalability of their findings across diverse urban morphologies. To bridge these gaps, the present study aims to provide a comprehensive comparative analysis of SHAP and LIME applied to multiple machine learning models for the prediction of the strength of EMF in several urban and semi-urban areas, collected using a hybrid system of fixed and portable measuring instruments. So, by coupling rigorous geospatial feature engineering with extensive evaluation of ML models and interpretable AI techniques, this work seeks to advance both the transparency and the scientific reliability of EMF exposure modeling in complex urban environments.",
            "2.1. Geospatial Methods for EMF and Environmental Monitoring": "In a widely referenced practical study, Phillips et al. [45] explore the use of geostatistical methods for radio environment mapping (REM). They evaluated kriging and geostatistical techniques for mapping radio environments from sparse measurements, demonstrating that geostatistics offers accurate interpolation and explicit uncertainty estimates. The study also suggests hybrid GP/physics approaches for handling complex urban environments, making it a valuable resource for REM construction and sensor placement. Tesfay and Clavier [46] propose a Gaussian Process regression approach for EMF spatial reconstruction from sparse sensor data. The GP gives both mean field estimates and spatial uncertainty; with an informative mean and properly chosen kernel it outperforms zero-mean GP and naive interpolation in their experiments. The study stresses the usefulness of GP uncertainty maps to guide sensor placement. In a more complex approach as part of consecutive novel studies, Kiouvrekis et al. [47] evaluated five geospatial modeling methods to create national-level EMF exposure maps, using 3621 measurements over a 9251 km2area. Gaussian Process Regression models, also known as Kriging models, outperformed others in accuracy; however, when outliers were removed, the classical nearest neighbor method showed comparable performance. In general, the study highlights the importance of model selection and data quality in accurately assessing EMF exposure for public health considerations. In a relevant study, Panagiotakopoulos et al. [48] investigated geospatial interpolation modeling techniques of electromagnetic field (EMF) exposure by also using Gaussian Process Regression (Kriging) and nearest-neighbor interpolation for large-scale EMF mapping. Using an enriched feature dataset of 3632 measurements collected in Paris, their study demonstrated that Kriging, particularly with the exponential covariance model, achieved superior predictive accuracy by effectively capturing spatial autocorrelation in the EMF data, while outlier removal further improved model performance by reducing both mean square error (MSE) and variability. Jorge Guillen-Pina et al. [24] in their study presented an innovative multi-method ensemble algorithm for optimization (PCRO-SL) to create EMF exposure maps using multi-method evolutionary ensembles, specifically genetic algorithms, to optimize measurement point selection. This approach allows for efficient exploration of the solution space to identify optimal configurations of measurement points and so significantly reduces the number of required measurements while maintaining the accuracy of the map, offering a resource-efficient solution for urban EMF monitoring.",
            "2.2. ML Models Used in EMF Prediction and Environmental Monitoring": "Artificial Intelligence, through innovative Machine Learning techniques, seeks to enhance the accuracy, adaptability, and efficiency of well-established mathematical geospatial methods in EMF predictions. In this spirit, Yarui Zhang et al. [28] proposed ExposNet, a deep learning framework that predicts the levels of the electric field in urban environments using real-world measurements and data from the base station. By encoding environmental features as multidimensional tensors for input to CNNs, the model captures complex spatial patterns and achieves improved prediction accuracy, demonstrating the potential of deep learning in environmental monitoring. In another study, Xinwei Song et al. [49] compared six predictive models for forecasting RF-EMF exposure in urban settings using two years of monitoring data from Novi Sad, Serbia. Among the models evaluated, PLS, CNN and Transformer models outperforming the others but with the Partial Least Squares Regression (PLS) demonstrated superior accuracy and efficiency above all, suggesting its suitability for real-time EMF exposure monitoring and public health protection initiatives. Wang and Wiart [50] developed a hybrid artificial neural network model designed to efficiently process various input data, including spatial and temporal variations in exposure to EMF to map urban exposure to EMF by integrating sensor data, drive tests and public BSA information. Applied to Paris‚Äôs 14th district, the model outperformed traditional methods, effectively capturing complex spatial variations in EMF exposure. The study offers a cost-effective and accurate approach to urban EMF monitoring. Mallik et al. [51] proposed a generative method, GLIP (Generative Local Image Prior), to reconstruct electromagnetic field (EMF) exposure maps from sparse sensor data. Instead of training a full GAN, GLIP employs a pretrained generator as an image prior, optimizing its latent space to match sparse measurements while preserving realistic spatial structures. Evaluated on simulated and real EMF datasets, GLIP outperformed traditional interpolation techniques (e.g., Kriging, nearest-neighbor) in accuracy and spatial coherence, demonstrating its effectiveness in data-sparse EMF mapping scenarios. Bilson et al. [52] proposed a physics-informed machine learning (PIML) framework to model RF-EMF exposure from 5G massive multiple-input-multiple-output (mMIMO) base stations. By integrating wave propagation physics and antenna radiation patterns into neural network training through physics-based loss terms, the approach enforces physically consistent predictions and improves extrapolation in sparse data regions. Evaluations in simulated and real scenarios show that the PIML model achieves lower RMSE and greater robustness than purely data-driven or physics-only methods, offering an efficient and physically grounded solution for large-scale EMF mapping. Shahid et al. [53] proposed ReVeal, a physics-informed neural network (PINN) for radio environment mapping (REM) that embeds a path-loss PDE into the learning objective to enforce physical consistency while fitting sparse RF measurements. By combining data-driven learning with propagation constraints, ReVeal achieves high-fidelity RSS maps with improved RMSE, robustness, and data efficiency compared to standard ML and classical models. Evaluations of real-world ARA testbed data demonstrate substantial accuracy gains and reduced sample requirements, highlighting ReVeal‚Äôs suitability for large-scale and resource-constrained EMF mapping.",
            "2.3. Use of XAI in Spatial and Environmental Predictions": "Although the wide application of machine learning techniques in the prediction of EMF and the implementation of environmental research has achieved remarkable results, the need for interpretability and comprehensive, human-understandable documentation of the results has become more evident than ever. In this spirit, the study of Abekoon et al. [54] developed a deep neural network (DNN) to predict soil nitrogen (N), phosphorus (P) and potassium (K) levels in cabbage cultivation based on plant growth metrics such as height, leaf count and leaf area. The study was applied over an 85-day period in Sri Lanka‚Äôs central highlands, with the model aimed to support soil fertility assessment without extensive chemical tests. To improve transparency and interpretability, the study used explainable AI (XAI) techniques, namely SHAP and LIME, offering clear insights into the contribution of the features and enhancing the relevance of the model for precision agriculture and sustainable soil management. Junhao Zhang‚Äôs study [55] applies the LightGBM model to predict water potability using various water quality indicators. To interpret the model‚Äôs predictions, the research compares SHAP and LIME techniques. SHAP offers a global view, identifying sulfate and pH as key features, while LIME provides local explanations for individual predictions through a local linear approximation. The study concludes that SHAP is suitable for understanding the overall behavior of the model, whereas LIME is more effective for interpreting specific instances. Kiouvrekis et al. [36] developed an explainable machine learning framework to map the strength of the electric field in Paris using an enriched dataset incorporating geographical features such as building volume and population density. Among 410 models evaluated, the k-nearest neighbors (kNN) algorithm demonstrated the most consistent performance, generating the lowest RMSE and standard deviation. To improve interpretability, SHapley Additive exPlanations (SHAP) were applied, revealing that the building volume around each antenna was the most influential predictor of EMF levels, followed by population density. The resulting dynamic maps offer valuable insights for urban planning and public health. Xianlin Ma et al. [56] developed interpretable machine learning models to predict tight gas well productivity using data from the Eastern Sulige field in China. They developed machine learning models, including Support Vector Machines (SVMs), Gradient Boosting Decision Trees (GBDTs), and Random Forest (RF), to forecast well productivity. Among these, the GBDT model demonstrated the most superior predictive performance. By applying SHAP and LIME, the study provided global and local explanations of model predictions, enhancing the understanding of key factors that affect productivity and helping in strategic decision-making. Nasir and Li [57] applied advanced machine learning models such as ANN, GBM, RF, XGBoost, and a hybrid RF-GBM, to predict key wastewater treatment plant (WWTP) variables such as BOD, TSS,ùëÅùêª3NH3, and P using multi-year operational data. To enhance interpretability, SHAP and LIME were employed, revealing the most influential features driving the model predictions. The results demonstrated high predictive accuracy(ùëÖ2‚âà0.98)(R2‚âà0.98)and strong generalization, while the integration of XAI improved transparency and informed data-driven optimization of WWTP operations. In another novel study, Kiouvrekis et al. [31] evaluated 566 machine learning models to predict electric field strength in eight French cities by integrating sensor-based EMF data with urban features such as population density, urbanization, and building characteristics. Among the six machine learning approaches tested, the ensemble methods, in particular Random Forest and XGBoost, achieved the highest predictive accuracy, especially when including geographic features as predictive variables. To enhance interpretability, SHAP was employed to interpret model outputs, revealing notable differences in feature importance rankings between tree-based and other machine learning models. A study by Kalatzis et al. [30], which conducted in Cyprus applied Explainable Artificial Intelligence (XAI) techniques to analyze electromagnetic field (EMF) exposure across multiple frequency bands (30 MHz‚Äì6 GHz) from mobile, radio, and television sources. Six machine learning models were compared, with ensemble methods, particularly Random Forests and LightGBM, achieving the highest predictive accuracy. Using SHAP, the study identified key spatial and demographic factors influencing EMF intensity, supporting transparent, data-driven exposure mapping for urban planning and public health assessment. However, despite the significant progress demonstrated in the reviewed studies, several limitations remain evident in the current body of research. Most existing work has focused on improving predictive accuracy through advanced machine learning or on enhancing spatial interpolation via geospatial models, yet few have systematically addressed the interpretability of EMF prediction models. Although SHAP has been increasingly adopted for global explainability, there is a notable absence of studies that directly compare SHAP and LIME in the context of EMF strength prediction, particularly when combined with heterogeneous geospatial and environmental datasets. Furthermore, existing explainable approaches often concentrate on a single urban area or frequency band, limiting the generalizability and scalability of their findings across diverse urban morphologies. To bridge these gaps, the present study aims to provide a comprehensive comparative analysis of SHAP and LIME applied to multiple machine learning models for the prediction of the strength of EMF in several urban and semi-urban areas, collected using a hybrid system of fixed and portable measuring instruments. So, by coupling rigorous geospatial feature engineering with extensive evaluation of ML models and interpretable AI techniques, this work seeks to advance both the transparency and the scientific reliability of EMF exposure modeling in complex urban environments.",
            "3. Materials and Methods": "3.1. The DatasetThe data set [58] used in this study originates from the official open data platform of the Spanish Government, which provides measurements of radio-frequency electromagnetic field (EMF) exposure collected through certified monitoring campaigns. During the period 2013‚Äì2015, within the framework of the Governan√ßa Radioel√®ctrica project [59], a comprehensive electromagnetic field (EMF) measurement program was carried out in Catalonia. For this purpose, a combined system of fixed and portable measuring instruments was used, allowing continuous monitoring and recording on-site in sensitive areas such as schools and hospitals (Figure 1). A total of 100 fixed monitoring stations were installed on roofs of buildings with a direct line-of-sight to the antennas. Ninety of them operated on photovoltaic power, while ten operated as electrically powered units. These stations measured either specific mobile telephony frequency bands (900, 1800, 2100 MHz) or the full spectrum from 100 kHz to 8 GHz. In parallel, 50 portable devices were distributed to municipalities and public bodies, enabling more than 19,322 measurements at 4598 locations.Figure 1.Spatial distribution of EMF (Electromagnetic Field) measurement points across Catalonia, Spain.Each record in the dataset corresponds to a specific measurement report, containing details such as location, date of assessment, measurement type and the recorded average field strength. The data set covers a wide geographic coverage in various municipalities and administrative regions, reflecting real-world environmental EMF conditions. Measurements are typically conducted using portable spectrum analyzers or fixed monitoring equipment, following standardized protocols to ensure reliability and comparability.The variables used in this study were derived from the Global Human Settlement Layer (GHSL) Data Package 2023 (GHS P2023) developed by the European Commission‚Äôs Joint Research Centre (JRC, 2024). These datasets provide globally consistent, open-access spatial information describing the built environment, population distribution, and settlement structure from 1975 to 2030. All datasets were resampled and aligned to a common grid of 100 m spatial resolution with geographic coordinates(ùë•,ùë¶)(x,y)corresponding to longitude and latitude of the grid cell centroid (seeTable 1).Table 1.Summary of GHSL datasets used in the study, including variable type, spatial resolution, temporal coverage, and coordinate reference.The GHS-BUILT-S R2023A variable represents the fraction of built-up area within each grid cell, derived from Sentinel-2 and Landsat imagery for epochs between 1975 and 2030 (5-year intervals). It provides sub-pixel built-up estimates at 10 m and aggregated 100 m resolution, distinguishing between residential (RES) and non-residential (NRES) domains. This layer serves as a measure of horizontal urban expansion and built-up density. The GHS-BUILT-H R2023A variable provides average building height (in metres) for each 100 m grid cell, estimated from the integration of AW3D30, SRTM30, and Sentinel-2 data (epoch 2018). It captures the vertical dimension of the built environment, enabling analyses of urban morphology and 3D structural variation. The GHS-BUILT-V R2023A variable combines the built-up surface and height layers to estimate built-up volume (m3/cell) for 1975‚Äì2030 at 100 m resolution. It expresses the volumetric density of constructed space, reflecting the total built infrastructure per unit area. This is a critical indicator of the intensity of urban development and infrastructure stock. The GHS-BUILT-C R2023A variable integrates built-up surface, height, and population layers to describe settlement characteristics and urban form. It includes metrics of built-up density, morphology, and functional classification at 10‚Äì100 m resolution for 2018. This variable enables the identification of heterogeneous settlement typologies and spatial organization patterns. The GHS_BUILT_C_MSZ layer classifies morphological settlement zones (MSZ) based on built-up density and spatial configuration, distinguishing continuous urban fabric, suburban, and rural morphologies. It provides categorical information useful for urban form classification and sprawl analysis. The GHS-POP R2023A variable provides multi-temporal population density grids (inhabitants per cell) for 1975‚Äì2030 in 5-year intervals at 100 m and 1 km resolutions. Population counts are derived by downscaling national and subnational census data using residential built-up volume (RES) as a weighting layer, aligned to the UN World Population Prospects (2022) and World Urbanization Prospects (2018). The GHS-SMOD R2023Avariable classifies the Earth‚Äôs surface into urban centres, dense urban clusters, and rural grid cells following the Degree of Urbanisation (DEGURBA) Stage I methodology. It integrates GHS-BUILT-S and GHS-POP data to describe the spatial hierarchy of settlement systems between 1975 and 2030. 3.2. Framework for Machine Learning and Explainable AIThis study follows a structured, reproducible workflow designed to ensure robust electromagnetic field (EMF) prediction and model interpretability, integrating geospatial preprocessing, systematic ML benchmarking, and dual XAI evaluation (Figure 2). The workflow begins with raw EMF measurements from the official Spanish governmental open-data platform, followed by comprehensive quality control including missing-value handling, physical-plausibility checks, distributional inspection, and robust outlier filtering techniques. All geospatial variables are formalized and enriched with refined urban-morphology attributes from the GHSL Data Package 2023, producing a unified spatial dataset suitable for ML experimentation.Figure 2.Machine Learning‚ÄîXAI pipeline.A randomized shuffling stage ensures independence in sampling order prior to model training. To mitigate spatial autocorrelation biases, we employ 50 stratified train‚Äìtest splits (80/20) with unique seeds, verifying no leakage and maintained target-variable representativeness across splits. This repeated partitioning strategy provides a low-variance generalization estimate and robust cross-validation. Six ML models were then benchmarked, Neural Networks, k-Nearest Neighbors, Decision Trees, Random Forests, LightGBM, and XGBoost, each trained over a structured hyperparameter grid and evaluated via mean RMSE, standard deviation, and 95% confidence intervals across all folds. Best configurations were subsequently retrained and independently assessed to confirm generalization.Model interpretability was conducted on the top-performing models using SHAP and LIME. SHAP provided global feature attribution and instance-level decomposition, while LIME offered localized surrogate explanations aggregated across samples. This dual-lens analysis enabled assessment of both global explanatory stability and local contextual behavior. Together, the pipeline workflow delivers a transparent, rigorously validated, and scientifically accountable framework for EMF estimation in urban environments, ensuring methodological robustness, reproducibility, and interpretability. 3.3. Description of Machine Learning MethodsMachine learning (ML), a core branch of artificial intelligence, facilitates data-driven modeling by identifying patterns and generating predictions with minimal human intervention. In this study, ML techniques are employed to accurately predict and map electromagnetic field (EMF) strength in urban environments, integrating spatial and structural features to enhance environmental monitoring. Before detailing the specific algorithms, architectures, and hyperparameters, it is important to highlight two fundamental hyperparameters common across most ML models: the learning rate(ùúÇ)(Œ∑)and the optimizer. The learning rate governs the magnitude of parameter updates during optimization, directly influencing model convergence and stability, while the optimizer determines the update strategy for minimizing the loss function. Proper selection or tuning of these parameters is essential to balance convergence speed, accuracy, and generalization across different ML methods.Neural Networks (NN) Neural Networks (NNs) are computational models inspired by biological neural systems, composed of interconnected neurons organized into input, hidden, and output layers. Each neuron processes weighted inputs and biases through nonlinear activation functions(ùúé)(œÉ)(e.g., ReLU, sigmoid, tanh), enabling the network to approximate complex nonlinear mappings. The network‚Äôs depth, determined by the hyperparameter that controls the number of hidden layers(ùêªùêø)(HL), which define the total depthùëì(ùë•;ùúÉ)=ùëì(ùêø)(ùëì(ùêø‚àí1)(‚Ä¶ùëì(1)(ùë•;ùëä(1),ùëè(1))‚Ä¶))f(x;Œ∏)=f(L)f(L‚àí1)(‚Ä¶f(1)(x;W(1),b(1))‚Ä¶)of the NN and directly influences its representational capacity but also affects overfitting risk. Training is performed via backpropagation using gradient descent-based optimization to minimize a loss function, such as Mean Squared Error for regression or Cross-Entropy for classification. In this study, a set of neural network models were employed, with hyperparameter tuning over the number of hidden layersùêªùêø‚àà{2,5,10,20,50}HL‚àà{2,5,10,20,50}, activation functionsùúé‚àà{identity,logistic,tanh,ReLU}œÉ‚àà{identity,logistic,tanh,ReLU}, and learning ratesùúÇ‚àà{0.01,0.1,0.2,0.5,1.0}Œ∑‚àà{0.01,0.1,0.2,0.5,1.0}, while the Adam optimizer was used and all the other hyperparameters kept at default settings.k-Nearest Neighbors (k-NN)The k-Nearest Neighbors (k-NN) algorithm is a non-parametric supervised learning method used for classification and regression, based on the principle that similar data points in the feature space exhibit similar outcomes. For a query pointx, the algorithm identifies theknearest samples and predicts the output as either the majority class (classification) or the mean/weighted mean of neighboring target values (regression). Model performance depends primarily on the choice ofk, the distance metric, and the weighting scheme. Distance can be computed using various norms‚ÄîEuclidean(ùêø2)(L2), Manhattan(ùêø1)(L1), or Minkowski‚Äîwhile weights may be uniform or distance-dependent, e.g., inverse distanceùë§ùëñ=ùëë(ùë•,ùë•ùëñ)‚àíùëùwi=d(x,xi)‚àípor Gaussianùë§ùëñ=ùëí‚àí(ùëë2ùëñ/2ùúé2)wi=e‚àí(di2/2œÉ2). In this study, three hyperparameters were optimized: the number of neighborsùëò‚àà{5,10,25,50,100,150}k‚àà{5,10,25,50,100,150}, distance metric parameterùëù‚àà{1,1.5,2,3}p‚àà{1,1.5,2,3}, and weighting scheme (uniform vs. distance-based).Decision Trees (DTs)Decision Trees (DTs) are nonparametric supervised learning models applicable to both classification and regression tasks. They recursively partition the feature space through hierarchical, feature-based splits to form a tree structure of decision and terminal nodes. Splits are chosen to maximize node purity using criteria such as Gini Impurityùê∫=1‚àí‚àëùëñùëù2ùëñG=1‚àí‚àëipi2or Entropyùêª=‚àí‚àëùëñùëùùëñlog2ùëùùëñH=‚àí‚àëipilog2pi, with information gain guiding the optimal partitioning. For regression, variance reduction serves as the splitting criterion. In this study, a minimal configuration was adopted to establish a baseline for comparison with ensemble tree models, varying only the hyperparameter controlling the minimum samples required to split a node within{2,5,10,20,50}{2,5,10,20,50}.Random Forest (RF)Random Forest (RF) is an ensemble learning algorithm that enhances predictive accuracy and generalization for both classification and regression by aggregating multiple Decision Trees trained on bootstrap samples (bagging) and random feature subsets. This dual randomness reduces inter-tree correlation and mitigates overfitting. The final prediction is obtained via majority voting (classification) or averaging (regression). Feature importance is derived from the mean impurity reduction across all trees, quantifying each variable‚Äôs contribution to model performance. In this study, the Random Forest was tuned by varying the number of treesùëÄ‚àà{50,100,200}M‚àà{50,100,200}, the splitting criterion{MSE,MAE,Friedman-MSE}{MSE,MAE,Friedman-MSE}, and fixing the maximum tree depth to 10.LightGBMLight Gradient Boosting Machine (LightGBM) is an efficient ensemble algorithm based on Gradient Boosting Decision Trees (GBDTs), optimized for speed and scalability. Unlike traditional level-wise boosting, LightGBM adopts a leaf-wise growth strategy, expanding the leaf with the greatest loss reduction to enhance accuracy while preserving efficiency through histogram-based feature binning. Each tree minimizes a differentiable loss function by fitting the residuals of prior predictions, updating the model asùë¶ÃÇ(ùë°)=ùë¶ÃÇ(ùë°‚àí1)+ùúÇùëìùë°(ùê±)y^(t)=y^(t‚àí1)+Œ∑ft(x)whereùëìùë°ftis the tree added at iterationtandùúÇŒ∑is the learning rate. In this study, four key hyperparameters were tuned: the number of leaves(ùêø‚àà{31,50,100})(L‚àà{31,50,100}), number of trees(ùëÄ‚àà{100,300,500})(M‚àà{100,300,500}), maximum depth(ùëöùëéùë•_ùëëùëíùëùùë°‚Ñé‚àà{5,10,20})(max_depth‚àà{5,10,20}), and learning rate(ùúÇ‚àà{0.01,0.05,0.1})(Œ∑‚àà{0.01,0.05,0.1}), balancing model complexity, convergence speed, and generalization performance.XGBoost (Extreme Gradient Boosting)Extreme Gradient Boosting (XGBoost) is an optimized implementation of the gradient boosting framework that constructs decision trees sequentially, with each tree correcting the residuals of its predecessors. It minimizes a regularized objective function,‚Ñí(ùë°)=‚àëùëñ=1ùëõùëô(ùë¶ùëñ,ùë¶ÃÇ(ùë°)ùëñ)+‚àëùë°=1ùëáŒ©(ùëìùë°)L(t)=‚àëi=1nl(yi,y^i(t))+‚àët=1TŒ©(ft)whereùëô(ùë¶ùëñ,ùë¶ÃÇ(ùë°)ùëñ)l(yi,y^i(t))denotes the loss andŒ©(ùëìùë°)=ùõæùëá+12ùúÜ‚àëùëáùëó=1ùë§2ùëóŒ©(ft)=Œ≥T+12Œª‚àëj=1Twj2penalizes model complexity, thereby enhancing generalization. Optimization is guided by first- and second-order gradients, enabling precise and efficient updates. XGBoost incorporates regularization, sparsity-aware learning, and parallelized computation, achieving both scalability and robustness.For this study, the tuned hyperparameters were the L2 regularization term(ùúÜ‚àà{1,5,10})(Œª‚àà{1,5,10}), the number of trees(ùëÄ‚àà{100,300,500})(M‚àà{100,300,500}), the maximum tree depth (fixed at 20), and the learning rate(ùúÇ‚àà{0.01,0.05,0.2})(Œ∑‚àà{0.01,0.05,0.2}), ensuring an optimal balance between bias, variance, and computational efficiency.TheTable 2above summarizes the hyperparameter configurations explored for each machine learning model. The selected ranges were designed to systematically probe the trade-off between model complexity, generalization capacity, and computational efficiency, ensuring a balanced and reproducible experimental framework. For Neural Networks (NN), variations in the number of hidden layers, activation functions, and learning rates capture the effect of network depth and nonlinear activation on convergence and overfitting control. In k-Nearest Neighbors (k-NN), tuning the number of neighbors, distance metric, and weighting scheme examines the locality dependencies of the feature space and the influence of metric geometry on model‚Äôs performance. For Decision Trees (DTs), the minimum samples per split parameter isolates the relationship between node purity and model variance, while keeping DTs as a simple, interpretable baseline for ensemble comparison. Random Forest (RF) tuning included ensemble size and splitting criterion, maintaining fixed depth to evaluate ensemble effects independently of tree complexity. In LightGBM, the number of leaves, boosting iterations, maximum depth, and learning rate were adjusted to control tree growth, regularization strength, and learning dynamics. Similarly, for XGBoost, tuning the L2 regularization term(ùúÜ)(Œª), number of trees, maximum depth, and learning rate allowed examination of the algorithm‚Äôs balance between model expressiveness, regularization, and convergence speed. The chosen hyperparameter combinations were designed to cover a range of low-to-high capacity levels across algorithms, enabling a thorough comparison of bias-variance behavior, stability, and computational cost, all within a consistent cross-validation framework.Table 2.Machine Learning Models Hyperparameters Configurations.A Grid Search ApproachIn this study, hyperparameter tuning was performed by using Grid Search, a systematic and exhaustive optimization technique that evaluates all possible combinations within a predefined discrete hyperparameter space in order to identify the configuration yielding the optimal model performance. Formally, Grid Search seeks the combination‚Ñé‚àó=argmin‚Ñé‚àà‚Ñãùëì(‚Ñé)h*=argminh‚ààHf(h), where‚Ñã=‚Ñã1√ó‚Ñã2√ó‚ãØ√ó‚ÑãùëòH=H1√óH2√ó‚ãØ√óHkis the cartesian product of the hyperparameter space andùëì(‚Ñé)f(h)represents the objective function, here defined as the mean RMSE obtained through cross-validation. Although computationally intensive due to its combinatorial nature, this approach ensures a thorough exploration of the hyperparameter space, providing a deterministic and reproducible framework for model selection and establishing a robust baseline for performance optimization. Additionally, it‚Äôs worth noting that, for the purposes of this study, we predefined the hyperparameter values based on other relevant and valid studies in the field [11,27,30,31,36,47]. This decision was made to address the exponential nature of grid search concerning the number of hyperparameters and to prevent potential computational infeasibility of the machine learning models. 3.4. Metrics-Accuracy EvaluationTo ensure the effective deployment of electromagnetic field (EMF) prediction models and exposure maps, it is crucial to establish robust measures of predictive reliability. Among the various error metrics used to evaluate model performance, the Root Mean Squared Error (RMSE) is one of the most widely employed in regression analysis and machine learning tasks. RMSE quantifies the square root of the average squared differences between predicted and observed values, providing an interpretable and sensitive indicator of overall model accuracy. Formally, it is defined as:RMSE=1ùëõ‚àëùëñ=1ùëõ(ùë¶ùëñ‚àíùë¶ÃÇùëñ)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àíÓÑ¥‚é∑ÓÑ≥ÓÑ≥RMSE=1n‚àëi=1n(yi‚àíy^i)2whereùë¶ùëñyidenotes the true value,ùë¶ÃÇùëñy^ithe predicted value, andnthe total number of observations. The squaring of the errors emphasizes larger deviations more than smaller ones, making RMSE particularly sensitive to outliers. As such, it serves as a comprehensive measure of model performance by combining both the variance and bias of the estimator into a single value expressed in the same units as the dependent variable. A lower RMSE indicates better model performance, with predictions closely aligned to actual values. However, the sensitivity of RMSE to large errors implies that it may not always be the best choice when robustness to outliers is required, a feature which must be taken under consideration during the design of the model. 3.5. Explainable AI MethodsResults without explanations, without reasoning, is just mechanistical stripped computations. As machine learning models become increasingly complex, often involving sophisticated mathematical formulations, their interpretability correspondingly diminishes. Moreover, with the growing integration of artificial intelligence (AI) and machine learning (ML) across a wide range of critical domains, the demand for transparent and interpretable models has become paramount. In response to this challenge, various explainable AI (XAI) techniques have been developed, among which SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) are prominent, offering systematic approaches to elucidate model behavior and foster trust in model predictions.Broadly speaking, in mathematics and formal sciences, a model refers to a mathematical structure used to interpret the symbols and formulas of a formal language or to represent a class of phenomena. Specifically, in the context of machine learning (ML), a model is a parameterized mathematical function (or family of functions) trained to approximate a relationship between inputs and outputs. Formally, given:an input spaceùí≥X,an output spaceùí¥Yandtraining dataùíü={(ùë•ùëñ,ùë¶ùëñ)}ùëõùëñ=1‚äÜùí≥√óùí¥D={(xi,yi)}i=1n‚äÜX√óY.Then a modelùëìùúÉ:ùí≥‚Üíùí¥fŒ∏:X‚ÜíYis a function parameterized byùúÉ‚ààŒòŒ∏‚ààŒò(whereŒòŒòis a parameter space). The goal is to find parametersùúÉŒ∏such thatùëìùúÉfŒ∏‚Äúbest fits‚Äù the data according to some loss function‚ÑíL, usually by solving an optimization problem like:ùúÉ‚òÖ=argminùúÉ‚ààŒò1ùëõ‚àëùëñ=1ùëõ‚Ñí(ùëìùúÉ(ùë•ùëñ),ùë¶ùëñ)Œ∏‚òÖ=argminŒ∏‚ààŒò1n‚àëi=1nL(fŒ∏(xi),yi)Thus, the model is a hypothesis in a hypothesis space‚Ñã={ùëìùúÉ:ùúÉ‚ààŒò}H={fŒ∏:Œ∏‚ààŒò}.Consider, for example, a linearly separable Support Vector Machine (SVM) problem with inputùë•‚àà‚Ñùùëëx‚ààRdand parametersùúÉ‚àà‚ÑùùëëŒ∏‚ààRd,ùëè‚àà‚Ñùb‚ààR. The general model for the class of linear SVMs can be expressed as:ùëìùúÉ(ùë•)=sign(ùúÉ‚ä§ùë•+ùëè)fŒ∏(x)=sign(Œ∏‚ä§x+b)For each distinct set of parametersùúÉ‚àà‚ÑùùëëŒ∏‚ààRd, a different model is obtained, along with a corresponding partition of the data. This partition is determined by the hyperplane that separates the classes with the maximum margin. The objective is to identify the optimal hyperplane that maximizes the margin between the classes, typically by minimizing an error metric or a loss function designed to achieve this goal. So, by realizing the inherent enormous complexity that machine learning models can have, the need to develop models with sufficient explainability of the results (valuations of the models) is imperative.3.5.1. SHAPSHapley Additive exPlanations (SHAP) is a unified framework for interpreting machine learning model predictions based on cooperative game theory. It leverages the concept of Shapley Values to attribute the output of a model to its input features fairly and consistently. In this framework, each feature is treated as a ‚Äúplayer‚Äù in a coalition, and the model‚Äôs output is considered the ‚Äúpayout‚Äù that the players collectively generate. The Shapley value for a given feature represents its average marginal contribution to the prediction across all possible subsets of features.The practical implementation of SHAP enables both global and local interpretability. Locally, SHAP values explain individual predictions by quantifying the positive or negative contribution of each feature relative to a baseline expectation (typically the mean model output). Globally, aggregating SHAP values across a dataset highlights the overall importance and influence of features on model behavior. Our approach employs a specific type of explanation model that aggregates the contributions of additive feature attribution methods, where the explanation model is represented as a linear function mathematically expressed such as:ùëî(ùëß‚Ä≤)=ùúô0+‚àëùëñ=1ùëõùúôùëñùëß‚Ä≤ùëñg(z‚Ä≤)=œï0+‚àëi=1nœïizi‚Ä≤whereùëß‚Ä≤z‚Ä≤denotes binary variables,ndenotes the number of simplified input features, andùúôùëñœïidenotes the attribute effect. The SHAP valueùúôùëñœïifor a featureiis defined as:ùúôùëñ=‚àëùëÜ‚äÜùëÅ‚àñ{ùëñ}|ùëÜ|!(|ùëÅ|‚àí|ùëÜ|‚àí1)!|ùëÅ|![ùëì(ùëÜ‚à™{ùëñ})‚àíùëì(ùëÜ)]œïi=‚àëS‚äÜN‚àñ{i}|S|!(|N|‚àí|S|‚àí1)!|N|!f(S‚à™{i})‚àíf(S)whereNdenotes the set of all features,Sis a subset of features not containingi,ùëì(ùëÜ)f(S)is the model output when only features inSare present, andùëì(ùëÜ‚à™{ùëñ})f(S‚à™{i})is the output when featureiis added toS. The above expression captures the average marginal contribution of featureiover all possible subsetsS, ensuring fairness in attribution, by satisfying properties such as:Local Accuracy: When the transformation‚Ñéùë•(ùë•‚Ä≤)hx(x‚Ä≤)is identified withx, then the explanation modelùëî(ùë•‚Ä≤)g(x‚Ä≤)matches the original model, i.e.,ùëì(ùë•)=ùëî(ùë•‚Ä≤)=ùúô0+‚àëùëõùëñ=1ùúôùëñùëß‚Ä≤ùëñf(x)=g(x‚Ä≤)=œï0+‚àëi=1nœïizi‚Ä≤.Missingness: Simply, ifùë•‚Ä≤ùëñ=0xi‚Ä≤=0, thenùúôùëñ=0œïi=0. This means that this feature has no attributable impact whenùë•‚Ä≤ùëñ=0xi‚Ä≤=0. Missingness implies that a missing feature receives an attribution of zero.Consistency: The attribution values remain unchanged unless there is a variation in a feature‚Äôs contribution to the prediction. More critically, the consistency property ensures that if a feature‚Äôs influence on the model‚Äôs output increases, its corresponding Shapley value must either increase or remain constant.SHAP offers an additive explanation model in which the prediction is decomposed into a sum of contributions from each feature, thereby enhancing transparency and trust in complex predictive systems.However, the exact computation of SHAP values is computationally expensive, as it requires evaluating the model on all2ùëÄ2Msubsets of features, which becomes infeasible for largeM. To mitigate this challenge, SHAP leverages approximation methods such as Kernel SHAP, Tree SHAP, and Deep SHAP, each offering either a model-agnostic or model-specific approach, depending on the computational context. These methods reduce the computational complexity from exponential to polynomial time, making SHAP a practical and scalable tool for interpreting complex models in real-world applications.3.5.2. LIMELocal Interpretable Model-agnostic Explanations (LIME) is another widely adopted method within the domain of explainable artificial intelligence (XAI) that aims to provide interpretable explanations for individual predictions of complex machine learning models. It operates under the principle that complex models can be locally approximated by simpler, interpretable models. So, as a model-agnostic technique, LIME is designed to be applicable across a broad range of classifiers and regressors without requiring access to the internal structure of the model. Its primary objective is to generate locally faithful approximations of a model‚Äôs behavior in the vicinity of a specific instance by constructing an interpretable surrogate model, typically a sparse linear regression model.Specifically, given a black-box modelùëì:‚Ñùùëë‚Üí‚Ñùf:Rd‚ÜíRand an instanceùë•‚àà‚Ñùùëëx‚ààRd, LIME seeks to approximatefin the vicinity ofxusing an interpretable modelùëî‚ààùê∫g‚ààG, whereGis a class of simple models (e.g., linear models, decision trees). The objective is to minimize the following loss function:‚Ñí(ùëì,ùëî,ùúãùë•)+Œ©(ùëî)L(f,g,œÄx)+Œ©(g)where:‚Ñí(ùëì,ùëî,ùúãùë•)L(f,g,œÄx)measures the fidelity ofgin approximatingfin the locality defined byùúãùë•œÄx.ùúãùë•(ùëß)œÄx(z)is a proximity measure between the instancezand the instancex, often defined as an exponential kernel:ùúãùë•(ùëß)=exp(‚àíùê∑(ùë•,ùëß)2ùúé2)œÄx(z)=exp‚àíD(x,z)2œÉ2ùê∑(ùë•,ùëß)D(x,z)is a distance metric (e.g., Euclidean distance), andùúéœÉcontrols the width of the neighborhood.Œ©(ùëî)Œ©(g)is a complexity penalty to ensure interpretability, such as the number of non-zero coefficients in a linear model.The loss function‚ÑíLis typically the weighted squared loss:‚Ñí(ùëì,ùëî,ùúãùë•)=‚àëùëß‚ààùëçùúãùë•(ùëß)¬∑(ùëì(ùëß)‚àíùëî(ùëß))2L(f,g,œÄx)=‚àëz‚ààZœÄx(z)¬∑(f(z)‚àíg(z))2whereZis a set of perturbed samples aroundx.The overall methodology involves perturbing the input instance to create a set of synthetic samples and evaluating these samples using the original black-box model to obtain predicted outcomes. In particular, it incorporates the following algorithmic steps:Data Perturbation: Generate a datasetZof perturbed samples around the instancex.Prediction: Obtain the black-box model‚Äôs predictionsùëì(ùëß)f(z)for each perturbed sampleùëß‚ààùëçz‚ààZ.Weighting: Compute the proximityùúãùë•(ùëß)œÄx(z)for eachzto weigh the importance of each sample in the local approximation.Interpretable Model Training: Fit the interpretable modelgon the datasetZwith weightsùúãùë•(ùëß)œÄx(z), minimizing the loss‚Ñí(ùëì,ùëî,ùúãùë•)L(f,g,œÄx)while considering the complexity penaltyŒ©(ùëî)Œ©(g).Explanation: Present the interpretable modelgas the explanation for the predictionùëì(ùë•)f(x). For linear models, the coefficients indicate the contribution of each feature.LIME‚Äôs focus on local fidelity ensures that the explanations accurately reflect the model‚Äôs behavior in the immediate region surrounding the prediction, thereby enhancing transparency and user trust. Moreover, its model-agnostic nature and intuitive output make LIME a versatile and effective tool for interpreting the decisions of complex machine learning systems in a wide array of application domains.",
            "3.1. The Dataset": "The data set [58] used in this study originates from the official open data platform of the Spanish Government, which provides measurements of radio-frequency electromagnetic field (EMF) exposure collected through certified monitoring campaigns. During the period 2013‚Äì2015, within the framework of the Governan√ßa Radioel√®ctrica project [59], a comprehensive electromagnetic field (EMF) measurement program was carried out in Catalonia. For this purpose, a combined system of fixed and portable measuring instruments was used, allowing continuous monitoring and recording on-site in sensitive areas such as schools and hospitals (Figure 1). A total of 100 fixed monitoring stations were installed on roofs of buildings with a direct line-of-sight to the antennas. Ninety of them operated on photovoltaic power, while ten operated as electrically powered units. These stations measured either specific mobile telephony frequency bands (900, 1800, 2100 MHz) or the full spectrum from 100 kHz to 8 GHz. In parallel, 50 portable devices were distributed to municipalities and public bodies, enabling more than 19,322 measurements at 4598 locations. Figure 1.Spatial distribution of EMF (Electromagnetic Field) measurement points across Catalonia, Spain. Each record in the dataset corresponds to a specific measurement report, containing details such as location, date of assessment, measurement type and the recorded average field strength. The data set covers a wide geographic coverage in various municipalities and administrative regions, reflecting real-world environmental EMF conditions. Measurements are typically conducted using portable spectrum analyzers or fixed monitoring equipment, following standardized protocols to ensure reliability and comparability. The variables used in this study were derived from the Global Human Settlement Layer (GHSL) Data Package 2023 (GHS P2023) developed by the European Commission‚Äôs Joint Research Centre (JRC, 2024). These datasets provide globally consistent, open-access spatial information describing the built environment, population distribution, and settlement structure from 1975 to 2030. All datasets were resampled and aligned to a common grid of 100 m spatial resolution with geographic coordinates(ùë•,ùë¶)(x,y)corresponding to longitude and latitude of the grid cell centroid (seeTable 1). Table 1.Summary of GHSL datasets used in the study, including variable type, spatial resolution, temporal coverage, and coordinate reference. The GHS-BUILT-S R2023A variable represents the fraction of built-up area within each grid cell, derived from Sentinel-2 and Landsat imagery for epochs between 1975 and 2030 (5-year intervals). It provides sub-pixel built-up estimates at 10 m and aggregated 100 m resolution, distinguishing between residential (RES) and non-residential (NRES) domains. This layer serves as a measure of horizontal urban expansion and built-up density. The GHS-BUILT-H R2023A variable provides average building height (in metres) for each 100 m grid cell, estimated from the integration of AW3D30, SRTM30, and Sentinel-2 data (epoch 2018). It captures the vertical dimension of the built environment, enabling analyses of urban morphology and 3D structural variation. The GHS-BUILT-V R2023A variable combines the built-up surface and height layers to estimate built-up volume (m3/cell) for 1975‚Äì2030 at 100 m resolution. It expresses the volumetric density of constructed space, reflecting the total built infrastructure per unit area. This is a critical indicator of the intensity of urban development and infrastructure stock. The GHS-BUILT-C R2023A variable integrates built-up surface, height, and population layers to describe settlement characteristics and urban form. It includes metrics of built-up density, morphology, and functional classification at 10‚Äì100 m resolution for 2018. This variable enables the identification of heterogeneous settlement typologies and spatial organization patterns. The GHS_BUILT_C_MSZ layer classifies morphological settlement zones (MSZ) based on built-up density and spatial configuration, distinguishing continuous urban fabric, suburban, and rural morphologies. It provides categorical information useful for urban form classification and sprawl analysis. The GHS-POP R2023A variable provides multi-temporal population density grids (inhabitants per cell) for 1975‚Äì2030 in 5-year intervals at 100 m and 1 km resolutions. Population counts are derived by downscaling national and subnational census data using residential built-up volume (RES) as a weighting layer, aligned to the UN World Population Prospects (2022) and World Urbanization Prospects (2018). The GHS-SMOD R2023Avariable classifies the Earth‚Äôs surface into urban centres, dense urban clusters, and rural grid cells following the Degree of Urbanisation (DEGURBA) Stage I methodology. It integrates GHS-BUILT-S and GHS-POP data to describe the spatial hierarchy of settlement systems between 1975 and 2030.",
            "3.2. Framework for Machine Learning and Explainable AI": "This study follows a structured, reproducible workflow designed to ensure robust electromagnetic field (EMF) prediction and model interpretability, integrating geospatial preprocessing, systematic ML benchmarking, and dual XAI evaluation (Figure 2). The workflow begins with raw EMF measurements from the official Spanish governmental open-data platform, followed by comprehensive quality control including missing-value handling, physical-plausibility checks, distributional inspection, and robust outlier filtering techniques. All geospatial variables are formalized and enriched with refined urban-morphology attributes from the GHSL Data Package 2023, producing a unified spatial dataset suitable for ML experimentation. Figure 2.Machine Learning‚ÄîXAI pipeline. A randomized shuffling stage ensures independence in sampling order prior to model training. To mitigate spatial autocorrelation biases, we employ 50 stratified train‚Äìtest splits (80/20) with unique seeds, verifying no leakage and maintained target-variable representativeness across splits. This repeated partitioning strategy provides a low-variance generalization estimate and robust cross-validation. Six ML models were then benchmarked, Neural Networks, k-Nearest Neighbors, Decision Trees, Random Forests, LightGBM, and XGBoost, each trained over a structured hyperparameter grid and evaluated via mean RMSE, standard deviation, and 95% confidence intervals across all folds. Best configurations were subsequently retrained and independently assessed to confirm generalization. Model interpretability was conducted on the top-performing models using SHAP and LIME. SHAP provided global feature attribution and instance-level decomposition, while LIME offered localized surrogate explanations aggregated across samples. This dual-lens analysis enabled assessment of both global explanatory stability and local contextual behavior. Together, the pipeline workflow delivers a transparent, rigorously validated, and scientifically accountable framework for EMF estimation in urban environments, ensuring methodological robustness, reproducibility, and interpretability.",
            "3.3. Description of Machine Learning Methods": "Machine learning (ML), a core branch of artificial intelligence, facilitates data-driven modeling by identifying patterns and generating predictions with minimal human intervention. In this study, ML techniques are employed to accurately predict and map electromagnetic field (EMF) strength in urban environments, integrating spatial and structural features to enhance environmental monitoring. Before detailing the specific algorithms, architectures, and hyperparameters, it is important to highlight two fundamental hyperparameters common across most ML models: the learning rate(ùúÇ)(Œ∑)and the optimizer. The learning rate governs the magnitude of parameter updates during optimization, directly influencing model convergence and stability, while the optimizer determines the update strategy for minimizing the loss function. Proper selection or tuning of these parameters is essential to balance convergence speed, accuracy, and generalization across different ML methods. Neural Networks (NN) Neural Networks (NNs) are computational models inspired by biological neural systems, composed of interconnected neurons organized into input, hidden, and output layers. Each neuron processes weighted inputs and biases through nonlinear activation functions(ùúé)(œÉ)(e.g., ReLU, sigmoid, tanh), enabling the network to approximate complex nonlinear mappings. The network‚Äôs depth, determined by the hyperparameter that controls the number of hidden layers(ùêªùêø)(HL), which define the total depthùëì(ùë•;ùúÉ)=ùëì(ùêø)(ùëì(ùêø‚àí1)(‚Ä¶ùëì(1)(ùë•;ùëä(1),ùëè(1))‚Ä¶))f(x;Œ∏)=f(L)f(L‚àí1)(‚Ä¶f(1)(x;W(1),b(1))‚Ä¶)of the NN and directly influences its representational capacity but also affects overfitting risk. Training is performed via backpropagation using gradient descent-based optimization to minimize a loss function, such as Mean Squared Error for regression or Cross-Entropy for classification. In this study, a set of neural network models were employed, with hyperparameter tuning over the number of hidden layersùêªùêø‚àà{2,5,10,20,50}HL‚àà{2,5,10,20,50}, activation functionsùúé‚àà{identity,logistic,tanh,ReLU}œÉ‚àà{identity,logistic,tanh,ReLU}, and learning ratesùúÇ‚àà{0.01,0.1,0.2,0.5,1.0}Œ∑‚àà{0.01,0.1,0.2,0.5,1.0}, while the Adam optimizer was used and all the other hyperparameters kept at default settings. k-Nearest Neighbors (k-NN) The k-Nearest Neighbors (k-NN) algorithm is a non-parametric supervised learning method used for classification and regression, based on the principle that similar data points in the feature space exhibit similar outcomes. For a query pointx, the algorithm identifies theknearest samples and predicts the output as either the majority class (classification) or the mean/weighted mean of neighboring target values (regression). Model performance depends primarily on the choice ofk, the distance metric, and the weighting scheme. Distance can be computed using various norms‚ÄîEuclidean(ùêø2)(L2), Manhattan(ùêø1)(L1), or Minkowski‚Äîwhile weights may be uniform or distance-dependent, e.g., inverse distanceùë§ùëñ=ùëë(ùë•,ùë•ùëñ)‚àíùëùwi=d(x,xi)‚àípor Gaussianùë§ùëñ=ùëí‚àí(ùëë2ùëñ/2ùúé2)wi=e‚àí(di2/2œÉ2). In this study, three hyperparameters were optimized: the number of neighborsùëò‚àà{5,10,25,50,100,150}k‚àà{5,10,25,50,100,150}, distance metric parameterùëù‚àà{1,1.5,2,3}p‚àà{1,1.5,2,3}, and weighting scheme (uniform vs. distance-based). Decision Trees (DTs) Decision Trees (DTs) are nonparametric supervised learning models applicable to both classification and regression tasks. They recursively partition the feature space through hierarchical, feature-based splits to form a tree structure of decision and terminal nodes. Splits are chosen to maximize node purity using criteria such as Gini Impurityùê∫=1‚àí‚àëùëñùëù2ùëñG=1‚àí‚àëipi2or Entropyùêª=‚àí‚àëùëñùëùùëñlog2ùëùùëñH=‚àí‚àëipilog2pi, with information gain guiding the optimal partitioning. For regression, variance reduction serves as the splitting criterion. In this study, a minimal configuration was adopted to establish a baseline for comparison with ensemble tree models, varying only the hyperparameter controlling the minimum samples required to split a node within{2,5,10,20,50}{2,5,10,20,50}. Random Forest (RF) Random Forest (RF) is an ensemble learning algorithm that enhances predictive accuracy and generalization for both classification and regression by aggregating multiple Decision Trees trained on bootstrap samples (bagging) and random feature subsets. This dual randomness reduces inter-tree correlation and mitigates overfitting. The final prediction is obtained via majority voting (classification) or averaging (regression). Feature importance is derived from the mean impurity reduction across all trees, quantifying each variable‚Äôs contribution to model performance. In this study, the Random Forest was tuned by varying the number of treesùëÄ‚àà{50,100,200}M‚àà{50,100,200}, the splitting criterion{MSE,MAE,Friedman-MSE}{MSE,MAE,Friedman-MSE}, and fixing the maximum tree depth to 10. LightGBM Light Gradient Boosting Machine (LightGBM) is an efficient ensemble algorithm based on Gradient Boosting Decision Trees (GBDTs), optimized for speed and scalability. Unlike traditional level-wise boosting, LightGBM adopts a leaf-wise growth strategy, expanding the leaf with the greatest loss reduction to enhance accuracy while preserving efficiency through histogram-based feature binning. Each tree minimizes a differentiable loss function by fitting the residuals of prior predictions, updating the model asùë¶ÃÇ(ùë°)=ùë¶ÃÇ(ùë°‚àí1)+ùúÇùëìùë°(ùê±)y^(t)=y^(t‚àí1)+Œ∑ft(x)whereùëìùë°ftis the tree added at iterationtandùúÇŒ∑is the learning rate. In this study, four key hyperparameters were tuned: the number of leaves(ùêø‚àà{31,50,100})(L‚àà{31,50,100}), number of trees(ùëÄ‚àà{100,300,500})(M‚àà{100,300,500}), maximum depth(ùëöùëéùë•_ùëëùëíùëùùë°‚Ñé‚àà{5,10,20})(max_depth‚àà{5,10,20}), and learning rate(ùúÇ‚àà{0.01,0.05,0.1})(Œ∑‚àà{0.01,0.05,0.1}), balancing model complexity, convergence speed, and generalization performance. XGBoost (Extreme Gradient Boosting) Extreme Gradient Boosting (XGBoost) is an optimized implementation of the gradient boosting framework that constructs decision trees sequentially, with each tree correcting the residuals of its predecessors. It minimizes a regularized objective function,‚Ñí(ùë°)=‚àëùëñ=1ùëõùëô(ùë¶ùëñ,ùë¶ÃÇ(ùë°)ùëñ)+‚àëùë°=1ùëáŒ©(ùëìùë°)L(t)=‚àëi=1nl(yi,y^i(t))+‚àët=1TŒ©(ft)whereùëô(ùë¶ùëñ,ùë¶ÃÇ(ùë°)ùëñ)l(yi,y^i(t))denotes the loss andŒ©(ùëìùë°)=ùõæùëá+12ùúÜ‚àëùëáùëó=1ùë§2ùëóŒ©(ft)=Œ≥T+12Œª‚àëj=1Twj2penalizes model complexity, thereby enhancing generalization. Optimization is guided by first- and second-order gradients, enabling precise and efficient updates. XGBoost incorporates regularization, sparsity-aware learning, and parallelized computation, achieving both scalability and robustness. For this study, the tuned hyperparameters were the L2 regularization term(ùúÜ‚àà{1,5,10})(Œª‚àà{1,5,10}), the number of trees(ùëÄ‚àà{100,300,500})(M‚àà{100,300,500}), the maximum tree depth (fixed at 20), and the learning rate(ùúÇ‚àà{0.01,0.05,0.2})(Œ∑‚àà{0.01,0.05,0.2}), ensuring an optimal balance between bias, variance, and computational efficiency. TheTable 2above summarizes the hyperparameter configurations explored for each machine learning model. The selected ranges were designed to systematically probe the trade-off between model complexity, generalization capacity, and computational efficiency, ensuring a balanced and reproducible experimental framework. For Neural Networks (NN), variations in the number of hidden layers, activation functions, and learning rates capture the effect of network depth and nonlinear activation on convergence and overfitting control. In k-Nearest Neighbors (k-NN), tuning the number of neighbors, distance metric, and weighting scheme examines the locality dependencies of the feature space and the influence of metric geometry on model‚Äôs performance. For Decision Trees (DTs), the minimum samples per split parameter isolates the relationship between node purity and model variance, while keeping DTs as a simple, interpretable baseline for ensemble comparison. Random Forest (RF) tuning included ensemble size and splitting criterion, maintaining fixed depth to evaluate ensemble effects independently of tree complexity. In LightGBM, the number of leaves, boosting iterations, maximum depth, and learning rate were adjusted to control tree growth, regularization strength, and learning dynamics. Similarly, for XGBoost, tuning the L2 regularization term(ùúÜ)(Œª), number of trees, maximum depth, and learning rate allowed examination of the algorithm‚Äôs balance between model expressiveness, regularization, and convergence speed. The chosen hyperparameter combinations were designed to cover a range of low-to-high capacity levels across algorithms, enabling a thorough comparison of bias-variance behavior, stability, and computational cost, all within a consistent cross-validation framework. Table 2.Machine Learning Models Hyperparameters Configurations. A Grid Search Approach In this study, hyperparameter tuning was performed by using Grid Search, a systematic and exhaustive optimization technique that evaluates all possible combinations within a predefined discrete hyperparameter space in order to identify the configuration yielding the optimal model performance. Formally, Grid Search seeks the combination‚Ñé‚àó=argmin‚Ñé‚àà‚Ñãùëì(‚Ñé)h*=argminh‚ààHf(h), where‚Ñã=‚Ñã1√ó‚Ñã2√ó‚ãØ√ó‚ÑãùëòH=H1√óH2√ó‚ãØ√óHkis the cartesian product of the hyperparameter space andùëì(‚Ñé)f(h)represents the objective function, here defined as the mean RMSE obtained through cross-validation. Although computationally intensive due to its combinatorial nature, this approach ensures a thorough exploration of the hyperparameter space, providing a deterministic and reproducible framework for model selection and establishing a robust baseline for performance optimization. Additionally, it‚Äôs worth noting that, for the purposes of this study, we predefined the hyperparameter values based on other relevant and valid studies in the field [11,27,30,31,36,47]. This decision was made to address the exponential nature of grid search concerning the number of hyperparameters and to prevent potential computational infeasibility of the machine learning models.",
            "3.4. Metrics-Accuracy Evaluation": "To ensure the effective deployment of electromagnetic field (EMF) prediction models and exposure maps, it is crucial to establish robust measures of predictive reliability. Among the various error metrics used to evaluate model performance, the Root Mean Squared Error (RMSE) is one of the most widely employed in regression analysis and machine learning tasks. RMSE quantifies the square root of the average squared differences between predicted and observed values, providing an interpretable and sensitive indicator of overall model accuracy. Formally, it is defined as:RMSE=1ùëõ‚àëùëñ=1ùëõ(ùë¶ùëñ‚àíùë¶ÃÇùëñ)2‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àíÓÑ¥‚é∑ÓÑ≥ÓÑ≥RMSE=1n‚àëi=1n(yi‚àíy^i)2whereùë¶ùëñyidenotes the true value,ùë¶ÃÇùëñy^ithe predicted value, andnthe total number of observations. The squaring of the errors emphasizes larger deviations more than smaller ones, making RMSE particularly sensitive to outliers. As such, it serves as a comprehensive measure of model performance by combining both the variance and bias of the estimator into a single value expressed in the same units as the dependent variable. A lower RMSE indicates better model performance, with predictions closely aligned to actual values. However, the sensitivity of RMSE to large errors implies that it may not always be the best choice when robustness to outliers is required, a feature which must be taken under consideration during the design of the model.",
            "3.5. Explainable AI Methods": "Results without explanations, without reasoning, is just mechanistical stripped computations. As machine learning models become increasingly complex, often involving sophisticated mathematical formulations, their interpretability correspondingly diminishes. Moreover, with the growing integration of artificial intelligence (AI) and machine learning (ML) across a wide range of critical domains, the demand for transparent and interpretable models has become paramount. In response to this challenge, various explainable AI (XAI) techniques have been developed, among which SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) are prominent, offering systematic approaches to elucidate model behavior and foster trust in model predictions. Broadly speaking, in mathematics and formal sciences, a model refers to a mathematical structure used to interpret the symbols and formulas of a formal language or to represent a class of phenomena. Specifically, in the context of machine learning (ML), a model is a parameterized mathematical function (or family of functions) trained to approximate a relationship between inputs and outputs. Formally, given: an input spaceùí≥X,an output spaceùí¥Yandtraining dataùíü={(ùë•ùëñ,ùë¶ùëñ)}ùëõùëñ=1‚äÜùí≥√óùí¥D={(xi,yi)}i=1n‚äÜX√óY. Then a modelùëìùúÉ:ùí≥‚Üíùí¥fŒ∏:X‚ÜíYis a function parameterized byùúÉ‚ààŒòŒ∏‚ààŒò(whereŒòŒòis a parameter space). The goal is to find parametersùúÉŒ∏such thatùëìùúÉfŒ∏‚Äúbest fits‚Äù the data according to some loss function‚ÑíL, usually by solving an optimization problem like:ùúÉ‚òÖ=argminùúÉ‚ààŒò1ùëõ‚àëùëñ=1ùëõ‚Ñí(ùëìùúÉ(ùë•ùëñ),ùë¶ùëñ)Œ∏‚òÖ=argminŒ∏‚ààŒò1n‚àëi=1nL(fŒ∏(xi),yi) Thus, the model is a hypothesis in a hypothesis space‚Ñã={ùëìùúÉ:ùúÉ‚ààŒò}H={fŒ∏:Œ∏‚ààŒò}. Consider, for example, a linearly separable Support Vector Machine (SVM) problem with inputùë•‚àà‚Ñùùëëx‚ààRdand parametersùúÉ‚àà‚ÑùùëëŒ∏‚ààRd,ùëè‚àà‚Ñùb‚ààR. The general model for the class of linear SVMs can be expressed as:ùëìùúÉ(ùë•)=sign(ùúÉ‚ä§ùë•+ùëè)fŒ∏(x)=sign(Œ∏‚ä§x+b) For each distinct set of parametersùúÉ‚àà‚ÑùùëëŒ∏‚ààRd, a different model is obtained, along with a corresponding partition of the data. This partition is determined by the hyperplane that separates the classes with the maximum margin. The objective is to identify the optimal hyperplane that maximizes the margin between the classes, typically by minimizing an error metric or a loss function designed to achieve this goal. So, by realizing the inherent enormous complexity that machine learning models can have, the need to develop models with sufficient explainability of the results (valuations of the models) is imperative. 3.5.1. SHAPSHapley Additive exPlanations (SHAP) is a unified framework for interpreting machine learning model predictions based on cooperative game theory. It leverages the concept of Shapley Values to attribute the output of a model to its input features fairly and consistently. In this framework, each feature is treated as a ‚Äúplayer‚Äù in a coalition, and the model‚Äôs output is considered the ‚Äúpayout‚Äù that the players collectively generate. The Shapley value for a given feature represents its average marginal contribution to the prediction across all possible subsets of features.The practical implementation of SHAP enables both global and local interpretability. Locally, SHAP values explain individual predictions by quantifying the positive or negative contribution of each feature relative to a baseline expectation (typically the mean model output). Globally, aggregating SHAP values across a dataset highlights the overall importance and influence of features on model behavior. Our approach employs a specific type of explanation model that aggregates the contributions of additive feature attribution methods, where the explanation model is represented as a linear function mathematically expressed such as:ùëî(ùëß‚Ä≤)=ùúô0+‚àëùëñ=1ùëõùúôùëñùëß‚Ä≤ùëñg(z‚Ä≤)=œï0+‚àëi=1nœïizi‚Ä≤whereùëß‚Ä≤z‚Ä≤denotes binary variables,ndenotes the number of simplified input features, andùúôùëñœïidenotes the attribute effect. The SHAP valueùúôùëñœïifor a featureiis defined as:ùúôùëñ=‚àëùëÜ‚äÜùëÅ‚àñ{ùëñ}|ùëÜ|!(|ùëÅ|‚àí|ùëÜ|‚àí1)!|ùëÅ|![ùëì(ùëÜ‚à™{ùëñ})‚àíùëì(ùëÜ)]œïi=‚àëS‚äÜN‚àñ{i}|S|!(|N|‚àí|S|‚àí1)!|N|!f(S‚à™{i})‚àíf(S)whereNdenotes the set of all features,Sis a subset of features not containingi,ùëì(ùëÜ)f(S)is the model output when only features inSare present, andùëì(ùëÜ‚à™{ùëñ})f(S‚à™{i})is the output when featureiis added toS. The above expression captures the average marginal contribution of featureiover all possible subsetsS, ensuring fairness in attribution, by satisfying properties such as:Local Accuracy: When the transformation‚Ñéùë•(ùë•‚Ä≤)hx(x‚Ä≤)is identified withx, then the explanation modelùëî(ùë•‚Ä≤)g(x‚Ä≤)matches the original model, i.e.,ùëì(ùë•)=ùëî(ùë•‚Ä≤)=ùúô0+‚àëùëõùëñ=1ùúôùëñùëß‚Ä≤ùëñf(x)=g(x‚Ä≤)=œï0+‚àëi=1nœïizi‚Ä≤.Missingness: Simply, ifùë•‚Ä≤ùëñ=0xi‚Ä≤=0, thenùúôùëñ=0œïi=0. This means that this feature has no attributable impact whenùë•‚Ä≤ùëñ=0xi‚Ä≤=0. Missingness implies that a missing feature receives an attribution of zero.Consistency: The attribution values remain unchanged unless there is a variation in a feature‚Äôs contribution to the prediction. More critically, the consistency property ensures that if a feature‚Äôs influence on the model‚Äôs output increases, its corresponding Shapley value must either increase or remain constant.SHAP offers an additive explanation model in which the prediction is decomposed into a sum of contributions from each feature, thereby enhancing transparency and trust in complex predictive systems.However, the exact computation of SHAP values is computationally expensive, as it requires evaluating the model on all2ùëÄ2Msubsets of features, which becomes infeasible for largeM. To mitigate this challenge, SHAP leverages approximation methods such as Kernel SHAP, Tree SHAP, and Deep SHAP, each offering either a model-agnostic or model-specific approach, depending on the computational context. These methods reduce the computational complexity from exponential to polynomial time, making SHAP a practical and scalable tool for interpreting complex models in real-world applications. 3.5.2. LIMELocal Interpretable Model-agnostic Explanations (LIME) is another widely adopted method within the domain of explainable artificial intelligence (XAI) that aims to provide interpretable explanations for individual predictions of complex machine learning models. It operates under the principle that complex models can be locally approximated by simpler, interpretable models. So, as a model-agnostic technique, LIME is designed to be applicable across a broad range of classifiers and regressors without requiring access to the internal structure of the model. Its primary objective is to generate locally faithful approximations of a model‚Äôs behavior in the vicinity of a specific instance by constructing an interpretable surrogate model, typically a sparse linear regression model.Specifically, given a black-box modelùëì:‚Ñùùëë‚Üí‚Ñùf:Rd‚ÜíRand an instanceùë•‚àà‚Ñùùëëx‚ààRd, LIME seeks to approximatefin the vicinity ofxusing an interpretable modelùëî‚ààùê∫g‚ààG, whereGis a class of simple models (e.g., linear models, decision trees). The objective is to minimize the following loss function:‚Ñí(ùëì,ùëî,ùúãùë•)+Œ©(ùëî)L(f,g,œÄx)+Œ©(g)where:‚Ñí(ùëì,ùëî,ùúãùë•)L(f,g,œÄx)measures the fidelity ofgin approximatingfin the locality defined byùúãùë•œÄx.ùúãùë•(ùëß)œÄx(z)is a proximity measure between the instancezand the instancex, often defined as an exponential kernel:ùúãùë•(ùëß)=exp(‚àíùê∑(ùë•,ùëß)2ùúé2)œÄx(z)=exp‚àíD(x,z)2œÉ2ùê∑(ùë•,ùëß)D(x,z)is a distance metric (e.g., Euclidean distance), andùúéœÉcontrols the width of the neighborhood.Œ©(ùëî)Œ©(g)is a complexity penalty to ensure interpretability, such as the number of non-zero coefficients in a linear model.The loss function‚ÑíLis typically the weighted squared loss:‚Ñí(ùëì,ùëî,ùúãùë•)=‚àëùëß‚ààùëçùúãùë•(ùëß)¬∑(ùëì(ùëß)‚àíùëî(ùëß))2L(f,g,œÄx)=‚àëz‚ààZœÄx(z)¬∑(f(z)‚àíg(z))2whereZis a set of perturbed samples aroundx.The overall methodology involves perturbing the input instance to create a set of synthetic samples and evaluating these samples using the original black-box model to obtain predicted outcomes. In particular, it incorporates the following algorithmic steps:Data Perturbation: Generate a datasetZof perturbed samples around the instancex.Prediction: Obtain the black-box model‚Äôs predictionsùëì(ùëß)f(z)for each perturbed sampleùëß‚ààùëçz‚ààZ.Weighting: Compute the proximityùúãùë•(ùëß)œÄx(z)for eachzto weigh the importance of each sample in the local approximation.Interpretable Model Training: Fit the interpretable modelgon the datasetZwith weightsùúãùë•(ùëß)œÄx(z), minimizing the loss‚Ñí(ùëì,ùëî,ùúãùë•)L(f,g,œÄx)while considering the complexity penaltyŒ©(ùëî)Œ©(g).Explanation: Present the interpretable modelgas the explanation for the predictionùëì(ùë•)f(x). For linear models, the coefficients indicate the contribution of each feature.LIME‚Äôs focus on local fidelity ensures that the explanations accurately reflect the model‚Äôs behavior in the immediate region surrounding the prediction, thereby enhancing transparency and user trust. Moreover, its model-agnostic nature and intuitive output make LIME a versatile and effective tool for interpreting the decisions of complex machine learning systems in a wide array of application domains.",
            "3.5.1. SHAP": "SHapley Additive exPlanations (SHAP) is a unified framework for interpreting machine learning model predictions based on cooperative game theory. It leverages the concept of Shapley Values to attribute the output of a model to its input features fairly and consistently. In this framework, each feature is treated as a ‚Äúplayer‚Äù in a coalition, and the model‚Äôs output is considered the ‚Äúpayout‚Äù that the players collectively generate. The Shapley value for a given feature represents its average marginal contribution to the prediction across all possible subsets of features. The practical implementation of SHAP enables both global and local interpretability. Locally, SHAP values explain individual predictions by quantifying the positive or negative contribution of each feature relative to a baseline expectation (typically the mean model output). Globally, aggregating SHAP values across a dataset highlights the overall importance and influence of features on model behavior. Our approach employs a specific type of explanation model that aggregates the contributions of additive feature attribution methods, where the explanation model is represented as a linear function mathematically expressed such as:ùëî(ùëß‚Ä≤)=ùúô0+‚àëùëñ=1ùëõùúôùëñùëß‚Ä≤ùëñg(z‚Ä≤)=œï0+‚àëi=1nœïizi‚Ä≤whereùëß‚Ä≤z‚Ä≤denotes binary variables,ndenotes the number of simplified input features, andùúôùëñœïidenotes the attribute effect. The SHAP valueùúôùëñœïifor a featureiis defined as:ùúôùëñ=‚àëùëÜ‚äÜùëÅ‚àñ{ùëñ}|ùëÜ|!(|ùëÅ|‚àí|ùëÜ|‚àí1)!|ùëÅ|![ùëì(ùëÜ‚à™{ùëñ})‚àíùëì(ùëÜ)]œïi=‚àëS‚äÜN‚àñ{i}|S|!(|N|‚àí|S|‚àí1)!|N|!f(S‚à™{i})‚àíf(S)whereNdenotes the set of all features,Sis a subset of features not containingi,ùëì(ùëÜ)f(S)is the model output when only features inSare present, andùëì(ùëÜ‚à™{ùëñ})f(S‚à™{i})is the output when featureiis added toS. The above expression captures the average marginal contribution of featureiover all possible subsetsS, ensuring fairness in attribution, by satisfying properties such as: Local Accuracy: When the transformation‚Ñéùë•(ùë•‚Ä≤)hx(x‚Ä≤)is identified withx, then the explanation modelùëî(ùë•‚Ä≤)g(x‚Ä≤)matches the original model, i.e.,ùëì(ùë•)=ùëî(ùë•‚Ä≤)=ùúô0+‚àëùëõùëñ=1ùúôùëñùëß‚Ä≤ùëñf(x)=g(x‚Ä≤)=œï0+‚àëi=1nœïizi‚Ä≤.Missingness: Simply, ifùë•‚Ä≤ùëñ=0xi‚Ä≤=0, thenùúôùëñ=0œïi=0. This means that this feature has no attributable impact whenùë•‚Ä≤ùëñ=0xi‚Ä≤=0. Missingness implies that a missing feature receives an attribution of zero.Consistency: The attribution values remain unchanged unless there is a variation in a feature‚Äôs contribution to the prediction. More critically, the consistency property ensures that if a feature‚Äôs influence on the model‚Äôs output increases, its corresponding Shapley value must either increase or remain constant. SHAP offers an additive explanation model in which the prediction is decomposed into a sum of contributions from each feature, thereby enhancing transparency and trust in complex predictive systems. However, the exact computation of SHAP values is computationally expensive, as it requires evaluating the model on all2ùëÄ2Msubsets of features, which becomes infeasible for largeM. To mitigate this challenge, SHAP leverages approximation methods such as Kernel SHAP, Tree SHAP, and Deep SHAP, each offering either a model-agnostic or model-specific approach, depending on the computational context. These methods reduce the computational complexity from exponential to polynomial time, making SHAP a practical and scalable tool for interpreting complex models in real-world applications.",
            "3.5.2. LIME": "Local Interpretable Model-agnostic Explanations (LIME) is another widely adopted method within the domain of explainable artificial intelligence (XAI) that aims to provide interpretable explanations for individual predictions of complex machine learning models. It operates under the principle that complex models can be locally approximated by simpler, interpretable models. So, as a model-agnostic technique, LIME is designed to be applicable across a broad range of classifiers and regressors without requiring access to the internal structure of the model. Its primary objective is to generate locally faithful approximations of a model‚Äôs behavior in the vicinity of a specific instance by constructing an interpretable surrogate model, typically a sparse linear regression model. Specifically, given a black-box modelùëì:‚Ñùùëë‚Üí‚Ñùf:Rd‚ÜíRand an instanceùë•‚àà‚Ñùùëëx‚ààRd, LIME seeks to approximatefin the vicinity ofxusing an interpretable modelùëî‚ààùê∫g‚ààG, whereGis a class of simple models (e.g., linear models, decision trees). The objective is to minimize the following loss function:‚Ñí(ùëì,ùëî,ùúãùë•)+Œ©(ùëî)L(f,g,œÄx)+Œ©(g)where: ‚Ñí(ùëì,ùëî,ùúãùë•)L(f,g,œÄx)measures the fidelity ofgin approximatingfin the locality defined byùúãùë•œÄx.ùúãùë•(ùëß)œÄx(z)is a proximity measure between the instancezand the instancex, often defined as an exponential kernel:ùúãùë•(ùëß)=exp(‚àíùê∑(ùë•,ùëß)2ùúé2)œÄx(z)=exp‚àíD(x,z)2œÉ2ùê∑(ùë•,ùëß)D(x,z)is a distance metric (e.g., Euclidean distance), andùúéœÉcontrols the width of the neighborhood.Œ©(ùëî)Œ©(g)is a complexity penalty to ensure interpretability, such as the number of non-zero coefficients in a linear model.The loss function‚ÑíLis typically the weighted squared loss:‚Ñí(ùëì,ùëî,ùúãùë•)=‚àëùëß‚ààùëçùúãùë•(ùëß)¬∑(ùëì(ùëß)‚àíùëî(ùëß))2L(f,g,œÄx)=‚àëz‚ààZœÄx(z)¬∑(f(z)‚àíg(z))2whereZis a set of perturbed samples aroundx. The overall methodology involves perturbing the input instance to create a set of synthetic samples and evaluating these samples using the original black-box model to obtain predicted outcomes. In particular, it incorporates the following algorithmic steps: Data Perturbation: Generate a datasetZof perturbed samples around the instancex.Prediction: Obtain the black-box model‚Äôs predictionsùëì(ùëß)f(z)for each perturbed sampleùëß‚ààùëçz‚ààZ.Weighting: Compute the proximityùúãùë•(ùëß)œÄx(z)for eachzto weigh the importance of each sample in the local approximation.Interpretable Model Training: Fit the interpretable modelgon the datasetZwith weightsùúãùë•(ùëß)œÄx(z), minimizing the loss‚Ñí(ùëì,ùëî,ùúãùë•)L(f,g,œÄx)while considering the complexity penaltyŒ©(ùëî)Œ©(g).Explanation: Present the interpretable modelgas the explanation for the predictionùëì(ùë•)f(x). For linear models, the coefficients indicate the contribution of each feature. LIME‚Äôs focus on local fidelity ensures that the explanations accurately reflect the model‚Äôs behavior in the immediate region surrounding the prediction, thereby enhancing transparency and user trust. Moreover, its model-agnostic nature and intuitive output make LIME a versatile and effective tool for interpreting the decisions of complex machine learning systems in a wide array of application domains.",
            "4. Results": "4.1. Machine Learning Model Selection ResultsThis section presents a comprehensive analysis of the results obtained from the systematic application of the machine learning methods described previously. For each method, hyperparameter optimization yielded a distinct set of models, enabling a thorough evaluation of their performance. We provide a detailed discussion of the optimal results for each algorithm individually, followed by a comparative analysis to assess their relative performance and effectiveness.To provide additional practical value, we report the computational burden associated with the hyperparameter grid and model training process. Across all experiments, the relative runtime order was as follows: kNN was the fastest method, followed by Decision Trees, which also completed training quickly. XGBoost and LightGBM required moderate computation time, while Neural Networks exhibited moderate to high runtime depending on the number of layers and complexity of the architecture. Random Forest showed the highest overall computational cost due to the large number of trees and repeated cross-validation. These observations help practitioners understand the trade-off between predictive accuracy and computational resources when selecting models for large-scale EMF prediction tasks.Neural Networks (NN)We implemented a large-scale evaluation of neural network architectures, systematically varying hyperparameters related to topology (number of layersL), activation functions (ùúéœÉ), and optimization settings (learning rateùúÇŒ∑, optimizer choice). The best-performing configurations achieved mean RMSE values between 0.909277 and 0.911257, with differences lying within the error margin, indicating comparable accuracy and robustness across cross-validation folds. The numerical optimal model consisted of two hidden layers with ReLU activation, Adam optimizer, andùúÇ=1.0Œ∑=1.0, yielding a mean RMSE of 0.909277 (ùëÜùê∑=0.070638SD=0.070638). Although ReLU combined with high learning rates produced competitive results, these models exhibited lower reproducibility.In contrast, networks with logistic or tanh activation functions, compact architectures (2‚Äì20 hidden units), and moderate learning rates (ùúÇ=0.01Œ∑=0.01, Adam) consistently achieved near-optimal accuracy while maintaining greater stability. These models dominated the top-ranked results, underscoring their reliability and efficiency in capturing nonlinear patterns in electric field strength prediction, as we can see in the summarized rankingTable 3. Conversely, identity activations and excessively large networks often diverged, producing inflated errors and instability, highlighting their unsuitability. Overall, compact architectures with logistic or tanh activations and moderate learning rates represent the most robust and practical solutions for deployment, while further statistical paired tests on fold-level predictions are required to claim that any of the best configurations is better than another.Table 3.Top 10 Neural Network Hyperparameter Combinations Ranked by Mean RMSE.k-Nearest Neighbors (kNN)The evaluation of the k-Nearest Neighbors (kNN) models revealed a consistent trend of better results by the use of distance-based weighting schemes (inverse-distance) over uniform weighting (equal weights), regardless of the number of neighbors or Minkowski distance parameterp. Specifically, the five best-performing configurations, as measured by mean RMSE, were very tightly grouped in the interval [0.623153, 0.627706], with the best of them(ùëù=1.5,ùëò=25,weights=distance)(p=1.5,k=25,weights=distance)achieving RMSE 0.623153 (SD 0.042993), closely followed by the second best(ùëù=1.0,ùëò=25,distance)(p=1.0,k=25,distance)with RMSE 0.623335 (SD 0.043059), and the rest of the top 5 as clearly depicted in summarizedTable 4, accompanied with their corresponding CIs.Table 4.Top 5 kNN hyperparameter configurations ranked by mean RMSE.The above results indicate that medium neighborhood sizes (ùëò=10k=10‚Äì25) combined with inverse distance weighting achieved an optimal balance between local sensitivity and generalization and thus yielded the lowest prediction errors. In contrast, uniform weighting schemes systematically underperformed, with RMSE values increasing substantially askgrew larger, underscoring the importance of accounting for proximity when aggregating neighbor contributions, especially for spatial applications like EMF predictions, because measurements are spatially autocorrelated and nearer sensors are more informative. Summing up, distance-weighted kNN models with moderate neighborhood sizes (ùëò=10k=10‚Äì25) yielded the most robust performance, with only minor RMSE variation across top settings. Moreover, the slightly best results atùëù=1.5p=1.5may indicate that the feature space geometry could be best captured by a non-integer continuous Minkowskipnorm.Decision Trees (DT)The evaluation of Decision Tree models demonstrated that adjustments to theùëöùëñùëõ_ùë†ùëéùëöùëùùëôùëíùë†_ùë†ùëùùëôùëñùë°min_samples_splithyperparameter, which controls the minimum number (or fraction) of samples required to split an internal node, produced a systematic bias‚Äìvariance trade-off consistent with theoretical expectations. Specifically, very small thresholds (e.g.,ùëöùëñùëõ_ùë†ùëéùëöùëùùëôùëíùë†_ùë†ùëùùëôùëñùë°=2min_samples_split=2) yielded the weakest performance (mean RMSE = 0.869772, SD = 0.075755), indicating overfitting and unstable predictions across folds. Increasing the threshold clearly improved generalization, with the best performance observed atùëöùëñùëõ_ùë†ùëéùëöùëùùëôùëíùë†_ùë†ùëùùëôùëñùë°=20min_samples_split=20(mean RMSE = 0.812665, SD = 0.060873). However, further increasing the threshold (e.g.,ùëöùëñùëõ_ùë†ùëéùëöùëùùëôùëíùë†_ùë†ùëùùëôùëñùë°=50min_samples_split=50) slightly degraded the performance (mean RMSE = 0.817838, SD = 0.061485), suggesting underfitting as the model became overly constrained.The above results indicate that moderate complexity constraints, specifically a splitting threshold around 20, yielded the best performing configuration and provide the most favorable balance between flexibility and stability, clearly indicated in the summarizedTable 5, by the lower mean RMSE score. Overall, this configuration achieved to reduce the variance without sacrificing his predictive accuracy, yielding the most robust results between DTs models for EMF strength prediction in urban environments.Table 5.Ranking of Decision Tree hyperparameter configurations based on RMSE performance.Random Forest (RF)The evaluation of Random Forest models indicates that the squared-error and Friedman MSE criteria consistently yielded superior performance across all ensemble sizes. The best configuration with 200 estimators, squared-error splitting and maximum depth of 10, achieved the lowest mean RMSE of 0.757916 (SD = 0.066344). In contrast, the absolute-error criterion showed considerably weaker results (mean RMSE ‚âà 0.817), underscoring its unsuitability for accurate EMF estimation. Moreover, by increasing the number of estimators from 50 to 200 we obtained marginal improvements, with refinements of less than 0.002 in mean RMSE, improvements that, although consistent, were small relative to cross-validation variability, as we can see in the summarized top 5 rankingTable 6.Table 6.Top 5 ranked Random Forests hyperparameter configurations based on RMSE performance.Overall, performance differences among the top Random Forest settings (squared-error and Friedman MSE with 100‚Äì200 trees) were minor and largely within overlapping confidence intervals, suggesting further statistical testing (e.g., pairedt-tests) to confirm notable statistical significance. From a computational perspective, ensembles of ‚àº100 trees appear to provide an optimal trade-off between predictive accuracy and efficiency, while 200 trees offer slightly greater stability at higher computational cost.LightGBMThe evaluation of LightGBM models across a broad hyperparameter space revealed a consistent pattern in predictive accuracy, with the lowest errors obtained from larger tree structures combined with sufficient gradient boosting iterations. The best performing configuration with Num_Leaves = 100, N_Estimators = 500, Learning_Rate = 0.1, and Max_Depth = 20 achieved a mean RMSE of 0.716689 (SD = 0.053689), indicating both high accuracy and stable generalization across all folds. Comparable performance was observed for slightly less complex settings, such as N_Estimators = 300 or Learning_Rate = 0.05, which yielded mean RMSE values in the range 0.720822‚Äì0.722759, as summarized in theTable 7.Table 7.Top 10 ranked LightGBM hyperparameter configurations based on Mean RMSE.The above results confirm that LightGBM benefits from deeper and broader trees when combined with sufficient gradient boosting rounds, particularly under moderate learning rates that balance convergence speed and generalization. However, deeper architectures and larger ensembles increase computational demands and may increase overfitting risks if the available data are limited or noisy. Overall, while the results show that more complex configurations yield superior accuracy in the estimation of EMF, the marginal gains diminish beyond a certain complexity, e.g., differences between 300 and 500 estimators are modest, a fact that must taken under consideration in applications where the computational resources are limited. So, from a practical standpoint, configurations with 100 leaves, 300 estimators, and depth 20 may offer a near-optimal trade-off between accuracy and efficiency.XGBoostThe evaluation of XGBoost models across varied hyperparameter configurations revealed that the strongest predictive performance was consistently obtained with deep trees (Max_Depth = 20) combined with stronger L2 regularization parameters (i.e., Lambda_optionùúÜŒª= 10). Specifically, the best configuration, withùúÜŒª= 10, N_Estimators = 100 and Learning_RateùúÇŒ∑= 0.05, achieved the lowest mean RMSE = 0.711201 (SD = 0.059282). Comparable results were observed with lower learning rates (ùúÇŒ∑= 0.01) but considerably larger ensembles (300‚Äì500 estimators), as summarized inTable 8.Table 8.Top 10 XGBoost hyperparameter configurations ranked by Mean RMSE (lower is better).The above results highlight two effective strategic approaches that can be followed: (i) moderate learning rates (ùúÇŒ∑= 0.05) with fewer boosting rounds (N_Estimators ‚âà 100), which reduce computational cost while maintaining accuracy, and (ii) smaller learning rates (ùúÇŒ∑= 0.01) with considerably larger ensembles (300‚Äì500 estimators), which yield similar accuracy but at higher computational expense. The consistent presence of strong L2 regularization (ùúÜŒª= 5‚Äì10) among the top-performing models, suggests that high-capacity XGBoost trees tend to overfit in that specific settings unless they are adequately penalized and thus underscoring its critical role of regularization in mitigating overfitting in high-capacity deep trees.Overall, the best-performing configurations balance well the ability to capture complex spatial‚Äìenvironmental dependencies in the EMF data combined with stable generalization across folds. However, the narrow differences among the top results, coupled with overlapping confidence intervals, suggest that observed improvements are incremental and may depend on the specific dataset partitioning. From a practical standpoint, the configuration withùúÜŒª= 10, N_Estimators = 100 andùúÇŒ∑= 0.05 offers an effective trade-off between accuracy and efficiency, while low‚Äìlearning rate settings may be advantageous in occasions when computational resources are plentiful and marginal gains in stability are important.Comparative Analysis Machine Learning ModelsThe comparative analysis of the six machine learning models Neural Networks (NN), k-Nearest Neighbors (kNN), Decision Trees (DTs), Random Forest (RF), LightGBM and XGBoost demonstrates notable differences in their predictive accuracy, variability, and robustness. Specifically, models‚Äô evaluation (The models were ranked according to their mean RMSE values, reported with six-decimal precision.) was conducted using the root mean squared error (RMSE) as the primary performance metric, complemented by variability measures including the standard deviation of RMSE (SD RMSE) and 95% confidence intervals (CI).The best-performing model was kNN, achieving a mean RMSE of 0.623153 (SD = 0.042993; 95% CI: [0.611236, 0.635070]). This result indicates a strong predictive accuracy and comparatively low uncertainty, highlighting kNN‚Äôs capacity to capture local structures within the dataset. The second-best performer was XGBoost, with a mean RMSE of 0.711201 (SD = 0.059282; 95% CI: [0.694769, 0.727633]), followed closely by LightGBM at 0.716689 (SD = 0.053689; 95% CI: [0.701807, 0.731571]). Both gradient boosting methods demonstrated competitive performance with consistent generalization, benefiting from their ensemble-based optimization against weak learners (single learners, e.g., Decision Trees), aligns with theoretical expectations that ensemble and boosting methods reduce variance and bias through aggregation.Random Forest obtained a mean RMSE of 0.757916 (SD = 0.066344; 95% CI: [0.739527, 0.776306]), ranking fourth. It is worth mentioning that while Random Forest outperformed simpler models in terms of variance reduction, its predictive accuracy was inferior compared to boosting methods and kNN. Furthermore, the Decision Tree ML approach, a baseline of tree methods, yielded a mean RMSE of 0.812665 (SD = 0.060873; 95% CI: [0.795792, 0.829538]), underlining the limitations of single-tree learners, which are prone to high variance and overfitting. Finally, the Neural Networks configuration tested achieved the weakest performance, with a mean RMSE of 0.909277 (SD = 0.070638; 95% CI: [0.889697, 0.928857]). This result suggests that the chosen hyperparameter settings and network depth were not well-suited to the dataset, leading to suboptimal convergence, as clearly depicted inFigure 3.Figure 3.Comparative Analysis ML Models Ranking Table based on Mean RMSE.In summary, the results establish a clear ranking order of predictive EMF performance as follows:k-Nearest Neighbors: mean RMSE = 0.623153, SD = 0.042993, CI = [0.611236, 0.635070](ùëò=25k=25|ùëù=1.5p=1.5|ùëäùëíùëñùëî‚Ñéùë°ùë†=ùëëùëñùë†ùë°ùëéùëõùëêùëíWeights=distance)XGBoost: mean RMSE = 0.711201, SD = 0.059282, CI = [0.694769, 0.727633](ùúÜ=10Œª=10|ùëÅ_ùê∏ùë†ùë°.=100N_Est.=100|ùúÇ=0.05Œ∑=0.05|ùëÄùëéùë•_ùê∑ùëíùëùùë°‚Ñé=20Max_Depth=20)LightGBM: mean RMSE = 0.716689, SD = 0.053689, CI = [0.701807, 0.731571](ùëÅ_ùêøùëíùëéùë£ùëíùë†=100N_Leaves=100|ùëÅ_ùê∏ùë†ùë°.=500N_Est.=500|ùúÇ=0.1Œ∑=0.1|ùëÄùëéùë•_ùê∑ùëíùëùùë°‚Ñé=20Max_Depth=20)Random Forest: mean RMSE = 0.757916, SD = 0.066344, CI = [0.739527, 0.776306](ùëÄùëéùë•_ùê∑ùëíùëùùë°‚Ñé=10Max_Depth=10|ùëÅ_ùê∏ùë†ùë°ùëñùëöùëéùë°ùëúùëüùë†=200N_Estimators=200|ùê∂ùëüùëñùë°ùëíùëüùëñùëúùëõ=ùëÜùëûùë¢ùëéùëüùëíùëë_ùê∏ùëüùëüùëúùëüCriterion=Squared_Error)Decision Tree: mean RMSE = 0.812665, SD = 0.060873, CI = [0.795792, 0.829538](ùëÄùëñùëõ_ùëÜùëéùëöùëùùëôùëíùë†_ùëÜùëùùëôùëñùë°=20Min_Samples_Split=20)Neural Networks: mean RMSE = 0.909277, SD = 0.070638, CI = [0.889697, 0.928857](ùúé=ùëüùëíùëôùë¢|ùêªùëñùëëùëëùëíùëõ_ùêøùëéùë¶ùëíùëüùë†=2|ùúÇ=1.0œÉ=relu|Hidden_Layers=2|Œ∑=1.0)The superior accuracy of kNN highlights the effectiveness of instance-based learning for this specific EMF prediction task, though its computational efficiency may pose limitations for very large datasets. On the other hand, more novel gradient boosting approaches, such as XGBoost and LightGBM, provide a strong trade-off between predictive performance and scalability, while Random Forest remains a robust yet slightly less accurate alternative. Conversely, single-tree and neural architectures require careful hyperparameter refinement-calibration in order to approach the performance of ensemble-based methods. Summing up, these findings suggest that future work should prioritize boosting models or optimized kNN schemes, while indicating the potential of exploring hybrid approaches that integrate the effectiveness of instance-based learning with the interpretability of decision trees and the accuracy of ensemble learning methods.Figure 4showcase a comparative analysis of RMSEs distributions across all ML modes. Also, theFigure 5shows how the predicted values are mapped over the area of interest.Figure 4.ML Models Per-fold Violin Plots of RMSE Distributions.Figure 5.(a) Spatial interpolation of electromagnetic field (EMF) intensity across Catalonia, Spain, using k-NN method (b) Spatial interpolation of electromagnetic field (EMF) intensity across Catalonia, Spain, using XGBoost method. 4.2. XAI Results and Explanations4.2.1. SHAPThe SHAP (SHapley Additive exPlanations) framework offers a unified, model-agnostic approach for interpreting complex machine learning predictions by quantifying the marginal contribution of each feature to the model‚Äôs output. In particular, SHAP decomposes a model‚Äôs prediction into additive feature attributions, ensuring both local accuracy and global consistency across instances. Theglobal perspectiveaggregates feature contributions across all samples to identify the dominant predictors driving EMF variability at the population level. Conversely, thelocal perspectivefocuses on individual predictions, explaining how specific feature interactions influence the model‚Äôs output for a given observation.Specifically, in the context of electromagnetic field (EMF) strength prediction, the SHAP dual approach of the above two perspectives facilitate a comprehensive understanding of both the overall model behavior and case-specific explanatory patterns and so, enables a detailed examination of how spatial, demographic, and environmental variables-features influence model outputs, thereby providing transparent and theoretically grounded insights into the underlying decision mechanisms of the employed ML algorithms.In the subsequent subsections, SHAP results are presented for the top-performing ML models, including k-Nearest Neighbors (kNN) and XGBoost. The global SHAP summary plots (beeswarm plots) illustrate the average magnitude and direction of each feature‚Äôs contribution to EMF prediction, while local explanations (waterfall plots) reveal instance-specific deviations and contextual dependencies. This dual analysis enables the identification of consistent explanatory trends across models and environments, forming a robust interpretive basis for assessing predictive performance and environmental significance.kNN SHAP AnalysisFigure 6(beeswarm plot) visualizes both the magnitude and directionality of each feature‚Äôs impact, whereasFigure 7(waterfall plot) illustrates the additive effect of the most influential variables for a representative prediction instance. The signed mean SHAP values reflect whether a feature predominantly increases or decreases the predicted electromagnetic field (EMF) strength, while the absolute mean SHAP values capture the overall influence magnitude, independent of direction.Figure 6.kNN SHAP Beeswarm Plot.Figure 7.kNN SHAP Waterfall Plot.Among all predictors, SMOD (Degree of Urbanisation) emerges as the most influential variable (Figure 7), displaying the strongest negative SHAP contribution on average. This suggests that areas classified with higher urbanisation degrees correspond to a relative decrease in predicted EMF strength within the kNN model‚Äôs local decision structure, possibly due to attenuation effects or signal diffusion associated with denser morphological configurations. Conversely, Built_C_MSZ, representing the Morphological Settlement Zone classification, exerts a positive influence, indicating that denser or morphologically structured settlements tend to elevate EMF predictions, which likely reflects a potential increased infrastructure and emitter density in such zones.The features Built_H (average building height), Built_S (built-up surface), and Built_V (built-up volume) also show moderate positive contributions, emphasizing the importance of vertical and horizontal development intensity in shaping EMF propagation, whereas the feature variable POP (population density) appears to have a relatively minor but still meaningful impact, consistent with the idea that population distribution serves as an indirect indication for built infrastructure. Finally, Built_C_FUN (residential vs. non-residential functional classification) demonstrates minimal impact, suggesting that functional land-use type alone does not strongly determine EMF variations once structural characteristics are accounted for.Overall, the SHAP-based interpretability analysis reveals that urban morphology and spatial structural variables, particularly the degree of urbanisation, settlement delineation, and built environment metrics, constitute the most decisive factors in the kNN model‚Äôs EMF strength predictions. This evidence underscores the model‚Äôs sensitivity to fine-grained geospatial geometric attributes rather than purely demographic factors. From a methodological standpoint, SHAP confirms that the kNN model captures locally heterogeneous spatial geometrical dependencies, reinforcing the value of spatially explicit variables in EMF modeling frameworks.XGBoost SHAP AnalysisThe summary of the XGBoost SHAP analysis as depicted in (beeswarm) plot inFigure 8visualizes the distribution of SHAP values across all test samples, thereby quantifying both the magnitude and direction of each feature‚Äôs effect on the predicted EMF intensity. Meanwhile, the waterfall plot inFigure 9illustrates a single prediction instance, showing how the cumulative contributions of features shift the prediction from the baseline expectationùê∏[ùëì(ùëã)]E[f(X)]to the final model outputùëì(ùë•)f(x). The signed mean SHAP values reveal whether a feature tends to increase or decrease the predicted EMF, while the absolute mean SHAP values highlight its overall importance in shaping the model output.Figure 8.XGBoost SHAP Beeswarm Plot.Figure 9.XGBoost SHAP Waterfall Plot.Among the examined predictors, Built_H (building height), POP (population density), and SMOD (degree of urbanisation) emerge as the most influential variables, as clearly illustrated inFigure 9, in determining the model‚Äôs predictions. The SHAP values indicate that higher building heights and higher population densities generally lead to a decrease in predicted EMF levels, as shown by the negative SHAP contributions for these variables. This pattern can be attributed to the signal attenuation and scattering effects introduced by taller structures and densely populated urban cores, which can obstruct direct line-of-sight propagation. Similarly, SMOD, representing the urbanisation level, exerts a negative influence, suggesting that highly urbanized zones, often characterized by dense building clusters and infrastructure, tend to experience lower EMF intensities at ground level due to increased multipath fading and signal absorption.On the other hand, features related to morphological classification, such as the variables Built_C_MSZ (Morphological Settlement Zone) and Built_C_FUN (functional classification of the built environment), exhibit comparatively smaller yet non-negligible impacts. The weak positive or near-zero contributions of these variables suggest that while the categorical partitioning of urban morphology provides structural context, it alone does not substantially modify EMF predictions beyond the physical characteristics that already captured by building height and surface data. Similarly, Built_S (built-up surface) and Built_V (built-up volume) show marginal contributions, reinforcing the idea that geometric and structural aspects of the urban fabric, although interacting with EMF propagation mechanisms, still remain secondary to the building height and population-related attenuation factors.Overall, the SHAP analysis confirms that structural and demographic urban characteristics jointly govern EMF distribution, with XGBoost effectively capturing the nonlinear dependencies between physical urban morphology and exposure levels. The model assigns higher importance to attenuative factors (e.g., height, density) rather than purely spatial classifications, aligning well with established radio propagation theory. From an interpretability standpoint, these results validate the physical plausibility of the XGBoost model‚Äôs learning behavior, where the identified feature effects are consistent with empirical observations in urban electromagnetic environments, thereby enhancing confidence in its predictive transparency and reliability.SHAP DiscussionComparative dual SHAP interpretations reveal that the k-Nearest Neighbors (kNN) and XGBoost models achieve similar predictive accuracy but through fundamentally different explanatory mechanisms. The kNN model exhibits heightened sensitivity to spatial and morphological-topological structure, relying on neighborhood similarity in the feature space to infer electromagnetic field (EMF) strength. Its predictions are primarily influenced by local contextual features, particularly morphological indicators such as urban form and settlement classification, reflecting the model‚Äôs instance-based reasoning and dependence on spatial geometric distance-based proximity.In contrast, the XGBoost model emphasize on physical attenuation dynamics and quantitative urban metrics, capturing complex nonlinear interactions and cumulative effects among structural and demographic variables. This behavior aligns more closely with theoretical signal propagation principles, as the model generalizes more effectively across heterogeneous urban settings by learning hierarchical relationships rather than specific localized associations.Together, these complementary interpretability profiles demonstrate that kNN encodes better upon spatial similarity while XGBoost learns more effective parametric attenuation functions, two distinct but yet potential synergistic approaches to modeling EMF variability. Integrating both local (kNN-driven) and global (XGBoost-driven) interpretability perspectives yields a more comprehensive and physically consistent understanding of EMF behavior in urban environments, thereby advancing transparent, data-driven methodologies for environmental monitoring and urban planning.4.2.2. LIMEThe LIME (Local Interpretable Model-Agnostic Explanations) framework provides a complementary perspective to SHAP by focusing explicitly onlocal interpretability‚Äîthat is, understanding the reasoning behind individual model predictions. Unlike SHAP, which derives globally consistent feature attributions based on cooperative game theory, LIME approximates the complex behavior of a trained model in the vicinity of a specific instance by fitting a locally linear alternative substitute model. This approximation allows for the identification of the most influential features contributing to a given prediction, thereby offering interpretable, human-readable insights into the local decision boundaries of non-linear models.Specifically, in the context of electromagnetic field (EMF) strength prediction, LIME facilitates the examination of localized environmental, demographic, and infrastructural conditions that drive model predictions for specific spatial locations. This approach is particularly valuable for identifying context-sensitive deviations that may not be visible in more global XAI approaches like SHAP. By combining LIME‚Äôs fine-grained local analysis with SHAP‚Äôs globally consistent attributions, a comprehensive interpretability framework is achieved that can support both the validation of model behavior and the transparent communication of EMF prediction results for scientific and regulatory purposes. In correspondence to SHAP analysis, the following subsections present the LIME results for the top-performing ML models, k-Nearest Neighbors (kNN) and XGBoost.kNN LIME AnalysisConsecutive to the SHAP-based interpretability analysis, Local Interpretable Model-Agnostic Explanations (LIME) was employed in order to complement and examine feature contributions for the k-Nearest Neighbors (kNN) model. LIME was applied in a global aggregation framework, wherein instance-level explanations were averaged across the test set to obtain both absolute mean weightsFigure 10(indicating importance magnitude irrespective of direction) and signed mean weightsFigure 11(denoting whether features systematically increase or decrease EMF predictions). This dual representation provides a richer interpretability perspective, capturing the stability, strength, and polarity of each predictor‚Äôs influence on the urban electromagnetic field (EMF) estimations.Figure 10.LIME - kNN Global Absolute Values Feature Importance.Figure 11.LIME - kNN Global Signed Values Feature Importance.The LIME absolute mean weights analysis profile highlights Built_H (building height), SMOD (degree of urbanization), and Built_C_FUN (urban functional classification) as dominant drivers of kNN predictions, followed closely by POP (population density) and Built_C_MSZ (morphological settlement zoning), which collectively suggest that both vertical density and urban planning topology are central determinants in EMF variability. Built form intensity feature variables such as, Built_V (volume) and Built_S (surface area), exhibit smaller but nontrivial contributions, reflecting their secondary role in shaping propagation environments. Overall, the absolute LIME map reinforces the hypothesis that the kNN model relies primarily on local morphological and socio-spatial gradients, consistent with its distance-based learning nature.The signed LIME contributions (Figure 11) reveal coherent directional patterns. Higher values of Built_H, POP, and SMOD tend to increase EMF levels, consistent with urban densification leading to heightened wireless infrastructure density and reflective interference effects. In contrast, certain categories of Built_C_MSZ and low-rise built environments exhibit negative directional effects, suggesting attenuation or less intensive radiative environments in suburban or semi-urban contexts. The consistent polarity of Built_H and SMOD across most instances underscores their robust explanatory power in shaping EMF exposure gradients, while the mixed-sign behavior of built-form intensity variables (Built_S and Built_V) reflects context-dependent effects, likely driven by variations in infrastructure placement, open-space fragmentation, and various micro-propagation phenomena. Collectively, the LIME interpretations confirm the spatially-adaptive and locally sensitive nature of kNN, which leverages micro-scale morphological attributes to interpolate EMF levels.XGBoost LIME AnalysisLIME-based interpretability analysis was performed in order to elucidate how the XGBoost model leverages geospatial and morphological urban indicators to predict electromagnetic field (EMF) levels. The global mean absolute LIME weights (Figure 12) reveal that Built_V (built-up volume) and Built_H (building height) emerge as the most influential spatial predictors, followed closely by POP (population density) and SMOD (degree of urbanisation). This dominance indicates that three-dimensional urban morphology and population intensity act as primary determinants shaping EMF variability. The importance of Built_V and Built_H implies a strong association between vertical structural complexity and electromagnetic propagation dynamics, aligning well with radiofrequency propagation theory wherein higher and denser built forms induce multipath effects, signal obstruction, and power variations. Similarly, the prominence of POP and SMOD underscores that densely populated urban cores, which tend to exhibit intensive telecommunication activity and infrastructure concentration, drive higher EMF exposure levels.Figure 12.LIME - XGBoost Global Absolute Values Feature Importance.The signed LIME weights (Figure 13) provide insight into directionality. SMOD, POP, and Built_H predominantly exhibit positive signed effects, suggesting that higher values of these features consistently increase model predictions‚Äîi.e., contribute to elevated EMF intensity. In contrast, Built_V demonstrates a mixed effect profile: while highly ranked in absolute magnitude, its signed values oscillate around zero across spatial samples, indicating heterogeneous influence that varies by urban context. This pattern reflects physical reality, where vertical density may either attenuate or amplify field strength depending on line-of-sight availability, antenna configuration, and propagation pathways. Built_C_MSZ (settlement morphology classification) and Built_C_FUN (functional class of built space) appear less influential overall but still contribute meaningfully to local decision boundaries, particularly in delineating residential vs. non-residential urban fabric, which indirectly governs telecom antenna distribution and usage patterns.Figure 13.LIME‚ÄîXGBoost Global Signed Values Feature Importance.Collectively, the LIME results demonstrate that the XGBoost model learns coherent, physically interpretable associations grounded in urban morphology structure allocation and population density. The convergence of high absolute importance for Built_V, Built_H, POP, and SMOD across diverse local samples reinforces the robustness of these predictors as primary drivers of EMF exposure spatial heterogeneity. The subtle positive/negative balance for some variables highlights the context-dependent nature of RF propagation, where interactions between terrain, built form, and antenna topology produce nonlinear effects. These findings validate the model‚Äôs capacity to reflect real-world physical propagation mechanisms while also providing transparent local explanations that support regulatory, environmental, and urban planning applications.LIME DiscussionThe LIME analysis provides complementary insights into the interpretive behavior of the kNN and XGBoost models by examining feature contributions at the individual-prediction level. Similar to the SHAP findings, both models consistently identify key built-environment and demographic variables, particularly Built_H (building height), Built_V (built volume), Built_S (built surface area) and POP (population density), as the most dominant predictors of EMF intensity. This convergence reinforces the central role of vertical urban form structures and population concentration in shaping radio-frequency propagation electrodynamics.For the kNN model, the largest LIME attribution magnitudes are associated with Built_H, SMOD, and Built_C_FUN, followed by POP and Built_C_MSZ. Signed contributions reveal context-dependent influences, where certain urban morphologies increase EMF strength while others attenuate it depending on local neighborhood topological similarity. This behavior is consistent with kNN‚Äôs instance-based learning paradigm, wherein predictions are strongly shaped by the characteristics of proximate observations. As a result, kNN exhibits more spatially heterogeneous and locally sensitive interpretability, making it particularly informative for micro-scale EMF variation in highly diverse urban settings.In contrast, XGBoost demonstrates a more structured and globally stable feature attribution pattern. Built_V and Built_H dominate the local explanations, with POP and SMOD also contributing consistently across instances. Directional effects for these variables align closely with already establised theoretical expectations: taller and denser urban environments, which often associated with increased antenna density and multipath propagation, exert strong influence on EMF levels. Compared to kNN, XGBoost yields smoother, more coherent interpretive profiles, reflecting its increased capacity to capture more complex nonlinear, system-level interactions rather than purely local distance-based analogies. Overall, LIME confirms that while kNN provides granular, location-specific interpretability, XGBoost delivers more consistent and physically aligned explanations, resulting in two complementary interpretive paradigms for urban EMF analysis. Together, these LIME results demonstrate that combining instance-level, context-sensitive explanations with broader model-based insights yields a more complete and operationally meaningful understanding of EMF prediction behavior, reinforcing the value of localized interpretability for urban exposure assessment and decision-support applications.4.2.3. Comparative Analysis Explainable AI ModelsA comparative analysis evaluation of SHAP and LIME was conducted in order to elucidate the interpretability characteristics of the employed machine learning models and to assess the consistency, stability, and physical plausibility of feature attributions in the context of EMF field prediction. Both frameworks identified urban morphology and demographic intensity as primary drivers of EMF variability, with highly consistent emphasis across models on variables capturing three-dimensional built form and urbanisation. Specifically, Built_H (building height), Built_V (built volume), POP (population density), and SMOD (degree of urbanisation) emerged as dominant predictors under both methods, reinforcing that vertical density, infrastructure concentration, and population-driven network intensity jointly shape radiofrequency propagation in dense urban environments. Secondary variables such as Built_S, Built_C_MSZ, and Built_C_FUN provided additional structural context, though with comparatively weaker influence. This convergence across SHAP and LIME substantiates the physical plausibility of the learned EMF patterns, aligning model interpretability outcomes with underlying electromagnetic propagation theory.Despite this overarching consistency, SHAP and LIME exhibited complementary interpretive behaviors reflecting their methodological underpinnings. SHAP provided globally stable, additive attributions with clear decomposition of prediction contributions, enabling systematic quantification of global importance patterns while preserving local fidelity. This was particularly evident for XGBoost, where SHAP exposed coherent attenuation patterns associated with increasing building height and population density, alongside structured nonlinear interactions across the urban morphology. LIME, by contrast, emphasized high-resolution local interpretability, capturing spatially varying effects driven by neighborhood-specific morphology‚Äîparticularly pronounced in kNN, where instance-based reasoning produced heterogeneous but physically meaningful feature impacts tied to local similarity relationships. Thus, while SHAP offered robust and globally consistent explanations suitable for model benchmarking and scientific inference, LIME revealed contextual micro-dynamics valuable for localized environmental assessment and scenario-specific interpretation.Overall, the comparative analysis demonstrates that SHAP and LIME act as complementary interpretability mechanisms. SHAP excels in establishing global causal consistency and theoretical conformity, whereas LIME enhances granularity in local diagnostic interpretation. The combined deployment of both methods results in a comprehensive transparency framework, enabling robust validation of model behavior, improved credibility of EMF predictions, and enhanced interpretive utility for urban-scale electromagnetic exposure monitoring and decision-support applications.Table 9andFigure 14summarize the explainability results obtained across SHAP and LIME, where feature importance analysis of kNN and XGBoost ML methods consistently highlight Built_H, Built_V, and population density POP as the primary drivers of EMF field variability, with SMOD providing additional contextual differentiation of urban form topology. These findings confirm that vertical urban morphology and demographic intensity exert the strongest influence on RF propagation patterns, reinforcing the physical plausibility and robustness of the learned predictive relationships. As shown in the newly addedFigure 14, we summarize the feature-importance patterns across kNN and XGBoost models using a heatmap that integrates both SHAP and LIME evaluations. In order to visualize the qualitative feature-importance categories in a unified manner, we encoded each textual level into an ordinal numerical scale as follows:12345=Weak,=Weak‚ÄìModerate/Medium‚ÄìLow,=Moderate,=Moderate‚ÄìStrong/High,=Strong/VeryHigh.1=Weak,2=Weak‚ÄìModerate/Medium‚ÄìLow,3=Moderate,4=Moderate‚ÄìStrong/High,5=Strong/VeryHigh.Figure 14.Combined SHAP and LIME feature importance and consistency across kNN and XGBoost models.Table 9.Feature importance alignment across kNN and XGBoost models using SHAP and LIME.This mapping enables consistent graphical representation of importance intensity across models and interpretability methods.",
            "4.1. Machine Learning Model Selection Results": "This section presents a comprehensive analysis of the results obtained from the systematic application of the machine learning methods described previously. For each method, hyperparameter optimization yielded a distinct set of models, enabling a thorough evaluation of their performance. We provide a detailed discussion of the optimal results for each algorithm individually, followed by a comparative analysis to assess their relative performance and effectiveness. To provide additional practical value, we report the computational burden associated with the hyperparameter grid and model training process. Across all experiments, the relative runtime order was as follows: kNN was the fastest method, followed by Decision Trees, which also completed training quickly. XGBoost and LightGBM required moderate computation time, while Neural Networks exhibited moderate to high runtime depending on the number of layers and complexity of the architecture. Random Forest showed the highest overall computational cost due to the large number of trees and repeated cross-validation. These observations help practitioners understand the trade-off between predictive accuracy and computational resources when selecting models for large-scale EMF prediction tasks. Neural Networks (NN) We implemented a large-scale evaluation of neural network architectures, systematically varying hyperparameters related to topology (number of layersL), activation functions (ùúéœÉ), and optimization settings (learning rateùúÇŒ∑, optimizer choice). The best-performing configurations achieved mean RMSE values between 0.909277 and 0.911257, with differences lying within the error margin, indicating comparable accuracy and robustness across cross-validation folds. The numerical optimal model consisted of two hidden layers with ReLU activation, Adam optimizer, andùúÇ=1.0Œ∑=1.0, yielding a mean RMSE of 0.909277 (ùëÜùê∑=0.070638SD=0.070638). Although ReLU combined with high learning rates produced competitive results, these models exhibited lower reproducibility. In contrast, networks with logistic or tanh activation functions, compact architectures (2‚Äì20 hidden units), and moderate learning rates (ùúÇ=0.01Œ∑=0.01, Adam) consistently achieved near-optimal accuracy while maintaining greater stability. These models dominated the top-ranked results, underscoring their reliability and efficiency in capturing nonlinear patterns in electric field strength prediction, as we can see in the summarized rankingTable 3. Conversely, identity activations and excessively large networks often diverged, producing inflated errors and instability, highlighting their unsuitability. Overall, compact architectures with logistic or tanh activations and moderate learning rates represent the most robust and practical solutions for deployment, while further statistical paired tests on fold-level predictions are required to claim that any of the best configurations is better than another. Table 3.Top 10 Neural Network Hyperparameter Combinations Ranked by Mean RMSE. k-Nearest Neighbors (kNN) The evaluation of the k-Nearest Neighbors (kNN) models revealed a consistent trend of better results by the use of distance-based weighting schemes (inverse-distance) over uniform weighting (equal weights), regardless of the number of neighbors or Minkowski distance parameterp. Specifically, the five best-performing configurations, as measured by mean RMSE, were very tightly grouped in the interval [0.623153, 0.627706], with the best of them(ùëù=1.5,ùëò=25,weights=distance)(p=1.5,k=25,weights=distance)achieving RMSE 0.623153 (SD 0.042993), closely followed by the second best(ùëù=1.0,ùëò=25,distance)(p=1.0,k=25,distance)with RMSE 0.623335 (SD 0.043059), and the rest of the top 5 as clearly depicted in summarizedTable 4, accompanied with their corresponding CIs. Table 4.Top 5 kNN hyperparameter configurations ranked by mean RMSE. The above results indicate that medium neighborhood sizes (ùëò=10k=10‚Äì25) combined with inverse distance weighting achieved an optimal balance between local sensitivity and generalization and thus yielded the lowest prediction errors. In contrast, uniform weighting schemes systematically underperformed, with RMSE values increasing substantially askgrew larger, underscoring the importance of accounting for proximity when aggregating neighbor contributions, especially for spatial applications like EMF predictions, because measurements are spatially autocorrelated and nearer sensors are more informative. Summing up, distance-weighted kNN models with moderate neighborhood sizes (ùëò=10k=10‚Äì25) yielded the most robust performance, with only minor RMSE variation across top settings. Moreover, the slightly best results atùëù=1.5p=1.5may indicate that the feature space geometry could be best captured by a non-integer continuous Minkowskipnorm. Decision Trees (DT) The evaluation of Decision Tree models demonstrated that adjustments to theùëöùëñùëõ_ùë†ùëéùëöùëùùëôùëíùë†_ùë†ùëùùëôùëñùë°min_samples_splithyperparameter, which controls the minimum number (or fraction) of samples required to split an internal node, produced a systematic bias‚Äìvariance trade-off consistent with theoretical expectations. Specifically, very small thresholds (e.g.,ùëöùëñùëõ_ùë†ùëéùëöùëùùëôùëíùë†_ùë†ùëùùëôùëñùë°=2min_samples_split=2) yielded the weakest performance (mean RMSE = 0.869772, SD = 0.075755), indicating overfitting and unstable predictions across folds. Increasing the threshold clearly improved generalization, with the best performance observed atùëöùëñùëõ_ùë†ùëéùëöùëùùëôùëíùë†_ùë†ùëùùëôùëñùë°=20min_samples_split=20(mean RMSE = 0.812665, SD = 0.060873). However, further increasing the threshold (e.g.,ùëöùëñùëõ_ùë†ùëéùëöùëùùëôùëíùë†_ùë†ùëùùëôùëñùë°=50min_samples_split=50) slightly degraded the performance (mean RMSE = 0.817838, SD = 0.061485), suggesting underfitting as the model became overly constrained. The above results indicate that moderate complexity constraints, specifically a splitting threshold around 20, yielded the best performing configuration and provide the most favorable balance between flexibility and stability, clearly indicated in the summarizedTable 5, by the lower mean RMSE score. Overall, this configuration achieved to reduce the variance without sacrificing his predictive accuracy, yielding the most robust results between DTs models for EMF strength prediction in urban environments. Table 5.Ranking of Decision Tree hyperparameter configurations based on RMSE performance. Random Forest (RF) The evaluation of Random Forest models indicates that the squared-error and Friedman MSE criteria consistently yielded superior performance across all ensemble sizes. The best configuration with 200 estimators, squared-error splitting and maximum depth of 10, achieved the lowest mean RMSE of 0.757916 (SD = 0.066344). In contrast, the absolute-error criterion showed considerably weaker results (mean RMSE ‚âà 0.817), underscoring its unsuitability for accurate EMF estimation. Moreover, by increasing the number of estimators from 50 to 200 we obtained marginal improvements, with refinements of less than 0.002 in mean RMSE, improvements that, although consistent, were small relative to cross-validation variability, as we can see in the summarized top 5 rankingTable 6. Table 6.Top 5 ranked Random Forests hyperparameter configurations based on RMSE performance. Overall, performance differences among the top Random Forest settings (squared-error and Friedman MSE with 100‚Äì200 trees) were minor and largely within overlapping confidence intervals, suggesting further statistical testing (e.g., pairedt-tests) to confirm notable statistical significance. From a computational perspective, ensembles of ‚àº100 trees appear to provide an optimal trade-off between predictive accuracy and efficiency, while 200 trees offer slightly greater stability at higher computational cost. LightGBM The evaluation of LightGBM models across a broad hyperparameter space revealed a consistent pattern in predictive accuracy, with the lowest errors obtained from larger tree structures combined with sufficient gradient boosting iterations. The best performing configuration with Num_Leaves = 100, N_Estimators = 500, Learning_Rate = 0.1, and Max_Depth = 20 achieved a mean RMSE of 0.716689 (SD = 0.053689), indicating both high accuracy and stable generalization across all folds. Comparable performance was observed for slightly less complex settings, such as N_Estimators = 300 or Learning_Rate = 0.05, which yielded mean RMSE values in the range 0.720822‚Äì0.722759, as summarized in theTable 7. Table 7.Top 10 ranked LightGBM hyperparameter configurations based on Mean RMSE. The above results confirm that LightGBM benefits from deeper and broader trees when combined with sufficient gradient boosting rounds, particularly under moderate learning rates that balance convergence speed and generalization. However, deeper architectures and larger ensembles increase computational demands and may increase overfitting risks if the available data are limited or noisy. Overall, while the results show that more complex configurations yield superior accuracy in the estimation of EMF, the marginal gains diminish beyond a certain complexity, e.g., differences between 300 and 500 estimators are modest, a fact that must taken under consideration in applications where the computational resources are limited. So, from a practical standpoint, configurations with 100 leaves, 300 estimators, and depth 20 may offer a near-optimal trade-off between accuracy and efficiency. XGBoost The evaluation of XGBoost models across varied hyperparameter configurations revealed that the strongest predictive performance was consistently obtained with deep trees (Max_Depth = 20) combined with stronger L2 regularization parameters (i.e., Lambda_optionùúÜŒª= 10). Specifically, the best configuration, withùúÜŒª= 10, N_Estimators = 100 and Learning_RateùúÇŒ∑= 0.05, achieved the lowest mean RMSE = 0.711201 (SD = 0.059282). Comparable results were observed with lower learning rates (ùúÇŒ∑= 0.01) but considerably larger ensembles (300‚Äì500 estimators), as summarized inTable 8. Table 8.Top 10 XGBoost hyperparameter configurations ranked by Mean RMSE (lower is better). The above results highlight two effective strategic approaches that can be followed: (i) moderate learning rates (ùúÇŒ∑= 0.05) with fewer boosting rounds (N_Estimators ‚âà 100), which reduce computational cost while maintaining accuracy, and (ii) smaller learning rates (ùúÇŒ∑= 0.01) with considerably larger ensembles (300‚Äì500 estimators), which yield similar accuracy but at higher computational expense. The consistent presence of strong L2 regularization (ùúÜŒª= 5‚Äì10) among the top-performing models, suggests that high-capacity XGBoost trees tend to overfit in that specific settings unless they are adequately penalized and thus underscoring its critical role of regularization in mitigating overfitting in high-capacity deep trees. Overall, the best-performing configurations balance well the ability to capture complex spatial‚Äìenvironmental dependencies in the EMF data combined with stable generalization across folds. However, the narrow differences among the top results, coupled with overlapping confidence intervals, suggest that observed improvements are incremental and may depend on the specific dataset partitioning. From a practical standpoint, the configuration withùúÜŒª= 10, N_Estimators = 100 andùúÇŒ∑= 0.05 offers an effective trade-off between accuracy and efficiency, while low‚Äìlearning rate settings may be advantageous in occasions when computational resources are plentiful and marginal gains in stability are important. Comparative Analysis Machine Learning ModelsThe comparative analysis of the six machine learning models Neural Networks (NN), k-Nearest Neighbors (kNN), Decision Trees (DTs), Random Forest (RF), LightGBM and XGBoost demonstrates notable differences in their predictive accuracy, variability, and robustness. Specifically, models‚Äô evaluation (The models were ranked according to their mean RMSE values, reported with six-decimal precision.) was conducted using the root mean squared error (RMSE) as the primary performance metric, complemented by variability measures including the standard deviation of RMSE (SD RMSE) and 95% confidence intervals (CI).The best-performing model was kNN, achieving a mean RMSE of 0.623153 (SD = 0.042993; 95% CI: [0.611236, 0.635070]). This result indicates a strong predictive accuracy and comparatively low uncertainty, highlighting kNN‚Äôs capacity to capture local structures within the dataset. The second-best performer was XGBoost, with a mean RMSE of 0.711201 (SD = 0.059282; 95% CI: [0.694769, 0.727633]), followed closely by LightGBM at 0.716689 (SD = 0.053689; 95% CI: [0.701807, 0.731571]). Both gradient boosting methods demonstrated competitive performance with consistent generalization, benefiting from their ensemble-based optimization against weak learners (single learners, e.g., Decision Trees), aligns with theoretical expectations that ensemble and boosting methods reduce variance and bias through aggregation.Random Forest obtained a mean RMSE of 0.757916 (SD = 0.066344; 95% CI: [0.739527, 0.776306]), ranking fourth. It is worth mentioning that while Random Forest outperformed simpler models in terms of variance reduction, its predictive accuracy was inferior compared to boosting methods and kNN. Furthermore, the Decision Tree ML approach, a baseline of tree methods, yielded a mean RMSE of 0.812665 (SD = 0.060873; 95% CI: [0.795792, 0.829538]), underlining the limitations of single-tree learners, which are prone to high variance and overfitting. Finally, the Neural Networks configuration tested achieved the weakest performance, with a mean RMSE of 0.909277 (SD = 0.070638; 95% CI: [0.889697, 0.928857]). This result suggests that the chosen hyperparameter settings and network depth were not well-suited to the dataset, leading to suboptimal convergence, as clearly depicted inFigure 3.Figure 3.Comparative Analysis ML Models Ranking Table based on Mean RMSE.In summary, the results establish a clear ranking order of predictive EMF performance as follows:k-Nearest Neighbors: mean RMSE = 0.623153, SD = 0.042993, CI = [0.611236, 0.635070](ùëò=25k=25|ùëù=1.5p=1.5|ùëäùëíùëñùëî‚Ñéùë°ùë†=ùëëùëñùë†ùë°ùëéùëõùëêùëíWeights=distance)XGBoost: mean RMSE = 0.711201, SD = 0.059282, CI = [0.694769, 0.727633](ùúÜ=10Œª=10|ùëÅ_ùê∏ùë†ùë°.=100N_Est.=100|ùúÇ=0.05Œ∑=0.05|ùëÄùëéùë•_ùê∑ùëíùëùùë°‚Ñé=20Max_Depth=20)LightGBM: mean RMSE = 0.716689, SD = 0.053689, CI = [0.701807, 0.731571](ùëÅ_ùêøùëíùëéùë£ùëíùë†=100N_Leaves=100|ùëÅ_ùê∏ùë†ùë°.=500N_Est.=500|ùúÇ=0.1Œ∑=0.1|ùëÄùëéùë•_ùê∑ùëíùëùùë°‚Ñé=20Max_Depth=20)Random Forest: mean RMSE = 0.757916, SD = 0.066344, CI = [0.739527, 0.776306](ùëÄùëéùë•_ùê∑ùëíùëùùë°‚Ñé=10Max_Depth=10|ùëÅ_ùê∏ùë†ùë°ùëñùëöùëéùë°ùëúùëüùë†=200N_Estimators=200|ùê∂ùëüùëñùë°ùëíùëüùëñùëúùëõ=ùëÜùëûùë¢ùëéùëüùëíùëë_ùê∏ùëüùëüùëúùëüCriterion=Squared_Error)Decision Tree: mean RMSE = 0.812665, SD = 0.060873, CI = [0.795792, 0.829538](ùëÄùëñùëõ_ùëÜùëéùëöùëùùëôùëíùë†_ùëÜùëùùëôùëñùë°=20Min_Samples_Split=20)Neural Networks: mean RMSE = 0.909277, SD = 0.070638, CI = [0.889697, 0.928857](ùúé=ùëüùëíùëôùë¢|ùêªùëñùëëùëëùëíùëõ_ùêøùëéùë¶ùëíùëüùë†=2|ùúÇ=1.0œÉ=relu|Hidden_Layers=2|Œ∑=1.0)The superior accuracy of kNN highlights the effectiveness of instance-based learning for this specific EMF prediction task, though its computational efficiency may pose limitations for very large datasets. On the other hand, more novel gradient boosting approaches, such as XGBoost and LightGBM, provide a strong trade-off between predictive performance and scalability, while Random Forest remains a robust yet slightly less accurate alternative. Conversely, single-tree and neural architectures require careful hyperparameter refinement-calibration in order to approach the performance of ensemble-based methods. Summing up, these findings suggest that future work should prioritize boosting models or optimized kNN schemes, while indicating the potential of exploring hybrid approaches that integrate the effectiveness of instance-based learning with the interpretability of decision trees and the accuracy of ensemble learning methods.Figure 4showcase a comparative analysis of RMSEs distributions across all ML modes. Also, theFigure 5shows how the predicted values are mapped over the area of interest.Figure 4.ML Models Per-fold Violin Plots of RMSE Distributions.Figure 5.(a) Spatial interpolation of electromagnetic field (EMF) intensity across Catalonia, Spain, using k-NN method (b) Spatial interpolation of electromagnetic field (EMF) intensity across Catalonia, Spain, using XGBoost method.",
            "Comparative Analysis Machine Learning Models": "The comparative analysis of the six machine learning models Neural Networks (NN), k-Nearest Neighbors (kNN), Decision Trees (DTs), Random Forest (RF), LightGBM and XGBoost demonstrates notable differences in their predictive accuracy, variability, and robustness. Specifically, models‚Äô evaluation (The models were ranked according to their mean RMSE values, reported with six-decimal precision.) was conducted using the root mean squared error (RMSE) as the primary performance metric, complemented by variability measures including the standard deviation of RMSE (SD RMSE) and 95% confidence intervals (CI). The best-performing model was kNN, achieving a mean RMSE of 0.623153 (SD = 0.042993; 95% CI: [0.611236, 0.635070]). This result indicates a strong predictive accuracy and comparatively low uncertainty, highlighting kNN‚Äôs capacity to capture local structures within the dataset. The second-best performer was XGBoost, with a mean RMSE of 0.711201 (SD = 0.059282; 95% CI: [0.694769, 0.727633]), followed closely by LightGBM at 0.716689 (SD = 0.053689; 95% CI: [0.701807, 0.731571]). Both gradient boosting methods demonstrated competitive performance with consistent generalization, benefiting from their ensemble-based optimization against weak learners (single learners, e.g., Decision Trees), aligns with theoretical expectations that ensemble and boosting methods reduce variance and bias through aggregation. Random Forest obtained a mean RMSE of 0.757916 (SD = 0.066344; 95% CI: [0.739527, 0.776306]), ranking fourth. It is worth mentioning that while Random Forest outperformed simpler models in terms of variance reduction, its predictive accuracy was inferior compared to boosting methods and kNN. Furthermore, the Decision Tree ML approach, a baseline of tree methods, yielded a mean RMSE of 0.812665 (SD = 0.060873; 95% CI: [0.795792, 0.829538]), underlining the limitations of single-tree learners, which are prone to high variance and overfitting. Finally, the Neural Networks configuration tested achieved the weakest performance, with a mean RMSE of 0.909277 (SD = 0.070638; 95% CI: [0.889697, 0.928857]). This result suggests that the chosen hyperparameter settings and network depth were not well-suited to the dataset, leading to suboptimal convergence, as clearly depicted inFigure 3. Figure 3.Comparative Analysis ML Models Ranking Table based on Mean RMSE. In summary, the results establish a clear ranking order of predictive EMF performance as follows: k-Nearest Neighbors: mean RMSE = 0.623153, SD = 0.042993, CI = [0.611236, 0.635070](ùëò=25k=25|ùëù=1.5p=1.5|ùëäùëíùëñùëî‚Ñéùë°ùë†=ùëëùëñùë†ùë°ùëéùëõùëêùëíWeights=distance)XGBoost: mean RMSE = 0.711201, SD = 0.059282, CI = [0.694769, 0.727633](ùúÜ=10Œª=10|ùëÅ_ùê∏ùë†ùë°.=100N_Est.=100|ùúÇ=0.05Œ∑=0.05|ùëÄùëéùë•_ùê∑ùëíùëùùë°‚Ñé=20Max_Depth=20)LightGBM: mean RMSE = 0.716689, SD = 0.053689, CI = [0.701807, 0.731571](ùëÅ_ùêøùëíùëéùë£ùëíùë†=100N_Leaves=100|ùëÅ_ùê∏ùë†ùë°.=500N_Est.=500|ùúÇ=0.1Œ∑=0.1|ùëÄùëéùë•_ùê∑ùëíùëùùë°‚Ñé=20Max_Depth=20)Random Forest: mean RMSE = 0.757916, SD = 0.066344, CI = [0.739527, 0.776306](ùëÄùëéùë•_ùê∑ùëíùëùùë°‚Ñé=10Max_Depth=10|ùëÅ_ùê∏ùë†ùë°ùëñùëöùëéùë°ùëúùëüùë†=200N_Estimators=200|ùê∂ùëüùëñùë°ùëíùëüùëñùëúùëõ=ùëÜùëûùë¢ùëéùëüùëíùëë_ùê∏ùëüùëüùëúùëüCriterion=Squared_Error)Decision Tree: mean RMSE = 0.812665, SD = 0.060873, CI = [0.795792, 0.829538](ùëÄùëñùëõ_ùëÜùëéùëöùëùùëôùëíùë†_ùëÜùëùùëôùëñùë°=20Min_Samples_Split=20)Neural Networks: mean RMSE = 0.909277, SD = 0.070638, CI = [0.889697, 0.928857](ùúé=ùëüùëíùëôùë¢|ùêªùëñùëëùëëùëíùëõ_ùêøùëéùë¶ùëíùëüùë†=2|ùúÇ=1.0œÉ=relu|Hidden_Layers=2|Œ∑=1.0) The superior accuracy of kNN highlights the effectiveness of instance-based learning for this specific EMF prediction task, though its computational efficiency may pose limitations for very large datasets. On the other hand, more novel gradient boosting approaches, such as XGBoost and LightGBM, provide a strong trade-off between predictive performance and scalability, while Random Forest remains a robust yet slightly less accurate alternative. Conversely, single-tree and neural architectures require careful hyperparameter refinement-calibration in order to approach the performance of ensemble-based methods. Summing up, these findings suggest that future work should prioritize boosting models or optimized kNN schemes, while indicating the potential of exploring hybrid approaches that integrate the effectiveness of instance-based learning with the interpretability of decision trees and the accuracy of ensemble learning methods.Figure 4showcase a comparative analysis of RMSEs distributions across all ML modes. Also, theFigure 5shows how the predicted values are mapped over the area of interest. Figure 4.ML Models Per-fold Violin Plots of RMSE Distributions. Figure 5.(a) Spatial interpolation of electromagnetic field (EMF) intensity across Catalonia, Spain, using k-NN method (b) Spatial interpolation of electromagnetic field (EMF) intensity across Catalonia, Spain, using XGBoost method.",
            "4.2. XAI Results and Explanations": "4.2.1. SHAPThe SHAP (SHapley Additive exPlanations) framework offers a unified, model-agnostic approach for interpreting complex machine learning predictions by quantifying the marginal contribution of each feature to the model‚Äôs output. In particular, SHAP decomposes a model‚Äôs prediction into additive feature attributions, ensuring both local accuracy and global consistency across instances. Theglobal perspectiveaggregates feature contributions across all samples to identify the dominant predictors driving EMF variability at the population level. Conversely, thelocal perspectivefocuses on individual predictions, explaining how specific feature interactions influence the model‚Äôs output for a given observation.Specifically, in the context of electromagnetic field (EMF) strength prediction, the SHAP dual approach of the above two perspectives facilitate a comprehensive understanding of both the overall model behavior and case-specific explanatory patterns and so, enables a detailed examination of how spatial, demographic, and environmental variables-features influence model outputs, thereby providing transparent and theoretically grounded insights into the underlying decision mechanisms of the employed ML algorithms.In the subsequent subsections, SHAP results are presented for the top-performing ML models, including k-Nearest Neighbors (kNN) and XGBoost. The global SHAP summary plots (beeswarm plots) illustrate the average magnitude and direction of each feature‚Äôs contribution to EMF prediction, while local explanations (waterfall plots) reveal instance-specific deviations and contextual dependencies. This dual analysis enables the identification of consistent explanatory trends across models and environments, forming a robust interpretive basis for assessing predictive performance and environmental significance.kNN SHAP AnalysisFigure 6(beeswarm plot) visualizes both the magnitude and directionality of each feature‚Äôs impact, whereasFigure 7(waterfall plot) illustrates the additive effect of the most influential variables for a representative prediction instance. The signed mean SHAP values reflect whether a feature predominantly increases or decreases the predicted electromagnetic field (EMF) strength, while the absolute mean SHAP values capture the overall influence magnitude, independent of direction.Figure 6.kNN SHAP Beeswarm Plot.Figure 7.kNN SHAP Waterfall Plot.Among all predictors, SMOD (Degree of Urbanisation) emerges as the most influential variable (Figure 7), displaying the strongest negative SHAP contribution on average. This suggests that areas classified with higher urbanisation degrees correspond to a relative decrease in predicted EMF strength within the kNN model‚Äôs local decision structure, possibly due to attenuation effects or signal diffusion associated with denser morphological configurations. Conversely, Built_C_MSZ, representing the Morphological Settlement Zone classification, exerts a positive influence, indicating that denser or morphologically structured settlements tend to elevate EMF predictions, which likely reflects a potential increased infrastructure and emitter density in such zones.The features Built_H (average building height), Built_S (built-up surface), and Built_V (built-up volume) also show moderate positive contributions, emphasizing the importance of vertical and horizontal development intensity in shaping EMF propagation, whereas the feature variable POP (population density) appears to have a relatively minor but still meaningful impact, consistent with the idea that population distribution serves as an indirect indication for built infrastructure. Finally, Built_C_FUN (residential vs. non-residential functional classification) demonstrates minimal impact, suggesting that functional land-use type alone does not strongly determine EMF variations once structural characteristics are accounted for.Overall, the SHAP-based interpretability analysis reveals that urban morphology and spatial structural variables, particularly the degree of urbanisation, settlement delineation, and built environment metrics, constitute the most decisive factors in the kNN model‚Äôs EMF strength predictions. This evidence underscores the model‚Äôs sensitivity to fine-grained geospatial geometric attributes rather than purely demographic factors. From a methodological standpoint, SHAP confirms that the kNN model captures locally heterogeneous spatial geometrical dependencies, reinforcing the value of spatially explicit variables in EMF modeling frameworks.XGBoost SHAP AnalysisThe summary of the XGBoost SHAP analysis as depicted in (beeswarm) plot inFigure 8visualizes the distribution of SHAP values across all test samples, thereby quantifying both the magnitude and direction of each feature‚Äôs effect on the predicted EMF intensity. Meanwhile, the waterfall plot inFigure 9illustrates a single prediction instance, showing how the cumulative contributions of features shift the prediction from the baseline expectationùê∏[ùëì(ùëã)]E[f(X)]to the final model outputùëì(ùë•)f(x). The signed mean SHAP values reveal whether a feature tends to increase or decrease the predicted EMF, while the absolute mean SHAP values highlight its overall importance in shaping the model output.Figure 8.XGBoost SHAP Beeswarm Plot.Figure 9.XGBoost SHAP Waterfall Plot.Among the examined predictors, Built_H (building height), POP (population density), and SMOD (degree of urbanisation) emerge as the most influential variables, as clearly illustrated inFigure 9, in determining the model‚Äôs predictions. The SHAP values indicate that higher building heights and higher population densities generally lead to a decrease in predicted EMF levels, as shown by the negative SHAP contributions for these variables. This pattern can be attributed to the signal attenuation and scattering effects introduced by taller structures and densely populated urban cores, which can obstruct direct line-of-sight propagation. Similarly, SMOD, representing the urbanisation level, exerts a negative influence, suggesting that highly urbanized zones, often characterized by dense building clusters and infrastructure, tend to experience lower EMF intensities at ground level due to increased multipath fading and signal absorption.On the other hand, features related to morphological classification, such as the variables Built_C_MSZ (Morphological Settlement Zone) and Built_C_FUN (functional classification of the built environment), exhibit comparatively smaller yet non-negligible impacts. The weak positive or near-zero contributions of these variables suggest that while the categorical partitioning of urban morphology provides structural context, it alone does not substantially modify EMF predictions beyond the physical characteristics that already captured by building height and surface data. Similarly, Built_S (built-up surface) and Built_V (built-up volume) show marginal contributions, reinforcing the idea that geometric and structural aspects of the urban fabric, although interacting with EMF propagation mechanisms, still remain secondary to the building height and population-related attenuation factors.Overall, the SHAP analysis confirms that structural and demographic urban characteristics jointly govern EMF distribution, with XGBoost effectively capturing the nonlinear dependencies between physical urban morphology and exposure levels. The model assigns higher importance to attenuative factors (e.g., height, density) rather than purely spatial classifications, aligning well with established radio propagation theory. From an interpretability standpoint, these results validate the physical plausibility of the XGBoost model‚Äôs learning behavior, where the identified feature effects are consistent with empirical observations in urban electromagnetic environments, thereby enhancing confidence in its predictive transparency and reliability.SHAP DiscussionComparative dual SHAP interpretations reveal that the k-Nearest Neighbors (kNN) and XGBoost models achieve similar predictive accuracy but through fundamentally different explanatory mechanisms. The kNN model exhibits heightened sensitivity to spatial and morphological-topological structure, relying on neighborhood similarity in the feature space to infer electromagnetic field (EMF) strength. Its predictions are primarily influenced by local contextual features, particularly morphological indicators such as urban form and settlement classification, reflecting the model‚Äôs instance-based reasoning and dependence on spatial geometric distance-based proximity.In contrast, the XGBoost model emphasize on physical attenuation dynamics and quantitative urban metrics, capturing complex nonlinear interactions and cumulative effects among structural and demographic variables. This behavior aligns more closely with theoretical signal propagation principles, as the model generalizes more effectively across heterogeneous urban settings by learning hierarchical relationships rather than specific localized associations.Together, these complementary interpretability profiles demonstrate that kNN encodes better upon spatial similarity while XGBoost learns more effective parametric attenuation functions, two distinct but yet potential synergistic approaches to modeling EMF variability. Integrating both local (kNN-driven) and global (XGBoost-driven) interpretability perspectives yields a more comprehensive and physically consistent understanding of EMF behavior in urban environments, thereby advancing transparent, data-driven methodologies for environmental monitoring and urban planning. 4.2.2. LIMEThe LIME (Local Interpretable Model-Agnostic Explanations) framework provides a complementary perspective to SHAP by focusing explicitly onlocal interpretability‚Äîthat is, understanding the reasoning behind individual model predictions. Unlike SHAP, which derives globally consistent feature attributions based on cooperative game theory, LIME approximates the complex behavior of a trained model in the vicinity of a specific instance by fitting a locally linear alternative substitute model. This approximation allows for the identification of the most influential features contributing to a given prediction, thereby offering interpretable, human-readable insights into the local decision boundaries of non-linear models.Specifically, in the context of electromagnetic field (EMF) strength prediction, LIME facilitates the examination of localized environmental, demographic, and infrastructural conditions that drive model predictions for specific spatial locations. This approach is particularly valuable for identifying context-sensitive deviations that may not be visible in more global XAI approaches like SHAP. By combining LIME‚Äôs fine-grained local analysis with SHAP‚Äôs globally consistent attributions, a comprehensive interpretability framework is achieved that can support both the validation of model behavior and the transparent communication of EMF prediction results for scientific and regulatory purposes. In correspondence to SHAP analysis, the following subsections present the LIME results for the top-performing ML models, k-Nearest Neighbors (kNN) and XGBoost.kNN LIME AnalysisConsecutive to the SHAP-based interpretability analysis, Local Interpretable Model-Agnostic Explanations (LIME) was employed in order to complement and examine feature contributions for the k-Nearest Neighbors (kNN) model. LIME was applied in a global aggregation framework, wherein instance-level explanations were averaged across the test set to obtain both absolute mean weightsFigure 10(indicating importance magnitude irrespective of direction) and signed mean weightsFigure 11(denoting whether features systematically increase or decrease EMF predictions). This dual representation provides a richer interpretability perspective, capturing the stability, strength, and polarity of each predictor‚Äôs influence on the urban electromagnetic field (EMF) estimations.Figure 10.LIME - kNN Global Absolute Values Feature Importance.Figure 11.LIME - kNN Global Signed Values Feature Importance.The LIME absolute mean weights analysis profile highlights Built_H (building height), SMOD (degree of urbanization), and Built_C_FUN (urban functional classification) as dominant drivers of kNN predictions, followed closely by POP (population density) and Built_C_MSZ (morphological settlement zoning), which collectively suggest that both vertical density and urban planning topology are central determinants in EMF variability. Built form intensity feature variables such as, Built_V (volume) and Built_S (surface area), exhibit smaller but nontrivial contributions, reflecting their secondary role in shaping propagation environments. Overall, the absolute LIME map reinforces the hypothesis that the kNN model relies primarily on local morphological and socio-spatial gradients, consistent with its distance-based learning nature.The signed LIME contributions (Figure 11) reveal coherent directional patterns. Higher values of Built_H, POP, and SMOD tend to increase EMF levels, consistent with urban densification leading to heightened wireless infrastructure density and reflective interference effects. In contrast, certain categories of Built_C_MSZ and low-rise built environments exhibit negative directional effects, suggesting attenuation or less intensive radiative environments in suburban or semi-urban contexts. The consistent polarity of Built_H and SMOD across most instances underscores their robust explanatory power in shaping EMF exposure gradients, while the mixed-sign behavior of built-form intensity variables (Built_S and Built_V) reflects context-dependent effects, likely driven by variations in infrastructure placement, open-space fragmentation, and various micro-propagation phenomena. Collectively, the LIME interpretations confirm the spatially-adaptive and locally sensitive nature of kNN, which leverages micro-scale morphological attributes to interpolate EMF levels.XGBoost LIME AnalysisLIME-based interpretability analysis was performed in order to elucidate how the XGBoost model leverages geospatial and morphological urban indicators to predict electromagnetic field (EMF) levels. The global mean absolute LIME weights (Figure 12) reveal that Built_V (built-up volume) and Built_H (building height) emerge as the most influential spatial predictors, followed closely by POP (population density) and SMOD (degree of urbanisation). This dominance indicates that three-dimensional urban morphology and population intensity act as primary determinants shaping EMF variability. The importance of Built_V and Built_H implies a strong association between vertical structural complexity and electromagnetic propagation dynamics, aligning well with radiofrequency propagation theory wherein higher and denser built forms induce multipath effects, signal obstruction, and power variations. Similarly, the prominence of POP and SMOD underscores that densely populated urban cores, which tend to exhibit intensive telecommunication activity and infrastructure concentration, drive higher EMF exposure levels.Figure 12.LIME - XGBoost Global Absolute Values Feature Importance.The signed LIME weights (Figure 13) provide insight into directionality. SMOD, POP, and Built_H predominantly exhibit positive signed effects, suggesting that higher values of these features consistently increase model predictions‚Äîi.e., contribute to elevated EMF intensity. In contrast, Built_V demonstrates a mixed effect profile: while highly ranked in absolute magnitude, its signed values oscillate around zero across spatial samples, indicating heterogeneous influence that varies by urban context. This pattern reflects physical reality, where vertical density may either attenuate or amplify field strength depending on line-of-sight availability, antenna configuration, and propagation pathways. Built_C_MSZ (settlement morphology classification) and Built_C_FUN (functional class of built space) appear less influential overall but still contribute meaningfully to local decision boundaries, particularly in delineating residential vs. non-residential urban fabric, which indirectly governs telecom antenna distribution and usage patterns.Figure 13.LIME‚ÄîXGBoost Global Signed Values Feature Importance.Collectively, the LIME results demonstrate that the XGBoost model learns coherent, physically interpretable associations grounded in urban morphology structure allocation and population density. The convergence of high absolute importance for Built_V, Built_H, POP, and SMOD across diverse local samples reinforces the robustness of these predictors as primary drivers of EMF exposure spatial heterogeneity. The subtle positive/negative balance for some variables highlights the context-dependent nature of RF propagation, where interactions between terrain, built form, and antenna topology produce nonlinear effects. These findings validate the model‚Äôs capacity to reflect real-world physical propagation mechanisms while also providing transparent local explanations that support regulatory, environmental, and urban planning applications.LIME DiscussionThe LIME analysis provides complementary insights into the interpretive behavior of the kNN and XGBoost models by examining feature contributions at the individual-prediction level. Similar to the SHAP findings, both models consistently identify key built-environment and demographic variables, particularly Built_H (building height), Built_V (built volume), Built_S (built surface area) and POP (population density), as the most dominant predictors of EMF intensity. This convergence reinforces the central role of vertical urban form structures and population concentration in shaping radio-frequency propagation electrodynamics.For the kNN model, the largest LIME attribution magnitudes are associated with Built_H, SMOD, and Built_C_FUN, followed by POP and Built_C_MSZ. Signed contributions reveal context-dependent influences, where certain urban morphologies increase EMF strength while others attenuate it depending on local neighborhood topological similarity. This behavior is consistent with kNN‚Äôs instance-based learning paradigm, wherein predictions are strongly shaped by the characteristics of proximate observations. As a result, kNN exhibits more spatially heterogeneous and locally sensitive interpretability, making it particularly informative for micro-scale EMF variation in highly diverse urban settings.In contrast, XGBoost demonstrates a more structured and globally stable feature attribution pattern. Built_V and Built_H dominate the local explanations, with POP and SMOD also contributing consistently across instances. Directional effects for these variables align closely with already establised theoretical expectations: taller and denser urban environments, which often associated with increased antenna density and multipath propagation, exert strong influence on EMF levels. Compared to kNN, XGBoost yields smoother, more coherent interpretive profiles, reflecting its increased capacity to capture more complex nonlinear, system-level interactions rather than purely local distance-based analogies. Overall, LIME confirms that while kNN provides granular, location-specific interpretability, XGBoost delivers more consistent and physically aligned explanations, resulting in two complementary interpretive paradigms for urban EMF analysis. Together, these LIME results demonstrate that combining instance-level, context-sensitive explanations with broader model-based insights yields a more complete and operationally meaningful understanding of EMF prediction behavior, reinforcing the value of localized interpretability for urban exposure assessment and decision-support applications. 4.2.3. Comparative Analysis Explainable AI ModelsA comparative analysis evaluation of SHAP and LIME was conducted in order to elucidate the interpretability characteristics of the employed machine learning models and to assess the consistency, stability, and physical plausibility of feature attributions in the context of EMF field prediction. Both frameworks identified urban morphology and demographic intensity as primary drivers of EMF variability, with highly consistent emphasis across models on variables capturing three-dimensional built form and urbanisation. Specifically, Built_H (building height), Built_V (built volume), POP (population density), and SMOD (degree of urbanisation) emerged as dominant predictors under both methods, reinforcing that vertical density, infrastructure concentration, and population-driven network intensity jointly shape radiofrequency propagation in dense urban environments. Secondary variables such as Built_S, Built_C_MSZ, and Built_C_FUN provided additional structural context, though with comparatively weaker influence. This convergence across SHAP and LIME substantiates the physical plausibility of the learned EMF patterns, aligning model interpretability outcomes with underlying electromagnetic propagation theory.Despite this overarching consistency, SHAP and LIME exhibited complementary interpretive behaviors reflecting their methodological underpinnings. SHAP provided globally stable, additive attributions with clear decomposition of prediction contributions, enabling systematic quantification of global importance patterns while preserving local fidelity. This was particularly evident for XGBoost, where SHAP exposed coherent attenuation patterns associated with increasing building height and population density, alongside structured nonlinear interactions across the urban morphology. LIME, by contrast, emphasized high-resolution local interpretability, capturing spatially varying effects driven by neighborhood-specific morphology‚Äîparticularly pronounced in kNN, where instance-based reasoning produced heterogeneous but physically meaningful feature impacts tied to local similarity relationships. Thus, while SHAP offered robust and globally consistent explanations suitable for model benchmarking and scientific inference, LIME revealed contextual micro-dynamics valuable for localized environmental assessment and scenario-specific interpretation.Overall, the comparative analysis demonstrates that SHAP and LIME act as complementary interpretability mechanisms. SHAP excels in establishing global causal consistency and theoretical conformity, whereas LIME enhances granularity in local diagnostic interpretation. The combined deployment of both methods results in a comprehensive transparency framework, enabling robust validation of model behavior, improved credibility of EMF predictions, and enhanced interpretive utility for urban-scale electromagnetic exposure monitoring and decision-support applications.Table 9andFigure 14summarize the explainability results obtained across SHAP and LIME, where feature importance analysis of kNN and XGBoost ML methods consistently highlight Built_H, Built_V, and population density POP as the primary drivers of EMF field variability, with SMOD providing additional contextual differentiation of urban form topology. These findings confirm that vertical urban morphology and demographic intensity exert the strongest influence on RF propagation patterns, reinforcing the physical plausibility and robustness of the learned predictive relationships. As shown in the newly addedFigure 14, we summarize the feature-importance patterns across kNN and XGBoost models using a heatmap that integrates both SHAP and LIME evaluations. In order to visualize the qualitative feature-importance categories in a unified manner, we encoded each textual level into an ordinal numerical scale as follows:12345=Weak,=Weak‚ÄìModerate/Medium‚ÄìLow,=Moderate,=Moderate‚ÄìStrong/High,=Strong/VeryHigh.1=Weak,2=Weak‚ÄìModerate/Medium‚ÄìLow,3=Moderate,4=Moderate‚ÄìStrong/High,5=Strong/VeryHigh.Figure 14.Combined SHAP and LIME feature importance and consistency across kNN and XGBoost models.Table 9.Feature importance alignment across kNN and XGBoost models using SHAP and LIME.This mapping enables consistent graphical representation of importance intensity across models and interpretability methods.",
            "4.2.1. SHAP": "The SHAP (SHapley Additive exPlanations) framework offers a unified, model-agnostic approach for interpreting complex machine learning predictions by quantifying the marginal contribution of each feature to the model‚Äôs output. In particular, SHAP decomposes a model‚Äôs prediction into additive feature attributions, ensuring both local accuracy and global consistency across instances. Theglobal perspectiveaggregates feature contributions across all samples to identify the dominant predictors driving EMF variability at the population level. Conversely, thelocal perspectivefocuses on individual predictions, explaining how specific feature interactions influence the model‚Äôs output for a given observation. Specifically, in the context of electromagnetic field (EMF) strength prediction, the SHAP dual approach of the above two perspectives facilitate a comprehensive understanding of both the overall model behavior and case-specific explanatory patterns and so, enables a detailed examination of how spatial, demographic, and environmental variables-features influence model outputs, thereby providing transparent and theoretically grounded insights into the underlying decision mechanisms of the employed ML algorithms. In the subsequent subsections, SHAP results are presented for the top-performing ML models, including k-Nearest Neighbors (kNN) and XGBoost. The global SHAP summary plots (beeswarm plots) illustrate the average magnitude and direction of each feature‚Äôs contribution to EMF prediction, while local explanations (waterfall plots) reveal instance-specific deviations and contextual dependencies. This dual analysis enables the identification of consistent explanatory trends across models and environments, forming a robust interpretive basis for assessing predictive performance and environmental significance. kNN SHAP Analysis Figure 6(beeswarm plot) visualizes both the magnitude and directionality of each feature‚Äôs impact, whereasFigure 7(waterfall plot) illustrates the additive effect of the most influential variables for a representative prediction instance. The signed mean SHAP values reflect whether a feature predominantly increases or decreases the predicted electromagnetic field (EMF) strength, while the absolute mean SHAP values capture the overall influence magnitude, independent of direction. Figure 6.kNN SHAP Beeswarm Plot. Figure 7.kNN SHAP Waterfall Plot. Among all predictors, SMOD (Degree of Urbanisation) emerges as the most influential variable (Figure 7), displaying the strongest negative SHAP contribution on average. This suggests that areas classified with higher urbanisation degrees correspond to a relative decrease in predicted EMF strength within the kNN model‚Äôs local decision structure, possibly due to attenuation effects or signal diffusion associated with denser morphological configurations. Conversely, Built_C_MSZ, representing the Morphological Settlement Zone classification, exerts a positive influence, indicating that denser or morphologically structured settlements tend to elevate EMF predictions, which likely reflects a potential increased infrastructure and emitter density in such zones. The features Built_H (average building height), Built_S (built-up surface), and Built_V (built-up volume) also show moderate positive contributions, emphasizing the importance of vertical and horizontal development intensity in shaping EMF propagation, whereas the feature variable POP (population density) appears to have a relatively minor but still meaningful impact, consistent with the idea that population distribution serves as an indirect indication for built infrastructure. Finally, Built_C_FUN (residential vs. non-residential functional classification) demonstrates minimal impact, suggesting that functional land-use type alone does not strongly determine EMF variations once structural characteristics are accounted for. Overall, the SHAP-based interpretability analysis reveals that urban morphology and spatial structural variables, particularly the degree of urbanisation, settlement delineation, and built environment metrics, constitute the most decisive factors in the kNN model‚Äôs EMF strength predictions. This evidence underscores the model‚Äôs sensitivity to fine-grained geospatial geometric attributes rather than purely demographic factors. From a methodological standpoint, SHAP confirms that the kNN model captures locally heterogeneous spatial geometrical dependencies, reinforcing the value of spatially explicit variables in EMF modeling frameworks. XGBoost SHAP Analysis The summary of the XGBoost SHAP analysis as depicted in (beeswarm) plot inFigure 8visualizes the distribution of SHAP values across all test samples, thereby quantifying both the magnitude and direction of each feature‚Äôs effect on the predicted EMF intensity. Meanwhile, the waterfall plot inFigure 9illustrates a single prediction instance, showing how the cumulative contributions of features shift the prediction from the baseline expectationùê∏[ùëì(ùëã)]E[f(X)]to the final model outputùëì(ùë•)f(x). The signed mean SHAP values reveal whether a feature tends to increase or decrease the predicted EMF, while the absolute mean SHAP values highlight its overall importance in shaping the model output. Figure 8.XGBoost SHAP Beeswarm Plot. Figure 9.XGBoost SHAP Waterfall Plot. Among the examined predictors, Built_H (building height), POP (population density), and SMOD (degree of urbanisation) emerge as the most influential variables, as clearly illustrated inFigure 9, in determining the model‚Äôs predictions. The SHAP values indicate that higher building heights and higher population densities generally lead to a decrease in predicted EMF levels, as shown by the negative SHAP contributions for these variables. This pattern can be attributed to the signal attenuation and scattering effects introduced by taller structures and densely populated urban cores, which can obstruct direct line-of-sight propagation. Similarly, SMOD, representing the urbanisation level, exerts a negative influence, suggesting that highly urbanized zones, often characterized by dense building clusters and infrastructure, tend to experience lower EMF intensities at ground level due to increased multipath fading and signal absorption. On the other hand, features related to morphological classification, such as the variables Built_C_MSZ (Morphological Settlement Zone) and Built_C_FUN (functional classification of the built environment), exhibit comparatively smaller yet non-negligible impacts. The weak positive or near-zero contributions of these variables suggest that while the categorical partitioning of urban morphology provides structural context, it alone does not substantially modify EMF predictions beyond the physical characteristics that already captured by building height and surface data. Similarly, Built_S (built-up surface) and Built_V (built-up volume) show marginal contributions, reinforcing the idea that geometric and structural aspects of the urban fabric, although interacting with EMF propagation mechanisms, still remain secondary to the building height and population-related attenuation factors. Overall, the SHAP analysis confirms that structural and demographic urban characteristics jointly govern EMF distribution, with XGBoost effectively capturing the nonlinear dependencies between physical urban morphology and exposure levels. The model assigns higher importance to attenuative factors (e.g., height, density) rather than purely spatial classifications, aligning well with established radio propagation theory. From an interpretability standpoint, these results validate the physical plausibility of the XGBoost model‚Äôs learning behavior, where the identified feature effects are consistent with empirical observations in urban electromagnetic environments, thereby enhancing confidence in its predictive transparency and reliability. SHAP Discussion Comparative dual SHAP interpretations reveal that the k-Nearest Neighbors (kNN) and XGBoost models achieve similar predictive accuracy but through fundamentally different explanatory mechanisms. The kNN model exhibits heightened sensitivity to spatial and morphological-topological structure, relying on neighborhood similarity in the feature space to infer electromagnetic field (EMF) strength. Its predictions are primarily influenced by local contextual features, particularly morphological indicators such as urban form and settlement classification, reflecting the model‚Äôs instance-based reasoning and dependence on spatial geometric distance-based proximity. In contrast, the XGBoost model emphasize on physical attenuation dynamics and quantitative urban metrics, capturing complex nonlinear interactions and cumulative effects among structural and demographic variables. This behavior aligns more closely with theoretical signal propagation principles, as the model generalizes more effectively across heterogeneous urban settings by learning hierarchical relationships rather than specific localized associations. Together, these complementary interpretability profiles demonstrate that kNN encodes better upon spatial similarity while XGBoost learns more effective parametric attenuation functions, two distinct but yet potential synergistic approaches to modeling EMF variability. Integrating both local (kNN-driven) and global (XGBoost-driven) interpretability perspectives yields a more comprehensive and physically consistent understanding of EMF behavior in urban environments, thereby advancing transparent, data-driven methodologies for environmental monitoring and urban planning.",
            "4.2.2. LIME": "The LIME (Local Interpretable Model-Agnostic Explanations) framework provides a complementary perspective to SHAP by focusing explicitly onlocal interpretability‚Äîthat is, understanding the reasoning behind individual model predictions. Unlike SHAP, which derives globally consistent feature attributions based on cooperative game theory, LIME approximates the complex behavior of a trained model in the vicinity of a specific instance by fitting a locally linear alternative substitute model. This approximation allows for the identification of the most influential features contributing to a given prediction, thereby offering interpretable, human-readable insights into the local decision boundaries of non-linear models. Specifically, in the context of electromagnetic field (EMF) strength prediction, LIME facilitates the examination of localized environmental, demographic, and infrastructural conditions that drive model predictions for specific spatial locations. This approach is particularly valuable for identifying context-sensitive deviations that may not be visible in more global XAI approaches like SHAP. By combining LIME‚Äôs fine-grained local analysis with SHAP‚Äôs globally consistent attributions, a comprehensive interpretability framework is achieved that can support both the validation of model behavior and the transparent communication of EMF prediction results for scientific and regulatory purposes. In correspondence to SHAP analysis, the following subsections present the LIME results for the top-performing ML models, k-Nearest Neighbors (kNN) and XGBoost. kNN LIME Analysis Consecutive to the SHAP-based interpretability analysis, Local Interpretable Model-Agnostic Explanations (LIME) was employed in order to complement and examine feature contributions for the k-Nearest Neighbors (kNN) model. LIME was applied in a global aggregation framework, wherein instance-level explanations were averaged across the test set to obtain both absolute mean weightsFigure 10(indicating importance magnitude irrespective of direction) and signed mean weightsFigure 11(denoting whether features systematically increase or decrease EMF predictions). This dual representation provides a richer interpretability perspective, capturing the stability, strength, and polarity of each predictor‚Äôs influence on the urban electromagnetic field (EMF) estimations. Figure 10.LIME - kNN Global Absolute Values Feature Importance. Figure 11.LIME - kNN Global Signed Values Feature Importance. The LIME absolute mean weights analysis profile highlights Built_H (building height), SMOD (degree of urbanization), and Built_C_FUN (urban functional classification) as dominant drivers of kNN predictions, followed closely by POP (population density) and Built_C_MSZ (morphological settlement zoning), which collectively suggest that both vertical density and urban planning topology are central determinants in EMF variability. Built form intensity feature variables such as, Built_V (volume) and Built_S (surface area), exhibit smaller but nontrivial contributions, reflecting their secondary role in shaping propagation environments. Overall, the absolute LIME map reinforces the hypothesis that the kNN model relies primarily on local morphological and socio-spatial gradients, consistent with its distance-based learning nature. The signed LIME contributions (Figure 11) reveal coherent directional patterns. Higher values of Built_H, POP, and SMOD tend to increase EMF levels, consistent with urban densification leading to heightened wireless infrastructure density and reflective interference effects. In contrast, certain categories of Built_C_MSZ and low-rise built environments exhibit negative directional effects, suggesting attenuation or less intensive radiative environments in suburban or semi-urban contexts. The consistent polarity of Built_H and SMOD across most instances underscores their robust explanatory power in shaping EMF exposure gradients, while the mixed-sign behavior of built-form intensity variables (Built_S and Built_V) reflects context-dependent effects, likely driven by variations in infrastructure placement, open-space fragmentation, and various micro-propagation phenomena. Collectively, the LIME interpretations confirm the spatially-adaptive and locally sensitive nature of kNN, which leverages micro-scale morphological attributes to interpolate EMF levels. XGBoost LIME Analysis LIME-based interpretability analysis was performed in order to elucidate how the XGBoost model leverages geospatial and morphological urban indicators to predict electromagnetic field (EMF) levels. The global mean absolute LIME weights (Figure 12) reveal that Built_V (built-up volume) and Built_H (building height) emerge as the most influential spatial predictors, followed closely by POP (population density) and SMOD (degree of urbanisation). This dominance indicates that three-dimensional urban morphology and population intensity act as primary determinants shaping EMF variability. The importance of Built_V and Built_H implies a strong association between vertical structural complexity and electromagnetic propagation dynamics, aligning well with radiofrequency propagation theory wherein higher and denser built forms induce multipath effects, signal obstruction, and power variations. Similarly, the prominence of POP and SMOD underscores that densely populated urban cores, which tend to exhibit intensive telecommunication activity and infrastructure concentration, drive higher EMF exposure levels. Figure 12.LIME - XGBoost Global Absolute Values Feature Importance. The signed LIME weights (Figure 13) provide insight into directionality. SMOD, POP, and Built_H predominantly exhibit positive signed effects, suggesting that higher values of these features consistently increase model predictions‚Äîi.e., contribute to elevated EMF intensity. In contrast, Built_V demonstrates a mixed effect profile: while highly ranked in absolute magnitude, its signed values oscillate around zero across spatial samples, indicating heterogeneous influence that varies by urban context. This pattern reflects physical reality, where vertical density may either attenuate or amplify field strength depending on line-of-sight availability, antenna configuration, and propagation pathways. Built_C_MSZ (settlement morphology classification) and Built_C_FUN (functional class of built space) appear less influential overall but still contribute meaningfully to local decision boundaries, particularly in delineating residential vs. non-residential urban fabric, which indirectly governs telecom antenna distribution and usage patterns. Figure 13.LIME‚ÄîXGBoost Global Signed Values Feature Importance. Collectively, the LIME results demonstrate that the XGBoost model learns coherent, physically interpretable associations grounded in urban morphology structure allocation and population density. The convergence of high absolute importance for Built_V, Built_H, POP, and SMOD across diverse local samples reinforces the robustness of these predictors as primary drivers of EMF exposure spatial heterogeneity. The subtle positive/negative balance for some variables highlights the context-dependent nature of RF propagation, where interactions between terrain, built form, and antenna topology produce nonlinear effects. These findings validate the model‚Äôs capacity to reflect real-world physical propagation mechanisms while also providing transparent local explanations that support regulatory, environmental, and urban planning applications. LIME Discussion The LIME analysis provides complementary insights into the interpretive behavior of the kNN and XGBoost models by examining feature contributions at the individual-prediction level. Similar to the SHAP findings, both models consistently identify key built-environment and demographic variables, particularly Built_H (building height), Built_V (built volume), Built_S (built surface area) and POP (population density), as the most dominant predictors of EMF intensity. This convergence reinforces the central role of vertical urban form structures and population concentration in shaping radio-frequency propagation electrodynamics. For the kNN model, the largest LIME attribution magnitudes are associated with Built_H, SMOD, and Built_C_FUN, followed by POP and Built_C_MSZ. Signed contributions reveal context-dependent influences, where certain urban morphologies increase EMF strength while others attenuate it depending on local neighborhood topological similarity. This behavior is consistent with kNN‚Äôs instance-based learning paradigm, wherein predictions are strongly shaped by the characteristics of proximate observations. As a result, kNN exhibits more spatially heterogeneous and locally sensitive interpretability, making it particularly informative for micro-scale EMF variation in highly diverse urban settings. In contrast, XGBoost demonstrates a more structured and globally stable feature attribution pattern. Built_V and Built_H dominate the local explanations, with POP and SMOD also contributing consistently across instances. Directional effects for these variables align closely with already establised theoretical expectations: taller and denser urban environments, which often associated with increased antenna density and multipath propagation, exert strong influence on EMF levels. Compared to kNN, XGBoost yields smoother, more coherent interpretive profiles, reflecting its increased capacity to capture more complex nonlinear, system-level interactions rather than purely local distance-based analogies. Overall, LIME confirms that while kNN provides granular, location-specific interpretability, XGBoost delivers more consistent and physically aligned explanations, resulting in two complementary interpretive paradigms for urban EMF analysis. Together, these LIME results demonstrate that combining instance-level, context-sensitive explanations with broader model-based insights yields a more complete and operationally meaningful understanding of EMF prediction behavior, reinforcing the value of localized interpretability for urban exposure assessment and decision-support applications.",
            "4.2.3. Comparative Analysis Explainable AI Models": "A comparative analysis evaluation of SHAP and LIME was conducted in order to elucidate the interpretability characteristics of the employed machine learning models and to assess the consistency, stability, and physical plausibility of feature attributions in the context of EMF field prediction. Both frameworks identified urban morphology and demographic intensity as primary drivers of EMF variability, with highly consistent emphasis across models on variables capturing three-dimensional built form and urbanisation. Specifically, Built_H (building height), Built_V (built volume), POP (population density), and SMOD (degree of urbanisation) emerged as dominant predictors under both methods, reinforcing that vertical density, infrastructure concentration, and population-driven network intensity jointly shape radiofrequency propagation in dense urban environments. Secondary variables such as Built_S, Built_C_MSZ, and Built_C_FUN provided additional structural context, though with comparatively weaker influence. This convergence across SHAP and LIME substantiates the physical plausibility of the learned EMF patterns, aligning model interpretability outcomes with underlying electromagnetic propagation theory. Despite this overarching consistency, SHAP and LIME exhibited complementary interpretive behaviors reflecting their methodological underpinnings. SHAP provided globally stable, additive attributions with clear decomposition of prediction contributions, enabling systematic quantification of global importance patterns while preserving local fidelity. This was particularly evident for XGBoost, where SHAP exposed coherent attenuation patterns associated with increasing building height and population density, alongside structured nonlinear interactions across the urban morphology. LIME, by contrast, emphasized high-resolution local interpretability, capturing spatially varying effects driven by neighborhood-specific morphology‚Äîparticularly pronounced in kNN, where instance-based reasoning produced heterogeneous but physically meaningful feature impacts tied to local similarity relationships. Thus, while SHAP offered robust and globally consistent explanations suitable for model benchmarking and scientific inference, LIME revealed contextual micro-dynamics valuable for localized environmental assessment and scenario-specific interpretation. Overall, the comparative analysis demonstrates that SHAP and LIME act as complementary interpretability mechanisms. SHAP excels in establishing global causal consistency and theoretical conformity, whereas LIME enhances granularity in local diagnostic interpretation. The combined deployment of both methods results in a comprehensive transparency framework, enabling robust validation of model behavior, improved credibility of EMF predictions, and enhanced interpretive utility for urban-scale electromagnetic exposure monitoring and decision-support applications. Table 9andFigure 14summarize the explainability results obtained across SHAP and LIME, where feature importance analysis of kNN and XGBoost ML methods consistently highlight Built_H, Built_V, and population density POP as the primary drivers of EMF field variability, with SMOD providing additional contextual differentiation of urban form topology. These findings confirm that vertical urban morphology and demographic intensity exert the strongest influence on RF propagation patterns, reinforcing the physical plausibility and robustness of the learned predictive relationships. As shown in the newly addedFigure 14, we summarize the feature-importance patterns across kNN and XGBoost models using a heatmap that integrates both SHAP and LIME evaluations. In order to visualize the qualitative feature-importance categories in a unified manner, we encoded each textual level into an ordinal numerical scale as follows:12345=Weak,=Weak‚ÄìModerate/Medium‚ÄìLow,=Moderate,=Moderate‚ÄìStrong/High,=Strong/VeryHigh.1=Weak,2=Weak‚ÄìModerate/Medium‚ÄìLow,3=Moderate,4=Moderate‚ÄìStrong/High,5=Strong/VeryHigh. Figure 14.Combined SHAP and LIME feature importance and consistency across kNN and XGBoost models. Table 9.Feature importance alignment across kNN and XGBoost models using SHAP and LIME. This mapping enables consistent graphical representation of importance intensity across models and interpretability methods.",
            "5. Conclusions": "This study conducted a rigorous, head-to-head evaluation of six machine-learning model paradigms for urban electric field (EMF) strength prediction, Neural Networks (NN), k-Nearest Neighbors (kNN), Decision Trees (DTs), Random Forest (RF), LightGBM, and XGBoost, augmented with model-agnostic interpretability via SHAP and LIME. Using cross-validated root mean squared error (RMSE) as the principal performance evaluation metric, we found a clear accuracy hierarchy: kNN achieved the lowest error (mean RMSE = 0.623153), followed by XGBoost (0.711201), LightGBM (0.716689), Random Forest (0.757916), Decision Trees (0.812665), and Neural Networks (0.909277). While the two gradient-boosting ensembles approached kNN‚Äôs accuracy with stronger scalability properties, the instance-based learner retained a decisive advantage on the present dataset, indicating a pronounced role of local similarity structure in EMF spatial variability. The interpretability analysis converged on a consistent physical interpretation. Across the top performing model methods, Built_H (building height), Built_V (built volume), POP (population density), and SMOD (degree of urbanisation) emerged as primary determinants of EMF levels, with Built_S, Built_C_MSZ, and Built_C_FUN providing secondary structural context attributions. SHAP exposed globally stable attribution patterns, particularly for XGBoost, highlighting attenuation and propagation effects tied to vertical density and demographic intensity, whereas LIME illuminated locally contingent behaviors, especially for kNN, where neighborhood-specific morphology modulates predictions. The alignment of SHAP and LIME with radio-propagation intuition strengthens confidence in the learned relations and supports the scientific validity of the models‚Äô decision processes. kNN provides the highest accuracy and strong local adaptability, although it can be memory-intensive at large scales. XGBoost and LightGBM offer an effective balance of accuracy, robustness, and inference speed, provided that regularization is carefully tuned. Random Forest remains a reliable baseline but is less accurate than boosting methods, while single Decision Trees and the tested Neural Networks underperformed in complex urban environments. These findings have practical implications for EMF exposure assessment. When maximum accuracy is required and computational resources allow, distance-weighted kNN with moderate neighborhood sizes (ùëò‚âà10k‚âà10‚Äì25) is recommended. For applications prioritizing scalability and generalization, XGBoost and LightGBM provide near-optimal accuracy with faster performance. The consistent influence of building height, built volume, population density, and urbanization degree further highlights their importance in shaping EMF patterns. In summary, this study establishes a strong benchmark for interpretable EMF modeling. kNN achieved the best accuracy, XGBoost and LightGBM offered scalable high performance, and the SHAP‚ÄìLIME analysis yielded coherent, physically meaningful explanations. Together, these results support more transparent EMF monitoring and guide future improvements in data enrichment, model design, and urban planning applications."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2079-9292/14/23/4766",
        "scraped_at": "2025-12-05 23:57:09"
    },
    {
        "title": "A Multifocal RSSeg Approach for Skeletal Age Estimation in an Indian Medicolegal Perspective",
        "authors": "byPriyanka Manchegowda,Manohar Nageshmurthy,Suresha RajuandDayananda Rudrappa",
        "journal": "Algorithms2025,18(12), 765; https://doi.org/10.3390/a18120765 (registering¬†DOI) - 4 Dec 2025",
        "abstract": "Estimating bone age is essential for accurate diagnoses, appropriate care based on biological age, and fairness in legal matters. In the Indian medicolegal context, determining age through a clinical approach involves analyzing multiple joints; however, the traditional method can be tedious and subjective, relying heavily on human expertise, which may lead to biased decisions in age-related legal disputes. Moreover, commonly used radiographs often exhibit pixel-level variations due to heterogeneous contrast, which complicate segmentation tasks and lead to inconsistencies and reduced model performance. The study presents a multifocal region-based symbolic segmentation technique to automatically retain the soft-tissue region that harbors a growth pattern of an ossification center. Experimental results demonstrate an 84.5% Jaccard similarity, an 81.4% Dice coefficient, an 88.3% precision, a 90.0% recall, and a 91.5% pixel accuracy on a novel multifocal dataset of Indian inhabitants. The proposed segmentation technique outperforms U-Net, Attention U-Net, TransU-Net, DeepLabV3+, Adaptive Otsu, and Watershed segmentation in terms of accuracy, indicating strong generalizability across joints and improving reliability. Compared with 86.4% without segmentation, the proposed integration of segmentation with VGG16 classification increases the overall accuracy to 93.8%, demonstrating that target-focused-region processing reduces unnecessary computations and improves feature discrimination without sacrificing accuracy.Keywords:multifocal radiographs;region-based symbolic segmentation;age estimation;Indian medicolegal;SDG 16-justice",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "The skeletal maturity assessment is essential for maintaining medicolegal investigations, assessing growth patterns, and diagnosing and treating endocrinological issues. From infancy to adulthood, a person‚Äôs bones undergo continuous changes in ossification centers, epiphyseal growth, and fusion. Following these morphological patterns is a reliable biological indicator of age. In the Indian medicolegal context, age estimation is typically inferred to be 1‚Äì21 years by examining the appearance and fusion patterns of various ossification centers in multiple joints‚Äîparticularly the wrist, elbow, shoulder, and pelvis [1,2]. These four joints collectively offer substantial diagnostic value, since they cover a wide range of maturation: early ossification changes are prominent in the wrist, reflections of transitional stages in the elbow and shoulder are noticeable, and the pelvis joint undergoes the best fusion process later. Although significant, existing automated systems predominantly focus on wrist radiographs, limiting their generalizability and applicability in forensic scenarios involving the Indian population. Despite the rise of deep learning and vision-based techniques for skeletal age determination, their performance remains limited by two main factors: poor representation of the relevant anatomical region of interest (ROI) and noisy, heterogeneous data that influence feature extraction. Precise segmentation is accordingly essential, as it separates the ossification center and discards overwhelming background layouts, encouraging the model to learn discriminative skeletal features more effectively. Without segmentation, the automated age estimation system tends to underperform, especially when input radiographs present challenges such as non-standard posture, plaster, and implants to treat fractured regions, varying contrast and illumination, radiological noise and arti-facts, and intra- and inter-class variations in background structures. The literature reveals that limited publicly available datasets exist, including those from the Radiological Society of North America (RSNA), the Digital Hand Atlas (DHA), and MURA, all originating from Western populations and predominantly containing wrist images. However, skeletal development factors, such as race, diet, sex [3], geographical location, nutritional status, physical activity [4,5], hormone imbalances, and metabolic abnormalities, significantly influence bone growth; investigators may detect differences in the appearance of ossification centers and the fusion process [1]. Studies show that populations of Asian and African descent generally exhibit more extended skeletal development than European groups, resulting in prominent ethnic variation in growth rates. Genetic roots in biological variations and environmental factors influence young children‚Äôs bone health, leading to racial differences in skeletal maturation that account for the variation [3,4,5,6,7,8]. Hence, Western reference data and models cannot be directly applied to Indian medicolegal evaluations, creating a critical gap and a pressing need to curate multi-joint datasets representing the Indian population, especially for forensic age estimation, where legal consequences demand high reliability. For sophisticated medical procedures and computer-aided diagnosis, segmenting medical images is a crucial preprocessing step, enabling efficient feature extraction by locating key structures that improve diagnostic effectiveness. Extracting anatomical cues from particular ossification centers rather than whole X-ray images greatly increases interpretability and accuracy for age estimation. Nevertheless, asFigure 1illustrates, medical segmentation in this field remains challenging. These elements highlight the need for a robust segmentation pipeline that can differentiate between soft and hard tissue boundaries with anatomical consistency, enabling the model to learn age-dependent skeletal features more effectively. To address these gaps, the proposed work introduces a comprehensive medicolegal-relevant solution for automated estimation from an Indian perspective. The significant contributions are as follows: A novel Indian dataset of wrist, elbow, shoulder, and pelvis X-rays highlights the population‚Äôs specific skeletal features, curated in accordance with ethical guidelines;A region-based symbolic segmentation (RSSeg) hierarchical patch-processing segmentation preserves soft-tissue cues to accurately retain ossification centers across multiple joints at an early stage;A classification model focuses on extracting anatomical cues from the segmented region, enhancing robustness, reliability, and generalizability for suitable medicolegal age estimation. Figure 1.Image samples of challenging conditions encountered during data acquisition: (a) non-standard pose; (b) plaster of Paris on the ROI; (c) fracture on the ossification joint; (d) implants on the ROI; (e) heterogeneous contrast; and (f) radiological artifacts. Figure 1.Image samples of challenging conditions encountered during data acquisition: (a) non-standard pose; (b) plaster of Paris on the ROI; (c) fracture on the ossification joint; (d) implants on the ROI; (e) heterogeneous contrast; and (f) radiological artifacts. The paper‚Äôs organization begins with a brief introduction and a thorough review of relevant research on skeletal age determination inSection 2.Section 3presents the proposed multifocal RSSeg model architecture and its detailed description. Further, the experiment and results are presented inSection 4, followed by a discussion. Finally,Section 5concludes by summarizing the paper and offering closing remarks.",
            "2. Related Work": "The literature focuses on developing a generalized segmentation model that is essential for precise bone age estimation, ROI capture, and the generation of critical clinical information for assessing skeletal maturity. Differences between chronological and bone ages indicate that bone growth is not progressing properly, which can help identify endocrine disorders and other metabolic or genetic problems [2,8]. Clinicians determine bone age by examining the ossification centers of the carpal bones and evaluating the emergence and fusion of epiphyses in both short and long tubular bones [8]. In recent years, many datasets have been made available for estimating age through bones. However, they often have only a few samples and can be hard for researchers to access when they want to build a robust system. Studies employ imaging techniques across retrospective datasets: RSNA [8], DHA [9], PACS [10]: wrist X-rays, teeth [11], elbow [12], shoulder X-ray (MURA) [13,14], shoulder 3.0 T MRI [15], pelvis radiographs [16,17], and pelvis CT [18]. However, these issues persist when using these datasets, which present several challenges, including uneven illumination, non-standard poses, blurriness, varying noise, and heterogeneous contrast, all of which can complicate the normalization process [8,9,10,18,19]. To overcome these dataset challenges, this study highlights several preprocessing techniques that enhance image quality, including histogram processing [16], gamma transformation [19], Contrast-Limited Adaptive Histogram Equalization (CLAHE) [8], and generative adversarial networks [20]. Multiple 2D transformation techniques‚Äîincluding angle rotation, zooming, and shearing‚Äîrestructure diverse hand poses. To balance training data in specific learning models, up-sampling techniques such as cropping, translation, flipping, and rotation address class imbalance [21]. Bilinear interpolation and Gaussian pyramid are used to scale the image, thereby avoiding information loss [22]. Grey-scale and min-max normalization regularize pixel values in X-rays [23]. The median filter, anisotropic diffusion filter, and wavelet packet decomposition suppress background noise through effective elimination [10]. The machine learning-based segmentation approaches, including threshold, region, edge, and cluster-based methods, focus on extracting the hand region. The watershed segmentation extracts the hand mask by first applying a Gaussian pyramid, a Sobel filter, and a median filter, then selecting the largest connected object [22]. Improved Adaptive Otsu thresholding segmentation, introduced by an optimization algorithm used on RSNA and manual datasets, yields better performance [24]. Texture-based Adaptive Crossed-reconstruction k-means clustering yields better homogeneity by varying k-values in the k-means cluster [25]. Template-matching-based Particle Swarm Optimization automates bone image segmentation by extracting edges [26]. Constrained Local Models integrate local models with global constraints to segment the hand region by examining intensity patterns at several landmark locations, defining the hand‚Äôs boundary, and predicting similar intensities using template matching [27]. Active shape model joint segmentation automatically crops the hand contour by grouping similar pixels using k-means clustering [28]. EMROI and CROI segmentation suppress the radiological marks, and hand rotation improves the segmentation performance [29]. Deep learning-based segmentation automatically identifies hand regions, achieving remarkable performance in separating the background from the foreground. Optimized U-Net deals with multi-objective segmentation using the Whale-based Class Topper Optimization activation function [30]. Replace the U-Net‚Äôs encoder with a VGG16 encoder pre-trained on ImageNet, and then, intentionally weaken the entire network to optimize mask generation [20]. Mask R-CNN includes a parallel branch to predict hand masks, which dramatically improves accuracy by utilizing a RoIAlign layer to address misalignment [31]. Modified k-means clustering segments the epiphysis bones by removing background noise via histogram equalization. The Hotelling T2 model and ADF suppress background noise. WPD extracts edges from the auto-cropped EROI and CROI regions of the H-image, obtained via texture and cluster analysis, which performs better than Iterative Thresholding and Adaptive K-Means Clustering [32]. To accomplish hand-semantic segmentation, Fully Connected DenseNet employs three transition-up and three transition-down blocks [33]. A Faster R-CNN to identify bone locations and the Global-Local Fusion Segmentation Network integrates global and local context for overlapping bones and extracts the ROI in a dataset of elbow radiographs [12]. Attention U-Net, Swin U-Net, and U-Net underwent five-fold cross-validation training using histogram-equalized pelvis X-rays. Among them, Swin U-Net achieved a specificity of 98.50%, indicating exceptional performance [18]. The comprehensive review emphasizes that there is a lack of studies on vision-based automation in the Indian medicolegal context, due to the limited availability of datasets for key growth regions, including the wrist, elbow, shoulder, and pelvis. Identifying ossification centers in these joints remains a complex process due to intra- and inter-pixel variations that obscure their appearance and to fusion conditions that overlap with the background, affecting the accuracy of age estimation. Therefore, developing a generalized segmentation model is crucial for identifying specific growth features while preserving the relevant foreground data across multiple joints for the medicolegal framework. In addition, this review not only highlights the importance of multi-joint datasets but also highlights the need for robust, reliable, and generalizable segmentation in skeletal age estimation solutions tailored to diverse anatomical structures and population-specific variations.",
            "3. Proposed Methodology": "This section details the development of a multifocal RSSeg approach for skeletal age estimation based on region-specific pixel variations in ossified joints and soft tissues, thereby capturing growth patterns and automating the process through enhanced techniques.Figure 2shows the proposed multifocal RSSeg approach for skeletal age estimation. Figure 2.The proposed multifocal region-based symbolic segmentation approach for skeletal age estimation from an Indian medicolegal perspective. 3.1. PreprocessingThe collected radiological image dataset faces several challenges, including noise and illumination. The methodology uses Gaussian filters to mitigate noise-induced image distortion [10], as shown in Equation (1):ùê∫(ùë•,ùë¶)=(1/(2ùúãùúé^2))ùëí^(‚àí(ùë•^2+ùë¶^2)/(2ùúé^2))G(x,y)=(1/(2œÄœÉ^2))e^(‚àí(x^2+y^2)/(2œÉ^2))(1)where theùê∫(ùë•,ùë¶)Gx,ydenotes the Gaussian function value at the point(ùë•,ùë¶)x,y, where(ùë•,ùë¶)x,yrepresents the coordinate of the filter, and sigma is the standard deviation of the Gaussian distribution.The CLAHE method enhances image contrast [8] by locally adjusting pixel intensities within small regions, called tiles. It clips the histogram to prevent noise amplification, redistributes overused pixels evenly and performs histogram equalization to provide clearer visibility of bone structures. Subsequently, bilinear interpolation is applied to combine the processed tiles, resulting in enhanced contrast of bone features without introducing artifacts, which significantly improved the model‚Äôs performance.The preprocessing technique involves comparing the original sample image with the enhanced image, as shown inFigure 3. Additionally, Equations (2) and (3) represent the mathematical formula for CLAHE.ùëî(ùë•,ùë¶)=[ùê∂ùê∑ùêπ(ùëì(ùë•,ùë¶))‚àíùê∂ùê∑ùêπùëöùëñùëõùëáùë•√óùëáùë¶‚àíùê∂ùê∑ùêπùëöùëñùëõ√ó(ùêø‚àí1)]gx,y=CDFfx,y‚àíCDFminTx√óTy‚àíCDFmin√óL‚àí1(2)ùêª‚Ä≤(ùëñ)=min(ùêª(ùëñ),ùê∂)+ùê∏ùêøH‚Ä≤i=minHi,C+EL(3)where the Cumulative Distribution Function(ùê∂ùê∑ùêπ)CDFis computed from the clipped histogramùêª‚Ä≤(ùëñ)H‚Ä≤i,ùêª(ùëñ)Hiis the histogram,ùê∂Cis the clip limit,ùëñirepresents the pixel intensity, the excess pixelsùê∏Eare redistributed evenly across all bins,ùêøLis the total intensity levels,ùëáùë•√óùëáùë¶Tx√óTyis the local tile size, andùê∂ùê∑ùêπùëöùëñùëõCDFminis the minimumùê∂ùê∑ùêπCDFvalue in the tile.Figure 3.Sample input images of multiple joints: (a) before enhancement; (b) after enhancement via CLAHE. 3.2. Region-Based Symbolic SegmentationSegmentation plays a vital role in identifying the specific age group for the Indian medicolegal process. The region-based symbolic segmentation process involves separating an image into two distinct regions: the foreground (the bone region) and the background [9,34]. The challenges, such as illumination and homogeneous contrast among bone, tissues, and artifacts, lead to intra- and inter-class similarity between bone and other background information, making the segmentation task difficult [8]. In medicolegal cases, the appearance and fusion of ossification centers are crucial in determining approximate age groups. The conventional segmentation approach overlooks vital details of various joint ossification centers due to uncertainty and the soft boundary between bone and background, resulting in the loss of the essential bone pattern.To preserve the appearance and fusion of ossification centers, the RSSeg technique efficiently handles ambiguous boundaries by capturing smooth variations within and between pixels. The use of mean and standard deviation of local patches to represent uncertainty in interval data while maintaining ossification center patterns, thereby improving segmentation efficiency.The RSSeg model efficiently retains the soft-tissue region, which exhibits a growth pattern of the ossification center, aiding age estimation. Initially, divide the enhanced image into a 10 √ó 10 grid of patches, resulting in a total of 100 patches, ranging from 1 to 100, and further divide each patch into 5 √ó 5 sub-patches, totaling 25, which are comprised of 1 to 25 sub-patches, to capture local interval variations within the regions, as illustrated inFigure 4. To measure the impurity of each patch(ùëÉùëê)(Pc)efficiently, compute the entropy(ùê∏)Efor each sub-patch(ùëÜùëù)(Sp)by calculating the probability of each bin pixel, which consists of 26 bins, and the entropy of the sub-patch is calculated using Equation (4). Reject the total entropy of the sub-patches if it is less than a specific threshold E. The mean and standard deviation fall within the intensity thresholds T1 and T2 of the sub-patch; the algorithm discards the region as background(ùêµùëî)(Bg). Otherwise, it retains the region as foreground(ùêπùëî)Fg, with Equation (5) providing the mathematical representation.ùê∏(ùëÉùëê,ùëÜùëù)=‚àí‚àëùëÉùëê=1100‚àëùëÜùëù=125‚àëùë•=026ùëÉ(ùë•)√ólog2(ùëÉ(ùë•))EPc,Sp=‚àí‚àëPc=1100‚àëSp=125‚àëx=026Px√ólog2Px(4)whereP(x) is the probability of the pixel bin (x) varying between 1 and 26.ùëÉùëéùë°ùëê‚ÑéùëÜùëíùëî={ùêµùëî,ùêπùëî,ùëñùëìùëìùê∏(ùëÉ)>ùëá1ùëéùëõùëëùëöùëíùëéùëõ(ùëÉ)<ùëá2ùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëíPatchSeg=Bg,Fg,iffEP>T1andmeanP<T2otherwise(5)Figure 4.The proposed RSSeg model is essential for hierarchically distinguishing foreground from background in the improved image while preserving region-specific growth features. Initially, a 10 √ó 10 patch of the image is processed; if the minimum criteria are met, then retain it. The retained patch is then further divided into 5 √ó 5 sub-patches, and the exact requirements for each sub-patch are measured to determine whether to maintain or discard it.The distribution and density of local muscles and other soft tissues vary in terms of pixel intensity, which plays a crucial role in shaping and maintaining the specific bone viscosity observed at the wrist, elbow, shoulder (mainly involved in movement and fine motor control), and pelvis (engaged in stable weight-bearing) functional requirements. Individually analyze the optimal empirical thresholds to account for each joint‚Äôs anatomical variation. The entropy threshold(ùê∏)Efilters out low-texture patches, while the mean-intensity threshold(ùëá1)T1and standard deviation(ùëá2)T2remove background. For wrist entropy(ùê∏ùë§)Ew, calculate Shannon entropy based on empirical observations, ifùê∏ùë§Ewvalues less than 0.1 are considered background information and discarded to avoid additional computational burden. On the other hand, to retain the growth pattern of the wrist, perform the empirical analysis of the mean(ùëáùë§1)Tw1and standard deviation(ùëáùë§2)Tw2intensities of sub-patches. Due to variations in pixel values across image sub-patches, the model dynamically estimates the local mean and standard deviation.ùëáùë§1Tw1varies from 55 to 150, andùëáùë§2Tw2varies from 30 to 70 based on the sub-patch foreground pixel. Similarly, elbow entropy(ùê∏ùëí)Eeis less than 0.13, andùëáùëí1Te1(mean of elbow) varies from 60 to 153, andùëáùëí2Te2(standard deviation of elbow) varies from 25 to 60, which are slightly higher thresholds that avoid soft-tissue blurring in mid-texture regions. Shoulder entropyùê∏ùë†Esis less than 0.125;ùëáùë†1Ts1(mean of shoulder) varies from 62 to 153, andùëáùë†2Ts2(standard deviation of shoulder) varies from 28 to 64; moderate thresholds retain edges while filtering other spaces. For the pelvis,ùê∏ùëùEp(pelvis entropy) is lower at 0.14;ùëáùëù1Tp1(mean of pelvis) varies from 52 to 160, andùëáùëù2Tp2(standard deviation of pelvis) varies from 26 to 170, while higher entropy reduces noise in pelvis regions with high soft-tissue overlap. These thresholds ensure that only patches containing meaningful bone and soft-tissue structures to retain the ossification center are preserved for symbolic segmentation, improving both efficiency and accuracy.The symbolic approach uses the mean-standard deviation interval representation to retain the intra-class pixel variations observed in X-ray samples [35]. This approach helps maintain the internal heterogeneity within samples from each region. Equations (6) and (7) show the mean and standard deviation for the sample region, respectively:ùúá=‚àëùëñ=0ùëö‚àëùëó=0ùëõùêº(ùëñ,ùëó)Œº=‚àëi=0m‚àëj=0nIi,j(6)ùúé=1(ùëö√óùëõ)‚àëùëñ=0ùëö‚àëùëó=0ùëõ(ùêº(ùëñ,ùëó)‚àíùúá)‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àíÓÑ¥‚é∑ÓÑ≥ÓÑ≥œÉ=1m√ón‚àëi=0m‚àëj=0n(Ii,j‚àíŒº)(7)whereùëñiandùëójindicate the number of rows and columns, respectively, and theùúáŒºandùúéœÉrepresent the mean and standard deviation for each X-ray image of the foreground and background images, respectively.To create an interval data representation for each sub-patch, calculate theùúáŒºandùúéœÉfor the relevant areas. An interval‚Äôs lower bound represents the difference between itsùúáŒºandùúéœÉ, while its upper bound represents the sum of itsùúáŒºandùúéœÉ. Ultimately,ùúáŒºandùúéœÉare the two instances of these intervals of every class found. Equations (8) and (9) formulate the class representative(ùê∂ùëÖ)CR:ùê∂ùëÖùëñ={[(ùúáùëìùëñ‚àíùúéùëìùëñ)(ùúáùëìùëñ+ùúéùëìùëñ)],[(ùúáùëèùëñ‚àíùúéùëèùëñ)(ùúáùëèùëñ+ùúéùëèùëñ)]}CRi=Œºif‚àíœÉifŒºif+œÉif,Œºib‚àíœÉibŒºib+œÉib(8)ùê∂ùëÖùëñ={[ùëÜ‚àíùëì,ùëÜ+ùëì],[ùëÜ‚àíùëè,ùëÜ+ùëè]}CRi=Sf‚àí,Sf+,Sb‚àí,Sb+(9)whereùëÜ‚àíùëì=(ùúáùëìùëñ‚àíùúéùëìùëñ),ùëÜ+ùëì=(ùúáùëìùëñ+ùúéùëìùëñ),ùëÜ‚àíùëè=(ùúáùëèùëñ‚àíùúéùëèùëñ),andùëÜ+ùëè=(ùúáùëèùëñ+ùúéùëèùëñ)Sf‚àí=Œºif‚àíœÉif,Sf+=Œºif+œÉif,Sb‚àí=Œºib‚àíœÉib,andSb+=Œºib+œÉibThe method computes the similarity between a pixel in a specific region and[ùëÜ‚àíùëì,ùëÜ+ùëì]Sf‚àí,Sf+to assess its foreground belongingness, and similarly calculates the background similarity using[ùëÜ‚àíùëè,ùëÜ+ùëè].[Sb‚àí,Sb+].The pixel classifies the region based on the highest similarity between the foreground and backgroundùê∂ùëÖùë†CRs.Suppose the crisp value falls within the upper(ùëÜ‚àíùëì)Sf‚àíand lower(ùëÜ+ùëì)(Sf+)bounds of the interval pixels; one is the similarity value; otherwise, it is zero, used to compute the reference interval region. This process repeats for the remaining pixels.For a test sample(ùë†ùëû)(sq)corresponding to two classes, the acceptance count(ùê¥ùê∂)ACis evaluated based on the summation of similarity between the test sample and class representatives given in Equation (10):ùê¥ùê∂=‚àëùëñ=12ùëÜùëñùëö(ùë†ùëû,ùê∂ùëÖ2ùëñ)AC=‚àëi=12Simsq,CRi2(10)whereùëÜùëñùëö(ùë†ùëû,ùê∂ùëÖ2ùëñ)={1,ùëñùëìùëÜùëû‚â•ùëÜ‚àíùëñùëéùëõùëëùëÜùëû‚â§ùëÜ+ùëñ0,ùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëíSimsq,CRi2=1,ifSq‚â•Si‚àíandSq‚â§Si+0,otherwiseFigure 5presents the segmentation results for four ossification bones, comparing the proposed RSSeg model with popular segmentation models, including U-Net, Attention U-Net, TransU-Net, DeepLabV3+, Adaptive Otsu, and Watershed, and testing against the ground truth.Figure 5.Segmentation results: (a) original image; (b) ground truth; (c) U-Net; (d) Attention U-Net; (e) TransU-Net; (f) DeepLabV3+; (g) Adaptive Otsu; (h) Watershed; and (i) proposed RSSeg model.",
            "3.1. Preprocessing": "The collected radiological image dataset faces several challenges, including noise and illumination. The methodology uses Gaussian filters to mitigate noise-induced image distortion [10], as shown in Equation (1):ùê∫(ùë•,ùë¶)=(1/(2ùúãùúé^2))ùëí^(‚àí(ùë•^2+ùë¶^2)/(2ùúé^2))G(x,y)=(1/(2œÄœÉ^2))e^(‚àí(x^2+y^2)/(2œÉ^2))(1)where theùê∫(ùë•,ùë¶)Gx,ydenotes the Gaussian function value at the point(ùë•,ùë¶)x,y, where(ùë•,ùë¶)x,yrepresents the coordinate of the filter, and sigma is the standard deviation of the Gaussian distribution. The CLAHE method enhances image contrast [8] by locally adjusting pixel intensities within small regions, called tiles. It clips the histogram to prevent noise amplification, redistributes overused pixels evenly and performs histogram equalization to provide clearer visibility of bone structures. Subsequently, bilinear interpolation is applied to combine the processed tiles, resulting in enhanced contrast of bone features without introducing artifacts, which significantly improved the model‚Äôs performance. The preprocessing technique involves comparing the original sample image with the enhanced image, as shown inFigure 3. Additionally, Equations (2) and (3) represent the mathematical formula for CLAHE.ùëî(ùë•,ùë¶)=[ùê∂ùê∑ùêπ(ùëì(ùë•,ùë¶))‚àíùê∂ùê∑ùêπùëöùëñùëõùëáùë•√óùëáùë¶‚àíùê∂ùê∑ùêπùëöùëñùëõ√ó(ùêø‚àí1)]gx,y=CDFfx,y‚àíCDFminTx√óTy‚àíCDFmin√óL‚àí1(2)ùêª‚Ä≤(ùëñ)=min(ùêª(ùëñ),ùê∂)+ùê∏ùêøH‚Ä≤i=minHi,C+EL(3)where the Cumulative Distribution Function(ùê∂ùê∑ùêπ)CDFis computed from the clipped histogramùêª‚Ä≤(ùëñ)H‚Ä≤i,ùêª(ùëñ)Hiis the histogram,ùê∂Cis the clip limit,ùëñirepresents the pixel intensity, the excess pixelsùê∏Eare redistributed evenly across all bins,ùêøLis the total intensity levels,ùëáùë•√óùëáùë¶Tx√óTyis the local tile size, andùê∂ùê∑ùêπùëöùëñùëõCDFminis the minimumùê∂ùê∑ùêπCDFvalue in the tile. Figure 3.Sample input images of multiple joints: (a) before enhancement; (b) after enhancement via CLAHE.",
            "3.2. Region-Based Symbolic Segmentation": "Segmentation plays a vital role in identifying the specific age group for the Indian medicolegal process. The region-based symbolic segmentation process involves separating an image into two distinct regions: the foreground (the bone region) and the background [9,34]. The challenges, such as illumination and homogeneous contrast among bone, tissues, and artifacts, lead to intra- and inter-class similarity between bone and other background information, making the segmentation task difficult [8]. In medicolegal cases, the appearance and fusion of ossification centers are crucial in determining approximate age groups. The conventional segmentation approach overlooks vital details of various joint ossification centers due to uncertainty and the soft boundary between bone and background, resulting in the loss of the essential bone pattern. To preserve the appearance and fusion of ossification centers, the RSSeg technique efficiently handles ambiguous boundaries by capturing smooth variations within and between pixels. The use of mean and standard deviation of local patches to represent uncertainty in interval data while maintaining ossification center patterns, thereby improving segmentation efficiency. The RSSeg model efficiently retains the soft-tissue region, which exhibits a growth pattern of the ossification center, aiding age estimation. Initially, divide the enhanced image into a 10 √ó 10 grid of patches, resulting in a total of 100 patches, ranging from 1 to 100, and further divide each patch into 5 √ó 5 sub-patches, totaling 25, which are comprised of 1 to 25 sub-patches, to capture local interval variations within the regions, as illustrated inFigure 4. To measure the impurity of each patch(ùëÉùëê)(Pc)efficiently, compute the entropy(ùê∏)Efor each sub-patch(ùëÜùëù)(Sp)by calculating the probability of each bin pixel, which consists of 26 bins, and the entropy of the sub-patch is calculated using Equation (4). Reject the total entropy of the sub-patches if it is less than a specific threshold E. The mean and standard deviation fall within the intensity thresholds T1 and T2 of the sub-patch; the algorithm discards the region as background(ùêµùëî)(Bg). Otherwise, it retains the region as foreground(ùêπùëî)Fg, with Equation (5) providing the mathematical representation.ùê∏(ùëÉùëê,ùëÜùëù)=‚àí‚àëùëÉùëê=1100‚àëùëÜùëù=125‚àëùë•=026ùëÉ(ùë•)√ólog2(ùëÉ(ùë•))EPc,Sp=‚àí‚àëPc=1100‚àëSp=125‚àëx=026Px√ólog2Px(4)whereP(x) is the probability of the pixel bin (x) varying between 1 and 26.ùëÉùëéùë°ùëê‚ÑéùëÜùëíùëî={ùêµùëî,ùêπùëî,ùëñùëìùëìùê∏(ùëÉ)>ùëá1ùëéùëõùëëùëöùëíùëéùëõ(ùëÉ)<ùëá2ùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëíPatchSeg=Bg,Fg,iffEP>T1andmeanP<T2otherwise(5) Figure 4.The proposed RSSeg model is essential for hierarchically distinguishing foreground from background in the improved image while preserving region-specific growth features. Initially, a 10 √ó 10 patch of the image is processed; if the minimum criteria are met, then retain it. The retained patch is then further divided into 5 √ó 5 sub-patches, and the exact requirements for each sub-patch are measured to determine whether to maintain or discard it. The distribution and density of local muscles and other soft tissues vary in terms of pixel intensity, which plays a crucial role in shaping and maintaining the specific bone viscosity observed at the wrist, elbow, shoulder (mainly involved in movement and fine motor control), and pelvis (engaged in stable weight-bearing) functional requirements. Individually analyze the optimal empirical thresholds to account for each joint‚Äôs anatomical variation. The entropy threshold(ùê∏)Efilters out low-texture patches, while the mean-intensity threshold(ùëá1)T1and standard deviation(ùëá2)T2remove background. For wrist entropy(ùê∏ùë§)Ew, calculate Shannon entropy based on empirical observations, ifùê∏ùë§Ewvalues less than 0.1 are considered background information and discarded to avoid additional computational burden. On the other hand, to retain the growth pattern of the wrist, perform the empirical analysis of the mean(ùëáùë§1)Tw1and standard deviation(ùëáùë§2)Tw2intensities of sub-patches. Due to variations in pixel values across image sub-patches, the model dynamically estimates the local mean and standard deviation.ùëáùë§1Tw1varies from 55 to 150, andùëáùë§2Tw2varies from 30 to 70 based on the sub-patch foreground pixel. Similarly, elbow entropy(ùê∏ùëí)Eeis less than 0.13, andùëáùëí1Te1(mean of elbow) varies from 60 to 153, andùëáùëí2Te2(standard deviation of elbow) varies from 25 to 60, which are slightly higher thresholds that avoid soft-tissue blurring in mid-texture regions. Shoulder entropyùê∏ùë†Esis less than 0.125;ùëáùë†1Ts1(mean of shoulder) varies from 62 to 153, andùëáùë†2Ts2(standard deviation of shoulder) varies from 28 to 64; moderate thresholds retain edges while filtering other spaces. For the pelvis,ùê∏ùëùEp(pelvis entropy) is lower at 0.14;ùëáùëù1Tp1(mean of pelvis) varies from 52 to 160, andùëáùëù2Tp2(standard deviation of pelvis) varies from 26 to 170, while higher entropy reduces noise in pelvis regions with high soft-tissue overlap. These thresholds ensure that only patches containing meaningful bone and soft-tissue structures to retain the ossification center are preserved for symbolic segmentation, improving both efficiency and accuracy. The symbolic approach uses the mean-standard deviation interval representation to retain the intra-class pixel variations observed in X-ray samples [35]. This approach helps maintain the internal heterogeneity within samples from each region. Equations (6) and (7) show the mean and standard deviation for the sample region, respectively:ùúá=‚àëùëñ=0ùëö‚àëùëó=0ùëõùêº(ùëñ,ùëó)Œº=‚àëi=0m‚àëj=0nIi,j(6)ùúé=1(ùëö√óùëõ)‚àëùëñ=0ùëö‚àëùëó=0ùëõ(ùêº(ùëñ,ùëó)‚àíùúá)‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àíÓÑ¥‚é∑ÓÑ≥ÓÑ≥œÉ=1m√ón‚àëi=0m‚àëj=0n(Ii,j‚àíŒº)(7)whereùëñiandùëójindicate the number of rows and columns, respectively, and theùúáŒºandùúéœÉrepresent the mean and standard deviation for each X-ray image of the foreground and background images, respectively. To create an interval data representation for each sub-patch, calculate theùúáŒºandùúéœÉfor the relevant areas. An interval‚Äôs lower bound represents the difference between itsùúáŒºandùúéœÉ, while its upper bound represents the sum of itsùúáŒºandùúéœÉ. Ultimately,ùúáŒºandùúéœÉare the two instances of these intervals of every class found. Equations (8) and (9) formulate the class representative(ùê∂ùëÖ)CR:ùê∂ùëÖùëñ={[(ùúáùëìùëñ‚àíùúéùëìùëñ)(ùúáùëìùëñ+ùúéùëìùëñ)],[(ùúáùëèùëñ‚àíùúéùëèùëñ)(ùúáùëèùëñ+ùúéùëèùëñ)]}CRi=Œºif‚àíœÉifŒºif+œÉif,Œºib‚àíœÉibŒºib+œÉib(8)ùê∂ùëÖùëñ={[ùëÜ‚àíùëì,ùëÜ+ùëì],[ùëÜ‚àíùëè,ùëÜ+ùëè]}CRi=Sf‚àí,Sf+,Sb‚àí,Sb+(9)whereùëÜ‚àíùëì=(ùúáùëìùëñ‚àíùúéùëìùëñ),ùëÜ+ùëì=(ùúáùëìùëñ+ùúéùëìùëñ),ùëÜ‚àíùëè=(ùúáùëèùëñ‚àíùúéùëèùëñ),andùëÜ+ùëè=(ùúáùëèùëñ+ùúéùëèùëñ)Sf‚àí=Œºif‚àíœÉif,Sf+=Œºif+œÉif,Sb‚àí=Œºib‚àíœÉib,andSb+=Œºib+œÉib The method computes the similarity between a pixel in a specific region and[ùëÜ‚àíùëì,ùëÜ+ùëì]Sf‚àí,Sf+to assess its foreground belongingness, and similarly calculates the background similarity using[ùëÜ‚àíùëè,ùëÜ+ùëè].[Sb‚àí,Sb+].The pixel classifies the region based on the highest similarity between the foreground and backgroundùê∂ùëÖùë†CRs. Suppose the crisp value falls within the upper(ùëÜ‚àíùëì)Sf‚àíand lower(ùëÜ+ùëì)(Sf+)bounds of the interval pixels; one is the similarity value; otherwise, it is zero, used to compute the reference interval region. This process repeats for the remaining pixels. For a test sample(ùë†ùëû)(sq)corresponding to two classes, the acceptance count(ùê¥ùê∂)ACis evaluated based on the summation of similarity between the test sample and class representatives given in Equation (10):ùê¥ùê∂=‚àëùëñ=12ùëÜùëñùëö(ùë†ùëû,ùê∂ùëÖ2ùëñ)AC=‚àëi=12Simsq,CRi2(10)whereùëÜùëñùëö(ùë†ùëû,ùê∂ùëÖ2ùëñ)={1,ùëñùëìùëÜùëû‚â•ùëÜ‚àíùëñùëéùëõùëëùëÜùëû‚â§ùëÜ+ùëñ0,ùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëíSimsq,CRi2=1,ifSq‚â•Si‚àíandSq‚â§Si+0,otherwise Figure 5presents the segmentation results for four ossification bones, comparing the proposed RSSeg model with popular segmentation models, including U-Net, Attention U-Net, TransU-Net, DeepLabV3+, Adaptive Otsu, and Watershed, and testing against the ground truth. Figure 5.Segmentation results: (a) original image; (b) ground truth; (c) U-Net; (d) Attention U-Net; (e) TransU-Net; (f) DeepLabV3+; (g) Adaptive Otsu; (h) Watershed; and (i) proposed RSSeg model.",
            "4. Experimentation and Result": "This section provides a detailed description of the dataset, evaluation metrics, experimental setup, and results of the RSSeg model. The proposed model was compared with state-of-the-art models, considering both individual- and multi-joint, to ensure the effectiveness of the study. Additionally, the evaluation compares the model against existing datasets to ensure consistent performance and efficiency. Finally, the visual analyses effectively illustrate the results. 4.1. DatasetThe study focuses on collecting X-ray data of two categories: retrospective and prospective data on various ossifications of bones, including the wrist, elbow, shoulder, and pelvis, from a diverse range of participants aged 0‚Äì21 years and above. The retrospective data were obtained from esteemed healthcare organizations with ethical approval from Mysore Medical College and Research Institute, Mysore, including Krishna Rajendra and Apollo BGS Hospitals, for training the model. Additionally, prospective data were collected from volunteers under 18 years old, with their parents‚Äô or guardians‚Äô approval, in radiological laboratories to assess the model‚Äôs effectiveness. In the Indian medicolegal context, age is categorized into seven distinct groups [1,2], as outlined inTable 1, which presents the class intervals in years, andFigure 6illustrates samples of multiple joints corresponding to these class intervals.Figure 6.A sample image of the appearance and fusion of various ossification centers across multiple joints in the Indian population.Table 1.Indian medicolegal class interval based on forensic expert intuition.Figure 7clarifies the anatomy of the multiple joints, as validated by the forensic expert, to ensure forensic accuracy, especially regarding ossification centers and joint margins relevant to skeletal age estimation. Specifically, class 1 represents individuals from infancy to toddlerhood (ages: 0.1 to 4 years), referred to as the Indian Medicolegal Infant-Toddler (IMIT). Class 2 represents individuals from 5 to 7 years, labeled as Indian Medicolegal Child (IMC); class 3 represents individuals from 8 to 12 years, labeled as Indian Medicolegal Pre-Adolescent (IMPA); class 4 represents individuals from 13 to 14 years, labeled as Indian Medicolegal Pre-Teen (IMPT); class 5 represents individuals from 15 to 18 years, labeled as Indian Medicolegal Teen (IMT); class 6 represents individuals from 19 to 21 years, labeled as Indian Medicolegal Young Adult (IMYA), while class 7 encompass individuals aged 21year and older, labeled as Indian Medicolegal Adult (IMA) [1].Figure 7.The appearance and fusion state of various joint anatomies (a) Wrist (IMIT: capitate, hamate, triquetral, lunate, radius base and 1st MC head and base, 3rd MC head, 5th MC head, 1st PP, 3rd PP, 5th PP, 3rd MP, 5th MP, 1st DP, and 3rd DP (appearance); IMC: appearance of trapezoid, trapezium, scaphoid, and ulna base, 5th DP (appearance); IMPA: pisiform (appearance), MC‚Äôs, PP‚Äôs, MP‚Äôs, and DP‚Äôs (non-union with separate epiphysis); IMPT: MC‚Äôs, PP‚Äôs, MP‚Äôs, and DP‚Äôs (partial union); IMT: MC‚Äôs, PP‚Äôs, MP‚Äôs, and DP‚Äôs (complete union); IMYA: nil; IMA (above): nil); (b) Elbow (IMIT: capitellum (appearance); IMC: medial/internal epicondyle and radius head (appearance); IMPA: trochlea, lateral/external epicondyle, and ulna head (appearance); IMPT: capitellum, trochlea, and lateral epicondyle (conjoint epiphysis); olecranon (appearance); IMT: medial epicondyle, radius head, and ulna head (conjoint epiphysis); olecranon (fuses with shaft of ulna seen in lateral view); IMYA: nil; IMA: nil); (c) Shoulder (IMIT: humerus head, greater tubercle (appearance); IMC: lesser tubercle (appearance); IMPA: humerus head, greater tubercle, and lesser tubercle (conjoint epiphysis); IMPT: acromion process (appearance); IMT: acromion process (appearance and conjoint epiphysis with the scapular); IMYA: humerus head, greater tubercle, and lesser tubercle (conjoint epiphysis fuses with shaft of humer); IMA: nil); (d) Pelvis (IMIT: femur head and greater trochanter (appearance); IMC: ischiopubic ramus (fusion); IMPA: lesser trochanter (appearance); IMPT: lesser trochanter and iliac crest (appearance); IMT: ischial tuberosity (appearance), femur head, greater trochanter, and lesser trochanter (fusion); IMYA: lliac crest and ischial tuberosity (fusion); IMA: nil) [1].Initially, the Institutional Ethical Committee of Mysore Medical College and Research Institute, along with the affiliated hospital, granted ethical approval for both prospective and retrospective data collection. A total of 5107 X-rays of various joints were collected, comprising 4959 samples from retrospective data and 148 from prospective data. Some challenges observed in the dataset include class imbalance, heterogeneous contrast, Gaussian noise, non-uniform sizes and resolutions, and different image formats, such as DICOM, JPEG, and PNG. The dataset, comprising 1356 wrist, 1228 elbow, 1506 shoulder, and 980 pelvis X-ray samples, is tabulated inTable 2.Table 2.Numbers of samples of multiple joints of the Indian population.Following the expert‚Äôs guidance, all annotations and ground truth were created and subsequently verified by the forensic expert to ensure anatomical and forensic accuracy, especially regarding ossification centers and joint margins relevant to skeletal age estimation. The ground truth using Photoshop was manually created and validated by the expert, serving as the baseline for comparing the proposed segmentation method. During the experiment, the performance of the proposed model was evaluated by comparing its outcomes with the ground truth and computing the percentage of matching area. It was also cross-verified under the supervision of a domain expert, andFigure 5displays samples of the ground truth image. 4.2. Evaluation MetricEvaluation metrics quantify the performance of the proposed study by examining the RSSeg model using well-known segmentation metrics and by evaluating various classification model behaviors with and without segmented data using classification metrics. Segmentation accuracy was measured using metrics such as Jaccard similarity, Dice-coefficient, precision, recall, and pixel accuracy [11]. Equations (10)‚Äì(14) represent the segmentation evaluation metrics.Jaccard similarity(ùêΩùëÜùëñùëö)JSimdetermines the similarity between the intersected regions‚Äô intensity of the ground truth(ùê∫ùëá)GTand the predicted region(ùëÉ)P:ùêΩùëÜùëñùëö(ùëÜ,ùëá)=|ùëÜ‚à©ùëá||ùëÜ|+|ùëá|‚àí|ùëÜ‚à©ùëá|JSimS,T=S‚à©TS+T‚àíS‚à©T(11)Dice coefficient(ùê∑ùëñùëêùëí)Dicedetermines the degree of relationship between the ground truth and the predicted region:ùê∑ùëñùëêùëí=2√ó|ùëÜ‚à©ùëá||ùëÜ|+|ùëá|Dice=2√óS‚à©TS+T(12)Precision(ùëÉùëüùëíùëê)Precis the ratio of the intersection to the entire number of predicted pixels:ùëÉùëüùëíùëê=‚àëùëñùëó(ùëáùëñùëó√óùëÜùëñùëó)‚àëùëñùëóùëáùëñùëóPrec=‚àëijTij√óSij‚àëijTij(13)Recall(ùëÖùëíùëê)Recis the ratio of the intersection to the entire number of ground truth pixels:ùëÖùëíùëê=‚àëùëñùëó(ùëáùëñùëó√óùëÜùëñùëó)‚àëùëñùëóùëÜùëñùëóRec=‚àëijTij√óSij‚àëijSij(14)whereùëÜ=ùê∫ùëáùëéùëüùëíùëéùëùùëñùë•ùëíùëôùë†S=GTareapixelsandùëá=ùëÉùëéùëüùëíùëéùëùùëñùë•ùëíùëôùë†T=Pareapixels.Pixel accuracy(ùëÉùê¥ùëêùëê)PAccis the proportion of intensities that are accurately classified in the image.ùëÉùê¥ùëêùëêPAccis an intuitive metric:ùëÉùê¥ùëêùëê=‚àëùëòùëñ=1ùëõùëñùëñ‚àëùëòùëñ=1ùë°ùëñPAcc=‚àëi=1knii‚àëi=1kti(15)whereùëõùëñùëñniiis the total number of intersected intensities of bothùëÉPandùê∫ùëáGTof theùëñùë°‚Ñéithclass andùë°ùëñtiis the total number of intensities in theùëñùë°‚Ñéithclass ofùê∫ùëáGT. 4.3. Experimental SetupPython 3.12.4, part of the Anaconda distribution, was used for the experiment conducted in Spyder IDE. The model was constructed and tested by using TensorFlow version 2.17.0 with integrated Keras. For GPU acceleration, the setup used CUDA and cuDNN on a machine equipped with an Intel(R) Core(TM) i7-10750H CPU, 16 GB of RAM, and a 6 GB NVIDIA GeForce RTX 3060 GPU. To maintain consistency across the experiments, all samples were resized to 250 √ó 250 √ó 3 pixels. A Gaussian filter was applied empirically with œÉ values of 1, 1.5, 2, 2.5, 3, and 3.5, among others. Notably, a œÉ value of 2.5 resulted in the best enhancement, particularly due to the presence of Gaussian noise. CLAHE was used to enhance contrast by setting the clip factor to 2 and using standard tile sizes of 25 √ó 25 pixels, which helped reduce local contrast artifacts. Bilinear interpolation was also frequently used to smooth transitions between tiles.In the dataset, the wrist consists of 1356 images, the elbow consists of 1228 total samples, the shoulder consists of 1506 total samples, and the pelvis consists of 980 total samples. To analyze the experimental results with U-Net [8,19], Attention U-Net [18], TransU-Net [11], and DeepLabV3+ [21] models, a patient-level stratified five-fold cross-validation was applied to preserve seven age-group proportions and prevent subject overlap between folds. The images were resized to 256 √ó 256 pixels, normalized and augmented with rotations and flips.The RSSeg segmentation model divides the enhanced image into a 10 √ó 10 grid of patches, resulting in 100 patches, and further divides each patch into 5 √ó 5 sub-patches, yielding 25 sub-patches to capture local interval variations within regions. To efficiently measure the impurity of each patch, compute the entropy for each sub-patch by calculating the probability of each bin pixel, which consists of 26 bins. Discard the total entropy of the sub-patch if it is less than the specific thresholds of joints. The mean and standard deviation fall within the thresholdsùëá1T1andùëá2T2for the sub-patch; the algorithm retains the region as foreground; otherwise, it discards it as background. These thresholds,ùëá1T1andùëá2T2, are entirely dependent on the local pattern variation within the sub-patch of a specific joint, due to the dynamic thresholds of the regional pattern, which are essential for preserving the symbolic representation of an ossification center. The proposed RSSeg approach retains such region-specific local patterns to improve the segmentation efficiency. To ensure model fairness, experiments were conducted and predicted regions with ground truth data were compared, yielding accuracies of 92.1% for the wrist, 93.7% for the elbow, 89.9% for the shoulder, and 90.3% for the pelvis.Similarly,Table 3andFigure 8present the evaluation results, comparing the proposed RSSeg model with U-Net [8,19], Attention U-Net [18], TransU-Net [11], DeepLabV3+ [21], Adaptive Otsu [24], and Watershed [22]. The following parameters were used to conduct the experiment and evaluate the model‚Äôs performance. The experiment used a dataset containing four joints and their corresponding ground-truth masks. Each model was evaluated on a single joint at a time, using stratified five-fold cross-validation. Each fold represents the distribution of the seven age categories more fairly and mitigates imbalance in deep segmentation models such as U-Net, Attention U-Net, TransU-Net, and DeepLabV3+.Figure 8.Performance of 5-fold cross-validation comparison of the proposed RSSeg model over state-of-the-art models for individual joints with respect to pixel accuracy.Table 3.Performance of 5-fold cross-validation of the proposed RSSeg model over state-of-the-art models for individual joints with respect to pixel accuracy.The U-Net model with a ResNet34 encoder was trained with each joint separately using the Adam optimizer with a learning rate of 1 √ó 10‚àí4and the Dice loss function for 50 epochs. The experimental results show improvements of 88.5 ¬± 0.50% for the wrist, 85.5 ¬± 0.51% for the elbow, 88.5 ¬± 0.12% for the shoulder, and 80.3 ¬± 0.27% for the pelvis region, outperforming Adaptive Ostu and Watershed segmentation.The Attention U-Net model was trained using the AdamW optimizer with a learning rate of 1 √ó 10‚àí4, binary cross-entropy loss, polynomial learning rate decay over 50 epochs, a batch size of 16, a fixed seed value of 42 on the multi-focal dataset, and a drop of 0.3 (decoder) with attention gates enhancing feature refinement in the skip connection. At the same time, a cosine annealing scheduler was used for stable convergence. The model achieves segmentation performances of 89.4 ¬± 0.89% for the wrist, 88.5 ¬± 0.54% for the elbow, 89.9 ¬± 0.10% for the shoulder, and 84.9 ¬± 0.25% for the pelvis.The TransU-Net model was trained using the AdamW optimizer with a learning rate of 1 √ó 10‚àí4(and a 5 √ó 10‚àí5warm-up for TransUNet), binary cross-entropy loss, polynomial learning rate decay over 50 epochs, a batch size of 16, a fixed seed value of 42 on the multi-focal dataset, a drop of 0.1 (Transformer blocks), and transformer-enhanced encoding for long-range context modeling. The TransU-Net model demonstrates segmentation performances of 91.3 ¬± 0.90% for the wrist, 89.2 ¬± 0.51% for the elbow, 90.9 ¬± 0.11% for the shoulder, and 85.9 ¬± 0.56% for the pelvis. Compared to U-Net, Attention U-Net, TransUNet, and DeepLabV3+, the latter three show a slight improvement in overall pixel accuracy and better performance on larger structures, but require more computational resources.The fine-tuning of DeepLabV3+ with a ResNet101 backbone pretrained on ImageNet uses the AdamW optimizer with a learning rate of 1 √ó 10‚àí4, applies cross-entropy loss and incorporates polynomial learning rate decay over 50 epochs, along with Atrous rates to capture multi-scale features. The model achieves a significant improvement in pixel accuracy, reaching segmentation performances of 91.9 ¬± 0.06% for the wrist, 89.6 ¬± 0.48% for the elbow, 91.3 ¬± 0.15% for the shoulder, and 86.9 ¬± 0.35% for the pelvis.The Adaptive Otsu method divides each image into 10 √ó 10 non-overlapping patches and applies local Otsu thresholding within each block to account for regional intensity variations. Post-processing included morphological closing to smooth bone boundaries and remove minor artifacts. The model achieves an overall pixel accuracy of 83.3%, with specific accuracies of 86.5% for the wrist, 77.8% for the elbow, 91.6% for the shoulder, and 77.4% for the pelvis, which are lower than those of other models.The watershed segmentation technique prevented over-segmentation in low-contrast areas by utilizing Otsu‚Äôs thresholding, distance transformations, and a marker-controlled watershed with background markers generated from morphological erosion and foreground markers derived from regional maxima. The model achieves an overall pixel accuracy of 84.0%, with specific accuracies of 91.1% for the wrist, 76.5% for the elbow, 76.3% for the pelvis, and 92.1% for the shoulder, outperforming other models.Table 3andFigure 8present the performance of individual joints, measured in terms of pixel accuracy.In comparison, various segmentation models were tested across multiple joints to measure the performance with the proposed RSSeg. The RSSeg shows better generalization, achieving an 84.5% Jaccard similarity, an 81.4% Dice coefficient, an 88.3% precision, a 90.0% recall, and a 91.5% pixel accuracy, making it suitable for real-time applications. A summary of the performance is provided inTable 4and visualized inFigure 9.Figure 9.Performance of 5-fold cross-validation analysis of the proposed RSSeg model over state-of-the-art models considering multiple joints.Table 4.Performance of 5-fold cross-validation analysis of the proposed RSSeg model over state-of-the-art models considering multiple joints.A performance comparison of DeepLabV3+ and the proposed RSSeg model, using a paired statistical analysis across several segmentation metrics, is depicted inTable 5. With mean differences of 1.1% in Jaccard similarity, 0.5% in Dice coefficient, 1.3% in precision, 0.8% in recall, and 1.6% in pixel accuracy, the results show that RSSeg consistently outperforms DeepLabV3+. The higher segmentation performance demonstrates that the proposed RSSeg surpasses DeepLabV3+ on the multifocal dataset, as indicated by a paired t-test (p< 0.05) across all evaluated parameters.Table 5.Performance comparison of the paired statistical test on the proposed RSSeg over DeepLabV3+.Table 6andFigure 10present the effectiveness of the proposed RSSeg model on several existing datasets. Despite noise and varying acquisition protocols, the proposed model achieves pixel accuracies of 92.1% on DHA and 96.7% on RSNA. The elbow‚Äôs 95.2% pixel-level accuracy demonstrates the model‚Äôs generalizability, and the shoulder‚Äôs higher recall of 91.5% further supports this. The model also demonstrates improved efficiency across different noise and illumination conditions, and was tested on a pelvis dataset, achieving a precision of 88.3%.Figure 10.Performance of the proposed RSSeg model over the existing datasets.Table 6.Performance of the proposed RSSeg model over the existing datasets.This study assessed the importance of segmentation by conducting experiments on segmented and non-segmented images and analyzing the impact of segmentation on deep-learning classification models, including VGG16 [19], ResNet50 [36], and InceptionV3 [37]. The study implemented all classification models using TensorFlow, initialized them with ImageNet-pretrained weights and trained them using various splits for training, validation, and testing data, as shown inTable 7, which details the sample splits. Images were resized to 250 √ó 250 pixels and normalized using channel-wise means and standard deviations. The models were trained with a batch size of 16, using the Adam optimizer (learning rate = 0.0001, decay = 1 √ó 10‚àí4), and employed data augmentation, including ¬±15¬∞ rotations and horizontal flips.Table 7.Sample distribution of the multiple joints‚Äô dataset for a varied training-validation-testing data split for age estimation.The VGG16 model was trained separately for 50, 53, 50, and 60 epochs on the wrist, elbow, shoulder, and pelvis regions, respectively, using early stopping with patience of 5 and cross-entropy loss. The implementation replaced the final layer with a two-layer dense network and a Softmax-activated classification layer. The ResNet50 model was trained for 40 epochs with early stopping (patience = 3) and binary cross-entropy loss. The architecture modified the final layers by implementing a global average pooling layer, followed by two ReLU-activated dense layers and a softmax classification layer. The InceptionV3 model was trained for 36 epochs with early stopping (patience = 5) and categorical cross-entropy loss. The top-layer replacement process used a global average pooling layer, followed by two dense layers with ReLU activations, and an output layer with a softmax activation function.The results of age estimation models (VGG16, ResNet50, and InceptionV3) trained with varying train‚àítest ratios (60‚Äì40, 70‚Äì30, and 80‚Äì20) were examined. The 80‚Äì20 split shows better performance on VGG16, ResNet50, and InceptionV3 than other splits, such as 60‚Äì40 and 70‚Äì30, which fail to capture more diverse growth patterns with limited training samples. The evaluation applies the training configuration mentioned above, with VGG16 demonstrating the best segmentation performance, achieving an overall accuracy of 93.8%, a precision of 92.6%, a recall of 92.9%, and an F1-score of 92.8%. Compared to its performance without segmentation, where the overall accuracy was 86.4%, this result represents a significant improvement. Interestingly, VGG16 delivers substantial improvements in the wrist and elbow regions, achieving an F1-score of nearly 94.0%, an accuracy above 94.0%, and a precision and a recall between 92.0% and 95.0%. These enhancements demonstrate that VGG16 effectively leverages segmented inputs to identify localized features crucial for age estimation. For the wrist, RSSeg results improved by 8.1% in accuracy with VGG16 (87.8‚Äì95.9%), 4.7% with ResNet50 (89.5‚Äì94.2%), and 8.0% with InceptionV3 (84.5‚Äì92.5%). For the elbow, an accuracy improved by 8.8% with VGG16 (85.8‚Äì94.6%), 4.6% with ResNet50 (88.5‚Äì93.1%), and 8.1% with InceptionV3 (85.6‚Äì93.7%). The carpal bones of the wrist and the CRITOE of the elbow ossification centers are visible at an early stage, and their appearance and fusion states are evident. VGG16 achieves better performance than others because of its ability to retain fine-grained local features. For the shoulder, the accuracies were 7.4% with VGG16 (86.5‚Äì93.9%), 5.6% with ResNet50 (86.9‚Äì92.5%), and 8.1% with InceptionV3 (83.8‚Äì91.9%). Hence, for the pelvis, the accuracies were 5.3% with VGG16 (85.5‚Äì90.8%), 6.4% with ResNet50 (84.6‚Äì91.0%), and 10.5% with InceptionV3 (80.1‚Äì90.6%). The humerus head and tubercles of the shoulder, as well as femur head, trochanters, and iliac crest of the pelvis, are complex, multi-scale anatomical features that were captured effectively by InceptionV3, which failed to be retained by ResNet50 and VGG16.Additionally, ResNet50 performs better; its overall accuracy rises from 87.4% without segmentation to 92.7% when segmented, while its precision, recall, and F1-score all improve from 85.1% to 92.0%, 87.3% to 92.6%, and 86.2% to 92.2%, respectively. InceptionV3 shows the overall precision improving from 82.9% to 90.0%, the recall from 82.7% to 90.9%, and the F1-score from 82.8% to 90.5%, while its accuracy increases from 83.5% to 92.2%. These results are tabulated inTable 8and visualized inFigure 11, which shows that the proposed RSSeg improves the model‚Äôs performance.Figure 11.Performance analysis of the state-of-the-art classification models considering multiple joints with RSSeg and without a segmentation approach for an 80:20 data split.Table 8.Performance analysis of the state-of-the-art classification models considering multiple joints with RSSeg and without a segmentation approach for various data split ratios.The five-fold cross-validation results show that the proposed RSSeg model performs consistently across multiple joints, with the highest accuracy of 92.0 ¬± 3.30% at the wrist and 88.5 ¬± 2.31% at the pelvis, which is slightly lower. Overall, the precision of 88.7 ¬± 3.68% and the recall of 89.5 ¬± 3.62% are similarly high, indicating reliable detection of relevant pixels, while the overall F1-score of 89.0 ¬± 3.58% confirms balanced, robust segmentation performance.Table 9andFigure 12show slightly higher variability in specific regions that reflects anatomical complexity, but overall, the model demonstrates accurate and consistent results across folds.Figure 12.Performance of the proposed RSSeg model over five-fold cross-validation on the multifocal dataset.Table 9.Performance of the proposed RSSeg model over five-fold cross-validation on the multifocal dataset.",
            "4.1. Dataset": "The study focuses on collecting X-ray data of two categories: retrospective and prospective data on various ossifications of bones, including the wrist, elbow, shoulder, and pelvis, from a diverse range of participants aged 0‚Äì21 years and above. The retrospective data were obtained from esteemed healthcare organizations with ethical approval from Mysore Medical College and Research Institute, Mysore, including Krishna Rajendra and Apollo BGS Hospitals, for training the model. Additionally, prospective data were collected from volunteers under 18 years old, with their parents‚Äô or guardians‚Äô approval, in radiological laboratories to assess the model‚Äôs effectiveness. In the Indian medicolegal context, age is categorized into seven distinct groups [1,2], as outlined inTable 1, which presents the class intervals in years, andFigure 6illustrates samples of multiple joints corresponding to these class intervals. Figure 6.A sample image of the appearance and fusion of various ossification centers across multiple joints in the Indian population. Table 1.Indian medicolegal class interval based on forensic expert intuition. Figure 7clarifies the anatomy of the multiple joints, as validated by the forensic expert, to ensure forensic accuracy, especially regarding ossification centers and joint margins relevant to skeletal age estimation. Specifically, class 1 represents individuals from infancy to toddlerhood (ages: 0.1 to 4 years), referred to as the Indian Medicolegal Infant-Toddler (IMIT). Class 2 represents individuals from 5 to 7 years, labeled as Indian Medicolegal Child (IMC); class 3 represents individuals from 8 to 12 years, labeled as Indian Medicolegal Pre-Adolescent (IMPA); class 4 represents individuals from 13 to 14 years, labeled as Indian Medicolegal Pre-Teen (IMPT); class 5 represents individuals from 15 to 18 years, labeled as Indian Medicolegal Teen (IMT); class 6 represents individuals from 19 to 21 years, labeled as Indian Medicolegal Young Adult (IMYA), while class 7 encompass individuals aged 21year and older, labeled as Indian Medicolegal Adult (IMA) [1]. Figure 7.The appearance and fusion state of various joint anatomies (a) Wrist (IMIT: capitate, hamate, triquetral, lunate, radius base and 1st MC head and base, 3rd MC head, 5th MC head, 1st PP, 3rd PP, 5th PP, 3rd MP, 5th MP, 1st DP, and 3rd DP (appearance); IMC: appearance of trapezoid, trapezium, scaphoid, and ulna base, 5th DP (appearance); IMPA: pisiform (appearance), MC‚Äôs, PP‚Äôs, MP‚Äôs, and DP‚Äôs (non-union with separate epiphysis); IMPT: MC‚Äôs, PP‚Äôs, MP‚Äôs, and DP‚Äôs (partial union); IMT: MC‚Äôs, PP‚Äôs, MP‚Äôs, and DP‚Äôs (complete union); IMYA: nil; IMA (above): nil); (b) Elbow (IMIT: capitellum (appearance); IMC: medial/internal epicondyle and radius head (appearance); IMPA: trochlea, lateral/external epicondyle, and ulna head (appearance); IMPT: capitellum, trochlea, and lateral epicondyle (conjoint epiphysis); olecranon (appearance); IMT: medial epicondyle, radius head, and ulna head (conjoint epiphysis); olecranon (fuses with shaft of ulna seen in lateral view); IMYA: nil; IMA: nil); (c) Shoulder (IMIT: humerus head, greater tubercle (appearance); IMC: lesser tubercle (appearance); IMPA: humerus head, greater tubercle, and lesser tubercle (conjoint epiphysis); IMPT: acromion process (appearance); IMT: acromion process (appearance and conjoint epiphysis with the scapular); IMYA: humerus head, greater tubercle, and lesser tubercle (conjoint epiphysis fuses with shaft of humer); IMA: nil); (d) Pelvis (IMIT: femur head and greater trochanter (appearance); IMC: ischiopubic ramus (fusion); IMPA: lesser trochanter (appearance); IMPT: lesser trochanter and iliac crest (appearance); IMT: ischial tuberosity (appearance), femur head, greater trochanter, and lesser trochanter (fusion); IMYA: lliac crest and ischial tuberosity (fusion); IMA: nil) [1]. Initially, the Institutional Ethical Committee of Mysore Medical College and Research Institute, along with the affiliated hospital, granted ethical approval for both prospective and retrospective data collection. A total of 5107 X-rays of various joints were collected, comprising 4959 samples from retrospective data and 148 from prospective data. Some challenges observed in the dataset include class imbalance, heterogeneous contrast, Gaussian noise, non-uniform sizes and resolutions, and different image formats, such as DICOM, JPEG, and PNG. The dataset, comprising 1356 wrist, 1228 elbow, 1506 shoulder, and 980 pelvis X-ray samples, is tabulated inTable 2. Table 2.Numbers of samples of multiple joints of the Indian population. Following the expert‚Äôs guidance, all annotations and ground truth were created and subsequently verified by the forensic expert to ensure anatomical and forensic accuracy, especially regarding ossification centers and joint margins relevant to skeletal age estimation. The ground truth using Photoshop was manually created and validated by the expert, serving as the baseline for comparing the proposed segmentation method. During the experiment, the performance of the proposed model was evaluated by comparing its outcomes with the ground truth and computing the percentage of matching area. It was also cross-verified under the supervision of a domain expert, andFigure 5displays samples of the ground truth image.",
            "4.2. Evaluation Metric": "Evaluation metrics quantify the performance of the proposed study by examining the RSSeg model using well-known segmentation metrics and by evaluating various classification model behaviors with and without segmented data using classification metrics. Segmentation accuracy was measured using metrics such as Jaccard similarity, Dice-coefficient, precision, recall, and pixel accuracy [11]. Equations (10)‚Äì(14) represent the segmentation evaluation metrics. Jaccard similarity(ùêΩùëÜùëñùëö)JSimdetermines the similarity between the intersected regions‚Äô intensity of the ground truth(ùê∫ùëá)GTand the predicted region(ùëÉ)P:ùêΩùëÜùëñùëö(ùëÜ,ùëá)=|ùëÜ‚à©ùëá||ùëÜ|+|ùëá|‚àí|ùëÜ‚à©ùëá|JSimS,T=S‚à©TS+T‚àíS‚à©T(11) Dice coefficient(ùê∑ùëñùëêùëí)Dicedetermines the degree of relationship between the ground truth and the predicted region:ùê∑ùëñùëêùëí=2√ó|ùëÜ‚à©ùëá||ùëÜ|+|ùëá|Dice=2√óS‚à©TS+T(12) Precision(ùëÉùëüùëíùëê)Precis the ratio of the intersection to the entire number of predicted pixels:ùëÉùëüùëíùëê=‚àëùëñùëó(ùëáùëñùëó√óùëÜùëñùëó)‚àëùëñùëóùëáùëñùëóPrec=‚àëijTij√óSij‚àëijTij(13) Recall(ùëÖùëíùëê)Recis the ratio of the intersection to the entire number of ground truth pixels:ùëÖùëíùëê=‚àëùëñùëó(ùëáùëñùëó√óùëÜùëñùëó)‚àëùëñùëóùëÜùëñùëóRec=‚àëijTij√óSij‚àëijSij(14)whereùëÜ=ùê∫ùëáùëéùëüùëíùëéùëùùëñùë•ùëíùëôùë†S=GTareapixelsandùëá=ùëÉùëéùëüùëíùëéùëùùëñùë•ùëíùëôùë†T=Pareapixels. Pixel accuracy(ùëÉùê¥ùëêùëê)PAccis the proportion of intensities that are accurately classified in the image.ùëÉùê¥ùëêùëêPAccis an intuitive metric:ùëÉùê¥ùëêùëê=‚àëùëòùëñ=1ùëõùëñùëñ‚àëùëòùëñ=1ùë°ùëñPAcc=‚àëi=1knii‚àëi=1kti(15)whereùëõùëñùëñniiis the total number of intersected intensities of bothùëÉPandùê∫ùëáGTof theùëñùë°‚Ñéithclass andùë°ùëñtiis the total number of intensities in theùëñùë°‚Ñéithclass ofùê∫ùëáGT.",
            "4.3. Experimental Setup": "Python 3.12.4, part of the Anaconda distribution, was used for the experiment conducted in Spyder IDE. The model was constructed and tested by using TensorFlow version 2.17.0 with integrated Keras. For GPU acceleration, the setup used CUDA and cuDNN on a machine equipped with an Intel(R) Core(TM) i7-10750H CPU, 16 GB of RAM, and a 6 GB NVIDIA GeForce RTX 3060 GPU. To maintain consistency across the experiments, all samples were resized to 250 √ó 250 √ó 3 pixels. A Gaussian filter was applied empirically with œÉ values of 1, 1.5, 2, 2.5, 3, and 3.5, among others. Notably, a œÉ value of 2.5 resulted in the best enhancement, particularly due to the presence of Gaussian noise. CLAHE was used to enhance contrast by setting the clip factor to 2 and using standard tile sizes of 25 √ó 25 pixels, which helped reduce local contrast artifacts. Bilinear interpolation was also frequently used to smooth transitions between tiles. In the dataset, the wrist consists of 1356 images, the elbow consists of 1228 total samples, the shoulder consists of 1506 total samples, and the pelvis consists of 980 total samples. To analyze the experimental results with U-Net [8,19], Attention U-Net [18], TransU-Net [11], and DeepLabV3+ [21] models, a patient-level stratified five-fold cross-validation was applied to preserve seven age-group proportions and prevent subject overlap between folds. The images were resized to 256 √ó 256 pixels, normalized and augmented with rotations and flips. The RSSeg segmentation model divides the enhanced image into a 10 √ó 10 grid of patches, resulting in 100 patches, and further divides each patch into 5 √ó 5 sub-patches, yielding 25 sub-patches to capture local interval variations within regions. To efficiently measure the impurity of each patch, compute the entropy for each sub-patch by calculating the probability of each bin pixel, which consists of 26 bins. Discard the total entropy of the sub-patch if it is less than the specific thresholds of joints. The mean and standard deviation fall within the thresholdsùëá1T1andùëá2T2for the sub-patch; the algorithm retains the region as foreground; otherwise, it discards it as background. These thresholds,ùëá1T1andùëá2T2, are entirely dependent on the local pattern variation within the sub-patch of a specific joint, due to the dynamic thresholds of the regional pattern, which are essential for preserving the symbolic representation of an ossification center. The proposed RSSeg approach retains such region-specific local patterns to improve the segmentation efficiency. To ensure model fairness, experiments were conducted and predicted regions with ground truth data were compared, yielding accuracies of 92.1% for the wrist, 93.7% for the elbow, 89.9% for the shoulder, and 90.3% for the pelvis. Similarly,Table 3andFigure 8present the evaluation results, comparing the proposed RSSeg model with U-Net [8,19], Attention U-Net [18], TransU-Net [11], DeepLabV3+ [21], Adaptive Otsu [24], and Watershed [22]. The following parameters were used to conduct the experiment and evaluate the model‚Äôs performance. The experiment used a dataset containing four joints and their corresponding ground-truth masks. Each model was evaluated on a single joint at a time, using stratified five-fold cross-validation. Each fold represents the distribution of the seven age categories more fairly and mitigates imbalance in deep segmentation models such as U-Net, Attention U-Net, TransU-Net, and DeepLabV3+. Figure 8.Performance of 5-fold cross-validation comparison of the proposed RSSeg model over state-of-the-art models for individual joints with respect to pixel accuracy. Table 3.Performance of 5-fold cross-validation of the proposed RSSeg model over state-of-the-art models for individual joints with respect to pixel accuracy. The U-Net model with a ResNet34 encoder was trained with each joint separately using the Adam optimizer with a learning rate of 1 √ó 10‚àí4and the Dice loss function for 50 epochs. The experimental results show improvements of 88.5 ¬± 0.50% for the wrist, 85.5 ¬± 0.51% for the elbow, 88.5 ¬± 0.12% for the shoulder, and 80.3 ¬± 0.27% for the pelvis region, outperforming Adaptive Ostu and Watershed segmentation. The Attention U-Net model was trained using the AdamW optimizer with a learning rate of 1 √ó 10‚àí4, binary cross-entropy loss, polynomial learning rate decay over 50 epochs, a batch size of 16, a fixed seed value of 42 on the multi-focal dataset, and a drop of 0.3 (decoder) with attention gates enhancing feature refinement in the skip connection. At the same time, a cosine annealing scheduler was used for stable convergence. The model achieves segmentation performances of 89.4 ¬± 0.89% for the wrist, 88.5 ¬± 0.54% for the elbow, 89.9 ¬± 0.10% for the shoulder, and 84.9 ¬± 0.25% for the pelvis. The TransU-Net model was trained using the AdamW optimizer with a learning rate of 1 √ó 10‚àí4(and a 5 √ó 10‚àí5warm-up for TransUNet), binary cross-entropy loss, polynomial learning rate decay over 50 epochs, a batch size of 16, a fixed seed value of 42 on the multi-focal dataset, a drop of 0.1 (Transformer blocks), and transformer-enhanced encoding for long-range context modeling. The TransU-Net model demonstrates segmentation performances of 91.3 ¬± 0.90% for the wrist, 89.2 ¬± 0.51% for the elbow, 90.9 ¬± 0.11% for the shoulder, and 85.9 ¬± 0.56% for the pelvis. Compared to U-Net, Attention U-Net, TransUNet, and DeepLabV3+, the latter three show a slight improvement in overall pixel accuracy and better performance on larger structures, but require more computational resources. The fine-tuning of DeepLabV3+ with a ResNet101 backbone pretrained on ImageNet uses the AdamW optimizer with a learning rate of 1 √ó 10‚àí4, applies cross-entropy loss and incorporates polynomial learning rate decay over 50 epochs, along with Atrous rates to capture multi-scale features. The model achieves a significant improvement in pixel accuracy, reaching segmentation performances of 91.9 ¬± 0.06% for the wrist, 89.6 ¬± 0.48% for the elbow, 91.3 ¬± 0.15% for the shoulder, and 86.9 ¬± 0.35% for the pelvis. The Adaptive Otsu method divides each image into 10 √ó 10 non-overlapping patches and applies local Otsu thresholding within each block to account for regional intensity variations. Post-processing included morphological closing to smooth bone boundaries and remove minor artifacts. The model achieves an overall pixel accuracy of 83.3%, with specific accuracies of 86.5% for the wrist, 77.8% for the elbow, 91.6% for the shoulder, and 77.4% for the pelvis, which are lower than those of other models. The watershed segmentation technique prevented over-segmentation in low-contrast areas by utilizing Otsu‚Äôs thresholding, distance transformations, and a marker-controlled watershed with background markers generated from morphological erosion and foreground markers derived from regional maxima. The model achieves an overall pixel accuracy of 84.0%, with specific accuracies of 91.1% for the wrist, 76.5% for the elbow, 76.3% for the pelvis, and 92.1% for the shoulder, outperforming other models.Table 3andFigure 8present the performance of individual joints, measured in terms of pixel accuracy. In comparison, various segmentation models were tested across multiple joints to measure the performance with the proposed RSSeg. The RSSeg shows better generalization, achieving an 84.5% Jaccard similarity, an 81.4% Dice coefficient, an 88.3% precision, a 90.0% recall, and a 91.5% pixel accuracy, making it suitable for real-time applications. A summary of the performance is provided inTable 4and visualized inFigure 9. Figure 9.Performance of 5-fold cross-validation analysis of the proposed RSSeg model over state-of-the-art models considering multiple joints. Table 4.Performance of 5-fold cross-validation analysis of the proposed RSSeg model over state-of-the-art models considering multiple joints. A performance comparison of DeepLabV3+ and the proposed RSSeg model, using a paired statistical analysis across several segmentation metrics, is depicted inTable 5. With mean differences of 1.1% in Jaccard similarity, 0.5% in Dice coefficient, 1.3% in precision, 0.8% in recall, and 1.6% in pixel accuracy, the results show that RSSeg consistently outperforms DeepLabV3+. The higher segmentation performance demonstrates that the proposed RSSeg surpasses DeepLabV3+ on the multifocal dataset, as indicated by a paired t-test (p< 0.05) across all evaluated parameters. Table 5.Performance comparison of the paired statistical test on the proposed RSSeg over DeepLabV3+. Table 6andFigure 10present the effectiveness of the proposed RSSeg model on several existing datasets. Despite noise and varying acquisition protocols, the proposed model achieves pixel accuracies of 92.1% on DHA and 96.7% on RSNA. The elbow‚Äôs 95.2% pixel-level accuracy demonstrates the model‚Äôs generalizability, and the shoulder‚Äôs higher recall of 91.5% further supports this. The model also demonstrates improved efficiency across different noise and illumination conditions, and was tested on a pelvis dataset, achieving a precision of 88.3%. Figure 10.Performance of the proposed RSSeg model over the existing datasets. Table 6.Performance of the proposed RSSeg model over the existing datasets. This study assessed the importance of segmentation by conducting experiments on segmented and non-segmented images and analyzing the impact of segmentation on deep-learning classification models, including VGG16 [19], ResNet50 [36], and InceptionV3 [37]. The study implemented all classification models using TensorFlow, initialized them with ImageNet-pretrained weights and trained them using various splits for training, validation, and testing data, as shown inTable 7, which details the sample splits. Images were resized to 250 √ó 250 pixels and normalized using channel-wise means and standard deviations. The models were trained with a batch size of 16, using the Adam optimizer (learning rate = 0.0001, decay = 1 √ó 10‚àí4), and employed data augmentation, including ¬±15¬∞ rotations and horizontal flips. Table 7.Sample distribution of the multiple joints‚Äô dataset for a varied training-validation-testing data split for age estimation. The VGG16 model was trained separately for 50, 53, 50, and 60 epochs on the wrist, elbow, shoulder, and pelvis regions, respectively, using early stopping with patience of 5 and cross-entropy loss. The implementation replaced the final layer with a two-layer dense network and a Softmax-activated classification layer. The ResNet50 model was trained for 40 epochs with early stopping (patience = 3) and binary cross-entropy loss. The architecture modified the final layers by implementing a global average pooling layer, followed by two ReLU-activated dense layers and a softmax classification layer. The InceptionV3 model was trained for 36 epochs with early stopping (patience = 5) and categorical cross-entropy loss. The top-layer replacement process used a global average pooling layer, followed by two dense layers with ReLU activations, and an output layer with a softmax activation function. The results of age estimation models (VGG16, ResNet50, and InceptionV3) trained with varying train‚àítest ratios (60‚Äì40, 70‚Äì30, and 80‚Äì20) were examined. The 80‚Äì20 split shows better performance on VGG16, ResNet50, and InceptionV3 than other splits, such as 60‚Äì40 and 70‚Äì30, which fail to capture more diverse growth patterns with limited training samples. The evaluation applies the training configuration mentioned above, with VGG16 demonstrating the best segmentation performance, achieving an overall accuracy of 93.8%, a precision of 92.6%, a recall of 92.9%, and an F1-score of 92.8%. Compared to its performance without segmentation, where the overall accuracy was 86.4%, this result represents a significant improvement. Interestingly, VGG16 delivers substantial improvements in the wrist and elbow regions, achieving an F1-score of nearly 94.0%, an accuracy above 94.0%, and a precision and a recall between 92.0% and 95.0%. These enhancements demonstrate that VGG16 effectively leverages segmented inputs to identify localized features crucial for age estimation. For the wrist, RSSeg results improved by 8.1% in accuracy with VGG16 (87.8‚Äì95.9%), 4.7% with ResNet50 (89.5‚Äì94.2%), and 8.0% with InceptionV3 (84.5‚Äì92.5%). For the elbow, an accuracy improved by 8.8% with VGG16 (85.8‚Äì94.6%), 4.6% with ResNet50 (88.5‚Äì93.1%), and 8.1% with InceptionV3 (85.6‚Äì93.7%). The carpal bones of the wrist and the CRITOE of the elbow ossification centers are visible at an early stage, and their appearance and fusion states are evident. VGG16 achieves better performance than others because of its ability to retain fine-grained local features. For the shoulder, the accuracies were 7.4% with VGG16 (86.5‚Äì93.9%), 5.6% with ResNet50 (86.9‚Äì92.5%), and 8.1% with InceptionV3 (83.8‚Äì91.9%). Hence, for the pelvis, the accuracies were 5.3% with VGG16 (85.5‚Äì90.8%), 6.4% with ResNet50 (84.6‚Äì91.0%), and 10.5% with InceptionV3 (80.1‚Äì90.6%). The humerus head and tubercles of the shoulder, as well as femur head, trochanters, and iliac crest of the pelvis, are complex, multi-scale anatomical features that were captured effectively by InceptionV3, which failed to be retained by ResNet50 and VGG16. Additionally, ResNet50 performs better; its overall accuracy rises from 87.4% without segmentation to 92.7% when segmented, while its precision, recall, and F1-score all improve from 85.1% to 92.0%, 87.3% to 92.6%, and 86.2% to 92.2%, respectively. InceptionV3 shows the overall precision improving from 82.9% to 90.0%, the recall from 82.7% to 90.9%, and the F1-score from 82.8% to 90.5%, while its accuracy increases from 83.5% to 92.2%. These results are tabulated inTable 8and visualized inFigure 11, which shows that the proposed RSSeg improves the model‚Äôs performance. Figure 11.Performance analysis of the state-of-the-art classification models considering multiple joints with RSSeg and without a segmentation approach for an 80:20 data split. Table 8.Performance analysis of the state-of-the-art classification models considering multiple joints with RSSeg and without a segmentation approach for various data split ratios. The five-fold cross-validation results show that the proposed RSSeg model performs consistently across multiple joints, with the highest accuracy of 92.0 ¬± 3.30% at the wrist and 88.5 ¬± 2.31% at the pelvis, which is slightly lower. Overall, the precision of 88.7 ¬± 3.68% and the recall of 89.5 ¬± 3.62% are similarly high, indicating reliable detection of relevant pixels, while the overall F1-score of 89.0 ¬± 3.58% confirms balanced, robust segmentation performance.Table 9andFigure 12show slightly higher variability in specific regions that reflects anatomical complexity, but overall, the model demonstrates accurate and consistent results across folds. Figure 12.Performance of the proposed RSSeg model over five-fold cross-validation on the multifocal dataset. Table 9.Performance of the proposed RSSeg model over five-fold cross-validation on the multifocal dataset.",
            "5. Discussion": "The RSSeg model‚Äôs hierarchical grid process retains the foreground growth patterns, which are essential for bone age estimation, by computing joint- and grid-specific entropies to avoid computational burden while effectively preserving growth patterns through symbolic segmentation.Figure 13a shows that the proposed method minimizes overhead while maintaining the soft-tissue areas essential for ossification pattern analysis.Figure 13b shows the segmentation result using DeepLabV3+, which yields low performance, especially in thin osification regions, cluttered backgrounds, and small datasets.Table 8shows the significant impact of the RSSeg model on age estimation. Figure 13.(a) The proposed RSSeg model reduces computational complexity in training and testing, preserving ossification centers, which are highlighted by the yellow bounding box region; (b) the DeepLabV3+ model yields a segmented result with the loss of ossification centers in the region, which are vital for age estimation and highlighted by the red bounding box area. The proposed RSSeg model captures the dynamic interval values to understand bone and ossification patterns by empirically calculating joint- and grid-specific entropies, means, and standard deviations. The model, which uses a 10 √ó 10 grid and neglects higher-level background regions with lower entropy and lower average pixel values, reduces unnecessary computation, as shown inFigure 4. However, the retained grids are further divided into 5 √ó 5 sub-grids, and the same retention and discarding criteria are applied to each sub-grid. Then, the region-specific symbolic segmentation is applied to the retained growth pattern, capturing local interval variations within each region and enabling the efficient handling of intra-class variations in both foreground and background. Applying a dynamic region-specific interval representation effectively handles regions with varied illumination, noise, insubstantial osification regions, and acquisition-related challenges in X-rays, thereby generalizing the segmentation task. Furthermore, the RSSeg model performs well on benchmark datasets such as RSNA and DHA, demonstrating its generality and robustness across diverse ethnic backgrounds. Even though the RSSeg model produces efficient results, the pelvis poses additional challenges, including overlapping bones, high-density anatomical features across joints, and regions that are overly noisy and difficult to distinguish from growth patterns. In addition, these challenges make both contextual and non-contextual segmentation difficult. In the future, integrating Transformer models such as MiDaS and ZoeDepth with an RSSeg model approach could help retain the depth anatomical features of high-density overlapping bone across joints to enhance effective segmentation. Further, to assess the impact of the RSSeg model, the segmented results were analyzed using the VGG16, ResNet50, and InceptionV3 models.Table 8shows that the proposed RSSeg improved overall accuracy and F1-score by 7.4% and 6.9% with VGG16, 5.3% and 6.0% with ResNet50, and 8.7% and 7.7% with InceptionV3, respectively. These results show that effective segmentation is vital for retaining growth patterns, which is essential for accurate age estimation. These results show that RSSeg is most beneficial when discriminative cues are small and local; when cues are large and global, the model becomes comparatively more critical. To examine the results of a deep learning-based age-estimation, a model trained with varying train‚àítest ratios was used. The models require a large number of training samples to capture contextual information. Due to limited dataset availability per class, the 80-20 split shows better performance on VGG16, ResNet50, and InceptionV3 than other splits, such as 60-40 and 70-30, which fail to capture more diverse growth patterns. To analyze the effectiveness of deep learning-based age estimation, the models were trained on various joints with different hyperparameters. VGG16 achieves overall performance, with a 93.8% accuracy, a 92.6% precision, a 92.9% recall, and a 92.8% F1-score, demonstrating the model‚Äôs ability to retain fine-grained local features and surpassing other models. However, ResNet-50 and Inception-V3 yield slightly lower performance, reflecting their greater focus on global shape and multi-scale context. In contrast, deeper pyramidal architectures partially capture growth patterns through depth-wise convolution and pooling. For the wrist, elbow, and shoulder, accuracies with VGG16 (95.9%, 94.6%, and 93.9%) were higher than with ResNet50 (94.2%, 93.1%, and 92.5%) or InceptionV3 (92.5%, 93.7%, and 91.9%). The carpal bones of the wrist, the CRITOE bones of the elbow, and the humeral head and tubercles of the shoulder ossification centers are visible at an early stage, and their fusion states gradually vary. Additionally, fusion of the shoulder joints is complete at the adolescent stage. Capturing minute variation among the dominant regions is essential without losing contextual information in the deeper pooling process. In contrast, ResNet50 shows a slight improvement in accuracy, reaching 91.0% compared to VGG16. The femur head, trochanters, and iliac crest of the pelvis; these high-density, varied anatomical features are captured effectively by ResNet50. Despite improvements, the pelvis remains the most challenging anatomical structure, resulting in lower segmentation performance and lower classification efficiency compared to other joints.",
            "6. Conclusions": "The RSSeg model offers a robust and automated solution for skeletal age estimation in the Indian medicolegal context, addressing the key limitations of existing methods. The study utilizes a novel dataset comprising both retrospective and prospective X-ray data that capture multiple ossification centers, including the wrist, elbow, shoulder, and pelvis, across a diverse age range from 0 to 21 years and above. The distinguishing characteristic of the proposed model is its retention of soft growth regions, which are crucial for extracting prominent growth features using small, multi-patch analysis. By leveraging entropy-based filtering, it reduces the computational burden and efficiently eliminates high-purity background regions. The symbolic segmentation applied to the foreground patches effectively retains the growth pattern. The RSSeg model, compared with popular existing segmented models (U-Net, Attention U-Net, TransU-Net, DeepLabV3+, Adaptive Otsu, and Watershed), achieves a notable improvement in pixel accuracy of 91.5% and generalizability across multiple joints, while maintaining lower computational overhead through targeted patch analysis. Furthermore, integrating RSSeg with the VGG16 classifier significantly improves overall accuracy, increasing from 83.5% to 92.2% compared to the case without segmentation, clearly indicating that focused-region processing minimizes redundant computations and enhances feature discrimination without compromising accuracy. This process has the potential to improve transparency and ensure fairness in legal matters and medical practice. Future work aims to scale the model by incorporating additional ossification centers and expanding the dataset to refine intra-class variability and improve class imbalance handling, thereby achieving even greater accuracy. Integrating transformer models with an RSSeg model approach that retains both contextual and non-contextual variation with depth anatomical features across joints for effective segmentation. The multi-joints collectively offer substantial diagnostic value, as they span a wide range of maturation. To address this effectively, a transformer-based weighted multifocal age estimation is required."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/1999-4893/18/12/765",
        "scraped_at": "2025-12-05 23:57:19"
    },
    {
        "title": "Clinically Oriented Evaluation of Transfer Learning Strategies for Cross-Site Breast Cancer Histopathology Classification",
        "authors": "byLiana StanescuandCosmin Stoica-Spahiu",
        "journal": "Appl. Sci.2025,15(23), 12819;https://doi.org/10.3390/app152312819- 4 Dec 2025",
        "abstract": "Background/Objectives: Breast cancer diagnosis based on histopathological examination remains the most reliable and widely accepted approach in clinical practice, despite being time-consuming and prone to inter-observer variability. While deep learning methods have achieved high accuracy in medical image classification, their cross-site generalization remains limited due to differences in staining protocols and image acquisition. This study aims to evaluate and compare three clinically relevant adaptation strategies to improve model robustness under domain shift.Methods: The ResNet50V2 model, pretrained on ImageNet and further fine-tuned on the Kaggle Breast Histopathology Images dataset, was subsequently adapted to the BreaKHis dataset under three clinically relevant transfer strategies: (i) threshold calibration without retraining (site calibration), (ii) head-only fine-tuning (light FT), and (iii) full fine-tuning (full FT). Experiments were performed on an internal balanced dataset and on the public BreaKHis dataset using strict patient-level splitting to avoid data leakage. Evaluation metrics included accuracy, precision, recall, F1-score, ROC-AUC, and PR-AUC, computed per magnification level (40√ó, 100√ó, 200√ó, 400√ó).Results: Full fine-tuning consistently yielded the highest performance across all magnifications, reaching up to 0.983 ROC-AUC and 0.980 sensitivity at 400√ó. At 40√ó and 100√ó, the model correctly identified over 90% of malignant cases, with ROC-AUC values of 0.9500 and 0.9332, respectively. Head-only fine-tuning led to moderate gains (e.g., sensitivity up to 0.859 at 200√ó), while threshold calibration showed limited improvements (ROC-AUC ranging between 0.60‚Äì0.73). Grad-CAM analysis revealed more stable and focused attention maps after full fine-tuning, though they did not always align with diagnostically relevant regions.Conclusions: Our findings confirm that full fine-tuning is essential for robust cross-site deployment of histopathology AI systems, particularly at high magnifications. Lighter strategies such as threshold calibration or head-only fine-tuning may serve as practical alternatives in resource-constrained environments where retraining is not feasible.Keywords:breast cancer;histopathological images;transfer learning;domain shift;model adaptation;clinical deployment",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Breast cancer remains one of the most prevalent malignancies worldwide and a leading cause of cancer-related mortality among women [1]. Histopathological analysis of tissue biopsies represents the reference standard for diagnosis, allowing pathologists to differentiate between benign and malignant lesions and to assess tumor subtype and grade. However, manual inspection of slides is time-consuming and subject to inter-observer variability, motivating the development of computer-aided diagnosis (CAD) systems. Deep learning has transformed medical image analysis, with convolutional neural networks (CNNs) and more recently transformer-based architectures achieving state-of-the-art performance across multiple applications [2,3]. ResNet [4], DenseNet [5], ConvNeXt [6], and Vision Transformers [7] have all been applied successfully to histopathological image classification. The Breast Cancer Histopathological Database (BreaKHis) [8] has become a widely used benchmark, comprising 7909 images from 82 patients labeled as benign or malignant across four magnification levels (40√ó, 100√ó, 200√ó, 400√ó). This magnification diversity makes it well suited for evaluating scale-dependent performance. In this study, the ResNet50V2 model was first fine-tuned on the Kaggle Breast Histopathology dataset to establish a balanced baseline and subsequently adapted to BreaKHis to assess cross-site generalization under controlled conditions. A key challenge in digital pathology is that models must generalize across acquisition sites, staining conditions, and patient populations. Many existing studies rely on random image-level splits, allowing patches from the same patient in both training and test sets and overestimating performance. In realistic clinical scenarios, however, models encounter entirely new patients, making patient-level independence essential. Although recent advances in attention mechanisms, stain normalization, and self-supervised learning have improved robustness, the relative effectiveness of different adaptation depths under domain shift remains insufficiently explored. No prior work has systematically compared threshold calibration, head-only fine-tuning, and full fine-tuning using strict patient-level separation and magnification-specific evaluation. This study offers the following contributions: A unified comparison of three adaptation strategies‚Äîthreshold calibration, head-only fine-tuning, and full fine-tuning‚Äîfor cross-site histopathology classification.Strict patient-level separation for both internal and BreaKHis datasets to prevent information leakage.A magnification-aware analysis quantifying the trade-off between adaptation cost and diagnostic performance.A clinically oriented discussion on model interpretability and realistic deployment scenarios. This perspective emphasizes robustness, reproducibility, and practical deployability within clinically realistic cross-site conditions.",
            "2. Related Work": "Deep learning has significantly advanced breast cancer histopathology classification, with numerous studies proposing increasingly sophisticated architectures and training strategies.Table 1summarizes representative approaches, highlighting major model types, datasets, and common advantages and limitations. Several publicly available datasets support research in histopathology image classification. Among these, the BreaKHis database [8] is one of the most widely used benchmarks, providing 7909 images from 82 patients across four magnifications (40√ó, 100√ó, 200√ó, 400√ó), which enables systematic assessment of scale-dependent performance. The Kaggle Breast Histopathology dataset contains over 250,000 balanced benign and malignant patches derived from whole-slide images, making it suitable for pretraining and establishing robust feature representations. These datasets are frequently adopted in the literature due to their accessibility, diversity, and relevance to clinical diagnostic workflows. Building on these benchmark datasets, early work on BreaKHis established foundational baselines for patch-level classification. Spanhol et al. [8] introduced baseline CNN models, while Araujo et al. [9] trained deeper networks from scratch to improve feature representation. Bayramoglu et al. [10] proposed a multi-scale CNN that integrates information across magnifications, demonstrating improved accuracy over single-scale approaches. As shown inTable 1, these initial methods laid the groundwork for deep learning in histopathology but remained limited by shallow architectures, high data requirements, or computational overhead. Transfer learning approaches soon emerged as a more efficient alternative. Studies employing pretrained architectures‚Äîsuch as ResNet, DenseNet, and related CNN families‚Äîreported substantial performance gains compared with training from scratch [11]. Structured deep learning models further enhanced interpretability and tissue representation by incorporating prior spatial relationships [12]. However, as indicated inTable 1, these frameworks typically rely on full fine-tuning and do not evaluate the effectiveness of lighter adaptation strategies. In addition to these CNN-based innovations, multiple studies explored modeling patch-level relevance and weakly supervised aggregation using attention-based multiple-instance learning, which improved lesion localization and interpretability in histopathology workflows [13]. Stain-normalization and domain adaptation techniques further addressed color variability across medical centers, yielding measurable gains in cross-site robustness [14]. More recently, self-supervised learning frameworks have leveraged large unlabeled repositories to extract transferable histopathology representations, reducing the dependence on extensive annotations [15]. Transformer-based architecture has recently gained prominence due to their ability to model long-range dependencies in tissue structure. Vision Transformer and Swin Transformer variants have achieved state-of-the-art results in lung adenocarcinoma [16], esophageal pathology [17], and laryngeal tumor grading [18,19]. Feature-disentangled transformers have further improved robustness and interpretability in squamous cell carcinoma grading [20]. Despite their strong performance,Table 1shows that these models often require large, annotated datasets and lack systematic evaluation under cross-site domain shift. Complementary lines of work have explored radiology-driven feature fusion [21], attention-enhanced CNNs [22], multi-branch and fusion frameworks [23], ensemble optimization [24], and multiscale deep networks [25,26]. While these approaches improve robustness across various medical imaging tasks, their computational complexity can hinder clinical deployment. A persistent methodological challenge in digital pathology is the widespread use of random image-level splits, which allow patches from the same patient to appear in both training and test sets. This practice introduces information leakage and leads to overly optimistic performance estimates. Campanella et al. [27] demonstrated that clinical-grade computational pathology requires strict slide- and patient-level independence to avoid unrealistic accuracy inflation. This underscores the importance of rigorous, patient-level data separation, particularly when evaluating cross-site generalization. Several studies have addressed domain shift using stain normalization [14] or adversarial domain adaptation [28], while self-supervised learning has shown promise for representation learning with limited labels [15]. However, the complexity and computational cost of these approaches, also summarized inTable 1, limit their scalability in low-resource clinical environments. Interpretability is another critical challenge. Grad-CAM [29] remains the most widely used saliency method in histopathology, but multiple studies have shown that its attention maps may not reliably correspond to diagnostically meaningful structures [30]. These limitations motivate the need to evaluate explainability methods alongside classification performance, particularly in clinically oriented workflows. Despite architectural progress, key gaps remain in the literature: Most studies rely on a single adaptation strategy, typically full fine-tuning;Lightweight strategies such as threshold calibration or head-only fine-tuning remain underexplored.Few works systematically quantify performance variation across magnification levels.Robustness under domain shift is not consistently evaluated. These limitations motivate the present study, which provides a unified comparison of adaptation strategies under clinically realistic, patient-level cross-site conditions. Table 1.Representative Deep Learning Approaches for Histopathology Classification. Table 1.Representative Deep Learning Approaches for Histopathology Classification.Study/ApproachModel Type/InnovationDataset(s)AdvantagesLimitationsSpanhol et al. [8]Baseline CNNsBreaKHisEstablishes initial benchmarksLimited depth; modest performanceAraujo et al. [9]Deeper CNNsBreaKHisImproved representational capacityRequires large datasets; trained from scratchBayramoglu et al. [10]Multi-scale CNNBreaKHisIntegrates multi-magnification cuesHigh computational costTransfer learning CNNs [11]Pretrained ResNet/DenseNetBreaKHisEfficient training; strong accuracyFocus on full fine-tuningStructured DL models [12]Structured/hierarchical CNNsBreaKHisBetter modeling of tissue structureIncreased architecture complexityTransformers [16,17,18,19,20]ViT, Swin, graph-attention, FDTVarious histopathology datasetsSOTA performance; improved interpretabilityRequires large datasets; limited cross-site testingFeature fusion & ensembles [23,24,25,26]CNN fusion, multi-branch, ensemble optimizationVarious medical imaging datasetsEnhanced robustness; multi-feature integrationHeavy models; risk of overfittingRadiology-driven approaches [21]Multi-fractal + fusionMammographyStrong texture encodingNot histopathology-specificStain normalization & adaptation [14,28]Stain transfer, adversarial adaptationHistopathologyMitigates domain shiftComplex pipeline; higher computeSelf-supervised learning [15]SSL pretrainingMedical Imaging DatasetsStrong features with few labelsLong training timeCampanella et al. [27]Weakly supervised WSI classificationWhole-slide imagesDemonstrates need for patient-level independenceShows risk of inflated accuracy under patch-level splits",
            "3. Materials and Methods": "This section provides a step-by-step description of the proposed workflow, encompassing dataset preparation, model architecture, training strategies, and implementation details. The overall pipeline consists of four main stages: (i) dataset preprocessing and patient-level splitting, (ii) baseline fine-tuning on an internal dataset (Kaggle IDC), (iii) cross-site adaptation and evaluation on the external BreaKHis dataset, and (iv) performance assessment under different magnifications (40√ó‚Äì400√ó). 3.1. Generalized Algorithm for Histopathological Image ClassificationTo place our experimental setup in a broader context, we first outline a generalized deep learning workflow for histopathological image classification. This abstract pipeline, summarized in Algorithm 1, captures the essential stages common to most modern CAD systems based on convolutional or transformer-based architectures, independently of the specific datasets or backbone models. In the subsequent subsections, we instantiate this workflow using ResNet50V2, the Kaggle IDC dataset for pretraining, and BreaKHis as the external target dataset.Algorithm 1.Generalized Workflow for Histopathological Image ClassificationInput:Histopathology dataset D = {(Ii, yi)} consisting of whole-slide images (WSIs)or pre-extracted patches Ii with class labels yi (e.g., benign/malignant)Output:Trained model M* and predicted labels ≈∑i for unseen samplesStage 1: Data preparation and splittingAcquire raw WSIs or image patches from one or more institutionsOptionally perform stain normalization and artefact removalIf WSIs are used:Tile each WSI into patchesDiscard background tilesResize all patches to a fixed input resolution (e.g., 224 √ó 224)Split patients (not images) into train, validation, and test setsEnsure that no patient appears in more than one split (patient-level separation)Stage 2: Model initializationChoose a backbone architecture (e.g., CNN or Vision Transformer)Initialize the backbone with pretrained weights (e.g., ImageNet) or random weightsReplace the final classification layer with a task-specific head (e.g., 2-class output)Select an adaptation strategy:(a) Threshold calibration only(b) Head-only fine-tuning(c) Full fine-tuningStage 3: Training/adaptationIf using threshold calibration:Apply the pretrained model to the target training/validation dataLearn an optimal decision threshold on the validation setElse:Freeze or unfreeze layers according to the chosen adaptation strategyTrain the model on the training set using a suitable loss (e.g., weighted cross-entropy)Monitor performance on the validation setSelect the best checkpoint M* based on a validation metric (e.g., F1-score)Stage 4: Inference and evaluationApply M* to the test set to obtain predicted probabilities piApply the chosen decision threshold to obtain final labels ≈∑iCompute evaluation metrics (accuracy, precision, recall, F1-score, ROC-AUC, PR-AUC)Optionally generate interpretability maps (e.g., Grad-CAM) for qualitative analysisReturn:Trained model M* and performance metrics on the test set 3.2. Proposed Training and Evaluation Procedure (Kaggle ‚Üí BreaKHis)The detailed training and evaluation procedure is summarized in Algorithm 2, which describes how the ResNet50V2 model, initialized with ImageNet weights, was first fine-tuned on the Kaggle IDC dataset at the patient level to obtain a balanced baseline model (M*). This pretrained model was then adapted and evaluated on the BreaKHis dataset under three strategies‚Äîthreshold calibration (SiteCalib), head-only fine-tuning (LightFT), and full fine-tuning (FullFT)‚Äîfor each magnification level.Evaluation metrics included accuracy, F1-score, ROC-AUC, and PR-AUC, computed per magnification level, along with Grad-CAM visualizations for interpretability. This design ensures strict patient-level independence and realistic cross-site generalization.Algorithm 2.Proposed Training and Evaluation Procedure for Kaggle ‚Üí BreaKHis Adap-tationInput: ResNet50V2 architecture M (initialized with ImageNet weights),internal dataset D_Kaggle = {train, val, test},external dataset D_BreaKHis = {train, val, test},magnifications = {40√ó, 100√ó, 200√ó, 400√ó}# Stage 1: Baseline fine-tuning on internal dataset (Kaggle)Train M on D_Kaggle[train] using weighted cross-entropy lossValidate on D_Kaggle[val]; select best checkpoint M*Save M* as pretrained baseline for adaptation# Stage 2: Adaptation and evaluation on BreaKHisfor each magnification m in magnifications dofor each adaptation strategy s in {SiteCalib, LightFT, FullFT} doif s == SiteCalib:Apply M* without retraining; calibrate decision threshold on val[m]if s == LightFT:Freeze backbone; train classifier head for 5 epochs on train[m]if s == FullFT:Unfreeze all layers; train for 5 epochs on train[m]Select best checkpoint by validation F1Optimize threshold on validation setEvaluate on test[m]; record ACC, F1, ROC-AUC, PR-AUC; generate Grad-CAMend forend forOutput: Comparative performance and interpretability for all strategies 3.3. DatasetsTwo datasets were employed in this study. The first is the Kaggle Breast Histopathology Images dataset (IDC) [Dataset S1], hereafter referred to as the initial dataset. It is acquired and organized at the patient level, with each patient contributing multiple image patches labeled as benign or malignant. To avoid data leakage, all patches from a given patient were assigned exclusively to the training, validation, or test split. The training set was balanced through random undersampling to ensure equal class distribution, whereas the validation and test sets preserved the natural prevalence (Table 2).Table 2.Composition of the Kaggle Breast Histopathology Images dataset [Dataset S1], split at patient level. The training set was class-balanced through undersampling; validation and test sets retained natural class distribution.The choice of the two datasets was motivated by their complementary characteristics. The Kaggle IDC dataset provides a large-scale, patient-organized collection of patches that enables stable pretraining and balanced feature extraction before cross-site adaptation. In contrast, the BreaKHis dataset is a widely used public benchmark offering multi-magnification (40√ó‚Äì400√ó) histopathological images collected under heterogeneous acquisition conditions, making it well suited for evaluating robustness under domain shift. Using Kaggle IDC for pretraining and BreaKHis for external evaluation models a realistic clinical scenario in which a system trained at one institution must generalize to images acquired elsewhere.The second dataset is BreaKHis (Breast Cancer Histopathological Database) [8], [Dataset S2], a public benchmark widely used for breast cancer classification.BreaKHis was selected because it provides multi-magnification (40√ó‚Äì400√ó) histopathological images acquired under diverse staining and imaging conditions, making it an ideal dataset for studying robustness under cross-site domain shift. Its widespread use in prior studies also allows direct comparison with existing methods and supports reproducibility.BreaKHis comprises benign and malignant images collected at four magnification levels (40√ó, 100√ó, 200√ó, and 400√ó). Each patient contributes multiple images organized into a directory structure by class, patient, and magnification.To ensure patient-level independence, we generated a new split into training, validation, and test subsets, disjoint at patient level. The split was performed using the unique patient identifier embedded in the folder name (e.g., SOB_B_A_14-22549AB), assigning all images from a single patient exclusively to one subset. The split ratios were approximately 70% for training, 15% for validation, and 15% for testing. This procedure was repeated independently for each magnification level (40√ó, 100√ó, 200√ó, 400√ó). The number of images per split and magnification is reported inTable 3.Table 3.Composition of the BreaKHis dataset [Dataset S2], showing the number of benign and malignant samples per split and magnification level (patient-level split). No class balancing was applied.Class imbalance was intentionally preserved to reflect real-world diagnostic variability. Although malignant samples are more frequent at higher magnifications, no artificial resampling or balancing was applied. Instead, to mitigate potential bias, the binary cross-entropy loss was weighted inversely to class frequency, and extensive data augmentation (random flips, rotations, cropping, and color jittering) was applied to increase the effective diversity of minority-class samples.To ensure consistency across datasets and experimental conditions, all images were resized to 224 √ó 224 pixels prior to training or evaluation. This standardization was applied uniformly to both the internal dataset and BreaKHis, regardless of their original resolution or magnification.Image-level splitting was intentionally avoided, as it can lead to information leakage when patches from the same patient appear in both training and test sets. This design choice ensures clinical realism, where the model is evaluated on entirely unseen patients. 3.4. Model ArchitectureIn this study we employed ResNet50V2, a deep residual convolutional neural network consisting of 50 layers organized into four residual stages. The architecture builds on the original residual learning framework introduced by He et al. [4] and incorporates the improved identity mappings with pre-activation blocks proposed in ResNet v2 [31]. These pre-activation residual units (BatchNorm ‚Üí ReLU ‚Üí Conv) enhance gradient flow and training stability, making them particularly effective in transfer learning settings.The backbone contains approximately 25.6 million parameters [4,31] and is composed of:an initial stem (7 √ó 7 convolution + max pooling),four residual stages with bottleneck blocks (1 √ó 1 ‚Üí 3 √ó 3 ‚Üí 1 √ó 1 convolutions),global average pooling,a fully connected classification head.To adapt the architecture to our binary classification task (benign vs. malignant), the original 1000-class fully connected layer was replaced with a two-unit classifier followed by a softmax activation. All convolutional layers retained ImageNet-pretrained weights, while the classifier head was initialized randomly.A schematic overview of ResNet50V2 is provided inFigure 1, illustrating the hierarchical progression from low-level texture features to high-level morphological patterns relevant to histopathology.Figure 1.ResNet50V2 architecture used in this study.This architecture was selected because:ResNet backbones have proven robust and widely adopted in computational pathology and biomedical imaging [4].The residual structure enables stable gradient propagation and reliable transfer learning even with limited training data [31].ResNet50V2 offers a favorable balance between representational depth and computational efficiency, making it suitable for both lightweight and full fine-tuning scenarios. 3.5. Training StrategiesThree strategies were compared for cross-site generalization:Site Calibration (Threshold Calibration): in this approach, the model pretrained on ImageNet is directly applied to the BreaKHis dataset. Only the classification threshold is recalibrated using the BreaKHis validation set, with no retraining of model weights. This setting simulates a realistic clinical scenario where model parameters cannot be updated and only decision calibration is feasible.Light Fine-Tuning: the backbone was frozen, and only the classifier head was retrained for five epochs on BreaKHis training data. The best checkpoint was selected on validation, and the threshold was recalibrated on validation.Full Fine-Tuning: all network layers were unfrozen and optimized jointly for five epochs on BreaKHis. The best model was again selected on validation, followed by threshold calibration.All strategies were applied following an initial fine-tuning stage on the Kaggle Breast Histopathology dataset, used to obtain a clinically relevant starting point before domain adaptation to BreaKHis.To account for the natural class imbalance present in BreaKHis, all training procedures employed a weighted cross-entropy loss, where class weights were computed inversely to class frequency within the training subset. This weighting helped ensure balanced gradient updates despite unequal class distributions.A visual summary of the three adaptation strategies is provided inFigure 2, illustrating the differences in model retraining scope, data usage, and evaluation setup.Figure 2.Overview of the three adaptation strategies evaluated in this study: site (threshold) calibration (no retraining), light fine-tuning (training only the classifier head), and full fine-tuning (training all layers). The diagram summarizes the data flow and model adaptation steps used in each strategy. 3.6. Implementation DetailsAll models were implemented in PyTorch 2.2. Optimization employed the AdamW optimizer with a weight decay of 1 √ó 10‚àí4. A standard classification loss function was used, as provided by the PyTorch 2.2 framework. For light fine-tuning, the learning rate was set to 1 √ó 10‚àí3, whereas for full fine-tuning a smaller rate of 1 √ó 10‚àí5was used to mitigate overfitting. The batch size was fixed at 32. Input images were resized to 224 √ó 224 pixels, normalized to ImageNet statistics, and augmented using geometric transformations. All experiments were conducted on a GPU-enabled workstation (CUDA).For each model and magnification level, the classification threshold was optimized on the validation set based on the F1-score. Model outputs were evaluated across thresholds in the range [0.0, 1.0], and the value that maximized the F1-score was applied to the test set. This procedure was performed independently for each adaptation strategy (threshold calibration, light fine-tuning, full fine-tuning) and each magnification level.During training, we applied common data augmentation techniques to enhance generalization, including random horizontal/vertical flips, rotations, cropping and resizing to 224 √ó 224 pixels, and occasional color jittering. Augmentations were applied only to the training set, while validation and test sets remained unaltered.Each fine-tuning procedure was limited to five epochs, a choice supported by preliminary convergence analysis showing that validation performance plateaued within the first few epochs. This setup was designed to emulate realistic low-resource adaptation scenarios, as the focus of this study was on lightweight transfer strategies rather than extended retraining. Overfitting was prevented through early stopping based on validation F1-score, weight decay regularization (1 √ó 10‚àí4), data augmentation, and strict patient-level separation across all subsets. 3.7. Evaluation MetricsTo quantitatively assess classification performance under cross-site domain shift, we employed commonly used metrics in medical image analysis: accuracy, precision, recall (sensitivity), F1-score, the Receiver Operating Characteristic Area Under the Curve (ROC-AUC), and the Precision‚ÄìRecall Area Under the Curve (PR-AUC). Their analytical definitions are provided below.LetTP = true positivesTN = true negativesFP = false positivesFN = false negativesAccuracy=TP+TNTP+TN+FP+FNAccuracy=TP+TNTP+TN+FP+FN(1)Precision=TPTP+FPPrecision=TPTP+FP(2)Recall=TPTP+FNRecall=TPTP+FN(3)F1=2‚ãÖPrecision‚ãÖRecallPrecision+RecallF1=2‚ãÖPrecision‚ãÖRecallPrecision+Recall(4)Since benign and malignant classes are imbalanced in BreaKHis, F1-score is preferred over accuracy because it better reflects the trade-off between false negatives and false positives.ROC-AUC measures the classifier‚Äôs ability to discriminate between classes across all decision thresholds. It is defined as:ùëÖùëÇùê∂-ùê¥ùëàùê∂=‚à´10ùëáùëÉùëÖ(ùêπùëÉùëÖ)ùëëùêπùëÉùëÖROC-AUC=‚à´01TPR(FPR)dFPR(5)where TPR = Recall and FPR = FP/(FP + TN).PR-AUC is more informative when the positive class is underrepresented:PR-AUC=‚à´10ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ(ùëÖùëíùëêùëéùëôùëô)ùëëùëÖùëíùëêùëéùëôùëôPR-AUC=‚à´01Precision(Recall)dRecall(6)These metrics were selected because:F1-score captures performance under class imbalance, which is critical when malignant cases must not be missed.ROC-AUC provides a threshold-independent assessment of separability and is widely used for medical classifiers.PR-AUC is particularly sensitive to false positives and false negatives in imbalanced datasets.Combining ROC-AUC and PR-AUC allows robust evaluation of model discrimination under domain shift (Kaggle ‚Üí BreaKHis).Together, these metrics offer a comprehensive evaluation framework aligned with clinical diagnostic requirements. 3.8. Interpretability Analysis (Grad-CAM)To explore model interpretability, we employed Gradient-weighted Class Activation Mapping (Grad-CAM) [29] as a post-hoc visualization method. Grad-CAM is not used during classification and has no influence on model predictions. Instead, it operates after the model has produced its output, tracing gradients back through the final convolutional layer to identify which spatial regions contributed most to the decision. The resulting saliency maps were upsampled and superimposed on the input images, highlighting areas of high model attention.For our experiments, Grad-CAM was applied to the last convolutional block of ResNet50V2 (layer4). We generated heatmaps for test images at all four magnifications (40√ó, 100√ó, 200√ó, 400√ó) under the three adaptation strategies (site calibration, light fine-tuning, full fine-tuning). For malignant samples, we targeted the malignant class; for the benign sample, we targeted the benign class (visualizing the predicted/prototypical evidence for each case). A number of representative cases (benign and malignant) were further reviewed by a board-certified pathologist to assess whether the highlighted regions corresponded to diagnostically meaningful structures.",
            "3.1. Generalized Algorithm for Histopathological Image Classification": "To place our experimental setup in a broader context, we first outline a generalized deep learning workflow for histopathological image classification. This abstract pipeline, summarized in Algorithm 1, captures the essential stages common to most modern CAD systems based on convolutional or transformer-based architectures, independently of the specific datasets or backbone models. In the subsequent subsections, we instantiate this workflow using ResNet50V2, the Kaggle IDC dataset for pretraining, and BreaKHis as the external target dataset.Algorithm 1.Generalized Workflow for Histopathological Image ClassificationInput:Histopathology dataset D = {(Ii, yi)} consisting of whole-slide images (WSIs)or pre-extracted patches Ii with class labels yi (e.g., benign/malignant)Output:Trained model M* and predicted labels ≈∑i for unseen samplesStage 1: Data preparation and splittingAcquire raw WSIs or image patches from one or more institutionsOptionally perform stain normalization and artefact removalIf WSIs are used:Tile each WSI into patchesDiscard background tilesResize all patches to a fixed input resolution (e.g., 224 √ó 224)Split patients (not images) into train, validation, and test setsEnsure that no patient appears in more than one split (patient-level separation)Stage 2: Model initializationChoose a backbone architecture (e.g., CNN or Vision Transformer)Initialize the backbone with pretrained weights (e.g., ImageNet) or random weightsReplace the final classification layer with a task-specific head (e.g., 2-class output)Select an adaptation strategy:(a) Threshold calibration only(b) Head-only fine-tuning(c) Full fine-tuningStage 3: Training/adaptationIf using threshold calibration:Apply the pretrained model to the target training/validation dataLearn an optimal decision threshold on the validation setElse:Freeze or unfreeze layers according to the chosen adaptation strategyTrain the model on the training set using a suitable loss (e.g., weighted cross-entropy)Monitor performance on the validation setSelect the best checkpoint M* based on a validation metric (e.g., F1-score)Stage 4: Inference and evaluationApply M* to the test set to obtain predicted probabilities piApply the chosen decision threshold to obtain final labels ≈∑iCompute evaluation metrics (accuracy, precision, recall, F1-score, ROC-AUC, PR-AUC)Optionally generate interpretability maps (e.g., Grad-CAM) for qualitative analysisReturn:Trained model M* and performance metrics on the test set",
            "3.2. Proposed Training and Evaluation Procedure (Kaggle ‚Üí BreaKHis)": "The detailed training and evaluation procedure is summarized in Algorithm 2, which describes how the ResNet50V2 model, initialized with ImageNet weights, was first fine-tuned on the Kaggle IDC dataset at the patient level to obtain a balanced baseline model (M*). This pretrained model was then adapted and evaluated on the BreaKHis dataset under three strategies‚Äîthreshold calibration (SiteCalib), head-only fine-tuning (LightFT), and full fine-tuning (FullFT)‚Äîfor each magnification level. Evaluation metrics included accuracy, F1-score, ROC-AUC, and PR-AUC, computed per magnification level, along with Grad-CAM visualizations for interpretability. This design ensures strict patient-level independence and realistic cross-site generalization.Algorithm 2.Proposed Training and Evaluation Procedure for Kaggle ‚Üí BreaKHis Adap-tationInput: ResNet50V2 architecture M (initialized with ImageNet weights),internal dataset D_Kaggle = {train, val, test},external dataset D_BreaKHis = {train, val, test},magnifications = {40√ó, 100√ó, 200√ó, 400√ó}# Stage 1: Baseline fine-tuning on internal dataset (Kaggle)Train M on D_Kaggle[train] using weighted cross-entropy lossValidate on D_Kaggle[val]; select best checkpoint M*Save M* as pretrained baseline for adaptation# Stage 2: Adaptation and evaluation on BreaKHisfor each magnification m in magnifications dofor each adaptation strategy s in {SiteCalib, LightFT, FullFT} doif s == SiteCalib:Apply M* without retraining; calibrate decision threshold on val[m]if s == LightFT:Freeze backbone; train classifier head for 5 epochs on train[m]if s == FullFT:Unfreeze all layers; train for 5 epochs on train[m]Select best checkpoint by validation F1Optimize threshold on validation setEvaluate on test[m]; record ACC, F1, ROC-AUC, PR-AUC; generate Grad-CAMend forend forOutput: Comparative performance and interpretability for all strategies",
            "3.3. Datasets": "Two datasets were employed in this study. The first is the Kaggle Breast Histopathology Images dataset (IDC) [Dataset S1], hereafter referred to as the initial dataset. It is acquired and organized at the patient level, with each patient contributing multiple image patches labeled as benign or malignant. To avoid data leakage, all patches from a given patient were assigned exclusively to the training, validation, or test split. The training set was balanced through random undersampling to ensure equal class distribution, whereas the validation and test sets preserved the natural prevalence (Table 2). Table 2.Composition of the Kaggle Breast Histopathology Images dataset [Dataset S1], split at patient level. The training set was class-balanced through undersampling; validation and test sets retained natural class distribution. The choice of the two datasets was motivated by their complementary characteristics. The Kaggle IDC dataset provides a large-scale, patient-organized collection of patches that enables stable pretraining and balanced feature extraction before cross-site adaptation. In contrast, the BreaKHis dataset is a widely used public benchmark offering multi-magnification (40√ó‚Äì400√ó) histopathological images collected under heterogeneous acquisition conditions, making it well suited for evaluating robustness under domain shift. Using Kaggle IDC for pretraining and BreaKHis for external evaluation models a realistic clinical scenario in which a system trained at one institution must generalize to images acquired elsewhere. The second dataset is BreaKHis (Breast Cancer Histopathological Database) [8], [Dataset S2], a public benchmark widely used for breast cancer classification. BreaKHis was selected because it provides multi-magnification (40√ó‚Äì400√ó) histopathological images acquired under diverse staining and imaging conditions, making it an ideal dataset for studying robustness under cross-site domain shift. Its widespread use in prior studies also allows direct comparison with existing methods and supports reproducibility. BreaKHis comprises benign and malignant images collected at four magnification levels (40√ó, 100√ó, 200√ó, and 400√ó). Each patient contributes multiple images organized into a directory structure by class, patient, and magnification. To ensure patient-level independence, we generated a new split into training, validation, and test subsets, disjoint at patient level. The split was performed using the unique patient identifier embedded in the folder name (e.g., SOB_B_A_14-22549AB), assigning all images from a single patient exclusively to one subset. The split ratios were approximately 70% for training, 15% for validation, and 15% for testing. This procedure was repeated independently for each magnification level (40√ó, 100√ó, 200√ó, 400√ó). The number of images per split and magnification is reported inTable 3. Table 3.Composition of the BreaKHis dataset [Dataset S2], showing the number of benign and malignant samples per split and magnification level (patient-level split). No class balancing was applied. Class imbalance was intentionally preserved to reflect real-world diagnostic variability. Although malignant samples are more frequent at higher magnifications, no artificial resampling or balancing was applied. Instead, to mitigate potential bias, the binary cross-entropy loss was weighted inversely to class frequency, and extensive data augmentation (random flips, rotations, cropping, and color jittering) was applied to increase the effective diversity of minority-class samples. To ensure consistency across datasets and experimental conditions, all images were resized to 224 √ó 224 pixels prior to training or evaluation. This standardization was applied uniformly to both the internal dataset and BreaKHis, regardless of their original resolution or magnification. Image-level splitting was intentionally avoided, as it can lead to information leakage when patches from the same patient appear in both training and test sets. This design choice ensures clinical realism, where the model is evaluated on entirely unseen patients.",
            "3.4. Model Architecture": "In this study we employed ResNet50V2, a deep residual convolutional neural network consisting of 50 layers organized into four residual stages. The architecture builds on the original residual learning framework introduced by He et al. [4] and incorporates the improved identity mappings with pre-activation blocks proposed in ResNet v2 [31]. These pre-activation residual units (BatchNorm ‚Üí ReLU ‚Üí Conv) enhance gradient flow and training stability, making them particularly effective in transfer learning settings. The backbone contains approximately 25.6 million parameters [4,31] and is composed of: an initial stem (7 √ó 7 convolution + max pooling),four residual stages with bottleneck blocks (1 √ó 1 ‚Üí 3 √ó 3 ‚Üí 1 √ó 1 convolutions),global average pooling,a fully connected classification head. To adapt the architecture to our binary classification task (benign vs. malignant), the original 1000-class fully connected layer was replaced with a two-unit classifier followed by a softmax activation. All convolutional layers retained ImageNet-pretrained weights, while the classifier head was initialized randomly. A schematic overview of ResNet50V2 is provided inFigure 1, illustrating the hierarchical progression from low-level texture features to high-level morphological patterns relevant to histopathology. Figure 1.ResNet50V2 architecture used in this study. This architecture was selected because: ResNet backbones have proven robust and widely adopted in computational pathology and biomedical imaging [4].The residual structure enables stable gradient propagation and reliable transfer learning even with limited training data [31].ResNet50V2 offers a favorable balance between representational depth and computational efficiency, making it suitable for both lightweight and full fine-tuning scenarios.",
            "3.5. Training Strategies": "Three strategies were compared for cross-site generalization: Site Calibration (Threshold Calibration): in this approach, the model pretrained on ImageNet is directly applied to the BreaKHis dataset. Only the classification threshold is recalibrated using the BreaKHis validation set, with no retraining of model weights. This setting simulates a realistic clinical scenario where model parameters cannot be updated and only decision calibration is feasible.Light Fine-Tuning: the backbone was frozen, and only the classifier head was retrained for five epochs on BreaKHis training data. The best checkpoint was selected on validation, and the threshold was recalibrated on validation.Full Fine-Tuning: all network layers were unfrozen and optimized jointly for five epochs on BreaKHis. The best model was again selected on validation, followed by threshold calibration. All strategies were applied following an initial fine-tuning stage on the Kaggle Breast Histopathology dataset, used to obtain a clinically relevant starting point before domain adaptation to BreaKHis. To account for the natural class imbalance present in BreaKHis, all training procedures employed a weighted cross-entropy loss, where class weights were computed inversely to class frequency within the training subset. This weighting helped ensure balanced gradient updates despite unequal class distributions. A visual summary of the three adaptation strategies is provided inFigure 2, illustrating the differences in model retraining scope, data usage, and evaluation setup. Figure 2.Overview of the three adaptation strategies evaluated in this study: site (threshold) calibration (no retraining), light fine-tuning (training only the classifier head), and full fine-tuning (training all layers). The diagram summarizes the data flow and model adaptation steps used in each strategy.",
            "3.6. Implementation Details": "All models were implemented in PyTorch 2.2. Optimization employed the AdamW optimizer with a weight decay of 1 √ó 10‚àí4. A standard classification loss function was used, as provided by the PyTorch 2.2 framework. For light fine-tuning, the learning rate was set to 1 √ó 10‚àí3, whereas for full fine-tuning a smaller rate of 1 √ó 10‚àí5was used to mitigate overfitting. The batch size was fixed at 32. Input images were resized to 224 √ó 224 pixels, normalized to ImageNet statistics, and augmented using geometric transformations. All experiments were conducted on a GPU-enabled workstation (CUDA). For each model and magnification level, the classification threshold was optimized on the validation set based on the F1-score. Model outputs were evaluated across thresholds in the range [0.0, 1.0], and the value that maximized the F1-score was applied to the test set. This procedure was performed independently for each adaptation strategy (threshold calibration, light fine-tuning, full fine-tuning) and each magnification level. During training, we applied common data augmentation techniques to enhance generalization, including random horizontal/vertical flips, rotations, cropping and resizing to 224 √ó 224 pixels, and occasional color jittering. Augmentations were applied only to the training set, while validation and test sets remained unaltered. Each fine-tuning procedure was limited to five epochs, a choice supported by preliminary convergence analysis showing that validation performance plateaued within the first few epochs. This setup was designed to emulate realistic low-resource adaptation scenarios, as the focus of this study was on lightweight transfer strategies rather than extended retraining. Overfitting was prevented through early stopping based on validation F1-score, weight decay regularization (1 √ó 10‚àí4), data augmentation, and strict patient-level separation across all subsets.",
            "3.7. Evaluation Metrics": "To quantitatively assess classification performance under cross-site domain shift, we employed commonly used metrics in medical image analysis: accuracy, precision, recall (sensitivity), F1-score, the Receiver Operating Characteristic Area Under the Curve (ROC-AUC), and the Precision‚ÄìRecall Area Under the Curve (PR-AUC). Their analytical definitions are provided below. Let TP = true positivesTN = true negativesFP = false positivesFN = false negatives Accuracy=TP+TNTP+TN+FP+FNAccuracy=TP+TNTP+TN+FP+FN(1) Precision=TPTP+FPPrecision=TPTP+FP(2) Recall=TPTP+FNRecall=TPTP+FN(3) F1=2‚ãÖPrecision‚ãÖRecallPrecision+RecallF1=2‚ãÖPrecision‚ãÖRecallPrecision+Recall(4) Since benign and malignant classes are imbalanced in BreaKHis, F1-score is preferred over accuracy because it better reflects the trade-off between false negatives and false positives. ROC-AUC measures the classifier‚Äôs ability to discriminate between classes across all decision thresholds. It is defined as:ùëÖùëÇùê∂-ùê¥ùëàùê∂=‚à´10ùëáùëÉùëÖ(ùêπùëÉùëÖ)ùëëùêπùëÉùëÖROC-AUC=‚à´01TPR(FPR)dFPR(5)where TPR = Recall and FPR = FP/(FP + TN). PR-AUC is more informative when the positive class is underrepresented:PR-AUC=‚à´10ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ(ùëÖùëíùëêùëéùëôùëô)ùëëùëÖùëíùëêùëéùëôùëôPR-AUC=‚à´01Precision(Recall)dRecall(6) These metrics were selected because: F1-score captures performance under class imbalance, which is critical when malignant cases must not be missed.ROC-AUC provides a threshold-independent assessment of separability and is widely used for medical classifiers.PR-AUC is particularly sensitive to false positives and false negatives in imbalanced datasets.Combining ROC-AUC and PR-AUC allows robust evaluation of model discrimination under domain shift (Kaggle ‚Üí BreaKHis). Together, these metrics offer a comprehensive evaluation framework aligned with clinical diagnostic requirements.",
            "3.8. Interpretability Analysis (Grad-CAM)": "To explore model interpretability, we employed Gradient-weighted Class Activation Mapping (Grad-CAM) [29] as a post-hoc visualization method. Grad-CAM is not used during classification and has no influence on model predictions. Instead, it operates after the model has produced its output, tracing gradients back through the final convolutional layer to identify which spatial regions contributed most to the decision. The resulting saliency maps were upsampled and superimposed on the input images, highlighting areas of high model attention. For our experiments, Grad-CAM was applied to the last convolutional block of ResNet50V2 (layer4). We generated heatmaps for test images at all four magnifications (40√ó, 100√ó, 200√ó, 400√ó) under the three adaptation strategies (site calibration, light fine-tuning, full fine-tuning). For malignant samples, we targeted the malignant class; for the benign sample, we targeted the benign class (visualizing the predicted/prototypical evidence for each case). A number of representative cases (benign and malignant) were further reviewed by a board-certified pathologist to assess whether the highlighted regions corresponded to diagnostically meaningful structures.",
            "4. Results": "4.1. Initial DatasetOn the Kaggle IDC dataset (reorganized at patient level), the balanced training set achieved ROC-AUC = 0.983 and F1w = 0.975. Validation and test sets preserved natural prevalence and reached ROC-AUC = 0.898/0.876 and F1w = 0.886/0.854, respectively (Table 4).Table 4.Initial dataset results (patient-level split). Train set was balanced; validation and test preserved natural prevalence. 4.2. Site Calibration on BreaKHisApplying the internal ResNet50V2 model directly to BreaKHis with threshold calibration produced modest improvements. Performance remained heterogeneous across magnifications, with ROC-AUC ranging from 0.60 (40√ó) to 0.73 (200√ó). Weighted F1 varied between 0.58 and 0.72, and balanced accuracy between 0.57 and 0.68 (Table 5).Table 5.Results of site calibration on BreaKHis (patient-level split). 4.3. Light Fine-TuningTraining only the classifier head for 5 epochs on BreaKHis increased sensitivity and F1 at intermediate magnifications. ROC-AUC ranged from 0.64 (400√ó) to 0.75 (200√ó), while F1w reached up to 0.73 at 200√ó (Table 6).Table 6.Light fine-tuning (head-only, 5 epochs) on BreaKHis test set. Best head selected on validation; thresholds selected on validation. 4.4. Full Fine-TuningOptimizing all ResNet50V2 layers for 5 epochs yielded the best performance across magnifications. ROC-AUC ranged from 0.92 to 0.95, with F1w between 0.86 and 0.93. Balanced accuracy exceeded 0.86 at all magnifications (Table 7).Table 7.Results of full fine-tuning on BreaKHis.These results suggest that full fine-tuning is particularly suited for clinical usage, especially in early-stage diagnosis scenarios where missing malignant samples must be avoided.In terms of computational efficiency, site calibration required only threshold adjustment (‚âà2 min per magnification). Head-only fine-tuning completed within ‚âà45 min per magnification, whereas full fine-tuning required ‚âà3.5‚Äì4 h on a single NVIDIA RTX 3090 GPU (NVIDIA, Santa Clara, CA, USA) (batch size 32). GPU memory usage was approximately 5.2 GB for head-only and 9.8 GB for full fine-tuning. These results illustrate the trade-off between computational cost and diagnostic gain, underscoring the practicality of lighter strategies in resource-limited hospitals. 4.5. Comparative AnalysisTable 8summarizes the comparative performance across the three strategies. Threshold calibration remained the weakest, light fine-tuning achieved moderate improvements, and full fine-tuning consistently achieved the best results across magnifications. In addition,Table 9reports the number and proportion of malignant cases correctly classified under each strategy, stratified by magnification.Table 8.Comparative results on BreaKHis (per magnification, across methods).Table 9.Malignant test cases correctly classified, per magnification and adaptation strategy. Each row shows the number and percentage of malignant images from the test set that were correctly classified using one of the three adaptation strategies. The results reflect classification performance at each magnification level, highlighting the improved detection with full fine-tuning.In addition to accuracy and F1-score, we evaluated the diagnostic sensitivity, specificity, and ROC-AUC for each adaptation strategy and magnification level. These metrics are summarized inTable 10and offer a more clinically interpretable view of model performance, illustrating the balance between correctly identifying malignant cases and avoiding false positives. It is important to note that the ROC-AUC values reported inTable 8are macro-averaged across classes, whereasTable 10presents binary ROC-AUC for malignancy, which is more directly aligned with clinical relevance.Table 10.Sensitivity, specificity, and ROC-AUC for each adaptation strategy and magnification level on the BreaKHis test set. 4.6. Grad-CAM VisualizationTo further examine model interpretability, we generated Grad-CAM heatmaps for representative benign and malignant cases under all three adaptation strategies (SiteCalib, LightFT, and FullFT).Figure 3,Figure 4andFigure 5illustrate the overlaid heatmaps for three diagnostic scenarios: a benign fibroadenoma at 40√ó, an invasive lobular carcinoma at 40√ó, and a mucinous carcinoma at 400√ó. Each figure compares the original histopathology image with Grad-CAM outputs from the three adaptation methods. While the highlighted regions differ across strategies and magnification levels, their diagnostic relevance‚Äîparticularly the limited correspondence between saliency maps and true morphological hallmarks‚Äîis discussed in detail inSection 5.Figure 3.Original benign fibroadenoma image at 40√ó and corresponding Grad-CAM visualizations generated by the three adaptation strategies (SiteCalib, LightFT, FullFT).Figure 4.Original image of invasive lobular carcinoma at 40√ó and the associated Grad-CAM maps produced by SiteCalib, LightFT, and FullFT.Figure 5.Original mucinous carcinoma image at 400√ó along with Grad-CAM heatmaps for the three adaptation strategies (SiteCalib, LightFT, FullFT).",
            "4.1. Initial Dataset": "On the Kaggle IDC dataset (reorganized at patient level), the balanced training set achieved ROC-AUC = 0.983 and F1w = 0.975. Validation and test sets preserved natural prevalence and reached ROC-AUC = 0.898/0.876 and F1w = 0.886/0.854, respectively (Table 4). Table 4.Initial dataset results (patient-level split). Train set was balanced; validation and test preserved natural prevalence.",
            "4.2. Site Calibration on BreaKHis": "Applying the internal ResNet50V2 model directly to BreaKHis with threshold calibration produced modest improvements. Performance remained heterogeneous across magnifications, with ROC-AUC ranging from 0.60 (40√ó) to 0.73 (200√ó). Weighted F1 varied between 0.58 and 0.72, and balanced accuracy between 0.57 and 0.68 (Table 5). Table 5.Results of site calibration on BreaKHis (patient-level split).",
            "4.3. Light Fine-Tuning": "Training only the classifier head for 5 epochs on BreaKHis increased sensitivity and F1 at intermediate magnifications. ROC-AUC ranged from 0.64 (400√ó) to 0.75 (200√ó), while F1w reached up to 0.73 at 200√ó (Table 6). Table 6.Light fine-tuning (head-only, 5 epochs) on BreaKHis test set. Best head selected on validation; thresholds selected on validation.",
            "4.4. Full Fine-Tuning": "Optimizing all ResNet50V2 layers for 5 epochs yielded the best performance across magnifications. ROC-AUC ranged from 0.92 to 0.95, with F1w between 0.86 and 0.93. Balanced accuracy exceeded 0.86 at all magnifications (Table 7). Table 7.Results of full fine-tuning on BreaKHis. These results suggest that full fine-tuning is particularly suited for clinical usage, especially in early-stage diagnosis scenarios where missing malignant samples must be avoided. In terms of computational efficiency, site calibration required only threshold adjustment (‚âà2 min per magnification). Head-only fine-tuning completed within ‚âà45 min per magnification, whereas full fine-tuning required ‚âà3.5‚Äì4 h on a single NVIDIA RTX 3090 GPU (NVIDIA, Santa Clara, CA, USA) (batch size 32). GPU memory usage was approximately 5.2 GB for head-only and 9.8 GB for full fine-tuning. These results illustrate the trade-off between computational cost and diagnostic gain, underscoring the practicality of lighter strategies in resource-limited hospitals.",
            "4.5. Comparative Analysis": "Table 8summarizes the comparative performance across the three strategies. Threshold calibration remained the weakest, light fine-tuning achieved moderate improvements, and full fine-tuning consistently achieved the best results across magnifications. In addition,Table 9reports the number and proportion of malignant cases correctly classified under each strategy, stratified by magnification. Table 8.Comparative results on BreaKHis (per magnification, across methods). Table 9.Malignant test cases correctly classified, per magnification and adaptation strategy. Each row shows the number and percentage of malignant images from the test set that were correctly classified using one of the three adaptation strategies. The results reflect classification performance at each magnification level, highlighting the improved detection with full fine-tuning. In addition to accuracy and F1-score, we evaluated the diagnostic sensitivity, specificity, and ROC-AUC for each adaptation strategy and magnification level. These metrics are summarized inTable 10and offer a more clinically interpretable view of model performance, illustrating the balance between correctly identifying malignant cases and avoiding false positives. It is important to note that the ROC-AUC values reported inTable 8are macro-averaged across classes, whereasTable 10presents binary ROC-AUC for malignancy, which is more directly aligned with clinical relevance. Table 10.Sensitivity, specificity, and ROC-AUC for each adaptation strategy and magnification level on the BreaKHis test set.",
            "4.6. Grad-CAM Visualization": "To further examine model interpretability, we generated Grad-CAM heatmaps for representative benign and malignant cases under all three adaptation strategies (SiteCalib, LightFT, and FullFT).Figure 3,Figure 4andFigure 5illustrate the overlaid heatmaps for three diagnostic scenarios: a benign fibroadenoma at 40√ó, an invasive lobular carcinoma at 40√ó, and a mucinous carcinoma at 400√ó. Each figure compares the original histopathology image with Grad-CAM outputs from the three adaptation methods. While the highlighted regions differ across strategies and magnification levels, their diagnostic relevance‚Äîparticularly the limited correspondence between saliency maps and true morphological hallmarks‚Äîis discussed in detail inSection 5. Figure 3.Original benign fibroadenoma image at 40√ó and corresponding Grad-CAM visualizations generated by the three adaptation strategies (SiteCalib, LightFT, FullFT). Figure 4.Original image of invasive lobular carcinoma at 40√ó and the associated Grad-CAM maps produced by SiteCalib, LightFT, and FullFT. Figure 5.Original mucinous carcinoma image at 400√ó along with Grad-CAM heatmaps for the three adaptation strategies (SiteCalib, LightFT, FullFT).",
            "5. Discussion": "5.1. Key FindingsThe comparative results reported inTable 8highlight the effect of different adaptation strategies on cross-site generalization. Site calibration alone achieves moderate performance, with ROC-AUC values ranging between 0.6045 (40√ó) and 0.7300 (200√ó). Introducing light fine-tuning, where only the classifier head is retrained on target-site data, consistently improves performance across most magnifications. At 40√ó, ROC-AUC increases from 0.6045 to 0.6954, although F1-weighted remains stable around 0.68, suggesting improved discrimination without a proportional gain in balanced decision-making. The most notable improvement occurs at 100√ó, where ROC-AUC rises from 0.6371 to 0.6875 and F1-weighted from 0.5790 to 0.6772, indicating that head adaptation successfully mitigates domain shift at this scale. At 200√ó, which already exhibited the strongest baseline performance, light fine-tuning yields marginal but consistent gains (ROC-AUC from 0.7300 to 0.7456; F1w from 0.7189 to 0.7305), confirming its robustness. Conversely, at 400√ó, the two approaches converge (AUC ‚âà 0.6458; F1w ‚âà 0.6383), suggesting that light fine-tuning is insufficient to address the larger discrepancy at this magnification. Full fine-tuning, however, substantially outperforms both prior strategies, reaching ROC-AUC values between 0.9332 and 0.9832 and F1w between 0.8631 and 0.9332 across all magnifications, with the highest score at 40√ó (AUC = 0.9500, F1w = 0.9306). These results indicate that while calibration and head-only adaptation provide partial mitigation of domain shift, full fine-tuning remains necessary for optimal cross-site generalization.Figure 6shows the comparative performance of site calibration, light fine-tuning, and full fine-tuning across the four magnifications. Full fine-tuning consistently outperformed the other strategies, with the largest gap observed at 40√ó and 100√ó.Figure 6.Comparative performance across adaptation strategies (SiteCalib, LightFT, FullFT) and magnifications (40√ó, 100√ó, 200√ó, 400√ó) on the BreaKHis test sets. Bars indicate ROC-AUC (left) and F1-weighted (right). Full fine-tuning consistently outperforms the other strategies, with the largest improvements at 40√ó and 100√ó. 5.2. Interpretation of Magnification-Dependent BehaviorDifferences between Light Fine-Tuning and Full Fine-Tuning were magnification dependent. At low magnifications (40√ó, 100√ó), diagnostic cues derive largely from global architectural structures such as lobular arrangement, stromal composition, and tissue organization‚Äîfeatures that pretrained representations capture reasonably well. At higher magnifications (200√ó, 400√ó), discrimination increasingly relies on subtle nuclear abnormalities (pleomorphism, nucleolar prominence) and mitotic activity, which require deeper adaptation of convolutional filters. This histopathological distinction explains why Full Fine-Tuning consistently outperformed Light Fine-Tuning, particularly at higher magnifications. 5.3. Comparison with Prior WorkWhen comparing our findings with previous research on the BreaKHis dataset, several patterns emerge. Early CNN-based approaches by Spanhol et al. [8] and Araujo et al. [9] reported accuracies between 77‚Äì83% under image-level splits, while Bayramoglu et al. [10] improved robustness through multi-scale feature aggregation. Later transfer learning studies leveraging ResNet or DenseNet architectures achieved AUC values around 0.85‚Äì0.90 [12], and more recent transformer-based methods, such as attention-based multiple-instance learning models [13], reported AUC values approaching 0.90‚Äì0.92. However, all these studies relied on image-level splits, enabling patches from the same patient to appear in both training and test sets‚Äîa practice known to artificially inflate performance due to information leakage.More recent CNN and ensemble-based architectures have reported even higher performance, including attention-guided CNNs [22], equilibrium-optimized ensembles [24], multiscale feature aggregation frameworks such as FabNet [25], and hybrid deep learning ensembles [26], frequently exceeding 90% accuracy. Yet, like earlier work, these studies also used non-patient-level data splits, making direct comparison challenging.Table 11andTable 12summarize representative approaches from literature.Table 11.Breast Cancer Histopathology (BreaKHis, Patch-Level ‚Üí Patient-Level Comparison).Table 12.Transformers in Other Histopathology Domains (not directly comparable).In contrast to these methods, our evaluation strictly enforces patient-level independence across both internal and external datasets, providing a more clinically realistic estimate of generalization. Under this protocol, full fine-tuning achieved ROC-AUC values of 0.92‚Äì0.95 and weighted F1-scores of 0.86‚Äì0.93 across magnifications, which are competitive with or superior to previously reported results. Unlike prior studies that typically evaluated a single transfer learning pipeline, our work systematically compares three adaptation strategies‚Äîthreshold calibration, head-only fine-tuning, and full fine-tuning‚Äîunder identical conditions of domain shift and magnification-specific evaluation. This design isolates the effect of adaptation depth and reveals the practical trade-offs between computational cost and diagnostic performance.By integrating interpretability analysis and expert review, our study extends previous literature by linking model performance with clinical explainability‚Äîan aspect rarely addressed in histopathology-focused transfer learning studies. Overall, our results reinforce the importance of patient-level separation and full model adaptation for reliable cross-site deployment. 5.4. Clinical Applicability and Deployment ScenariosBeyond quantitative performance, it is important to consider how each adaptation strategy aligns with realistic clinical workflows.From a clinical perspective, lightweight adaptation strategies may be practical in triage or second-reader workflows, where the model assists pathologists in flagging potentially malignant slides for priority review. Threshold calibration or head-only fine-tuning can thus provide a feasible compromise between computational efficiency and diagnostic support, particularly when data access or computing resources are limited. Conversely, full fine-tuning remains indispensable in high-stakes diagnostic settings such as confirmatory diagnosis or multi-institutional deployment, where performance stability and inter-site robustness are critical for patient safety.The consistent improvement in malignant detection across magnifications and adaptation strategies highlights the practical value of full fine-tuning in clinical environments. Identifying over 90% of malignant images at lower magnifications (40√ó, 100√ó) is particularly valuable in diagnostic triage or screening settings. Moreover, the threshold calibration approach, though limited, offers a lightweight alternative when retraining is not feasible‚Äîsupporting deployment in resource-constrained institutions. 5.5. Interpretability and Limitations of Grad-CAMGrad-CAM was expected to provide visual cues supporting AI-based predictions by highlighting diagnostically relevant regions, such as malignant nuclei or tumor epithelium. However, expert pathologist review of several representative cases (shown inFigure 3,Figure 4andFigure 5) revealed limited clinical interpretability.In the benign fibroadenoma case at 40√ó (Figure 3), the hematoxylin‚Äìeosin image shows mammary glandular tissue completely occupied by fibroadenoma, characterized by distorted ductal structures embedded in fibrous stroma. None of the AI-generated Grad-CAM maps‚Äîregardless of adaptation strategy‚Äîcaptured this global architectural alteration that defines the benign diagnosis.In the lobular carcinoma example at 40√ó (Figure 4), light fine-tuning and full fine-tuning produced more spatially focused heatmaps compared with SiteCalib, yet the highlighted regions still did not consistently correspond to genuine lobular patterns such as non-cohesive tumor cells arranged in single-file infiltration.In the mucinous carcinoma patch at 400√ó (Figure 5), none of the strategies reliably emphasized the characteristic cytoplasmic mucin or the floating tumor cell clusters. Heatmaps frequently concentrated on background tissue or nonspecific stromal areas, missing the true diagnostic features entirely.These observations underscore a fundamental limitation of Grad-CAM in histopathology: because it relies on coarse feature maps from the last convolutional layer, it cannot capture the subtle nuclear details or fine-grained morphological cues essential for pathological interpretation. As a result, Grad-CAM often reflects broad regions of statistical model attention rather than clinically meaningful structures. This behavior is consistent with prior reports on the limited diagnostic reliability of saliency-based methods [29,30].Although Grad-CAM remains useful for assessing the internal consistency of learned feature representations, particularly the improved spatial concentration observed after full fine-tuning‚Äîour findings indicate that it should not be interpreted as a standalone diagnostic explanation.Quantitative interpretability metrics such as IoU or Dice coefficients could not be computed due to the absence of region-level annotations in either dataset. Future work will incorporate expert-annotated ROIs or nuclei segmentation masks to enable objective alignment analysis. Beyond Grad-CAM, more advanced interpretability frameworks‚Äîsuch as multi-scale attention maps, concept-based attribution, or nuclei-level saliency‚Äîmay provide more precise localization and improved clinical trustworthiness. 5.6. Limitations and Future WorkAlthough the proposed workflow demonstrates strong cross-site generalization and competitive diagnostic accuracy, several limitations must be acknowledged.First, the study focused on a single backbone architecture (ResNet50V2), which, although robust and widely validated, may not capture all relevant morphological features that transformer-based or hybrid architectures could exploit.Second, the fine-tuning process was intentionally limited to five epochs to emulate a lightweight clinical adaptation scenario. While this configuration demonstrated fast convergence and good generalization, longer training or advanced optimization schedules could potentially yield further improvements.Third, the present work addressed binary classification (benign vs. malignant) without analyzing specific histological subtypes, which may limit clinical granularity. In addition, the interpretability analysis relied primarily on Grad-CAM, which provides coarse heatmaps and may not accurately localize fine cellular details.Finally, external validation was performed on BreaKHis only; extending the evaluation to additional multi-institutional datasets would be necessary to confirm robustness across diverse acquisition protocols.These limitations highlight important directions for future research, including full fine-tuning of multiple architectures, incorporation of multi-class pathology subtypes, and integration of more advanced explainability techniques.Both datasets used in this study may contain inherent biases, including limited demographic diversity, acquisition from a small number of institutions, and variability in staining protocols. Such biases can affect model generalization in clinical deployment. Addressing these issues will require future validation on multi-center datasets with balanced demographic representation and standardized acquisition protocols.Statistical significance testing (e.g., DeLong test for ROC-AUC comparison) was not performed due to the absence of per-patient paired ROC data. Future work will incorporate such analyses to strengthen comparisons across adaptation strategies.Moreover, the interpretability evaluation relied mainly on Grad-CAM; quantitative alignment metrics and nuclei-level saliency analysis were not available but will be explored in future work.",
            "5.1. Key Findings": "The comparative results reported inTable 8highlight the effect of different adaptation strategies on cross-site generalization. Site calibration alone achieves moderate performance, with ROC-AUC values ranging between 0.6045 (40√ó) and 0.7300 (200√ó). Introducing light fine-tuning, where only the classifier head is retrained on target-site data, consistently improves performance across most magnifications. At 40√ó, ROC-AUC increases from 0.6045 to 0.6954, although F1-weighted remains stable around 0.68, suggesting improved discrimination without a proportional gain in balanced decision-making. The most notable improvement occurs at 100√ó, where ROC-AUC rises from 0.6371 to 0.6875 and F1-weighted from 0.5790 to 0.6772, indicating that head adaptation successfully mitigates domain shift at this scale. At 200√ó, which already exhibited the strongest baseline performance, light fine-tuning yields marginal but consistent gains (ROC-AUC from 0.7300 to 0.7456; F1w from 0.7189 to 0.7305), confirming its robustness. Conversely, at 400√ó, the two approaches converge (AUC ‚âà 0.6458; F1w ‚âà 0.6383), suggesting that light fine-tuning is insufficient to address the larger discrepancy at this magnification. Full fine-tuning, however, substantially outperforms both prior strategies, reaching ROC-AUC values between 0.9332 and 0.9832 and F1w between 0.8631 and 0.9332 across all magnifications, with the highest score at 40√ó (AUC = 0.9500, F1w = 0.9306). These results indicate that while calibration and head-only adaptation provide partial mitigation of domain shift, full fine-tuning remains necessary for optimal cross-site generalization.Figure 6shows the comparative performance of site calibration, light fine-tuning, and full fine-tuning across the four magnifications. Full fine-tuning consistently outperformed the other strategies, with the largest gap observed at 40√ó and 100√ó. Figure 6.Comparative performance across adaptation strategies (SiteCalib, LightFT, FullFT) and magnifications (40√ó, 100√ó, 200√ó, 400√ó) on the BreaKHis test sets. Bars indicate ROC-AUC (left) and F1-weighted (right). Full fine-tuning consistently outperforms the other strategies, with the largest improvements at 40√ó and 100√ó.",
            "5.2. Interpretation of Magnification-Dependent Behavior": "Differences between Light Fine-Tuning and Full Fine-Tuning were magnification dependent. At low magnifications (40√ó, 100√ó), diagnostic cues derive largely from global architectural structures such as lobular arrangement, stromal composition, and tissue organization‚Äîfeatures that pretrained representations capture reasonably well. At higher magnifications (200√ó, 400√ó), discrimination increasingly relies on subtle nuclear abnormalities (pleomorphism, nucleolar prominence) and mitotic activity, which require deeper adaptation of convolutional filters. This histopathological distinction explains why Full Fine-Tuning consistently outperformed Light Fine-Tuning, particularly at higher magnifications.",
            "5.3. Comparison with Prior Work": "When comparing our findings with previous research on the BreaKHis dataset, several patterns emerge. Early CNN-based approaches by Spanhol et al. [8] and Araujo et al. [9] reported accuracies between 77‚Äì83% under image-level splits, while Bayramoglu et al. [10] improved robustness through multi-scale feature aggregation. Later transfer learning studies leveraging ResNet or DenseNet architectures achieved AUC values around 0.85‚Äì0.90 [12], and more recent transformer-based methods, such as attention-based multiple-instance learning models [13], reported AUC values approaching 0.90‚Äì0.92. However, all these studies relied on image-level splits, enabling patches from the same patient to appear in both training and test sets‚Äîa practice known to artificially inflate performance due to information leakage. More recent CNN and ensemble-based architectures have reported even higher performance, including attention-guided CNNs [22], equilibrium-optimized ensembles [24], multiscale feature aggregation frameworks such as FabNet [25], and hybrid deep learning ensembles [26], frequently exceeding 90% accuracy. Yet, like earlier work, these studies also used non-patient-level data splits, making direct comparison challenging.Table 11andTable 12summarize representative approaches from literature. Table 11.Breast Cancer Histopathology (BreaKHis, Patch-Level ‚Üí Patient-Level Comparison). Table 12.Transformers in Other Histopathology Domains (not directly comparable). In contrast to these methods, our evaluation strictly enforces patient-level independence across both internal and external datasets, providing a more clinically realistic estimate of generalization. Under this protocol, full fine-tuning achieved ROC-AUC values of 0.92‚Äì0.95 and weighted F1-scores of 0.86‚Äì0.93 across magnifications, which are competitive with or superior to previously reported results. Unlike prior studies that typically evaluated a single transfer learning pipeline, our work systematically compares three adaptation strategies‚Äîthreshold calibration, head-only fine-tuning, and full fine-tuning‚Äîunder identical conditions of domain shift and magnification-specific evaluation. This design isolates the effect of adaptation depth and reveals the practical trade-offs between computational cost and diagnostic performance. By integrating interpretability analysis and expert review, our study extends previous literature by linking model performance with clinical explainability‚Äîan aspect rarely addressed in histopathology-focused transfer learning studies. Overall, our results reinforce the importance of patient-level separation and full model adaptation for reliable cross-site deployment.",
            "5.4. Clinical Applicability and Deployment Scenarios": "Beyond quantitative performance, it is important to consider how each adaptation strategy aligns with realistic clinical workflows. From a clinical perspective, lightweight adaptation strategies may be practical in triage or second-reader workflows, where the model assists pathologists in flagging potentially malignant slides for priority review. Threshold calibration or head-only fine-tuning can thus provide a feasible compromise between computational efficiency and diagnostic support, particularly when data access or computing resources are limited. Conversely, full fine-tuning remains indispensable in high-stakes diagnostic settings such as confirmatory diagnosis or multi-institutional deployment, where performance stability and inter-site robustness are critical for patient safety. The consistent improvement in malignant detection across magnifications and adaptation strategies highlights the practical value of full fine-tuning in clinical environments. Identifying over 90% of malignant images at lower magnifications (40√ó, 100√ó) is particularly valuable in diagnostic triage or screening settings. Moreover, the threshold calibration approach, though limited, offers a lightweight alternative when retraining is not feasible‚Äîsupporting deployment in resource-constrained institutions.",
            "5.5. Interpretability and Limitations of Grad-CAM": "Grad-CAM was expected to provide visual cues supporting AI-based predictions by highlighting diagnostically relevant regions, such as malignant nuclei or tumor epithelium. However, expert pathologist review of several representative cases (shown inFigure 3,Figure 4andFigure 5) revealed limited clinical interpretability. In the benign fibroadenoma case at 40√ó (Figure 3), the hematoxylin‚Äìeosin image shows mammary glandular tissue completely occupied by fibroadenoma, characterized by distorted ductal structures embedded in fibrous stroma. None of the AI-generated Grad-CAM maps‚Äîregardless of adaptation strategy‚Äîcaptured this global architectural alteration that defines the benign diagnosis. In the lobular carcinoma example at 40√ó (Figure 4), light fine-tuning and full fine-tuning produced more spatially focused heatmaps compared with SiteCalib, yet the highlighted regions still did not consistently correspond to genuine lobular patterns such as non-cohesive tumor cells arranged in single-file infiltration. In the mucinous carcinoma patch at 400√ó (Figure 5), none of the strategies reliably emphasized the characteristic cytoplasmic mucin or the floating tumor cell clusters. Heatmaps frequently concentrated on background tissue or nonspecific stromal areas, missing the true diagnostic features entirely. These observations underscore a fundamental limitation of Grad-CAM in histopathology: because it relies on coarse feature maps from the last convolutional layer, it cannot capture the subtle nuclear details or fine-grained morphological cues essential for pathological interpretation. As a result, Grad-CAM often reflects broad regions of statistical model attention rather than clinically meaningful structures. This behavior is consistent with prior reports on the limited diagnostic reliability of saliency-based methods [29,30]. Although Grad-CAM remains useful for assessing the internal consistency of learned feature representations, particularly the improved spatial concentration observed after full fine-tuning‚Äîour findings indicate that it should not be interpreted as a standalone diagnostic explanation. Quantitative interpretability metrics such as IoU or Dice coefficients could not be computed due to the absence of region-level annotations in either dataset. Future work will incorporate expert-annotated ROIs or nuclei segmentation masks to enable objective alignment analysis. Beyond Grad-CAM, more advanced interpretability frameworks‚Äîsuch as multi-scale attention maps, concept-based attribution, or nuclei-level saliency‚Äîmay provide more precise localization and improved clinical trustworthiness.",
            "5.6. Limitations and Future Work": "Although the proposed workflow demonstrates strong cross-site generalization and competitive diagnostic accuracy, several limitations must be acknowledged. First, the study focused on a single backbone architecture (ResNet50V2), which, although robust and widely validated, may not capture all relevant morphological features that transformer-based or hybrid architectures could exploit. Second, the fine-tuning process was intentionally limited to five epochs to emulate a lightweight clinical adaptation scenario. While this configuration demonstrated fast convergence and good generalization, longer training or advanced optimization schedules could potentially yield further improvements. Third, the present work addressed binary classification (benign vs. malignant) without analyzing specific histological subtypes, which may limit clinical granularity. In addition, the interpretability analysis relied primarily on Grad-CAM, which provides coarse heatmaps and may not accurately localize fine cellular details. Finally, external validation was performed on BreaKHis only; extending the evaluation to additional multi-institutional datasets would be necessary to confirm robustness across diverse acquisition protocols. These limitations highlight important directions for future research, including full fine-tuning of multiple architectures, incorporation of multi-class pathology subtypes, and integration of more advanced explainability techniques. Both datasets used in this study may contain inherent biases, including limited demographic diversity, acquisition from a small number of institutions, and variability in staining protocols. Such biases can affect model generalization in clinical deployment. Addressing these issues will require future validation on multi-center datasets with balanced demographic representation and standardized acquisition protocols. Statistical significance testing (e.g., DeLong test for ROC-AUC comparison) was not performed due to the absence of per-patient paired ROC data. Future work will incorporate such analyses to strengthen comparisons across adaptation strategies. Moreover, the interpretability evaluation relied mainly on Grad-CAM; quantitative alignment metrics and nuclei-level saliency analysis were not available but will be explored in future work.",
            "6. Conclusions": "This study provides a systematic comparison of three adaptation strategies‚Äîthreshold calibration, head-only fine-tuning, and full fine-tuning‚Äîfor cross-site breast histopathology image classification using ResNet50V2. Threshold calibration offered only modest gains, and head-only fine-tuning improved performance primarily at intermediate magnifications. In contrast, full fine-tuning consistently achieved the highest accuracy, with ROC-AUC values between 0.9332 and 0.9832 and F1-weighted scores between 0.8631 and 0.9332 across all magnifications, detecting over 90% of malignant cases at 40√ó and 100√ó. These findings demonstrate that full model adaptation is essential for reliable cross-site generalization and should be preferred in clinical deployment scenarios. Interpretability analysis revealed important limitations of Grad-CAM. Although attention maps became more stable after full fine-tuning, they often failed to highlight diagnostically meaningful structures, such as the global architectural patterns in benign fibroadenoma or the nuclear abnormalities characteristic of mucinous carcinoma. This confirms that saliency-based visualization methods provide limited clinical insight and must be complemented by expert pathological review. Overall, the proposed evaluation framework highlights practical trade-offs between adaptation cost and diagnostic reliability and can support informed decisions regarding model deployment in digital pathology workflows. Future work will focus on exploring more advanced interpretability methods tailored to histopathology‚Äîsuch as multi-scale attention mechanisms, nuclei-level attribution, or weakly supervised localization‚Äîand extending cross-site validation to multi-institutional datasets and modern architectures (e.g., Vision Transformers, ConvNeXt). Addressing these challenges is essential for strengthening clinical trustworthiness and enabling safe, routine integration of AI-assisted histopathology in medical practice. The main contributions of this study can be summarized as follows: We propose a unified, patient-level evaluation pipeline for cross-site breast histopathology classification, eliminating information leakage.We conduct the first systematic comparison of three adaptation strategies (threshold calibration, head-only fine-tuning, full fine-tuning) under identical domain-shift conditions and across four magnifications.We provide a clinically grounded analysis of malignant detection rates, demonstrating where lightweight adaptation is feasible and where full model retraining is required.We include a pathologist-reviewed interpretability assessment, highlighting fundamental limitations of Grad-CAM for clinical reasoning.We design a lightweight, reproducible cross-site adaptation framework that reflects realistic constraints in medical institutions. From a methodological perspective, this study represents a complete end-to-end pipeline designed, implemented, and validated by the authors‚Äîincluding dataset reorganization, patient-level splitting, adaptation strategy design, model training, quantitative evaluation, and interpretability assessment. The systematic analysis carried out by the authors provides a transparent and reproducible framework for future cross-site studies, aligning the conclusions with the specific methodological and analytical contributions documented in the manuscript."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2076-3417/15/23/12819",
        "scraped_at": "2025-12-05 23:57:29"
    }
]