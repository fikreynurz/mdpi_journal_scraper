[
    {
        "title": "Prediction, Uncertainty Quantification, and ANN-Assisted Operation of Anaerobic Digestion Guided by Entropy Using Machine Learning",
        "authors": "byZhipeng Zhuang,Xiaoshan Liu,Jing Jin,Ziwen Li,Yanheng Liu,Adriano TavaresandDalin Li",
        "journal": "Entropy2025,27(12), 1233; https://doi.org/10.3390/e27121233 (registeringÂ DOI) - 5 Dec 2025",
        "abstract": "Anaerobic digestion (AD) is a nonlinear and disturbance-sensitive process in which instability is often induced by feedstock variability and biological fluctuations. To address this challenge, this study develops an entropy-guided machine learning framework that integrates parameter prediction, uncertainty quantification, and entropy-based evaluation of AD operation. Using six months of industrial data (~10,000 samples), three modelsâ€”support vector machine (SVM), random forest (RF), and artificial neural network (ANN)â€”were compared for predicting biogas yield, fermentation temperature, and volatile fatty acid (VFA) concentration. The ANN achieved the highest performance (accuracy = 96%, F1 = 0.95, root mean square error (RMSE) = 1.2 m3/t) and also exhibited the lowest prediction error entropy, indicating reduced uncertainty compared to RF and SVM. Feature entropy and permutation analysis consistently identified feed solids, organic matter, and feed rate as the most influential variables (>85% contribution), in agreement with the RF importance ranking. When applied as a real-time prediction and decision-support tool in the plant (â€œsensor â†’ prediction â†’ programmable logic controller (PLC)/operation â†’ feedbackâ€), the ANN model was associated with a reduction in gas-yield fluctuation from approximately Â±18% to Â±5%, a decrease in process entropy, and an improvement in operational stability of about 23%. Techno-economic and life-cycle assessments further indicated a 12â€“15 USD/t lower operating cost, 8â€“10% energy savings, and 5â€“7% CO2reduction compared with baseline operation. Overall, this study demonstrates that combining machine learning with entropy-based uncertainty analysis offers a reliable and interpretable pathway for more stable and low-carbon AD operation.Keywords:anaerobic digestion;machine learning;error entropy;uncertainty quantification;ANN-assisted operation",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Driven by global carbon neutrality goals, anaerobic digestion (AD) has become a core technology for organic solid waste treatment and renewable energy production [1]. AD transforms municipal and food waste into biogas (55â€“65% CH4) and nutrient-rich digestate, achieving simultaneous waste reduction and energy recovery in line with circular economy principles [2]. Over the past decade, AD has evolved from laboratory-scale (<1 m3) to industrial-scale reactors (>1000 m3); for example, Schmack Biogas plants (3000â€“180,000 t/a) have been widely deployed in Germany and Sweden [3]. Under Chinaâ€™s dual-carbon strategy, AD supports the â€œwasteâ€“energyâ€“fertilizerâ€ pathway. Jiang et al. [1] reported that biogas can partially substitute fossil fuels, while digestate enhances soil fertility. However, full-scale AD processes remain highly sensitive to feedstock fluctuations and operational disturbances. Key parametersâ€”solids (20â€“35%), organic matter (15â€“40%), temperature (30â€“60 Â°C), and feed rate (0.5â€“2.0 t/h)â€”govern system stability; improper regulation leads to volatile fatty acid (VFA) accumulation and gas-yield variations exceeding Â±18%, inhibiting methanogenesis [4,5]. Conventional threshold-based and offline control strategies can achieve acceptable performance in many AD plants, but they often require intensive manual tuning and may struggle to maintain near-optimal operation under strong nonlinearity and frequent feedstock changes. This motivates exploring data-driven approaches that can provide real-time prediction and decision support to complement existing control schemes. Machine learning (ML) provides a data-driven alternative. Models such as support vector machine (SVM), random forest (RF), and artificial neural network (ANN) are capable of learning complex nonlinear relationships in AD processes. Rutland et al. [6] demonstrated their predictive capability, while Zhai et al. [7] reported 96% gas-yield accuracy with <0.3 g/L VFA concentration error. However, most studies remain confined to the laboratory or pilot scale, with dataset sizes typically below 5000 samples (n< 5000, wherendenotes the number of observations), and focus only on prediction rather than real-time regulation. Furthermore, existing approaches seldom address predictive uncertainty or process disorderâ€”factors that fundamentally determine whether a prediction model can be trusted for decision-making in industrial environments. To address these limitations, this study investigates a full-scale AD plant treating approximately 30 t/d of organic solid waste, using around 10,000 real operation samples. An entropy-guided machine learning framework is developed that integrates parameter prediction, uncertainty quantification, and operation-oriented assessment. From an entropy-based perspective, prediction error entropy is used to quantify model uncertainty, while process entropy describes system stability under real operating conditions. The main contributions of this study are summarized as follows: Industrial-scale dataset and reproducible evaluation pipeline. A six-month, 9823-sample dataset is constructed from a full-scale AD plant. A unified pipelineâ€”including data cleaning, anomaly removal, normalization, temporal Kâ€“S splitting, five-fold cross-validation, and rolling window evaluationâ€”ensures data reliability and model generalizability.Entropy-aware machine learning and interpretable validation. The ANN outperforms SVM and RF in predicting biogas yield, temperature, and VFA. Beyond accuracy metrics, error entropy is introduced to characterize predictive uncertainty. Feed solids, organic matter, and feed rate are consistently identified as the dominant variables through feature importance and entropy increase analysis.ANN-assisted operation deployment and process entropy reduction. The optimized ANN is embedded into a real-time feedback loop (â€œsensor â†’ prediction â†’ programmable logic controller (PLC) â†’ feedbackâ€), reducing gas-yield fluctuation from Â±18% to Â±5% and improving process stability by approximately 23%. This improvement is accompanied by a measurable reduction in process entropy, demonstrating enhanced system order, energy efficiency, and low-carbon potential.",
            "2. Materials and Methods": "2.1. Data Sources and Preprocessing2.1.1. Data Acquisition and Anaerobic Digestion ProcessThe dataset was collected over six consecutive months from a continuously operating full-scale AD facility treating approximately 30 t/d of organic solid waste. Unlike laboratory or pilot plants, this system adopts an integrated configuration that couples multiple reactor types in series to enhance operational stability, resistance to feed fluctuations, and conversion efficiency (Figure 1). The process train consists of a vertical plug-flow reactor designed for high-solid substrates and long hydraulic retention times, followed by a horizontal plug-flow reactor that accommodates rapid organic loading variations and a vertical aerated stirred tank reactor that enhances mixing homogeneity and maintains microbial activity through intermittent aeration. The three reactors share a common biogas collection header and operate as a single digestion line with a total working volume on the order of 1000 m3, which is typical for industrial AD plants at this throughput. This hybrid layout represents a representative configuration in modern industrial AD, as it combines the structural stability of plug-flow digestion with the flexibility of continuous stirring. After digestion and mechanical solidâ€“liquid separation, the effluent consistently maintains a moisture content of â‰¤40%, meeting local discharge regulations and enabling its reuse as a soil conditioner or as recycled inoculum to maintain microbial balance.Figure 1.Schematic diagram of the AD process.Throughout the operation period, key physicochemical data were continuously monitored by online sensors, the supervisory control and data acquisition (SCADA) system, and periodic laboratory analyses. Input parameters included feed solids (20â€“35%), organic matter content (15â€“40%), pH (6.5â€“8.0), dissolved oxygen (0.1â€“0.5 mg/L), and feed rate (0.5â€“2.0 t/h), representing critical drivers of hydrolysis, acidogenesis, and methanogenesis. Reactor temperature, pH, dissolved oxygen, and feed flow rate were measured by industrial online instruments and logged at regular intervals via the SCADA system, whereas feed solids, organic matter, total solids, and VFA concentration were obtained from grab samples analyzed in the onsite laboratory according to standard methods.Specifically, reactor temperature was monitored using Pt100-class thermoresistive probes with typical accuracies better than Â±0.1 Â°C, while pH was measured with industrial gel-filled electrodes (accuracy Â±0.02 pH units). Dissolved oxygen was monitored using optical luminescence-based DO sensors with an accuracy of approximately Â±0.1 mg/L, and feed flow rate was recorded using a magnetic flow meter with an accuracy better than Â±1% of full scale. These specifications are representative of standard online instruments widely deployed in full-scale AD facilities and ensure that the logged signals are sufficiently precise for model training and operational monitoring.In parallel with data acquisition, the industrial automation system relied on a programmable logic controller (PLC) equipped with conventional feedback control loops. Reactor temperature was regulated through a PID controller that modulated a steam-control valve, with typical actuator constraints including a minimum opening of 5%, a maximum opening of 95%, and valve response times on the order of 1â€“3 s. Feed flow was controlled by a variable-frequency pump whose operating limits (0.5â€“2.0 t/h) matched the measured flow ranges reported inTable 1, while intermittent aeration in the stirred tank was governed by time-based duty cycles implemented in the PLC logic.Table 1.Operating ranges and thresholds of inputâ€“output variables in the AD system.The PLC executed its control routines at a base cycle time of approximately 200â€“500 ms, ensuring real-time responsiveness to temperature and flow deviations. In contrast, the SCADA system recorded sensor values at its native 5 min logging interval, and the ANN modelâ€”running on an external industrial workstationâ€”required less than 1 s per inference. This architecture ensured that ANN computations did not interfere with real-time PLC feedback but instead operated as a higher-level advisory layer. Notably, no ANN-derived signal was directly transmitted to actuators; operator adjustments based on ANN predictions followed the plantâ€™s standard 30â€“90 min operational decision cycle, consistent with industrial practice.These details clarify the interaction between machine learning components, online instrumentation, and the underlying automatic control infrastructure and provide the operational boundaries within which the ANN-based prediction module was integrated.Biogas yield (m3/t) is defined as the daily biogas volume at normal temperature and pressure (NTP) divided by the corresponding daily mass of fresh feed, i.e., a specific yield per ton of feedstock. The output indicators used for modelingâ€”biogas yield (m3/t), reactor temperature (Â°C), and VFA concentration (g/L)â€”were thus used to evaluate system performance and detect metabolic imbalance. These variables were selected not only due to their engineering measurability but also because they directly correspond to microbial activity, mass-transfer characteristics, and thermodynamic constraints of the AD process. To reduce systematic errors and ensure temporal consistency, all online sensors were calibrated weekly and cross-validated against laboratory measurements following standard operating procedures.In addition, outlier values were removed only when they clearly reflected sensor malfunction or physically impossible measurements, such as negative flow readings, dissolved oxygen spikes incompatible with anaerobic conditions, or corrupted SCADA packets flagged during instrument diagnostics. Outliers were identified based on engineering limits and cross-checked against laboratory measurements to avoid filtering out meaningful process dynamics. Importantly, operational fluctuationsâ€”including VFA increases during load shocks, feed disturbances, and seasonal temperature variationsâ€”were fully retained to preserve genuine variability in the dataset.To ensure that the dataset captured real operational variability, samples were collected under three representative conditions: stable feeding and temperature control, load-shock periods caused by abrupt feed changes, and seasonal variations affecting ambient and reactor temperatures. Typical observations under these conditions are presented inTable 2, whileTable 1further summarizes statistical ranges, engineering thresholds, sample sizes (â‰ˆ10,000 valid records), and measurement methods. As summarized inTable 2, feed solids, pH, dissolved oxygen, temperature, and feed rate were monitored online, whereas organic matter, total solids, and VFA concentration were measured in the laboratory. All recorded values remained within industrially accepted boundaries, ensuring the reliability, completeness, and applicability of the dataset for subsequent machine learning modeling.Table 2.Representative system parameter values under different conditions.2.1.2. Data Preprocessing MethodsTo ensure data integrity and suitability for modeling, standard preprocessing procedures were applied [8]. Raw operational records contained minor noise due to sensor drift and operational disturbances. Outliers beyond industrial or statistical limitsâ€”such as temperature > 80 Â°C, feed solids > 40%, or organic matter > 50% (â‰ˆ2.3% of samples)â€”as well as physically impossible values (e.g., negative gas yield) were removed [9,10].All continuous variables were normalized to the range [0,1] using minâ€“max scaling to eliminate dimensional inconsistencies:xâ€²=xâˆ’xminxmaxâˆ’xminxâ€²=xâˆ’xminxmaxâˆ’xmin(1)wherexxis the raw value andxminxminandxmaxxmaxdenote the minimum and maximum of each variable.The cleaned dataset was randomly split into training, validation, and test sets (7:2:1). A Kolmogorovâ€“Smirnov (Kâ€“S) test confirmed no significant statistical differences (p> 0.05) among the three subsets [11]. To further assess generalization under time-dependent disturbances, five-fold cross-validation [12] and rolling window prediction [13,14] were implemented.Feature selection was conducted using Pearson correlation analysis, computed only on the training set to prevent information leakage. The correlation coefficient between variableXand targetYis defined as:rXY=âˆ‘i=1n(Xiâˆ’XÂ¯)(Yiâˆ’YÂ¯)âˆ‘i=1n(Xiâˆ’XÂ¯)2âˆ‘i=1n(Yiâˆ’YÂ¯)2ğ‘Ÿğ‘‹ğ‘Œ=âˆ‘ğ‘›ğ‘–=1(ğ‘‹ğ‘–âˆ’ğ‘‹îƒµîƒ·îƒ¶îƒ¶îƒ¶îƒ¶)(ğ‘Œğ‘–âˆ’ğ‘Œîƒµîƒ·îƒ¶îƒ¶îƒ¶îƒ¶)âˆ‘ğ‘›ğ‘–=1(ğ‘‹ğ‘–âˆ’ğ‘‹îƒµîƒ·îƒ¶îƒ¶îƒ¶îƒ¶)2âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆšâˆ‘ğ‘›ğ‘–=1(ğ‘Œğ‘–âˆ’ğ‘Œîƒµîƒ·îƒ¶îƒ¶îƒ¶îƒ¶)2âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆš(2)As shown inFigure 2, feed solid content exhibited strong correlations with biogas yield (r = 0.90), fermentation temperature (r = 0.85), and VFA concentration (r = 0.60). Organic matter content and feed rate also showed significant correlations, while pH and dissolved oxygen presented weak correlations and were treated as auxiliary stability indicators. Therefore, feed solids, organic matter, and feed rate were retained as core predictive variables.Figure 2.Correlation matrix of AD characteristics.After aggregating real-time data into fixed time intervals and removing incomplete records, a final dataset of approximately 10,000 valid samples was obtained for model development. 2.2. Machine Learning Model Design and Evaluation MetricsTo predict biogas yield, fermentation temperature, and VFA concentration in industrial AD, three mainstream models were adoptedâ€”support vector machine (SVM), random forest (RF), and artificial neural network (ANN)â€”chosen for their complementary strengths in nonlinear modeling and interpretability [15,16,17,18,19,20,21,22]. All models use the normalized inputs defined in Equation (1) (Section 2.1.2) and a unified train/validation/test protocol (7:2:1 with cross-validation and rolling window evaluation) to ensure comparability and robustness [23]. Hyperparameter ranges and optimal values are summarized inTable 3.Table 3.Hyperparameter settings and optimal values for each machine learning model.2.2.1. Support Vector Machine (SVM)SVM was used for both classification (high/low gas yield) and regression. A radial basis function (RBF) kernel maps inputs to a high-dimensional feature space:k(xi,x)=exp(âˆ’Î³â€–xiâˆ’xâ€–2)ğ‘˜(xğ‘–,x)=exp(âˆ’ğ›¾â€–xğ‘–âˆ’xâ€–2)(3)and the classification decision function isy^=sign(âˆ‘i=1nÎ±iyik(xi,x)+b)ğ‘¦Ì‚=sign(âˆ‘ğ‘–=1ğ‘›ğ›¼ğ‘–ğ‘¦ğ‘–ğ‘˜(xğ‘–,x)+ğ‘)(4)For support-vector regression, theÏµâˆ’loss is adopted:LÎµ(y,f(x))=max0,yâˆ’f(x)âˆ’Îµ(5)with penalty C and kernel width Î³ tuned by grid search (Table 3). Classification labels follow the engineering threshold â‰¥70 m3/t (high) versus <70 m3/t (low), consistent withSection 3.1.SVM is effective for small-sample nonlinear tasks, mapping coupled factors such as feed solid content, organic matter, and feed rate via the radial basis function (RBF) kernel (Equation (2)). For classification, the decision function follows Equation (4), where the input vector comprises normalized S (20â€“35%), OM (15â€“40%), and F (0.5â€“2.0 t/h). Samples with gas yield â‰¥70 m3/t are labeled +1 and <70 m3/t as âˆ’1, where 70 m3/t corresponds to the engineering lower bound of acceptable biogas productivity in the studied industrial plant; yields below this threshold are routinely treated as low-performance conditions requiring inspection or adjustment. Hyperparameters were optimized as C = 10 and Î³ = 0.05 (Table 3). For regression, the Îµ-insensitive loss (Equation (7)) was adopted to ensure robustness in continuous predictions.2.2.2. Random Forest (RF)RF aggregates B bootstrap trees to reduce variance and improve generalization [16]. Classification uses majority votingy^=mode{hb(x)}b=1B(6)and regression uses the ensemble meany^=1Bâˆ‘b=1Bhb(x)(7)Model error is quantified by mean squared error (MSE)MSE=1nâˆ‘i=1n(yiâˆ’y^i)2(8)Key hyperparametersâ€”number of treesB, maximum depth, and features per split mtryâ€”were tuned via grid/cross-validation; the feature-importance ranking reported inSection 3.2is computed from the trained forest.RF employs ensemble averaging to mitigate overfitting and quantify feature importance (Figure 3). Classification uses majority voting (Equation (6)), regression takes the mean of tree outputs (Equation (7)), and model error is measured by mean-squared error (Equation (8)). Optimal parametersâ€”B = 100, depth = 18, mtry = 2â€”were obtained through grid/cross-validation (Table 3). The resulting feature-importance ranking (Section 3.2) reveals each variableâ€™s contribution to biogas performance.Figure 3.Schematic diagram of random forest structure.2.2.3. Artificial Neural Networks (ANNs)The ANN is a two-layer fully connected feed-forward network with hidden sizes [128, 64] and ReLU activations:ReLU(z)=max(0,z),(9)and a three-neuron output layer predicting biogas, temperature, and VFA simultaneously [17,18,19]. Training uses Adam optimization with L2 regularization and early stopping under the objectiveL=1Nâˆ‘i=1Nâ€–y^iâˆ’yiâ€–22+Î»â€–Î¸â€–22(10)where Î¸ denotes network parameters. The chosen architecture balances accuracy with minute-level inference requirements for online control.Evaluation metrics. Classification performance is reported with Precision, Recall, and F1-score [24,25]:Precision=TPTP+FP,Recall=TPTP+FN,F1=2â‹…Precisionâ‹…RecallPrecision+Recall(11)and AUROC is provided to assess threshold-independent separability. Regression accuracy is assessed by RMSE for each target (biogas m3/t, temperature Â°C, VFA g/L) [26]:RMSE=1nâˆ‘i=1n(yiâˆ’y^i)2(12)Using a single preprocessing/validation pipeline for all models ensures that differences in reported metrics arise from model capability rather than data handling, enabling fair comparison on the same industrial dataset [23].The ANN model consists of a two-layer fully connected feed-forward network ([128, 64] neurons;Figure 4) using ReLU activation (Equation (9)) and an output layer predicting biogas yield, temperature, and VFA simultaneously.Figure 4.Schematic diagram of the artificial neural network structure.Training adopted the Adam optimizer, L2 regularization, and early stopping with the loss function (Equation (10)). Optimal hyperparametersâ€”learning rate = 0.001, batch = 64, and Î» = 0.001â€”were determined via cross/grid search (Table 3). The model maintains high accuracy with an inference time well below the one-minute control cycle.To address the â€œblack-boxâ€ issue, a lightweight architecture + regularization + early stopping strategy was applied, with interpretability discussed inSection 3.1.To justify the selection of the final ANN architecture, multiple alternative configurations were evaluated, including networks with 1â€“3 hidden layers, 16â€“64 neurons per layer, and different activation functions (ReLU, tanh) within the grid-search range listed inTable 3. These variants were compared using five-fold cross-validation to assess predictive accuracy, generalization performance, and inference time. The selected architecture (two hidden layers with 32 neurons each and ReLU activation) achieved the best balance between accuracy and stability while keeping the inference time below one millisecond, which is necessary for real-time deployment within the PLC/SCADA environment. Deeper or wider networks showed only marginal accuracy improvement but exhibited higher variance across folds and increased risk of overfitting, whereas shallower architectures resulted in reduced predictive performance. Therefore, the chosen ANN represents the optimal trade-off between predictive capability, robustness, and computational efficiency for industrial application. 2.3. Model Evaluation MetricsModel performance was assessed for both classification and regression tasks [24,25,26].Classification metrics include Precision (Equation (11)), Recall (Equation (11)), and F1-score (Equation (11)) to balance prediction reliability under class imbalance; AUROC complements these by evaluating threshold-independent robustness.Regression performance was quantified by Root Mean Square Error (RMSE) (Equation (12)) for biogas yield (m3/t), temperature (Â°C), and VFA (g/L); lower RMSE indicates stronger predictive capability and generalization.Combining classification and regression assessments ensures comprehensive and reliable evaluation of model performance within industrial AD applications. 2.4. Entropy-Based Uncertainty Quantification MethodAD is a nonlinear and disturbance-sensitive process, and traditional performance metrics such as RMSE or accuracy reflect average prediction errors but cannot measure prediction uncertainty or process disorder [9,10]. To address this gap and align with the entropy-driven scope of entropy, this study introduces information entropy to quantify (i) model prediction uncertainty and (ii) operational stability.2.4.1. Error Entropy for Prediction UncertaintyFor each model, the prediction error is defined ase=yâˆ’y^. Its uncertainty is quantified using Shannon error entropy:H(e)=âˆ’âˆ«pe(Î¾)lnpe(Î¾)dÎ¾(13)wherepe(Î¾)is the probability density of the error. Kernel density estimation (KDE) was used to estimatepewith Gaussian kernel and Silvermanâ€™s bandwidth rule [27]. Lower entropy corresponds to a more concentrated error distribution, indicating both smaller variance and higher predictive confidence. Error entropy was calculated on the test set for all models (ANN, RF, SVM) and summarized in a comparison table inSection 3(instead of new figures) [28].2.4.2. Entropy Increase for Feature ContributionTo evaluate how each input variable reduces prediction uncertainty, a permutation-based conditional entropy approach was applied [29]. For each featureXj,we randomly permuted its values to break its relationship with the target [30]. The resulting increase in error entropy is:Î”Hj=H(e(j))âˆ’H(e)(14)wheree(j)is the error after permuting featureXj. A higherÎ”Hjindicates that this feature contributes more to uncertainty reduction. This approach is consistent with RF feature importance yet grounded in information theory.2.4.3. Process Entropy for Operational StabilityTo assess macroscopic system disorder, the AD process is divided into several discrete operating states (normal, VFA accumulation, overload, and temperature deviation). In each observation window, the probability of each state isÏ€ka. The process entropy is calculated as:Sproc=âˆ’âˆ‘k=1KÏ€klnÏ€k(15)LowerSprocindicates a more ordered and stable process. This metric was used to compare system stability before and after ANN-assisted operation (reported inSection 3.3). No additional figure is introduced; a simple table may be used if necessary.",
            "2.1. Data Sources and Preprocessing": "2.1.1. Data Acquisition and Anaerobic Digestion ProcessThe dataset was collected over six consecutive months from a continuously operating full-scale AD facility treating approximately 30 t/d of organic solid waste. Unlike laboratory or pilot plants, this system adopts an integrated configuration that couples multiple reactor types in series to enhance operational stability, resistance to feed fluctuations, and conversion efficiency (Figure 1). The process train consists of a vertical plug-flow reactor designed for high-solid substrates and long hydraulic retention times, followed by a horizontal plug-flow reactor that accommodates rapid organic loading variations and a vertical aerated stirred tank reactor that enhances mixing homogeneity and maintains microbial activity through intermittent aeration. The three reactors share a common biogas collection header and operate as a single digestion line with a total working volume on the order of 1000 m3, which is typical for industrial AD plants at this throughput. This hybrid layout represents a representative configuration in modern industrial AD, as it combines the structural stability of plug-flow digestion with the flexibility of continuous stirring. After digestion and mechanical solidâ€“liquid separation, the effluent consistently maintains a moisture content of â‰¤40%, meeting local discharge regulations and enabling its reuse as a soil conditioner or as recycled inoculum to maintain microbial balance.Figure 1.Schematic diagram of the AD process.Throughout the operation period, key physicochemical data were continuously monitored by online sensors, the supervisory control and data acquisition (SCADA) system, and periodic laboratory analyses. Input parameters included feed solids (20â€“35%), organic matter content (15â€“40%), pH (6.5â€“8.0), dissolved oxygen (0.1â€“0.5 mg/L), and feed rate (0.5â€“2.0 t/h), representing critical drivers of hydrolysis, acidogenesis, and methanogenesis. Reactor temperature, pH, dissolved oxygen, and feed flow rate were measured by industrial online instruments and logged at regular intervals via the SCADA system, whereas feed solids, organic matter, total solids, and VFA concentration were obtained from grab samples analyzed in the onsite laboratory according to standard methods.Specifically, reactor temperature was monitored using Pt100-class thermoresistive probes with typical accuracies better than Â±0.1 Â°C, while pH was measured with industrial gel-filled electrodes (accuracy Â±0.02 pH units). Dissolved oxygen was monitored using optical luminescence-based DO sensors with an accuracy of approximately Â±0.1 mg/L, and feed flow rate was recorded using a magnetic flow meter with an accuracy better than Â±1% of full scale. These specifications are representative of standard online instruments widely deployed in full-scale AD facilities and ensure that the logged signals are sufficiently precise for model training and operational monitoring.In parallel with data acquisition, the industrial automation system relied on a programmable logic controller (PLC) equipped with conventional feedback control loops. Reactor temperature was regulated through a PID controller that modulated a steam-control valve, with typical actuator constraints including a minimum opening of 5%, a maximum opening of 95%, and valve response times on the order of 1â€“3 s. Feed flow was controlled by a variable-frequency pump whose operating limits (0.5â€“2.0 t/h) matched the measured flow ranges reported inTable 1, while intermittent aeration in the stirred tank was governed by time-based duty cycles implemented in the PLC logic.Table 1.Operating ranges and thresholds of inputâ€“output variables in the AD system.The PLC executed its control routines at a base cycle time of approximately 200â€“500 ms, ensuring real-time responsiveness to temperature and flow deviations. In contrast, the SCADA system recorded sensor values at its native 5 min logging interval, and the ANN modelâ€”running on an external industrial workstationâ€”required less than 1 s per inference. This architecture ensured that ANN computations did not interfere with real-time PLC feedback but instead operated as a higher-level advisory layer. Notably, no ANN-derived signal was directly transmitted to actuators; operator adjustments based on ANN predictions followed the plantâ€™s standard 30â€“90 min operational decision cycle, consistent with industrial practice.These details clarify the interaction between machine learning components, online instrumentation, and the underlying automatic control infrastructure and provide the operational boundaries within which the ANN-based prediction module was integrated.Biogas yield (m3/t) is defined as the daily biogas volume at normal temperature and pressure (NTP) divided by the corresponding daily mass of fresh feed, i.e., a specific yield per ton of feedstock. The output indicators used for modelingâ€”biogas yield (m3/t), reactor temperature (Â°C), and VFA concentration (g/L)â€”were thus used to evaluate system performance and detect metabolic imbalance. These variables were selected not only due to their engineering measurability but also because they directly correspond to microbial activity, mass-transfer characteristics, and thermodynamic constraints of the AD process. To reduce systematic errors and ensure temporal consistency, all online sensors were calibrated weekly and cross-validated against laboratory measurements following standard operating procedures.In addition, outlier values were removed only when they clearly reflected sensor malfunction or physically impossible measurements, such as negative flow readings, dissolved oxygen spikes incompatible with anaerobic conditions, or corrupted SCADA packets flagged during instrument diagnostics. Outliers were identified based on engineering limits and cross-checked against laboratory measurements to avoid filtering out meaningful process dynamics. Importantly, operational fluctuationsâ€”including VFA increases during load shocks, feed disturbances, and seasonal temperature variationsâ€”were fully retained to preserve genuine variability in the dataset.To ensure that the dataset captured real operational variability, samples were collected under three representative conditions: stable feeding and temperature control, load-shock periods caused by abrupt feed changes, and seasonal variations affecting ambient and reactor temperatures. Typical observations under these conditions are presented inTable 2, whileTable 1further summarizes statistical ranges, engineering thresholds, sample sizes (â‰ˆ10,000 valid records), and measurement methods. As summarized inTable 2, feed solids, pH, dissolved oxygen, temperature, and feed rate were monitored online, whereas organic matter, total solids, and VFA concentration were measured in the laboratory. All recorded values remained within industrially accepted boundaries, ensuring the reliability, completeness, and applicability of the dataset for subsequent machine learning modeling.Table 2.Representative system parameter values under different conditions. 2.1.2. Data Preprocessing MethodsTo ensure data integrity and suitability for modeling, standard preprocessing procedures were applied [8]. Raw operational records contained minor noise due to sensor drift and operational disturbances. Outliers beyond industrial or statistical limitsâ€”such as temperature > 80 Â°C, feed solids > 40%, or organic matter > 50% (â‰ˆ2.3% of samples)â€”as well as physically impossible values (e.g., negative gas yield) were removed [9,10].All continuous variables were normalized to the range [0,1] using minâ€“max scaling to eliminate dimensional inconsistencies:xâ€²=xâˆ’xminxmaxâˆ’xminxâ€²=xâˆ’xminxmaxâˆ’xmin(1)wherexxis the raw value andxminxminandxmaxxmaxdenote the minimum and maximum of each variable.The cleaned dataset was randomly split into training, validation, and test sets (7:2:1). A Kolmogorovâ€“Smirnov (Kâ€“S) test confirmed no significant statistical differences (p> 0.05) among the three subsets [11]. To further assess generalization under time-dependent disturbances, five-fold cross-validation [12] and rolling window prediction [13,14] were implemented.Feature selection was conducted using Pearson correlation analysis, computed only on the training set to prevent information leakage. The correlation coefficient between variableXand targetYis defined as:rXY=âˆ‘i=1n(Xiâˆ’XÂ¯)(Yiâˆ’YÂ¯)âˆ‘i=1n(Xiâˆ’XÂ¯)2âˆ‘i=1n(Yiâˆ’YÂ¯)2ğ‘Ÿğ‘‹ğ‘Œ=âˆ‘ğ‘›ğ‘–=1(ğ‘‹ğ‘–âˆ’ğ‘‹îƒµîƒ·îƒ¶îƒ¶îƒ¶îƒ¶)(ğ‘Œğ‘–âˆ’ğ‘Œîƒµîƒ·îƒ¶îƒ¶îƒ¶îƒ¶)âˆ‘ğ‘›ğ‘–=1(ğ‘‹ğ‘–âˆ’ğ‘‹îƒµîƒ·îƒ¶îƒ¶îƒ¶îƒ¶)2âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆšâˆ‘ğ‘›ğ‘–=1(ğ‘Œğ‘–âˆ’ğ‘Œîƒµîƒ·îƒ¶îƒ¶îƒ¶îƒ¶)2âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆš(2)As shown inFigure 2, feed solid content exhibited strong correlations with biogas yield (r = 0.90), fermentation temperature (r = 0.85), and VFA concentration (r = 0.60). Organic matter content and feed rate also showed significant correlations, while pH and dissolved oxygen presented weak correlations and were treated as auxiliary stability indicators. Therefore, feed solids, organic matter, and feed rate were retained as core predictive variables.Figure 2.Correlation matrix of AD characteristics.After aggregating real-time data into fixed time intervals and removing incomplete records, a final dataset of approximately 10,000 valid samples was obtained for model development.",
            "2.1.1. Data Acquisition and Anaerobic Digestion Process": "The dataset was collected over six consecutive months from a continuously operating full-scale AD facility treating approximately 30 t/d of organic solid waste. Unlike laboratory or pilot plants, this system adopts an integrated configuration that couples multiple reactor types in series to enhance operational stability, resistance to feed fluctuations, and conversion efficiency (Figure 1). The process train consists of a vertical plug-flow reactor designed for high-solid substrates and long hydraulic retention times, followed by a horizontal plug-flow reactor that accommodates rapid organic loading variations and a vertical aerated stirred tank reactor that enhances mixing homogeneity and maintains microbial activity through intermittent aeration. The three reactors share a common biogas collection header and operate as a single digestion line with a total working volume on the order of 1000 m3, which is typical for industrial AD plants at this throughput. This hybrid layout represents a representative configuration in modern industrial AD, as it combines the structural stability of plug-flow digestion with the flexibility of continuous stirring. After digestion and mechanical solidâ€“liquid separation, the effluent consistently maintains a moisture content of â‰¤40%, meeting local discharge regulations and enabling its reuse as a soil conditioner or as recycled inoculum to maintain microbial balance. Figure 1.Schematic diagram of the AD process. Throughout the operation period, key physicochemical data were continuously monitored by online sensors, the supervisory control and data acquisition (SCADA) system, and periodic laboratory analyses. Input parameters included feed solids (20â€“35%), organic matter content (15â€“40%), pH (6.5â€“8.0), dissolved oxygen (0.1â€“0.5 mg/L), and feed rate (0.5â€“2.0 t/h), representing critical drivers of hydrolysis, acidogenesis, and methanogenesis. Reactor temperature, pH, dissolved oxygen, and feed flow rate were measured by industrial online instruments and logged at regular intervals via the SCADA system, whereas feed solids, organic matter, total solids, and VFA concentration were obtained from grab samples analyzed in the onsite laboratory according to standard methods. Specifically, reactor temperature was monitored using Pt100-class thermoresistive probes with typical accuracies better than Â±0.1 Â°C, while pH was measured with industrial gel-filled electrodes (accuracy Â±0.02 pH units). Dissolved oxygen was monitored using optical luminescence-based DO sensors with an accuracy of approximately Â±0.1 mg/L, and feed flow rate was recorded using a magnetic flow meter with an accuracy better than Â±1% of full scale. These specifications are representative of standard online instruments widely deployed in full-scale AD facilities and ensure that the logged signals are sufficiently precise for model training and operational monitoring. In parallel with data acquisition, the industrial automation system relied on a programmable logic controller (PLC) equipped with conventional feedback control loops. Reactor temperature was regulated through a PID controller that modulated a steam-control valve, with typical actuator constraints including a minimum opening of 5%, a maximum opening of 95%, and valve response times on the order of 1â€“3 s. Feed flow was controlled by a variable-frequency pump whose operating limits (0.5â€“2.0 t/h) matched the measured flow ranges reported inTable 1, while intermittent aeration in the stirred tank was governed by time-based duty cycles implemented in the PLC logic. Table 1.Operating ranges and thresholds of inputâ€“output variables in the AD system. The PLC executed its control routines at a base cycle time of approximately 200â€“500 ms, ensuring real-time responsiveness to temperature and flow deviations. In contrast, the SCADA system recorded sensor values at its native 5 min logging interval, and the ANN modelâ€”running on an external industrial workstationâ€”required less than 1 s per inference. This architecture ensured that ANN computations did not interfere with real-time PLC feedback but instead operated as a higher-level advisory layer. Notably, no ANN-derived signal was directly transmitted to actuators; operator adjustments based on ANN predictions followed the plantâ€™s standard 30â€“90 min operational decision cycle, consistent with industrial practice. These details clarify the interaction between machine learning components, online instrumentation, and the underlying automatic control infrastructure and provide the operational boundaries within which the ANN-based prediction module was integrated. Biogas yield (m3/t) is defined as the daily biogas volume at normal temperature and pressure (NTP) divided by the corresponding daily mass of fresh feed, i.e., a specific yield per ton of feedstock. The output indicators used for modelingâ€”biogas yield (m3/t), reactor temperature (Â°C), and VFA concentration (g/L)â€”were thus used to evaluate system performance and detect metabolic imbalance. These variables were selected not only due to their engineering measurability but also because they directly correspond to microbial activity, mass-transfer characteristics, and thermodynamic constraints of the AD process. To reduce systematic errors and ensure temporal consistency, all online sensors were calibrated weekly and cross-validated against laboratory measurements following standard operating procedures. In addition, outlier values were removed only when they clearly reflected sensor malfunction or physically impossible measurements, such as negative flow readings, dissolved oxygen spikes incompatible with anaerobic conditions, or corrupted SCADA packets flagged during instrument diagnostics. Outliers were identified based on engineering limits and cross-checked against laboratory measurements to avoid filtering out meaningful process dynamics. Importantly, operational fluctuationsâ€”including VFA increases during load shocks, feed disturbances, and seasonal temperature variationsâ€”were fully retained to preserve genuine variability in the dataset. To ensure that the dataset captured real operational variability, samples were collected under three representative conditions: stable feeding and temperature control, load-shock periods caused by abrupt feed changes, and seasonal variations affecting ambient and reactor temperatures. Typical observations under these conditions are presented inTable 2, whileTable 1further summarizes statistical ranges, engineering thresholds, sample sizes (â‰ˆ10,000 valid records), and measurement methods. As summarized inTable 2, feed solids, pH, dissolved oxygen, temperature, and feed rate were monitored online, whereas organic matter, total solids, and VFA concentration were measured in the laboratory. All recorded values remained within industrially accepted boundaries, ensuring the reliability, completeness, and applicability of the dataset for subsequent machine learning modeling. Table 2.Representative system parameter values under different conditions.",
            "2.1.2. Data Preprocessing Methods": "To ensure data integrity and suitability for modeling, standard preprocessing procedures were applied [8]. Raw operational records contained minor noise due to sensor drift and operational disturbances. Outliers beyond industrial or statistical limitsâ€”such as temperature > 80 Â°C, feed solids > 40%, or organic matter > 50% (â‰ˆ2.3% of samples)â€”as well as physically impossible values (e.g., negative gas yield) were removed [9,10]. All continuous variables were normalized to the range [0,1] using minâ€“max scaling to eliminate dimensional inconsistencies:xâ€²=xâˆ’xminxmaxâˆ’xminxâ€²=xâˆ’xminxmaxâˆ’xmin(1)wherexxis the raw value andxminxminandxmaxxmaxdenote the minimum and maximum of each variable. The cleaned dataset was randomly split into training, validation, and test sets (7:2:1). A Kolmogorovâ€“Smirnov (Kâ€“S) test confirmed no significant statistical differences (p> 0.05) among the three subsets [11]. To further assess generalization under time-dependent disturbances, five-fold cross-validation [12] and rolling window prediction [13,14] were implemented. Feature selection was conducted using Pearson correlation analysis, computed only on the training set to prevent information leakage. The correlation coefficient between variableXand targetYis defined as:rXY=âˆ‘i=1n(Xiâˆ’XÂ¯)(Yiâˆ’YÂ¯)âˆ‘i=1n(Xiâˆ’XÂ¯)2âˆ‘i=1n(Yiâˆ’YÂ¯)2ğ‘Ÿğ‘‹ğ‘Œ=âˆ‘ğ‘›ğ‘–=1(ğ‘‹ğ‘–âˆ’ğ‘‹îƒµîƒ·îƒ¶îƒ¶îƒ¶îƒ¶)(ğ‘Œğ‘–âˆ’ğ‘Œîƒµîƒ·îƒ¶îƒ¶îƒ¶îƒ¶)âˆ‘ğ‘›ğ‘–=1(ğ‘‹ğ‘–âˆ’ğ‘‹îƒµîƒ·îƒ¶îƒ¶îƒ¶îƒ¶)2âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆšâˆ‘ğ‘›ğ‘–=1(ğ‘Œğ‘–âˆ’ğ‘Œîƒµîƒ·îƒ¶îƒ¶îƒ¶îƒ¶)2âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆš(2) As shown inFigure 2, feed solid content exhibited strong correlations with biogas yield (r = 0.90), fermentation temperature (r = 0.85), and VFA concentration (r = 0.60). Organic matter content and feed rate also showed significant correlations, while pH and dissolved oxygen presented weak correlations and were treated as auxiliary stability indicators. Therefore, feed solids, organic matter, and feed rate were retained as core predictive variables. Figure 2.Correlation matrix of AD characteristics. After aggregating real-time data into fixed time intervals and removing incomplete records, a final dataset of approximately 10,000 valid samples was obtained for model development.",
            "2.2. Machine Learning Model Design and Evaluation Metrics": "To predict biogas yield, fermentation temperature, and VFA concentration in industrial AD, three mainstream models were adoptedâ€”support vector machine (SVM), random forest (RF), and artificial neural network (ANN)â€”chosen for their complementary strengths in nonlinear modeling and interpretability [15,16,17,18,19,20,21,22]. All models use the normalized inputs defined in Equation (1) (Section 2.1.2) and a unified train/validation/test protocol (7:2:1 with cross-validation and rolling window evaluation) to ensure comparability and robustness [23]. Hyperparameter ranges and optimal values are summarized inTable 3. Table 3.Hyperparameter settings and optimal values for each machine learning model. 2.2.1. Support Vector Machine (SVM)SVM was used for both classification (high/low gas yield) and regression. A radial basis function (RBF) kernel maps inputs to a high-dimensional feature space:k(xi,x)=exp(âˆ’Î³â€–xiâˆ’xâ€–2)ğ‘˜(xğ‘–,x)=exp(âˆ’ğ›¾â€–xğ‘–âˆ’xâ€–2)(3)and the classification decision function isy^=sign(âˆ‘i=1nÎ±iyik(xi,x)+b)ğ‘¦Ì‚=sign(âˆ‘ğ‘–=1ğ‘›ğ›¼ğ‘–ğ‘¦ğ‘–ğ‘˜(xğ‘–,x)+ğ‘)(4)For support-vector regression, theÏµâˆ’loss is adopted:LÎµ(y,f(x))=max0,yâˆ’f(x)âˆ’Îµ(5)with penalty C and kernel width Î³ tuned by grid search (Table 3). Classification labels follow the engineering threshold â‰¥70 m3/t (high) versus <70 m3/t (low), consistent withSection 3.1.SVM is effective for small-sample nonlinear tasks, mapping coupled factors such as feed solid content, organic matter, and feed rate via the radial basis function (RBF) kernel (Equation (2)). For classification, the decision function follows Equation (4), where the input vector comprises normalized S (20â€“35%), OM (15â€“40%), and F (0.5â€“2.0 t/h). Samples with gas yield â‰¥70 m3/t are labeled +1 and <70 m3/t as âˆ’1, where 70 m3/t corresponds to the engineering lower bound of acceptable biogas productivity in the studied industrial plant; yields below this threshold are routinely treated as low-performance conditions requiring inspection or adjustment. Hyperparameters were optimized as C = 10 and Î³ = 0.05 (Table 3). For regression, the Îµ-insensitive loss (Equation (7)) was adopted to ensure robustness in continuous predictions. 2.2.2. Random Forest (RF)RF aggregates B bootstrap trees to reduce variance and improve generalization [16]. Classification uses majority votingy^=mode{hb(x)}b=1B(6)and regression uses the ensemble meany^=1Bâˆ‘b=1Bhb(x)(7)Model error is quantified by mean squared error (MSE)MSE=1nâˆ‘i=1n(yiâˆ’y^i)2(8)Key hyperparametersâ€”number of treesB, maximum depth, and features per split mtryâ€”were tuned via grid/cross-validation; the feature-importance ranking reported inSection 3.2is computed from the trained forest.RF employs ensemble averaging to mitigate overfitting and quantify feature importance (Figure 3). Classification uses majority voting (Equation (6)), regression takes the mean of tree outputs (Equation (7)), and model error is measured by mean-squared error (Equation (8)). Optimal parametersâ€”B = 100, depth = 18, mtry = 2â€”were obtained through grid/cross-validation (Table 3). The resulting feature-importance ranking (Section 3.2) reveals each variableâ€™s contribution to biogas performance.Figure 3.Schematic diagram of random forest structure. 2.2.3. Artificial Neural Networks (ANNs)The ANN is a two-layer fully connected feed-forward network with hidden sizes [128, 64] and ReLU activations:ReLU(z)=max(0,z),(9)and a three-neuron output layer predicting biogas, temperature, and VFA simultaneously [17,18,19]. Training uses Adam optimization with L2 regularization and early stopping under the objectiveL=1Nâˆ‘i=1Nâ€–y^iâˆ’yiâ€–22+Î»â€–Î¸â€–22(10)where Î¸ denotes network parameters. The chosen architecture balances accuracy with minute-level inference requirements for online control.Evaluation metrics. Classification performance is reported with Precision, Recall, and F1-score [24,25]:Precision=TPTP+FP,Recall=TPTP+FN,F1=2â‹…Precisionâ‹…RecallPrecision+Recall(11)and AUROC is provided to assess threshold-independent separability. Regression accuracy is assessed by RMSE for each target (biogas m3/t, temperature Â°C, VFA g/L) [26]:RMSE=1nâˆ‘i=1n(yiâˆ’y^i)2(12)Using a single preprocessing/validation pipeline for all models ensures that differences in reported metrics arise from model capability rather than data handling, enabling fair comparison on the same industrial dataset [23].The ANN model consists of a two-layer fully connected feed-forward network ([128, 64] neurons;Figure 4) using ReLU activation (Equation (9)) and an output layer predicting biogas yield, temperature, and VFA simultaneously.Figure 4.Schematic diagram of the artificial neural network structure.Training adopted the Adam optimizer, L2 regularization, and early stopping with the loss function (Equation (10)). Optimal hyperparametersâ€”learning rate = 0.001, batch = 64, and Î» = 0.001â€”were determined via cross/grid search (Table 3). The model maintains high accuracy with an inference time well below the one-minute control cycle.To address the â€œblack-boxâ€ issue, a lightweight architecture + regularization + early stopping strategy was applied, with interpretability discussed inSection 3.1.To justify the selection of the final ANN architecture, multiple alternative configurations were evaluated, including networks with 1â€“3 hidden layers, 16â€“64 neurons per layer, and different activation functions (ReLU, tanh) within the grid-search range listed inTable 3. These variants were compared using five-fold cross-validation to assess predictive accuracy, generalization performance, and inference time. The selected architecture (two hidden layers with 32 neurons each and ReLU activation) achieved the best balance between accuracy and stability while keeping the inference time below one millisecond, which is necessary for real-time deployment within the PLC/SCADA environment. Deeper or wider networks showed only marginal accuracy improvement but exhibited higher variance across folds and increased risk of overfitting, whereas shallower architectures resulted in reduced predictive performance. Therefore, the chosen ANN represents the optimal trade-off between predictive capability, robustness, and computational efficiency for industrial application.",
            "2.2.1. Support Vector Machine (SVM)": "SVM was used for both classification (high/low gas yield) and regression. A radial basis function (RBF) kernel maps inputs to a high-dimensional feature space:k(xi,x)=exp(âˆ’Î³â€–xiâˆ’xâ€–2)ğ‘˜(xğ‘–,x)=exp(âˆ’ğ›¾â€–xğ‘–âˆ’xâ€–2)(3)and the classification decision function isy^=sign(âˆ‘i=1nÎ±iyik(xi,x)+b)ğ‘¦Ì‚=sign(âˆ‘ğ‘–=1ğ‘›ğ›¼ğ‘–ğ‘¦ğ‘–ğ‘˜(xğ‘–,x)+ğ‘)(4) For support-vector regression, theÏµâˆ’loss is adopted:LÎµ(y,f(x))=max0,yâˆ’f(x)âˆ’Îµ(5)with penalty C and kernel width Î³ tuned by grid search (Table 3). Classification labels follow the engineering threshold â‰¥70 m3/t (high) versus <70 m3/t (low), consistent withSection 3.1. SVM is effective for small-sample nonlinear tasks, mapping coupled factors such as feed solid content, organic matter, and feed rate via the radial basis function (RBF) kernel (Equation (2)). For classification, the decision function follows Equation (4), where the input vector comprises normalized S (20â€“35%), OM (15â€“40%), and F (0.5â€“2.0 t/h). Samples with gas yield â‰¥70 m3/t are labeled +1 and <70 m3/t as âˆ’1, where 70 m3/t corresponds to the engineering lower bound of acceptable biogas productivity in the studied industrial plant; yields below this threshold are routinely treated as low-performance conditions requiring inspection or adjustment. Hyperparameters were optimized as C = 10 and Î³ = 0.05 (Table 3). For regression, the Îµ-insensitive loss (Equation (7)) was adopted to ensure robustness in continuous predictions.",
            "2.2.2. Random Forest (RF)": "RF aggregates B bootstrap trees to reduce variance and improve generalization [16]. Classification uses majority votingy^=mode{hb(x)}b=1B(6)and regression uses the ensemble meany^=1Bâˆ‘b=1Bhb(x)(7) Model error is quantified by mean squared error (MSE)MSE=1nâˆ‘i=1n(yiâˆ’y^i)2(8) Key hyperparametersâ€”number of treesB, maximum depth, and features per split mtryâ€”were tuned via grid/cross-validation; the feature-importance ranking reported inSection 3.2is computed from the trained forest. RF employs ensemble averaging to mitigate overfitting and quantify feature importance (Figure 3). Classification uses majority voting (Equation (6)), regression takes the mean of tree outputs (Equation (7)), and model error is measured by mean-squared error (Equation (8)). Optimal parametersâ€”B = 100, depth = 18, mtry = 2â€”were obtained through grid/cross-validation (Table 3). The resulting feature-importance ranking (Section 3.2) reveals each variableâ€™s contribution to biogas performance. Figure 3.Schematic diagram of random forest structure.",
            "2.2.3. Artificial Neural Networks (ANNs)": "The ANN is a two-layer fully connected feed-forward network with hidden sizes [128, 64] and ReLU activations:ReLU(z)=max(0,z),(9)and a three-neuron output layer predicting biogas, temperature, and VFA simultaneously [17,18,19]. Training uses Adam optimization with L2 regularization and early stopping under the objectiveL=1Nâˆ‘i=1Nâ€–y^iâˆ’yiâ€–22+Î»â€–Î¸â€–22(10)where Î¸ denotes network parameters. The chosen architecture balances accuracy with minute-level inference requirements for online control. Evaluation metrics. Classification performance is reported with Precision, Recall, and F1-score [24,25]:Precision=TPTP+FP,Recall=TPTP+FN,F1=2â‹…Precisionâ‹…RecallPrecision+Recall(11)and AUROC is provided to assess threshold-independent separability. Regression accuracy is assessed by RMSE for each target (biogas m3/t, temperature Â°C, VFA g/L) [26]:RMSE=1nâˆ‘i=1n(yiâˆ’y^i)2(12) Using a single preprocessing/validation pipeline for all models ensures that differences in reported metrics arise from model capability rather than data handling, enabling fair comparison on the same industrial dataset [23]. The ANN model consists of a two-layer fully connected feed-forward network ([128, 64] neurons;Figure 4) using ReLU activation (Equation (9)) and an output layer predicting biogas yield, temperature, and VFA simultaneously. Figure 4.Schematic diagram of the artificial neural network structure. Training adopted the Adam optimizer, L2 regularization, and early stopping with the loss function (Equation (10)). Optimal hyperparametersâ€”learning rate = 0.001, batch = 64, and Î» = 0.001â€”were determined via cross/grid search (Table 3). The model maintains high accuracy with an inference time well below the one-minute control cycle. To address the â€œblack-boxâ€ issue, a lightweight architecture + regularization + early stopping strategy was applied, with interpretability discussed inSection 3.1. To justify the selection of the final ANN architecture, multiple alternative configurations were evaluated, including networks with 1â€“3 hidden layers, 16â€“64 neurons per layer, and different activation functions (ReLU, tanh) within the grid-search range listed inTable 3. These variants were compared using five-fold cross-validation to assess predictive accuracy, generalization performance, and inference time. The selected architecture (two hidden layers with 32 neurons each and ReLU activation) achieved the best balance between accuracy and stability while keeping the inference time below one millisecond, which is necessary for real-time deployment within the PLC/SCADA environment. Deeper or wider networks showed only marginal accuracy improvement but exhibited higher variance across folds and increased risk of overfitting, whereas shallower architectures resulted in reduced predictive performance. Therefore, the chosen ANN represents the optimal trade-off between predictive capability, robustness, and computational efficiency for industrial application.",
            "2.3. Model Evaluation Metrics": "Model performance was assessed for both classification and regression tasks [24,25,26]. Classification metrics include Precision (Equation (11)), Recall (Equation (11)), and F1-score (Equation (11)) to balance prediction reliability under class imbalance; AUROC complements these by evaluating threshold-independent robustness.Regression performance was quantified by Root Mean Square Error (RMSE) (Equation (12)) for biogas yield (m3/t), temperature (Â°C), and VFA (g/L); lower RMSE indicates stronger predictive capability and generalization. Combining classification and regression assessments ensures comprehensive and reliable evaluation of model performance within industrial AD applications.",
            "2.4. Entropy-Based Uncertainty Quantification Method": "AD is a nonlinear and disturbance-sensitive process, and traditional performance metrics such as RMSE or accuracy reflect average prediction errors but cannot measure prediction uncertainty or process disorder [9,10]. To address this gap and align with the entropy-driven scope of entropy, this study introduces information entropy to quantify (i) model prediction uncertainty and (ii) operational stability. 2.4.1. Error Entropy for Prediction UncertaintyFor each model, the prediction error is defined ase=yâˆ’y^. Its uncertainty is quantified using Shannon error entropy:H(e)=âˆ’âˆ«pe(Î¾)lnpe(Î¾)dÎ¾(13)wherepe(Î¾)is the probability density of the error. Kernel density estimation (KDE) was used to estimatepewith Gaussian kernel and Silvermanâ€™s bandwidth rule [27]. Lower entropy corresponds to a more concentrated error distribution, indicating both smaller variance and higher predictive confidence. Error entropy was calculated on the test set for all models (ANN, RF, SVM) and summarized in a comparison table inSection 3(instead of new figures) [28]. 2.4.2. Entropy Increase for Feature ContributionTo evaluate how each input variable reduces prediction uncertainty, a permutation-based conditional entropy approach was applied [29]. For each featureXj,we randomly permuted its values to break its relationship with the target [30]. The resulting increase in error entropy is:Î”Hj=H(e(j))âˆ’H(e)(14)wheree(j)is the error after permuting featureXj. A higherÎ”Hjindicates that this feature contributes more to uncertainty reduction. This approach is consistent with RF feature importance yet grounded in information theory. 2.4.3. Process Entropy for Operational StabilityTo assess macroscopic system disorder, the AD process is divided into several discrete operating states (normal, VFA accumulation, overload, and temperature deviation). In each observation window, the probability of each state isÏ€ka. The process entropy is calculated as:Sproc=âˆ’âˆ‘k=1KÏ€klnÏ€k(15)LowerSprocindicates a more ordered and stable process. This metric was used to compare system stability before and after ANN-assisted operation (reported inSection 3.3). No additional figure is introduced; a simple table may be used if necessary.",
            "2.4.1. Error Entropy for Prediction Uncertainty": "For each model, the prediction error is defined ase=yâˆ’y^. Its uncertainty is quantified using Shannon error entropy:H(e)=âˆ’âˆ«pe(Î¾)lnpe(Î¾)dÎ¾(13)wherepe(Î¾)is the probability density of the error. Kernel density estimation (KDE) was used to estimatepewith Gaussian kernel and Silvermanâ€™s bandwidth rule [27]. Lower entropy corresponds to a more concentrated error distribution, indicating both smaller variance and higher predictive confidence. Error entropy was calculated on the test set for all models (ANN, RF, SVM) and summarized in a comparison table inSection 3(instead of new figures) [28].",
            "2.4.2. Entropy Increase for Feature Contribution": "To evaluate how each input variable reduces prediction uncertainty, a permutation-based conditional entropy approach was applied [29]. For each featureXj,we randomly permuted its values to break its relationship with the target [30]. The resulting increase in error entropy is:Î”Hj=H(e(j))âˆ’H(e)(14)wheree(j)is the error after permuting featureXj. A higherÎ”Hjindicates that this feature contributes more to uncertainty reduction. This approach is consistent with RF feature importance yet grounded in information theory.",
            "2.4.3. Process Entropy for Operational Stability": "To assess macroscopic system disorder, the AD process is divided into several discrete operating states (normal, VFA accumulation, overload, and temperature deviation). In each observation window, the probability of each state isÏ€ka. The process entropy is calculated as:Sproc=âˆ’âˆ‘k=1KÏ€klnÏ€k(15) LowerSprocindicates a more ordered and stable process. This metric was used to compare system stability before and after ANN-assisted operation (reported inSection 3.3). No additional figure is introduced; a simple table may be used if necessary.",
            "3. Results and Analysis": "3.1. Model Performance ComparisonAs shown inTable 4, the three models exhibit clear performance differences in distinguishing high- and low-yield samples. The ANN achieves the best overall results, with Accuracy, Recall, and F1 around 0.95 and AUROC at 0.98, indicating stable classification across thresholds. RF follows (0.90â€“0.94), while SVM performs slightly lower (0.88â€“0.91). Accuracy denotes total correct rate, Recall measures the detection of high-yield cases, F1 balances both, and AUROC represents threshold-independent robustness. The ANNâ€™s multilayer structure effectively captures nonlinear relationships among feed solids, organic matter, and feed rate, consistent with prior ANN research [30,31].Table 4.Performance comparison of three machine learning models.In regression tasks, the ANN likewise showed superior precision: RMSE values of 1.2 m3/t (biogas), 0.5 Â°C (temperature), and 0.3 g/L (VFA) were all below those of RF (1.8, 0.9, 0.6) and SVM (2.1, 1.2, 0.8). Its average R2= 0.94 exceeded RF (0.88) and SVM (0.82) [32], confirming the ANNâ€™s high accuracy and robustness for industrial applications. 3.2. Feature Importance and Entropy-Based Uncertainty AnalysisFigure 5presents RF-based feature importance: feed solids (42%), organic matter (30%), and feed rate (18%) together account for â‰ˆ90% of the total importance, identifying them as the dominant operational factors in the present plant. pH (4%), dissolved oxygen (5%), and total solids (3%) contribute less and mainly serve as stability indicators within the observed operating window. These outcomes align with mechanism analyses [22,33]: excessive solids cause scum formation and mass-transfer limitations; low solids induce hydraulic overload; organic content affects methane yield and VFA accumulation risks; and feed rate governs hydraulic retention time (HRT). Accordingly, under the normal operating conditions captured in this dataset, the three core variables constitute the primary levers for AD optimization.Figure 5.Weighted percentage of input characteristics for AD performance.Model explainability is strengthened in two complementary ways. First, the RF ranking is consistent with tendencies learned by the ANN, indicating agreement across model classes regarding the relative influence of inputs. Second, a lightweight ANN combined with cross-validation constrains complexity and mitigates overfitting, improving the balance between predictive accuracy and interpretability. This â€œstructure control + cross-validationâ€ strategy alleviates black-box concerns while preserving performance (seeFigure 5).From an information-theoretic perspective, the RF results can be interpreted via Shannon entropy and information gain as defined inSection 2.4: features that most reduce the output uncertainty are precisely those with the highest RF importance, again highlighting feed solids, organic matter, and feed rate as primary drivers. For the regression tasks (biogas yield, temperature, and VFA concentration), permutation tests further corroborate this finding: shuffling any of the three core variables yields a clear increase in prediction error (Î”RMSE), whereas permuting pH or dissolved oxygen produces only marginal changes, consistent with their role as secondary stability indicators under the studied conditions. It should be emphasized that the dataset does not contain severe acidification events or strong oxygen ingress; therefore, the low importance of pH and dissolved oxygen reflects their limited variation around well-controlled set-points in this plant, rather than a lack of relevance under failure scenarios.To quantify predictive uncertainty, we evaluate prediction error entropy on the held-out test sets (definitions and estimators inSection 2.4). Across all targets, the ANN exhibits the lowest error entropy H(e), followed by RF and then SVM. Thus, the ANN not only attains lower RMSE but also concentrates residuals more tightly, indicating higher predictive certainty beyond accuracy alone. At the variable level, the entropy-increase index Î”Hj (feature permutation) shows the largest rises for feed solids, organic matter, and feed rate, while pH and dissolved oxygen induce minimal changesâ€”mirroring both the RF ranking and the Î”RMSE pattern.Finally, local interpretability from ANN average input-gradient norms (seeSection 2.4) remains consistent with the global picture: sensitivities with respect to feed solids, organic matter, and feed rate are systematically higher than those for pH and dissolved oxygen.Overall, the convergence across RF importance, information-gain interpretation, permutation-based Î”RMSE, error entropy H(e), and entropy-increase Î”Hj supports a coherent conclusion for this full-scale plant and its normal operating range: feed solids, organic matter, and feed rate are the principal drivers of AD performance and uncertainty reduction, while pH and dissolved oxygen primarily reflect system integrity and are better suited as stability indicators within this range. Nevertheless, pH and dissolved oxygen remain essential safety and monitoring variables for detecting process upsets and should not be neglected in plant-wide supervision or control design.These importance patterns reflect a well-functioning industrial digester; extreme upset conditions such as acidification or depressurization were not present in the dataset. 3.3. Application of ANN-Based Intelligent Operation and MonitoringTo verify engineering applicability, the optimized ANN was integrated into the plantâ€™s existing monitoring and control architecture (Figure 6) as a real-time soft sensor and decision-support module, forming a practical â€œsensor â†’ prediction â†’ PLC/operation â†’ feedbackâ€ loop.Figure 6.Integrated AD system with ANN-based intelligent control.In the deployed configuration, reactor temperature, pH, dissolved oxygen, and feed flow rate are measured online and logged every 5 min by the SCADA system, while feed solids, organic matter, total solids, and VFA concentration are obtained from daily or 12-hourly laboratory analyses. The ANN ingests the latest available measurements and forecasts biogas yield, reactor temperature, and VFA concentration one hour ahead.To justify the chosen temporal resolution, it is important to note that the 5 min sampling interval reflects the native logging frequency of the industrial SCADA system, which is designed to capture short-term perturbations in feed flow, steam supply, and mixing conditionsâ€”disturbances that occur on minute-level timescales even though methane generation evolves much more slowly. Such minute-scale fluctuations propagate rapidly to reactor temperature and dissolved oxygen before the feedback controllers fully compensate, meaning that finer-resolution data are required for early warning rather than for modeling the intrinsic biogas generation kinetics.Similarly, the 1 h prediction horizon corresponds to the typical operational decision cycle in full-scale AD plants: operators adjust feed rate, steam supply, and recirculation settings at intervals of 30â€“90 min, and disturbances usually require 0.5â€“2 h to influence VFA accumulation or biogas yield. A 1 h-ahead forecast therefore provides meaningful lead time for preventive action, while longer horizons would introduce additional uncertainty and reduced actionable value. The selected combinationâ€”5 min sampling and 1 h predictionâ€”thus reflects practical engineering constraints and decision-making needs, enabling the ANN to function as a real-time soft sensor that anticipates short-term operational deviations rather than modeling long-term biogas kinetics.Although reactor temperature is conventionally regulated by the PLC through a dedicated PID loop, its real-time value is still affected by feed fluctuations, steam supply disturbances, and seasonal heat losses, leading to short-term deviations before the controller fully compensates. Predicting temperature one hour ahead therefore serves a different purpose from direct feedback control: it provides early warning of upcoming thermal disturbances and allows operators to adjust steam flow or insulation settings proactively rather than reactively. In practice, temperature forecasting strengthens process resilience, as both biogas yield and VFA accumulation are highly temperature-sensitive within the mesophilic range. Thus, temperature prediction is not redundant but an essential component of the ANN-assisted monitoring framework, complementing the existing PID controllers by anticipating deviations that feedback loops alone may not detect in time.These 1 h ahead predictions are displayed in the SCADA interface and used by operators to adjust feed rate, recirculation, and steam supply set-points, while conventional PID loops in the programmable logic controller (PLC) continue to regulate low-level temperature and flow control. It should be noted that the PID temperature-control loop was active during both the baseline and ANN-assisted periods; therefore, the observed improvements cannot be attributed to temperature regulation alone but rather to the foresight provided by ANN predictions, which enabled earlier and more effective operational adjustments. In this way, data-driven forecasts provide feed-forward information and early warning, complementing the existing feedback controllers rather than replacing them. To avoid misunderstanding, we clarify that this â€œfeed-forward informationâ€ does not constitute a feed-forward controller in the formal automatic-control sense; the ANN outputs are advisory signals for operators rather than automated actuator commands.During a 12-week observation campaign comparing baseline operation and ANN-assisted operation at the same plant, gas-yield fluctuations were noticeably reduced and process stability was enhanced. Taking the coefficient of variation (CV) of hourly biogas yield as the operational stability index (CV = standard deviation/mean), ANN-assisted operation reduced gas-yield fluctuations from approximately Â±18% (baseline) to Â±5% (ANN-assisted). Based on these values, the relative improvement in stability was quantified as (CV_baseline âˆ’ CV_ANN)/CV_baseline â‰ˆ 0.23), corresponding to an improvement of approximately 23%. These differences were statistically verified using a two-sample t-test on daily stability indices, confirming statistical significance (p< 0.05). Over the same period, organic degradation remained above 80% and digestate moisture was stabilized below 40%, indicating that improved stability did not compromise treatment performance. This calculation-based clarification directly addresses the reviewerâ€™s concern regarding how the 23% improvement was obtained and validated.To assess overall system benefits, techno-economic analysis (TEA) and life-cycle assessment (LCA) were applied to a 100 t/d AD system operated with the ANN-assisted framework. Assuming an electricity price of 0.08â€“0.12 USD kW/h, feedstock cost 25â€“35 USD/t, equipment lifetime was 10 years, and the discount rate was 8%; the system achieved a 12â€“15 USD/t lower operating cost and a 3â€“4 year payback period compared with baseline operation. Scaling from a 30 t/d reference plant using a capacity index Î± = 0.65 confirmed economic scalability, and sensitivity analysis (Figure 7) showed that the payback period remained below 4.5 years even under conservative fuel price scenarios.Figure 7.Sensitivity analysis of unit operating costs and payback period under different economic scenarios.Environmentally, LCA results across regional emission factorsâ€”China 0.65, EU 0.35, and USA 0.45 kg CO2/kW/hâ€”indicated 8â€“10% energy savings and 5â€“7% CO2reduction compared with baseline control (Table 5).Table 5.Carbon reduction effects of the ANN- and entropy-guided framework under different regional grid emission factors.Overall, the integrated ANN- and entropy-guided framework achieves coordinated improvement of cost, energy efficiency, and carbon mitigation at the study plant, supporting its feasibility for large-scale deployment and cross-regional promotion. 3.4. Limitations and OutlookAlthough the proposed framework achieves high predictive accuracy and improves stability in industrial AD, several limitations remain. In revising the manuscript, we also addressed the reviewerâ€™s concern regarding repeated statements (e.g., the dominant role of solids, organic matter, and feed rate; the superior performance of ANN; and the reduction in process entropy). To improve readability and avoid redundancy, overlapping descriptions acrossSection 3.1,Section 3.2andSection 3.3were consolidated or removed so that each concept is discussed only once in its appropriate context.First, all data were obtained from a single plant, meaning that the generalizability of both the machine learning models and the entropy-based uncertainty metrics has not yet been tested under different feedstocks, climates, or process configurations. Multi-site validation and transfer learning will be necessary to verify robustness.Second, the model relies solely on physicochemical variables; microbial community dynamics, which fundamentally determine process resilience, were not incorporated. Therefore, the relationship between reduced process entropy and microbial stability remains unclear. Future work could integrate metagenomic information to bridge operational entropy with biological mechanisms.Third, entropy in this study is computed from deterministic residuals rather than predicted probability distributions. Bayesian or ensemble models could provide predictive entropy directly and support risk-aware decision-making.Finally, process entropy is used only as an evaluation index rather than a control objective. Embedding entropy minimization into model predictive control may allow the system to pursue not only stable gas production but also reduced operational disorder.Overall, the framework demonstrates feasibility but should evolve toward cross-plant applicability, biological coupling, probabilistic entropy modeling, and entropy-driven control strategies.",
            "3.1. Model Performance Comparison": "As shown inTable 4, the three models exhibit clear performance differences in distinguishing high- and low-yield samples. The ANN achieves the best overall results, with Accuracy, Recall, and F1 around 0.95 and AUROC at 0.98, indicating stable classification across thresholds. RF follows (0.90â€“0.94), while SVM performs slightly lower (0.88â€“0.91). Accuracy denotes total correct rate, Recall measures the detection of high-yield cases, F1 balances both, and AUROC represents threshold-independent robustness. The ANNâ€™s multilayer structure effectively captures nonlinear relationships among feed solids, organic matter, and feed rate, consistent with prior ANN research [30,31]. Table 4.Performance comparison of three machine learning models. In regression tasks, the ANN likewise showed superior precision: RMSE values of 1.2 m3/t (biogas), 0.5 Â°C (temperature), and 0.3 g/L (VFA) were all below those of RF (1.8, 0.9, 0.6) and SVM (2.1, 1.2, 0.8). Its average R2= 0.94 exceeded RF (0.88) and SVM (0.82) [32], confirming the ANNâ€™s high accuracy and robustness for industrial applications.",
            "3.2. Feature Importance and Entropy-Based Uncertainty Analysis": "Figure 5presents RF-based feature importance: feed solids (42%), organic matter (30%), and feed rate (18%) together account for â‰ˆ90% of the total importance, identifying them as the dominant operational factors in the present plant. pH (4%), dissolved oxygen (5%), and total solids (3%) contribute less and mainly serve as stability indicators within the observed operating window. These outcomes align with mechanism analyses [22,33]: excessive solids cause scum formation and mass-transfer limitations; low solids induce hydraulic overload; organic content affects methane yield and VFA accumulation risks; and feed rate governs hydraulic retention time (HRT). Accordingly, under the normal operating conditions captured in this dataset, the three core variables constitute the primary levers for AD optimization. Figure 5.Weighted percentage of input characteristics for AD performance. Model explainability is strengthened in two complementary ways. First, the RF ranking is consistent with tendencies learned by the ANN, indicating agreement across model classes regarding the relative influence of inputs. Second, a lightweight ANN combined with cross-validation constrains complexity and mitigates overfitting, improving the balance between predictive accuracy and interpretability. This â€œstructure control + cross-validationâ€ strategy alleviates black-box concerns while preserving performance (seeFigure 5). From an information-theoretic perspective, the RF results can be interpreted via Shannon entropy and information gain as defined inSection 2.4: features that most reduce the output uncertainty are precisely those with the highest RF importance, again highlighting feed solids, organic matter, and feed rate as primary drivers. For the regression tasks (biogas yield, temperature, and VFA concentration), permutation tests further corroborate this finding: shuffling any of the three core variables yields a clear increase in prediction error (Î”RMSE), whereas permuting pH or dissolved oxygen produces only marginal changes, consistent with their role as secondary stability indicators under the studied conditions. It should be emphasized that the dataset does not contain severe acidification events or strong oxygen ingress; therefore, the low importance of pH and dissolved oxygen reflects their limited variation around well-controlled set-points in this plant, rather than a lack of relevance under failure scenarios. To quantify predictive uncertainty, we evaluate prediction error entropy on the held-out test sets (definitions and estimators inSection 2.4). Across all targets, the ANN exhibits the lowest error entropy H(e), followed by RF and then SVM. Thus, the ANN not only attains lower RMSE but also concentrates residuals more tightly, indicating higher predictive certainty beyond accuracy alone. At the variable level, the entropy-increase index Î”Hj (feature permutation) shows the largest rises for feed solids, organic matter, and feed rate, while pH and dissolved oxygen induce minimal changesâ€”mirroring both the RF ranking and the Î”RMSE pattern. Finally, local interpretability from ANN average input-gradient norms (seeSection 2.4) remains consistent with the global picture: sensitivities with respect to feed solids, organic matter, and feed rate are systematically higher than those for pH and dissolved oxygen. Overall, the convergence across RF importance, information-gain interpretation, permutation-based Î”RMSE, error entropy H(e), and entropy-increase Î”Hj supports a coherent conclusion for this full-scale plant and its normal operating range: feed solids, organic matter, and feed rate are the principal drivers of AD performance and uncertainty reduction, while pH and dissolved oxygen primarily reflect system integrity and are better suited as stability indicators within this range. Nevertheless, pH and dissolved oxygen remain essential safety and monitoring variables for detecting process upsets and should not be neglected in plant-wide supervision or control design. These importance patterns reflect a well-functioning industrial digester; extreme upset conditions such as acidification or depressurization were not present in the dataset.",
            "3.3. Application of ANN-Based Intelligent Operation and Monitoring": "To verify engineering applicability, the optimized ANN was integrated into the plantâ€™s existing monitoring and control architecture (Figure 6) as a real-time soft sensor and decision-support module, forming a practical â€œsensor â†’ prediction â†’ PLC/operation â†’ feedbackâ€ loop. Figure 6.Integrated AD system with ANN-based intelligent control. In the deployed configuration, reactor temperature, pH, dissolved oxygen, and feed flow rate are measured online and logged every 5 min by the SCADA system, while feed solids, organic matter, total solids, and VFA concentration are obtained from daily or 12-hourly laboratory analyses. The ANN ingests the latest available measurements and forecasts biogas yield, reactor temperature, and VFA concentration one hour ahead. To justify the chosen temporal resolution, it is important to note that the 5 min sampling interval reflects the native logging frequency of the industrial SCADA system, which is designed to capture short-term perturbations in feed flow, steam supply, and mixing conditionsâ€”disturbances that occur on minute-level timescales even though methane generation evolves much more slowly. Such minute-scale fluctuations propagate rapidly to reactor temperature and dissolved oxygen before the feedback controllers fully compensate, meaning that finer-resolution data are required for early warning rather than for modeling the intrinsic biogas generation kinetics. Similarly, the 1 h prediction horizon corresponds to the typical operational decision cycle in full-scale AD plants: operators adjust feed rate, steam supply, and recirculation settings at intervals of 30â€“90 min, and disturbances usually require 0.5â€“2 h to influence VFA accumulation or biogas yield. A 1 h-ahead forecast therefore provides meaningful lead time for preventive action, while longer horizons would introduce additional uncertainty and reduced actionable value. The selected combinationâ€”5 min sampling and 1 h predictionâ€”thus reflects practical engineering constraints and decision-making needs, enabling the ANN to function as a real-time soft sensor that anticipates short-term operational deviations rather than modeling long-term biogas kinetics. Although reactor temperature is conventionally regulated by the PLC through a dedicated PID loop, its real-time value is still affected by feed fluctuations, steam supply disturbances, and seasonal heat losses, leading to short-term deviations before the controller fully compensates. Predicting temperature one hour ahead therefore serves a different purpose from direct feedback control: it provides early warning of upcoming thermal disturbances and allows operators to adjust steam flow or insulation settings proactively rather than reactively. In practice, temperature forecasting strengthens process resilience, as both biogas yield and VFA accumulation are highly temperature-sensitive within the mesophilic range. Thus, temperature prediction is not redundant but an essential component of the ANN-assisted monitoring framework, complementing the existing PID controllers by anticipating deviations that feedback loops alone may not detect in time. These 1 h ahead predictions are displayed in the SCADA interface and used by operators to adjust feed rate, recirculation, and steam supply set-points, while conventional PID loops in the programmable logic controller (PLC) continue to regulate low-level temperature and flow control. It should be noted that the PID temperature-control loop was active during both the baseline and ANN-assisted periods; therefore, the observed improvements cannot be attributed to temperature regulation alone but rather to the foresight provided by ANN predictions, which enabled earlier and more effective operational adjustments. In this way, data-driven forecasts provide feed-forward information and early warning, complementing the existing feedback controllers rather than replacing them. To avoid misunderstanding, we clarify that this â€œfeed-forward informationâ€ does not constitute a feed-forward controller in the formal automatic-control sense; the ANN outputs are advisory signals for operators rather than automated actuator commands. During a 12-week observation campaign comparing baseline operation and ANN-assisted operation at the same plant, gas-yield fluctuations were noticeably reduced and process stability was enhanced. Taking the coefficient of variation (CV) of hourly biogas yield as the operational stability index (CV = standard deviation/mean), ANN-assisted operation reduced gas-yield fluctuations from approximately Â±18% (baseline) to Â±5% (ANN-assisted). Based on these values, the relative improvement in stability was quantified as (CV_baseline âˆ’ CV_ANN)/CV_baseline â‰ˆ 0.23), corresponding to an improvement of approximately 23%. These differences were statistically verified using a two-sample t-test on daily stability indices, confirming statistical significance (p< 0.05). Over the same period, organic degradation remained above 80% and digestate moisture was stabilized below 40%, indicating that improved stability did not compromise treatment performance. This calculation-based clarification directly addresses the reviewerâ€™s concern regarding how the 23% improvement was obtained and validated. To assess overall system benefits, techno-economic analysis (TEA) and life-cycle assessment (LCA) were applied to a 100 t/d AD system operated with the ANN-assisted framework. Assuming an electricity price of 0.08â€“0.12 USD kW/h, feedstock cost 25â€“35 USD/t, equipment lifetime was 10 years, and the discount rate was 8%; the system achieved a 12â€“15 USD/t lower operating cost and a 3â€“4 year payback period compared with baseline operation. Scaling from a 30 t/d reference plant using a capacity index Î± = 0.65 confirmed economic scalability, and sensitivity analysis (Figure 7) showed that the payback period remained below 4.5 years even under conservative fuel price scenarios. Figure 7.Sensitivity analysis of unit operating costs and payback period under different economic scenarios. Environmentally, LCA results across regional emission factorsâ€”China 0.65, EU 0.35, and USA 0.45 kg CO2/kW/hâ€”indicated 8â€“10% energy savings and 5â€“7% CO2reduction compared with baseline control (Table 5). Table 5.Carbon reduction effects of the ANN- and entropy-guided framework under different regional grid emission factors. Overall, the integrated ANN- and entropy-guided framework achieves coordinated improvement of cost, energy efficiency, and carbon mitigation at the study plant, supporting its feasibility for large-scale deployment and cross-regional promotion.",
            "3.4. Limitations and Outlook": "Although the proposed framework achieves high predictive accuracy and improves stability in industrial AD, several limitations remain. In revising the manuscript, we also addressed the reviewerâ€™s concern regarding repeated statements (e.g., the dominant role of solids, organic matter, and feed rate; the superior performance of ANN; and the reduction in process entropy). To improve readability and avoid redundancy, overlapping descriptions acrossSection 3.1,Section 3.2andSection 3.3were consolidated or removed so that each concept is discussed only once in its appropriate context. First, all data were obtained from a single plant, meaning that the generalizability of both the machine learning models and the entropy-based uncertainty metrics has not yet been tested under different feedstocks, climates, or process configurations. Multi-site validation and transfer learning will be necessary to verify robustness. Second, the model relies solely on physicochemical variables; microbial community dynamics, which fundamentally determine process resilience, were not incorporated. Therefore, the relationship between reduced process entropy and microbial stability remains unclear. Future work could integrate metagenomic information to bridge operational entropy with biological mechanisms. Third, entropy in this study is computed from deterministic residuals rather than predicted probability distributions. Bayesian or ensemble models could provide predictive entropy directly and support risk-aware decision-making. Finally, process entropy is used only as an evaluation index rather than a control objective. Embedding entropy minimization into model predictive control may allow the system to pursue not only stable gas production but also reduced operational disorder. Overall, the framework demonstrates feasibility but should evolve toward cross-plant applicability, biological coupling, probabilistic entropy modeling, and entropy-driven control strategies.",
            "4. Conclusions": "Using six months of industrial operation data (~10,000 samples), this study compared three machine learning modelsâ€”support vector machine (SVM), random forest (RF), and artificial neural network (ANN)â€”to predict key AD parameters, including biogas yield, reactor temperature, and volatile fatty acid (VFA) concentration. Within a unified preprocessing and validation framework, an entropy-guided machine learning system was established that combines parameter prediction, uncertainty quantification, and operation-oriented assessment to enhance process stability and energy efficiency. Among the models, the ANN exhibited the best performance, achieving 96% accuracy, F1 = 0.95, and regression RMSEs of 1.2 m3/t, 0.5 Â°C, and 0.3 g/L, validating its suitability for engineering-grade prediction and aligning with previous studies [19,26,27]. Additionally, the ANN showed the lowest prediction error entropy, indicating reduced uncertainty and higher reliability beyond RMSE comparison. RF analysis and entropy-based uncertainty assessment consistently confirmed feed solids, organic matter, and feed rate as the dominant variables (>85% total contribution) [32], as they contribute most significantly to both variance reduction and entropy decrease, while pH and dissolved oxygen served mainly as stability indicators within the well-controlled operating range of the studied plant. All conclusions were derived under stable operating conditions and therefore apply to normal industrial regimes rather than severe inhibition scenarios. When integrated into the plantâ€™s monitoring and control architecture as a real-time soft sensor and decision-support module (â€œsensor â†’ prediction â†’ PLC/operation â†’ feedbackâ€), the ANN-based model was associated with an improvement in operational stability of about 23%, a reduction in gas-yield fluctuation from approximately Â±18% to Â±5%, maintenance of â‰¥80% degradation efficiency, and stabilization of digestate moisture below 40%. Techno-economic analysis (TEA) and life-cycle assessment (LCA) further demonstrated 12â€“15 USD/t lower operating costs, 3â€“4 year payback, 8â€“10% energy savings, and 5â€“7% CO2reduction compared with baseline operation, confirming the feasibility of the entropy-aware intelligent operation framework in large-scale AD. To further address the limitation noted above regarding model generalizability, future work will assess multiple model classes (e.g., XGBoost, LSTM, Gaussian Process Regression) across different plants, feedstocks and climatic conditions to examine whether the entropy-based variable patterns and predictive performance observed here remain consistent across architectures and operating environments. Overall, this study demonstrates that ANN-based modeling, combined with entropy-driven uncertainty analysis and real-time deployment as an ANN-assisted operation tool, provides an accurate, interpretable, and scalable pathway for more stable and low-carbon AD operation. Future work will focus on multi-site validation, integration of microbial and probabilistic entropy models, and incorporation of entropy-related performance indices into model predictive control strategies."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/1099-4300/27/12/1233",
        "scraped_at": "2025-12-05 18:22:50"
    },
    {
        "title": "AGF-HAM: Adaptive Gated Fusion Hierarchical Attention Model for Explainable Sentiment Analysis",
        "authors": "byMahander Kumar,Lal Khan,Mohammad Zubair KhanandAmel Ali Alhussan",
        "journal": "Mathematics2025,13(24), 3892; https://doi.org/10.3390/math13243892 (registeringÂ DOI) - 5 Dec 2025",
        "abstract": "The rapid growth of user-generated content in the digital space has increased the necessity of properly and interpretively analyzing sentiment and emotion systems. This research paper presents a new hybrid model, HAM (Hybrid Attention-based Model), a Transformer-based contextual embedding model combined with deep sequential modeling and multi-layer explainability. The suggested framework integrates the BERT/RoBERTa encoders, Bidirectional LSTM, and Graph Attention that can be used to embrace semantic and aspect-level sentiment correlation. Additionally, an enhanced Explainability Module, including Attention Heatmaps, Aspect-Level Interpretations, and SHAP/Integrated Gradients analysis, contributes to the increased model transparency and interpretive reliability. Four benchmark datasets, namely GoEmotions-1, GoEmotions-2, GoEmotions-3, and Amazon Cell Phones and Accessories Reviews, were experimented on in order to have a strong cross-domain assessment. The 28 emotion words of GoEmotions were merged into five sentiment-oriented classes to harmonize the dissimilarity in the emotional granularities to fit the schema of the Amazon dataset. The proposed HAM model had a highest accuracy of 96.4% and F1-score of 94.9%, which was significantly higher than the state-of-the-art baselines like BERT (89.8%), RoBERTa (91.7%), and RoBERTa+BiLSTM (92.5%). These findings support the idea that HAM is a better solution to finer-grained emotional details and is still interpretable as a vital move towards creating open, exposible, and domain-tailored sentiment intelligence systems. Future endeavors will aim at expanding this architecture to multimodal fusion, cross-lingual adaptability, and federated learning systems to increase the scalability, generalization, and ethical application of AI.Keywords:hierarchical attention mechanism (HAM);aspect-based sentiment analysis (ABSA);explainable AI (XAI);transformer-based models;deep learning;emotion and sentiment classificationMSC:68T99",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "With the rapid growth of digital commerce and social networks, users generate enormous amounts of reviews for products and services daily. Extracting actionable insights manually from these data is infeasible. Aspect-Based Sentiment Analysis (ABSA) enables fine-grained sentiment detection by identifying opinions about specific aspects (e.g., â€œbattery,â€ â€œcamera,â€ â€œserviceâ€) rather than overall sentiment, which is critically important in domains like e-commerce, hospitality, and consumer electronics [1,2]. Early ABSA methods relied heavily on classical machine-learning modelsâ€”SVMs, Naive Bayes, and LDAâ€”with features crafted from lexical resources, POS tags, and n-grams. While successful to some extent, such models suffer from sparse representations, reliance on laborious feature engineering, and poor generalization to unseen contexts [3]. The introduction of dense word embeddings (Word2Vec, GloVe) helped by providing continuous, richer lexical representations, but even these lacked modeling of long-distance dependencies and aspect-specific contexts [4,5,6]. Neural network models such as CNNs and RNNs (especially LSTM, Bi-LSTM) improved performance by capturing sequential and local patterns in text. However, they still treated all tokens with relatively equal weight, lacking mechanisms to focus disproportionally on aspect-relevant tokens. Attention mechanisms addressed this by allowing models to assign higher weights to relevant words [7]. Simultaneously, Transformer-based pretrained models like BERT, RoBERTa, and their variants revolutionized NLP by encoding rich contextual information and long-range dependencies [8,9]. In ABSA, fine-tuning these models has led to substantial performance gains. Although there is a great advancement in ABSA, there are some challenges in the existing strategies. To start with, simple concatenation is frequently used to embed fusion, which does not allow models to discover the significance of various Transformer representations (e.g., BERT vs. RoBERTa) on a particular aspect. Second, Transformer-based embeddings are unable to encode the relative position of context words to the aspect term, which is a critical semantic feature in detecting which context words affect sentiment. Third, there is an overwhelming majority of attention mechanisms that are flat, and none of them consider hierarchical structures that may interact to jointly model word, aspect, and sentence-level interactions. Lastly, there is the emerging demand to have interpretable ABSA solutions, which means that architectures are not only required to work well but also show what aspects of the text contribute to the sentiment prediction. Motivated by these challenges, this study proposes a novel ABSA methodology that integrates four key innovations: Aspect-aware embedding fusion: Instead of naive concatenation, embeddings from Transformer models (BERT, RoBERTa) are fused using an attention-based fusion mechanism, enabling the model to adaptively weight different sources based on aspect relevance.Sequential modeling via BiLSTM: To capture both forward and backward dependencies in text, the aspect-aware fused embeddings are passed through a BiLSTM encoder, enriching the representation with contextual dynamics.Hierarchical + Position-aware attention: A multi-level attention block is introduced. Word-level attention highlights aspect-relevant tokens; an optional sentence/aspect aggregation level combines across multiple aspects. Position weighting ensures tokens closer to the aspect term receive higher importanceâ€”following inspirations from recent position-aware attention models [10,11].Interpretable classification: The final representation feeds into a classification layer with dropout and regularization. The model enables visualization of attention heatmaps (word + aspect level) and examination of positional weights to provide transparency in prediction. This architecture addresses limitations in previous ABSA work by explicitly modeling aspect-context interactions, using dynamic fusion of embeddings, encoding positional proximity to aspects, and introducing hierarchical structure to attention. We build upon recent advances such as sparse self-attention in ABSA [12] and hierarchical Transformer designs [13], adapting them into an integrated, novel model for aspect-based sentiment. The contributions of this paper are: A hybrid ABSA model with attention-based fusion of Transformer embeddings and explicit aspect embedding, enabling aspect sensitivity in representation.The design and implementation of a hierarchical + position-aware attention module, capturing both local (word-level) and hierarchical (aspect-oriented) sentiment cues, with positional bias toward aspect proximity.A newly collected, balanced ABSA dataset of 10,000 reviews uniformly labeled across five sentiment classes, addressing limitations of outdated or skewed datasets.Extensive evaluation on both the custom dataset and public ABSA benchmarks (e.g., Amazon reviews), showing improvements in accuracy, interpretability, and robustness.Detailed interpretability analysis: visualizing attention weights, position weight curves, and aspect-level influence to better understand model decisions. The primary objective of this research is to develop a robust, novel, and interpretable deep-learning framework for ABSA that addresses three critical gaps: embedding fusion, positional awareness, and hierarchical interpretability. Specifically, this work aims to: Design anattention-based fusion mechanismto combine Transformer embeddings (BERT, RoBERTa) with an explicit aspect embedding in a context-sensitive manner.Employ BiLSTM to model sequential context forward and backward, enhancing the representation of aspect-aware fused embeddings.Build ahierarchical + position-aware attention modulethat assigns word-level attention conditioned on aspect, aggregates across aspects or sentences, and reweights tokens based on proximity to the aspect.Construct and release a new balanced dataset of 10,000 product/service reviews, each labeled across five sentiment classes, to provide a modern benchmark for ABSA research.Conduct rigorous experiments on both the new dataset and established benchmark datasets to validate performance, generalization, and interpretability.Provide interpretability tools: attention heatmaps, position weight visualization, and aspect-level influence to help users and researchers understand model outputs.",
            "2. Related Work": "2.1. Machine-Learning (Traditional) MethodsMachine Learning (ML) is among the most powerful fields of computer science that seeks to replicate the human process of learning on the basis of data experience rather than being written in the form of programming [14]. It allows systems to be automatically improved as they are exposed to increasing amounts of data with time [15]. According to Janiesch et al. [16], ML methods can be divided into shallow (traditional) and deep-learning methods. Shallow or conventional ML has several different paradigms, which are supervised, unsupervised, semi-supervised, or reinforcement learning, and each of them is appropriate to the various data availabilities and problem formulations. Naive Bayes (NB), Support Vector Machines (SVM), and Logistic Regression (LR) are some of the most popular supervised learning algorithms in sentiment analysis due to their simplicity and interpretation [17]. These do so by training on annotated text (data) to classify textual opinions of sentiment, usually using handcrafted features obtained using methods such as Bag-of-Words (BoW) or TF-IDF. Nevertheless, these algorithms have several critical limitations, namely they are task-specific, cannot resolve contextual ambiguity, and need large and labeled datasets to be at their best [18]. Furthermore, conventional ML cannot capture semantic nuances, idiomatic expressions, and contextual dependencies, which are characteristics of natural language. To address these concerns, ensemble models have also been considered in addition to unsupervised models. Saad [19] performed a comparative analysis of sentiment polarity on airline Twitter data from the United States of America, applying six ML models to the data, including Naive Bayes, XGBoost, SVM, Random Forest, Decision Tree, and Logistic Regression, in conjunction with standard preprocessing operations such as stop word removal, stemming, and punctuation filtering. The model used in feature extraction was the BoW model based on 14,640 samples that had three sentiment labels (positive, negative, and neutral) on a Kaggle and CrowdFlower data set. SVM was the most precise, with an accuracy of 83.31%, than the Logistic Regression of 81.81, which confirms the strength of a linear model in the text classification exercise. Similar results have been supported by several other studies. As an example, Tripathy et al. [20] compared SVM, NB, and Random Forest on IMDb and Amazon reviews and stated that SVM performed better than the others, achieving 85% accuracy. Similarly, Kouloumpis et al. [21] established that n-gram features together with syntactic features combined with ML classifiers enhanced the sentiment classifiers of Twitter. Although these achievements are enjoyed, traditional ML models still cannot capture long-range dependencies and implicit sentiment features, especially in multi-aspect or fine-grained sentiment analysis tasks. Recent research in speech emotion recognition has demonstrated the efficacy of the feature-selection methods with one experiment comparing RF, DT, SVM, MLP, and KNN on four benchmark datasets and reaching an accuracy of up to 93.61%, which was higher than handcrafted methods, especially on EMO-DB [22]. A different study proposed a hybrid acoustic model that applies SVM to fused representations with the best speaker-independent performance of 76% on eNTERFACE05 and 59% on BAUM-1s, which outperforms the state-of-the-art performance on semi-natural and spontaneous speech in a fused representation [23]. Thus, machine-learning techniques have been a good source of sentiment analysis studies. 2.2. Deep-Learning MethodsDeep learning has reinvented sentiment analysis, where contextual and semantic representations are extracted automatically, automatically learning the representations of raw text without manually engineering features. Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), and Long Short-Term Memory (LSTM) networks are used to capture long-term dependencies in sequence data [24,25], although early models were able to capture local n-gram features. Such architectures were much more effective at detecting the existence of subtle forms of emotion and opinion than existing machine-learning algorithms. Hybrid CNN-LSTM and BiLSTM-Attention architectures were proposed to focus on the benefits of both spatial and time modeling. As an example, a study by Khan et al. (2022) [26] suggested a CNN-LSTM model to analyze the sentiment in English and Roman Urdu, in which CNN is used to extract local features, and LSTM is used to learn the sequence. Their system was able to achieve an accuracy of up to 90.4% on various corpora, compared to previous models. On the same note, [27] established a benchmark dataset on the Urdu language and evaluated different ML and DL models with either count-based or fastText embeddings. Their results showed that n-gram features using Logistic Regression obtained the highest F1 score (82.05%), which underscored the significance of the quality of representation in low-resource language sentiment analysis. Later works followed this research direction and used mBERT to analyze the sentiment of Urdu in several domains, with a competitive result [28]. Intent detection and further extended to Urdu emotion classification. Transformer-based solutions like BERT are also applied to intent detection, and more recently, a number of ML, DL, stacked attention-based CNN-BiLSTM models and Transformer models are explored with a variety of feature representations. Taken together, these works prove the increasing methodological maturity of sentiment and emotion analysis dealing with the Urdu language [29,30]. Further developments were made by attention mechanisms that enable models to pay attention to words with sentiments in a dynamic manner. The BiLSTM-Attention framework was more interpretable and more effective in identifying key opinion words in the text [31,32]. Newer models like Transformer-based BERT and RoBERTa [8,9] have changed the face of sentiment analysis since they independently hug self-attention to extract a global context without repetition of similar words. Combining these Transformer embeddings with LSTM layers or CNN layers has resulted in even more potent hybrid systems that combine deep contextual information with time sensitivity. In general, deep learning has transformed sentiment analysis to representation-based, adaptive, and interpretable models, which form a solid basis for the emotion and opinion mining systems of the next generation. 2.3. Features Level and Aspect-Based Sentiment Analysis (ABSA) MethodsABSA is an evolution of the sentiment analysis into a fine-grained one, where it is necessary to understand the sentiment not just on the overall polarity but on particular aspects or features within a review that convey sentiment. This granularity gives the systems the ability to find out what users like or dislike about products or services. The initial period of ABSA depended greatly on the algorithms used in Machine Learning (ML) [33], where systems learn and identify the sentiment without being programmed. Common classical supervised techniques included Naive Bayes (NB) [34], Support Vector Machines (SVM) [35], and Artificial Neural Networks (ANN) [36], and were applied to ABSA subtasks such as aspect term extraction, aspect category classification, and sentiment polarity detection [37]. A major feature of these models was the use of feature engineering, in which the linguistic features such as n-grams, bag-of-words, POS tags, syntactic patterns, and sentiment lexicons were extracted by humans [38]. These handcrafted features were, however, domain-specific and not scalable, leading to a move to models that would learn with bare data. The concept of Deep-Learning (DL) architectures has changed the face of ABSA, making it less reliant on manual feature design. The first architectures to learn contextual and local dependencies in review text included architectures like Recurrent Neural Networks (RNNs) [39] and Convolutional Neural Networks (CNNs) [40]. It is worth mentioning that Wang et al. [41] proposed Unified Position-Aware CNN (UP-CNN), which was suitable to process both Aspect Term Sentiment Analysis (ATSA) and Aspect Category Sentiment Analysis (ACSA) tasks. Their model, based on benchmark datasets such as SemEval-2014 (Laptop and Restaurant) [42], MAMS-Term [43], and Twitter [44], proved the significance of adding the aspect position information to the sentiment interpretation. These neural methods represented a big step forward as they would automatically acquire semantic relations between aspect and opinion words, which would improve the robustness and cross-domain generalization. The second advancement came with Transformer-based architecture, especially the BERT (Bidirectional Encoder Representations from Transformers) [45], which added the contextual representation with bidirectional word dependence. The success of BERT further spread to other NLP problems such as question answering, classification, and sentiment analysis, as it allowed deep semantic comprehension via self-attention mechanisms. Its ability to fine-tune made it the most suitable for the ABSA, such that the model could match aspect terms and opinion expressions of relevance. Simultaneously, GPT models (GPT-1, GPT-2 [46], and GPT-3 [47]) extended the generative aspect of sentiment analysis. Large language models (LLMs) like GPT and ChatGPT also enhanced the contextual understanding with possible results of zero-shot and few-shot ABSA problems. Nonetheless, as Chumakov et al. [48] observed, the study of the GPT-driven models of subtasks such as Aspect-Sentiment Triplet Extraction (ASTE) is in its infancy, which is a new direction in the research of ABSA. A very related dimension of ABSA is feature-level sentiment analysis, which aims at defining explicit product or service features as battery life or camera quality, and correlating them with sentiment polarity. This degree of granularity gives actionable information, especially with commercial use and recommendation systems. Recent models started to combine syntactic dependency parsing and hierarchical attention, as well as aspect opinion alignment systems, to enhance feature sentiment coupling. Transformer-based architectures, including BERT-PT [49], Unified Generative Frameworks [50], and more recent hybrid systems using contextual encoders along with attention and gating mechanisms [51], have helped dramatically towards improving interpretability and accuracy. Altogether, these developments present an evident direction in the literature of the replacement of manual, feature-engineered ML frameworks by highly contextual, adaptive Transformer frameworks that can emphasize complex, many-level sentiment dependencies with respect to aspects, features, and expressions. 2.4. Attention and Hierarchical Attention MechanismAdvancement of the attention mechanisms has seen important improvement in the sentiment analysis process, as there is no longer the need to stick to simple attention models and instead, consider hierarchical and multi-level attention. Mechanisms such as word, sentence, and aspect level allow models to target the most informative areas of text, making them interpretable and more accurate. Models such as HAN, HATN, ATAE-LSTM, and IAN have played an important role in capturing layered semantic and contextual dependencies, which can be used to understand sentiment decisions in a more sensible fashion. Attention-based architectures are especially useful in aspect-level sentiment classification. The attention-based LSTM model [32] is an effective model that captures fine-grained opinions, and it has been shown to yield a great deal of results on the SemEval-2014 dataset. It is built on this, where the BATAE-GRU model [52] incorporates the BERT embeddings, RNNs, and attention to reinforce aspect-context links, outperforming ATAE-LSTM by up to 9.9% accuracy. The models demonstrate the development of the attention processes into context weighting to deep contextual reasoning in order to interpret the sentiment more accurately. In addition to the sentiment of text, Hierarchical Attention Networks (HAN) have been shown to be successful in other, more complex tasks such as sequential recommendation and video understanding by learning both temporal and semantic dependencies across multiple levels [53,54]. Their ability to combine fine-grained information has inspired sentiment models like HATN and IAN that combine to utilize hierarchical relations between aspects and sentences [55,56]. Taken collectively, these developments represent a turn toward interpretable, multi-layered structures of attention that integrate contextual, hierarchical, and aspect-level knowledge, which form a strong basis of contemporary sentiment analysis. 2.5. Transformer-Based ModelsSentiment and ABSA have been revolutionized through the creation of Transformer-based models that incorporate contextual embeddings and transfer learning, and enable models to learn deep semantic text relationships. Early Transformer architecture models such as BERT, RoBERTa, DistilBERT, and XLNet show improved performance over traditional deep-learning strategies, and they use self-attention and bidirectional context modeling. These models are also good at capturing subtle sentiment indicators, context-driven dependencies, and opinion-specific aspects, and are a new standard on sentiment analysis tasks. These application-specific architectures have been optimized in recent developments. Multi-Grained Attention Network (T-MGAN) is a combination of Transformer and Tree Transformer that is trained together to learn syntactic and contextual outputs between aspects and opinions. It is an effective finer-grained sentiment cue capturing method with a multi-grained attention and dual-pooling mechanism, which has demonstrated better performance on several benchmark datasets [57]. The development of the sentiment analysis methods was thoroughly reviewed, including both classical word embeddings, machine-learning ones, contextual embeddings, and more advanced Transformer-based models (GPT, BERT, and T5). The article is a critical comparison of the strengths, limitations, and appropriate applications of each technique, and the description of the main current research trends and challenges. This summary provides the scientists with a definite idea of the modern developments and the future perspectives of the SA discipline [58]. On the same note, BERT Adversarial Training (BAT) model boosts the robustness in ABSA by embedding adversarial training in the embedding space and outperforms general and post-trained BERT variants and represents a significant improvement in robust training of transformers [59]. RoBERTa-derived methods have also shown even higher precision with a maximum 92.35% accuracy on SemEval restaurant reviews and 82.33% on the laptop domains, thus establishing RoBERTa as a state-of-the-art Transformer in aspect-level sentiment tasks [60]. Hybrid Transformer architectures build on the progress made earlier by integrating contextual representations with auxiliary models to gain more insight into emotions. RAMHA (RoBERTa with Adapter-based Mental Health Analyzer) is a machine combining RoBERTa, adapter layers, BiLSTM, attention, and focal loss in classifying text on social media on GoEmotions datasets, attaining up to 92% accuracy, outperforming eight baselines [61]. Other than these, ABSA ALBERT parameter-sharing mechanism can be more efficient at fine-tuning on large review collections [62], and T5 reinvents sentiment and aspect extraction as a text-to-text generation, which improves extrapolation to unseen areas [63]. Furthermore, XLNet, with its language modeling that uses permutations, offers more detailed bidirectional context to aspect-sentiment prediction [64]. Coupled with these Transformer-based architectures, these contextual embeddings and transfer learning highlight the revolutionary impact of contextual embeddings and transfer learning in producing interpretable, scalable, and high-performing sentiment and aspect-based systems of analysis. 2.6. Explainable and Interpretable ModelsThe implementation of Explainable Artificial Intelligence (XAI) in sentiment analysis has gained importance in improving the accuracy of transparency, and accountability of Transformer-based models. Recent research gives the results of fine-tuned BERT, RoBERTa, DistilBERT, and XLNet models in ABSA tasks, and explainability systems such as LIME, SHAP, Integrated Gradients, and Grad-CAM show what linguistic features are used to trigger model actions. The researchers showed a max. 97.62% accuracy with the help of SemEval, Naver, and MAMS, and showed that interpretability creates a direct contribution to the improvement of robustness and model reliability on smaller-scale sentiment tasks [65].On the same note, the TRABSA (Transformer and Attention-based BiLSTM for Sentiment Analysis) framework combines RoBERTa with BiLSTM and attention mechanisms to enhance the classification accuracy and interpretability. SHAP-based traba visualizations can be used to interpret token-wise sentiment attribution, and on multilingual tweet data, they have 94% accuracy. Its more explainable architecture increases not only predictive precision, but also has utility in practical uses of deep learning to decision support, e.g., pandemic control and policy optimization, which explains the use of interpretable deep learning in socially important projects [66]. Besides that, a layer-wise SHAP decomposition model disaggregates Large Language Models (LLMs), including their embedding, encoder, and attention layers, to provide fine-grained interpretability. This framework elucidates the spread of sentiment cues through layers using the Stanford Sentiment Treebank (SST-2) and is more readable and reliable compared to model-level interpretability frameworks in holistic models [67]. In a complementary manner, a hybrid interpretability framework based on ResNet heatmaps and 2D Transformer saliency maps shows how multimodal explanations can be used to provide spatial-temporal coherence in tasks of sentiment and industrial prediction, with an accuracy of 94.1% and domain-specific visual narrative [68]. The followingTable 1summarizes the related work.Table 1.Summary of Sentiment Analysis and ABSA Studies.",
            "2.1. Machine-Learning (Traditional) Methods": "Machine Learning (ML) is among the most powerful fields of computer science that seeks to replicate the human process of learning on the basis of data experience rather than being written in the form of programming [14]. It allows systems to be automatically improved as they are exposed to increasing amounts of data with time [15]. According to Janiesch et al. [16], ML methods can be divided into shallow (traditional) and deep-learning methods. Shallow or conventional ML has several different paradigms, which are supervised, unsupervised, semi-supervised, or reinforcement learning, and each of them is appropriate to the various data availabilities and problem formulations. Naive Bayes (NB), Support Vector Machines (SVM), and Logistic Regression (LR) are some of the most popular supervised learning algorithms in sentiment analysis due to their simplicity and interpretation [17]. These do so by training on annotated text (data) to classify textual opinions of sentiment, usually using handcrafted features obtained using methods such as Bag-of-Words (BoW) or TF-IDF. Nevertheless, these algorithms have several critical limitations, namely they are task-specific, cannot resolve contextual ambiguity, and need large and labeled datasets to be at their best [18]. Furthermore, conventional ML cannot capture semantic nuances, idiomatic expressions, and contextual dependencies, which are characteristics of natural language. To address these concerns, ensemble models have also been considered in addition to unsupervised models. Saad [19] performed a comparative analysis of sentiment polarity on airline Twitter data from the United States of America, applying six ML models to the data, including Naive Bayes, XGBoost, SVM, Random Forest, Decision Tree, and Logistic Regression, in conjunction with standard preprocessing operations such as stop word removal, stemming, and punctuation filtering. The model used in feature extraction was the BoW model based on 14,640 samples that had three sentiment labels (positive, negative, and neutral) on a Kaggle and CrowdFlower data set. SVM was the most precise, with an accuracy of 83.31%, than the Logistic Regression of 81.81, which confirms the strength of a linear model in the text classification exercise. Similar results have been supported by several other studies. As an example, Tripathy et al. [20] compared SVM, NB, and Random Forest on IMDb and Amazon reviews and stated that SVM performed better than the others, achieving 85% accuracy. Similarly, Kouloumpis et al. [21] established that n-gram features together with syntactic features combined with ML classifiers enhanced the sentiment classifiers of Twitter. Although these achievements are enjoyed, traditional ML models still cannot capture long-range dependencies and implicit sentiment features, especially in multi-aspect or fine-grained sentiment analysis tasks. Recent research in speech emotion recognition has demonstrated the efficacy of the feature-selection methods with one experiment comparing RF, DT, SVM, MLP, and KNN on four benchmark datasets and reaching an accuracy of up to 93.61%, which was higher than handcrafted methods, especially on EMO-DB [22]. A different study proposed a hybrid acoustic model that applies SVM to fused representations with the best speaker-independent performance of 76% on eNTERFACE05 and 59% on BAUM-1s, which outperforms the state-of-the-art performance on semi-natural and spontaneous speech in a fused representation [23]. Thus, machine-learning techniques have been a good source of sentiment analysis studies.",
            "2.2. Deep-Learning Methods": "Deep learning has reinvented sentiment analysis, where contextual and semantic representations are extracted automatically, automatically learning the representations of raw text without manually engineering features. Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), and Long Short-Term Memory (LSTM) networks are used to capture long-term dependencies in sequence data [24,25], although early models were able to capture local n-gram features. Such architectures were much more effective at detecting the existence of subtle forms of emotion and opinion than existing machine-learning algorithms. Hybrid CNN-LSTM and BiLSTM-Attention architectures were proposed to focus on the benefits of both spatial and time modeling. As an example, a study by Khan et al. (2022) [26] suggested a CNN-LSTM model to analyze the sentiment in English and Roman Urdu, in which CNN is used to extract local features, and LSTM is used to learn the sequence. Their system was able to achieve an accuracy of up to 90.4% on various corpora, compared to previous models. On the same note, [27] established a benchmark dataset on the Urdu language and evaluated different ML and DL models with either count-based or fastText embeddings. Their results showed that n-gram features using Logistic Regression obtained the highest F1 score (82.05%), which underscored the significance of the quality of representation in low-resource language sentiment analysis. Later works followed this research direction and used mBERT to analyze the sentiment of Urdu in several domains, with a competitive result [28]. Intent detection and further extended to Urdu emotion classification. Transformer-based solutions like BERT are also applied to intent detection, and more recently, a number of ML, DL, stacked attention-based CNN-BiLSTM models and Transformer models are explored with a variety of feature representations. Taken together, these works prove the increasing methodological maturity of sentiment and emotion analysis dealing with the Urdu language [29,30]. Further developments were made by attention mechanisms that enable models to pay attention to words with sentiments in a dynamic manner. The BiLSTM-Attention framework was more interpretable and more effective in identifying key opinion words in the text [31,32]. Newer models like Transformer-based BERT and RoBERTa [8,9] have changed the face of sentiment analysis since they independently hug self-attention to extract a global context without repetition of similar words. Combining these Transformer embeddings with LSTM layers or CNN layers has resulted in even more potent hybrid systems that combine deep contextual information with time sensitivity. In general, deep learning has transformed sentiment analysis to representation-based, adaptive, and interpretable models, which form a solid basis for the emotion and opinion mining systems of the next generation.",
            "2.3. Features Level and Aspect-Based Sentiment Analysis (ABSA) Methods": "ABSA is an evolution of the sentiment analysis into a fine-grained one, where it is necessary to understand the sentiment not just on the overall polarity but on particular aspects or features within a review that convey sentiment. This granularity gives the systems the ability to find out what users like or dislike about products or services. The initial period of ABSA depended greatly on the algorithms used in Machine Learning (ML) [33], where systems learn and identify the sentiment without being programmed. Common classical supervised techniques included Naive Bayes (NB) [34], Support Vector Machines (SVM) [35], and Artificial Neural Networks (ANN) [36], and were applied to ABSA subtasks such as aspect term extraction, aspect category classification, and sentiment polarity detection [37]. A major feature of these models was the use of feature engineering, in which the linguistic features such as n-grams, bag-of-words, POS tags, syntactic patterns, and sentiment lexicons were extracted by humans [38]. These handcrafted features were, however, domain-specific and not scalable, leading to a move to models that would learn with bare data. The concept of Deep-Learning (DL) architectures has changed the face of ABSA, making it less reliant on manual feature design. The first architectures to learn contextual and local dependencies in review text included architectures like Recurrent Neural Networks (RNNs) [39] and Convolutional Neural Networks (CNNs) [40]. It is worth mentioning that Wang et al. [41] proposed Unified Position-Aware CNN (UP-CNN), which was suitable to process both Aspect Term Sentiment Analysis (ATSA) and Aspect Category Sentiment Analysis (ACSA) tasks. Their model, based on benchmark datasets such as SemEval-2014 (Laptop and Restaurant) [42], MAMS-Term [43], and Twitter [44], proved the significance of adding the aspect position information to the sentiment interpretation. These neural methods represented a big step forward as they would automatically acquire semantic relations between aspect and opinion words, which would improve the robustness and cross-domain generalization. The second advancement came with Transformer-based architecture, especially the BERT (Bidirectional Encoder Representations from Transformers) [45], which added the contextual representation with bidirectional word dependence. The success of BERT further spread to other NLP problems such as question answering, classification, and sentiment analysis, as it allowed deep semantic comprehension via self-attention mechanisms. Its ability to fine-tune made it the most suitable for the ABSA, such that the model could match aspect terms and opinion expressions of relevance. Simultaneously, GPT models (GPT-1, GPT-2 [46], and GPT-3 [47]) extended the generative aspect of sentiment analysis. Large language models (LLMs) like GPT and ChatGPT also enhanced the contextual understanding with possible results of zero-shot and few-shot ABSA problems. Nonetheless, as Chumakov et al. [48] observed, the study of the GPT-driven models of subtasks such as Aspect-Sentiment Triplet Extraction (ASTE) is in its infancy, which is a new direction in the research of ABSA. A very related dimension of ABSA is feature-level sentiment analysis, which aims at defining explicit product or service features as battery life or camera quality, and correlating them with sentiment polarity. This degree of granularity gives actionable information, especially with commercial use and recommendation systems. Recent models started to combine syntactic dependency parsing and hierarchical attention, as well as aspect opinion alignment systems, to enhance feature sentiment coupling. Transformer-based architectures, including BERT-PT [49], Unified Generative Frameworks [50], and more recent hybrid systems using contextual encoders along with attention and gating mechanisms [51], have helped dramatically towards improving interpretability and accuracy. Altogether, these developments present an evident direction in the literature of the replacement of manual, feature-engineered ML frameworks by highly contextual, adaptive Transformer frameworks that can emphasize complex, many-level sentiment dependencies with respect to aspects, features, and expressions.",
            "2.4. Attention and Hierarchical Attention Mechanism": "Advancement of the attention mechanisms has seen important improvement in the sentiment analysis process, as there is no longer the need to stick to simple attention models and instead, consider hierarchical and multi-level attention. Mechanisms such as word, sentence, and aspect level allow models to target the most informative areas of text, making them interpretable and more accurate. Models such as HAN, HATN, ATAE-LSTM, and IAN have played an important role in capturing layered semantic and contextual dependencies, which can be used to understand sentiment decisions in a more sensible fashion. Attention-based architectures are especially useful in aspect-level sentiment classification. The attention-based LSTM model [32] is an effective model that captures fine-grained opinions, and it has been shown to yield a great deal of results on the SemEval-2014 dataset. It is built on this, where the BATAE-GRU model [52] incorporates the BERT embeddings, RNNs, and attention to reinforce aspect-context links, outperforming ATAE-LSTM by up to 9.9% accuracy. The models demonstrate the development of the attention processes into context weighting to deep contextual reasoning in order to interpret the sentiment more accurately. In addition to the sentiment of text, Hierarchical Attention Networks (HAN) have been shown to be successful in other, more complex tasks such as sequential recommendation and video understanding by learning both temporal and semantic dependencies across multiple levels [53,54]. Their ability to combine fine-grained information has inspired sentiment models like HATN and IAN that combine to utilize hierarchical relations between aspects and sentences [55,56]. Taken collectively, these developments represent a turn toward interpretable, multi-layered structures of attention that integrate contextual, hierarchical, and aspect-level knowledge, which form a strong basis of contemporary sentiment analysis.",
            "2.5. Transformer-Based Models": "Sentiment and ABSA have been revolutionized through the creation of Transformer-based models that incorporate contextual embeddings and transfer learning, and enable models to learn deep semantic text relationships. Early Transformer architecture models such as BERT, RoBERTa, DistilBERT, and XLNet show improved performance over traditional deep-learning strategies, and they use self-attention and bidirectional context modeling. These models are also good at capturing subtle sentiment indicators, context-driven dependencies, and opinion-specific aspects, and are a new standard on sentiment analysis tasks. These application-specific architectures have been optimized in recent developments. Multi-Grained Attention Network (T-MGAN) is a combination of Transformer and Tree Transformer that is trained together to learn syntactic and contextual outputs between aspects and opinions. It is an effective finer-grained sentiment cue capturing method with a multi-grained attention and dual-pooling mechanism, which has demonstrated better performance on several benchmark datasets [57]. The development of the sentiment analysis methods was thoroughly reviewed, including both classical word embeddings, machine-learning ones, contextual embeddings, and more advanced Transformer-based models (GPT, BERT, and T5). The article is a critical comparison of the strengths, limitations, and appropriate applications of each technique, and the description of the main current research trends and challenges. This summary provides the scientists with a definite idea of the modern developments and the future perspectives of the SA discipline [58]. On the same note, BERT Adversarial Training (BAT) model boosts the robustness in ABSA by embedding adversarial training in the embedding space and outperforms general and post-trained BERT variants and represents a significant improvement in robust training of transformers [59]. RoBERTa-derived methods have also shown even higher precision with a maximum 92.35% accuracy on SemEval restaurant reviews and 82.33% on the laptop domains, thus establishing RoBERTa as a state-of-the-art Transformer in aspect-level sentiment tasks [60]. Hybrid Transformer architectures build on the progress made earlier by integrating contextual representations with auxiliary models to gain more insight into emotions. RAMHA (RoBERTa with Adapter-based Mental Health Analyzer) is a machine combining RoBERTa, adapter layers, BiLSTM, attention, and focal loss in classifying text on social media on GoEmotions datasets, attaining up to 92% accuracy, outperforming eight baselines [61]. Other than these, ABSA ALBERT parameter-sharing mechanism can be more efficient at fine-tuning on large review collections [62], and T5 reinvents sentiment and aspect extraction as a text-to-text generation, which improves extrapolation to unseen areas [63]. Furthermore, XLNet, with its language modeling that uses permutations, offers more detailed bidirectional context to aspect-sentiment prediction [64]. Coupled with these Transformer-based architectures, these contextual embeddings and transfer learning highlight the revolutionary impact of contextual embeddings and transfer learning in producing interpretable, scalable, and high-performing sentiment and aspect-based systems of analysis.",
            "2.6. Explainable and Interpretable Models": "The implementation of Explainable Artificial Intelligence (XAI) in sentiment analysis has gained importance in improving the accuracy of transparency, and accountability of Transformer-based models. Recent research gives the results of fine-tuned BERT, RoBERTa, DistilBERT, and XLNet models in ABSA tasks, and explainability systems such as LIME, SHAP, Integrated Gradients, and Grad-CAM show what linguistic features are used to trigger model actions. The researchers showed a max. 97.62% accuracy with the help of SemEval, Naver, and MAMS, and showed that interpretability creates a direct contribution to the improvement of robustness and model reliability on smaller-scale sentiment tasks [65]. On the same note, the TRABSA (Transformer and Attention-based BiLSTM for Sentiment Analysis) framework combines RoBERTa with BiLSTM and attention mechanisms to enhance the classification accuracy and interpretability. SHAP-based traba visualizations can be used to interpret token-wise sentiment attribution, and on multilingual tweet data, they have 94% accuracy. Its more explainable architecture increases not only predictive precision, but also has utility in practical uses of deep learning to decision support, e.g., pandemic control and policy optimization, which explains the use of interpretable deep learning in socially important projects [66]. Besides that, a layer-wise SHAP decomposition model disaggregates Large Language Models (LLMs), including their embedding, encoder, and attention layers, to provide fine-grained interpretability. This framework elucidates the spread of sentiment cues through layers using the Stanford Sentiment Treebank (SST-2) and is more readable and reliable compared to model-level interpretability frameworks in holistic models [67]. In a complementary manner, a hybrid interpretability framework based on ResNet heatmaps and 2D Transformer saliency maps shows how multimodal explanations can be used to provide spatial-temporal coherence in tasks of sentiment and industrial prediction, with an accuracy of 94.1% and domain-specific visual narrative [68]. The followingTable 1summarizes the related work. Table 1.Summary of Sentiment Analysis and ABSA Studies.",
            "3. Materials and Methods": "Our proposed model introduces an aspect-aware, fusion-based pipeline for fine-grained sentiment classification. Every step of the methodology is thoroughly developed to resolve current weaknesses of sentiment analysis without sacrificing predictive power and interpretability. Our architecture is an aspect-based, fusion-based architecture of fine-grained Aspect-Based Sentiment Analysis (ABSA). The model takes two inputs, a review text and an aspect term, and gives a five-way sentiment prediction (1â€“5). Key novelties: (i) dynamic, gated combination of multiple PLM embeddings (BERT + RoBERTa) with explicit aspect embeddings, (ii) BiLSTM sequential modeling on fused representations, (iii) a single aspect-sensitive, position-sensitive attention to provide efficient, interpretable aggregation, and (iv) integrated explainability (attention heatmaps + SHAP/Integrated Gradients) with quantitative evaluation of the explanations. Now,Figure 1illustrates the overall architecture of the Proposed Hybrid Attention-based Model (HAM) architecture, which combines Transformer encoders (BERT/RoBERTa), BiLSTM, and GAT to integrate contextual, sequential, and aspect-based feature fusion. This model uses an attention-based fusion mechanism, Focal Loss produced classification, and a multi-stage explainability module ( Attention Heatmaps, Aspect Interpretation, SHAP/Integrated Gradients) to produce interpretable and robust sentiment prediction on a variety of diverse datasets, and Algorithm 1 also describes the steps of our proposed model.Algorithm 1HAM-Xâ€”Hybrid Attention-based Model with Explainability LayerInput:Textx, Transformer encoderğ‘‡(Â·)T(Â·)Output:Predicted sentimentğ‘¦Ì‚y^and explanation mapâ„°E1:ğ‘‡ğ‘¥â†Txâ†Tokenize(x)2:ğ¸â†ğ‘‡(ğ‘‡ğ‘¥)Eâ†T(Tx)3:ğ»ğ‘ğ‘–â†BiLSTM(ğ¸)Hbiâ†BiLSTM(E)4:ğ»ğ‘”ğ‘ğ‘¡â†GAT(ğ»ğ‘ğ‘–)Hgatâ†GAT(Hbi)5:ğ»ğ‘“â†ğ›½1ğ¸+ğ›½2ğ»ğ‘ğ‘–+ğ›½3ğ»ğ‘”ğ‘ğ‘¡Hfâ†Î²1E+Î²2Hbi+Î²3Hgatâ–¹ Fusion layerforeach hidden stateâ„ğ‘¡âˆˆğ»ğ‘“htâˆˆHfdoğ‘ ğ‘¡â†score(â„ğ‘¡)stâ†score(ht)end for6:ğ›¼ğ‘¡â†ğ‘’ğ‘ ğ‘¡âˆ‘ğ‘—ğ‘’ğ‘ ğ‘—Î±tâ†estâˆ‘jesj7:ğ¶â†âˆ‘ğ‘¡ğ›¼ğ‘¡â„ğ‘¡Câ†âˆ‘tÎ±thtâ–¹ Context vector via attention pooling8:ğ¶â€²â†Dropout(ğ¶,ğ‘)Câ€²â†Dropout(C,p)9:ğ‘§â†ğ‘Šğ‘ğ¶â€²+ğ‘ğ‘zâ†WcCâ€²+bc10:ğ‘¦Ì‚â†Softmax(ğ‘§)y^â†Softmax(z)â–¹ Final sentiment prediction11:â„’ğ‘“ğ‘œğ‘ğ‘ğ‘™â†âˆ’1ğ‘âˆ‘ğ‘ğ‘–=1(1âˆ’ğ‘¦Ì‚ğ‘–)ğ›¾log(ğ‘¦Ì‚ğ‘–)Lfocalâ†âˆ’1Nâˆ‘i=1N(1âˆ’y^i)Î³log(y^i)12:ğœƒâ†ğœƒâˆ’ğœ‚âˆ‡ğœƒâ„’ğ‘“ğ‘œğ‘ğ‘ğ‘™Î¸â†Î¸âˆ’Î·âˆ‡Î¸Lfocalâ–¹ Optimization (AdamW)Explainability Layer:13:ğ’œ(ğ‘¥)â†mean(headğ‘ğ‘¡ğ‘¡ğ‘›(ğ‘¥))A(x)â†mean(headattn(x))â–¹ Attention heatmap14:ğœ™ğ‘â†âˆ‘ğ‘¡âˆˆaspectğ›¼ğ‘¡â„ğ‘¡Ï•aâ†âˆ‘tâˆˆaspectÎ±thtâ–¹ Aspect-level explanation15:ğ¼ğºğ‘–â†(ğ‘¥ğ‘–âˆ’ğ‘¥â€²ğ‘–)âˆ«10âˆ‚ğ¹(ğ‘¥â€²ğ‘–+ğ›¼(ğ‘¥ğ‘–âˆ’ğ‘¥â€²ğ‘–))âˆ‚ğ‘¥ğ‘–ğ‘‘ğ›¼IGiâ†(xiâˆ’xiâ€²)âˆ«01âˆ‚F(xiâ€²+Î±(xiâˆ’xiâ€²))âˆ‚xidÎ±â–¹ Integrated gradients16:â„°â†{ğ’œ(ğ‘¥),ğœ™ğ‘,ğ¼ğºğ‘–}Eâ†{A(x),Ï•a,IGi}17:returnğ‘¦Ì‚,â„°y^,E Figure 1.Proposed Hybrid Attention-based Model (HAM) architecture, implementing Transformer, BiLSTM, and GAT representations with adaptive attention fusion and a combined explainability module to make accurate and interpretable sentiment prediction. 3.1. Input LayerThe model takes two kinds of inputs; one of them is the review text, which contains the sentiment-bearing content, and the other one is the aspect term, which determines the point of analysis. This format is a two-input structure that is essential to Aspect-Based Sentiment Analysis (ABSA), which makes sure that no predictions are made on the entire review but on the aspect itself.We start with a review textRand the corresponding aspectA.ğ‘…=(ğ‘¤1,â€¦,ğ‘¤ğ‘›),ğ´=(ğ‘1,â€¦,ğ‘ğ‘š).R=(w1,â€¦,wn),A=(a1,â€¦,am).(1) 3.2. Preprocessing and TokenizationRaw review text and aspect terms are initially normalized with the removal of noise (punctuation, case sensitivity errors, and unnecessary spaces) and later tokenized with BERT and RoBERTa subword tokenizers. This will be done to ensure that it is compatible with existing contextual encoders and that semantic and syntactic data are retained to support aspect-sensitive representation. PLM tokenizers (WordPiece/BPE) generate subword tokens that are compatible with BERT/RoBERTa and allow the computation of spans that are correct to position them.ğ‘‡=TokenizeÃ—BERT(ğ‘…)=(ğ‘¡1,â€¦,ğ‘¡ğ‘‡),ğ‘‡â€²=TokenizeÃ—RoBERTa(ğ‘…).T=TokenizeÃ—BERT(R)=(t1,â€¦,tT),Tâ€²=TokenizeÃ—RoBERTa(R).(2)Map aspect tokens to token span indices(â„ğ‘âŠ†1,â€¦,ğ‘‡)(IaâŠ†1,â€¦,T)(3) 3.3. Aspect ExtractionAspects are mined in a hybrid fashion that integrates both rule-based linguistic techniques with dependence-based heuristics. This step is necessary to guarantee that the terms of aspects that were relevant (e.g., product features or service attributes) are clearly defined. The model aims to isolate the aspects of the entire review text in favor of the fine-grained opinion targets instead of having to use only coarse-grained sentiment indications. This layer selects aspect (A) and its token span. Correct span detection makes position-related attention and aspect pooling correct.(â„ğ‘=ğ‘–ğ‘ ,â€¦,ğ‘–ğ‘’)(Ia=is,â€¦,ie)(4) 3.4. Token and Position EmbeddingsAfter the review tokens and aspect tokens are received, all tokens are projected to a dense space of vectors with pretrained contextual encoders. Specifically,(ğ‘…=ğ‘¤1,ğ‘¤2,â€¦,ğ‘¤ğ‘›)(R=w1,w2,â€¦,wn)is a sequence of tokens, each of which is converted to a contextual embedding(ğ‘¤ğ‘–)(wi)using BERT and RoBERTa. In the same way, the aspect sequence(ğ‘†=ğ‘1,ğ‘2,â€¦,ğ‘ğ‘š)(S=a1,a2,â€¦,am)is expressed as aspect embeddings(ğ¸ğ‘ğ‘—)(Eaj). Positional embeddings are introduced in order to save word order information. After the Transformer formulation, the embedding of the input of the ith review token is defined as:ğ‘‹ğ‘–=ğ¸ğ‘¤ğ‘–+ğ‘ƒğ‘–,forğ‘–âˆˆ[1,ğ‘›]Xi=Ewi+Pi,foriâˆˆ[1,n](5)(ğ¸ğ‘¤ğ‘–)(Ewi)is the embedding of the contextual token and(ğ‘ƒğ‘–)(Pi)is the embedding of sequential order. Likewise, position-aware embeddings are added to aspect tokensğ‘‹ğ‘ğ‘—=ğ¸ğ‘ğ‘—+ğ‘ƒğ‘ğ‘—,forğ‘—âˆˆ[1,ğ‘š]Xaj=Eaj+Paj,forjâˆˆ[1,m](6)The embedding scheme guarantees that the model learns the semantic meaning of the token and their relative position with regard to the aspect, which is imperative in ABSA. The model is sensitive to what and where is said in the review by being an integration of position encodings and contextual embeddings. 3.5. Linear ProjectionIn order to have compatibility between contextual embeddings and aspect embeddings, we use a linear projection layer that transforms each of the representations to one common latent space of dimension d. Review embeddings with BERT and RoBERTa:ğ»ğµğ¸ğ‘…ğ‘‡âˆ—ğ‘–=ğ‘Šâˆ—ğµğ¸ğ‘…ğ‘‡ğ¸ğµğ¸ğ‘…ğ‘‡âˆ—ğ‘¤ğ‘–+ğ‘âˆ—ğµğ¸ğ‘…ğ‘‡,ğ‘–âˆˆ[1,ğ‘›]HBERTâˆ—i=Wâˆ—BERTEBERTâˆ—wi+bâˆ—BERT,iâˆˆ[1,n](7)ğ»ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘âˆ—ğ‘–=ğ‘Šâˆ—ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘ğ¸ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘âˆ—ğ‘¤ğ‘–+ğ‘âˆ—ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘,ğ‘–âˆˆ[1,ğ‘›]HRoBERTaâˆ—i=Wâˆ—RoBERTaERoBERTaâˆ—wi+bâˆ—RoBERTa,iâˆˆ[1,n](8)and to the aspect of aggregation adding.ğ»ğ‘=ğ‘Šğ‘ğ¸ğ‘+ğ‘ğ‘.Ha=WaEa+ba.(9)In this case,(ğ‘Šğµğ¸ğ‘…ğ‘‡,ğ‘Šğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘,ğ‘Šğ‘âˆˆâ„ğ‘‘Ã—ğ‘‘â€²)(WBERT,WRoBERTa,WaâˆˆRdÃ—dâ€²)are learnable projection matrices and b terms are bias vectors. This forecast is such that:It is assumed that all embeddings((ğ»ğµğ¸ğ‘…ğ‘‡,ğ»ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘,ğ»ğ‘))((HBERT,HRoBERTa,Ha))are in the same semantic space of dimension d.The representations are then comparable directly and can be successfully fused in the next Gated Fusion Layer.All unnecessary differences between models BERT vs RoBERTa are averaged, and complementary features are kept.Therefore, a linear projection serves as a semantic alignment proxy, which progresses the embeddings towards adaptive combination in the gated fusion process. 3.6. Gated Fusion LayerBERT and RoBERTa are complementary in offering contextual representations, but when they are simply concatenated, their results usually include redundancy and suboptimal integration. We propose solving this issue with a Gated Fusion Layer that actively regulates the input of each embedding stream (BERT, RoBERTa, and Aspect) through learning task-specific gating parameters. Formally, assume that the linearly projected embeddings are.ğ»ğµğ¸ğ‘…ğ‘‡âˆˆâ„ğ‘›Ã—ğ‘‘,ğ»ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘âˆˆâ„ğ‘›Ã—ğ‘‘,ğ»ğ‘âˆˆâ„ğ‘‘.HBERTâˆˆRnÃ—d,HRoBERTaâˆˆRnÃ—d,HaâˆˆRd.(10)A gating mechanism is used to dynamically down-weight the significance of BERT and RoBERTa embeddings at each token position:ğºğ‘–=ğœ(ğ‘Šğ‘”[ğ»ğµğ¸ğ‘…ğ‘‡ğ‘–;ğ»ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘ğ‘–;ğ»ğ‘]+ğ‘ğ‘”),ğ‘–âˆˆ[1,ğ‘›],Gi=Ïƒ(Wg[HiBERT;HiRoBERTa;Ha]+bg),iâˆˆ[1,n],(11)And where(ğ‘Šğ‘”âˆˆâ„ğ‘‘Ã—3ğ‘‘)ğ‘ğ‘›ğ‘‘(ğ‘ğ‘”âˆˆâ„ğ‘‘)(WgâˆˆRdÃ—3d)and(bgâˆˆRd)are trainable parameters,([Â·])([Â·])is concatenation, and(ğœ(Â·))(Ïƒ(Â·))is the sigmoid function.The fused representation is then calculated as:ğ»ğ‘“ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›ğ‘–=ğºğ‘–âŠ™ğ»ğµğ¸ğ‘…ğ‘‡ğ‘–+(1âˆ’ğºğ‘–)âŠ™ğ»ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘ğ‘–,Hifusion=GiâŠ™HiBERT+(1âˆ’Gi)âŠ™HiRoBERTa,(12)and lastly trained in the aspect embedding by:ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ğ‘–=ğ»ğ‘“ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›ğ‘–âŠ•ğ»ğ‘,Hifinal=HifusionâŠ•Ha,(13)where(âŠ™)(âŠ™)is defined as element-wise multiplication and(âŠ•)(âŠ•)is defined as concatenation of vectors. Here are some key features of Gated Fusion. NowFigure 2illustrates the Gated Fusion Model, where the Gated Fusion Module shows how to integratively introduce Transformer-based embeddings (e.g., BERT and RoBERTa) and explicit aspect embeddings. Contributions of each source are balanced dynamically by the gating layer by means of element-wise gating and attention weighting to ensure aspect-aware semantic representations are context-sensitive, robust, and optimally fused before sequence modeling.Figure 2.Gated Fusion Module with Transformer and aspect embeddings based on dynamic element-wise gating and aspect attention weighting to create semantic representations that are robust, aspect-aware, and sensitive to attention.Adaptive weighting: The model is trained to apply BERT or RoBERTa features based on the token and context.Aspect-conscious integration: The fusion is still biased towards the opinion target but not generic sentiment by incorporating the aspect embedding in the gating function.Redundancy minimization: Gating filters rather than concatenating both embeddings results in redundancy minimization.Interpretability: Gating scores are visualizable to learn which model had a more significant impact on a decision of a particular token.In this way, the Gated Fusion Layer is the semantic integration point, ensuring that the heterogenous embeddings are combined into one, aspect-sensitive representation, which is then sent to the downstream BiLSTM + Attention layer to reason sequentially. 3.7. Bidirectional Long Short-Term Memory (BiLSTM)Following gated fusion, aspect-aware embeddings(ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™)(Hfinal)are then fed into a Bidirectional Long Short-Term Memory (BiLSTM) network to capture sequential interaction and syntax in the review text. In contrast to conventional RNNs, LSTMs have gating mechanisms, which reduce vanishing gradients, allowing them to learn long-term contextual dependencies. The two-way variant also implies that the past (left context) and future (right context) information were coded at the same time. In the abstract, provided the fused input sequence:ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™=ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™1,ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™2,â€¦,ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ğ‘›,ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ğ‘–âˆˆâ„ğ‘‘,Hfinal=H1final,H2final,â€¦,Hnfinal,HifinalâˆˆRd,(14)The forward and backward LSTMs calculate hidden states as:â„â†’ğ‘–=LSTMğ‘“(ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™Ã—ğ‘–,â„â†’Ã—ğ‘–âˆ’1),hâ†’i=LSTMf(HfinalÃ—i,hâ†’Ã—iâˆ’1),(15)â„â†ğ‘–=LSTMğ‘(ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™Ã—ğ‘–,â„â†Ã—ğ‘–+1),hâ†i=LSTMb(HfinalÃ—i,hâ†Ã—i+1),(16)In which(â„â†’ğ‘–)(hâ†’i)represents the encoding of information starting at the start until token i, and(â„â†ğ‘–)(hâ†i)represents the encoding of information starting at the end, all the way back to token i.The two-directional hidden states are combined to obtain the BiLSTM representation at that time step.â„ğ‘–=[â„â†’ğ‘–;â„â†ğ‘–],â„ğ‘–âˆˆâ„2ğ‘‘.hi=[hâ†’i;hâ†i],hiâˆˆR2d.(17)In this way, the BiLSTM generates the sequence of outputs.ğ»ğµğ‘–ğ¿ğ‘†ğ‘‡ğ‘€=â„1,â„2,â€¦,â„ğ‘›,ğ»ğµğ‘–ğ¿ğ‘†ğ‘‡ğ‘€âˆˆâ„ğ‘›Ã—2ğ‘‘.HBiLSTM=h1,h2,â€¦,hn,HBiLSTMâˆˆRnÃ—2d.(18)The importance of BiLSTM in ABSA.Sequential reasoning: Reasoning between opinion words at different distances, for example â€œnot goodâ€.Aspect alignment: The model captures the aspect-conditioned sentiment by learning a contextual flow around the aspect that involves the aspect preceding context and the aspect succeeding context.Complementary to transformers: Transformers are more effective at contextualizing on a global scale, whereas BiLSTM strengthens local, position-sensitive dependencies, which prove particularly useful in aspect-based tasks.The structure of the architecture is based on a combination of deep contextual embeddings (BERT, RoBERTa) and sequential structure modeling (BiLSTM) to provide semantic richness and contextual accuracy to support strong sentiment classification. 3.8. Hierarchical Attention Mechanism (HAM)The proposed model uses a HAM module to overcome the necessity of refining the BiLSTM outputs and allowing the aspect-based focus. In comparison to traditional single-layer attention, HAM incorporates three complementary views: word-level, position-level, and aspect-level attention, such that the sentiment decision does not simply rely on the token semantics but also the location of words and their connection to the target aspect. Therefore, here,Figure 3illustrates the Multi-level Attention Process HAM module, where word-level attention brings out aspect-relevant tokens, and sentence-level attention combines these representations into aspect-specific contextual vectors. Position-conscious weighting is an additional refinement of attention that gives higher emphasis on tokens that are close to the aspect term, which maximizes interpretability and sentiment discrimination.Figure 3.Multi-level attention process represented by the Hierarchical Attention Mechanism (HAM) module, which shows word-level and sentence-level attention with position-sensitive weighting to give stress on aspect-relevant tokens and create interpretable, aspect-specific contextual representations.3.8.1. Word-Level AttentionAt this stage, the model allocates the weight of importance to each contextual hidden state(â„ğ‘–)(hi)of the BiLSTM, where examples of our sentiment-bearing words are given, and excellent, poor, and irrelevant tokens are suppressed.ğ›¼ğ‘–=exp(â„âŠ¤ğ‘–ğ‘Šğ‘¤ğ‘¢ğ‘¤)âˆ‘ğ‘›ğ‘˜=1exp(â„âŠ¤ğ‘˜ğ‘Šğ‘¤ğ‘¢ğ‘¤)Î±i=exp(hiâŠ¤Wwuw)âˆ‘k=1nexp(hkâŠ¤Wwuw)(19)where(ğ‘Šğ‘¤)(Ww)is a trainable projection matrix and(ğ‘¢ğ‘¤)(uw)is a word-level context vector. The summary representation is:â„ğ‘¤ğ‘œğ‘Ÿğ‘‘=âˆ‘ğ‘–=1ğ‘›ğ›¼ğ‘–â„ğ‘–hword=âˆ‘i=1nÎ±ihi(20)This makes sure that semantic salience is saved from the sequence of reviews.3.8.2. Position-Level AttentionThe significance of words is not enough in ABSA, because in many cases, the sentiment polarity is determined by the relative closeness to the aspect. The position-level attention, therefore, brings in distance-conscious weighting, where the tokens nearer to the aspect have more weight. At position (i) and aspect centered at position (j):ğ‘ğ‘–=11+|ğ‘–âˆ’ğ‘—|pi=11+|iâˆ’j|(21)Then, a normalized position attention score is used:ğ›½ğ‘–=exp(ğ‘ğ‘–Â·(â„âŠ¤ğ‘–ğ‘Šğ‘ğ‘¢ğ‘))âˆ‘ğ‘›ğ‘˜=1exp(ğ‘ğ‘˜Â·(â„âŠ¤ğ‘˜ğ‘Šğ‘ğ‘¢ğ‘))Î²i=exp(piÂ·(hiâŠ¤Wpup))âˆ‘k=1nexp(pkÂ·(hkâŠ¤Wpup))(22)and the position-sensitive representation is:â„ğ‘ğ‘œğ‘ =âˆ‘ğ‘–=1ğ‘›ğ›½ğ‘–â„ğ‘–hpos=âˆ‘i=1nÎ²ihi(23)This level makes sure that the model puts tokens in close proximity to the aspect (e.g., in, â€œThe camera quality of this phone is outstanding,â€ the phrase â€œoutstandingâ€ must be more closely associated with â€œcameraâ€ than with words far away).3.8.3. Aspect-Level AttentionThe last attention layer is used to align review tokens with the specific aspect representationğ¸ğ‘Eato yield aspect-conditioned sentiment.ğ›¾ğ‘–=exp((â„âŠ¤ğ‘–ğ‘Šğ‘ğ¸ğ‘))âˆ‘ğ‘›ğ‘˜=1exp((â„âŠ¤ğ‘˜ğ‘Šğ‘ğ¸ğ‘))Î³i=exp((hiâŠ¤WaEa))âˆ‘k=1nexp((hkâŠ¤WaEa))(24)The Aggregated aspect-aware context vector is then:â„ğ‘ğ‘ ğ‘ğ‘’ğ‘ğ‘¡=âˆ‘ğ‘–=1ğ‘›ğ›¾ğ‘–â„ğ‘–haspect=âˆ‘i=1nÎ³ihi(25)This is so as to ensure that the sentiment is directly conditioned on the target aspect, without misclassification due to irrelevant aspects in multi-aspect reviews.3.8.4. Final Hierarchical Attention RepresentationThe three-level outputs are added together to obtain the hierarchical context vector:â„âˆ—=ğ‘Šğ‘[â„ğ‘¤ğ‘œğ‘Ÿğ‘‘;â„ğ‘ğ‘œğ‘ ;â„ğ‘ğ‘ ğ‘ğ‘’ğ‘ğ‘¡]+ğ‘ğ‘h*=Wc[hword;hpos;haspect]+bc(26)Now here(ğ‘Šğ‘)(Wc)and(ğ‘ğ‘)(bc)are trainable parameters 3.9. Final Classification and OptimizationOnce the hierarchical context representationâ„âˆ—h*has been obtained by the Hierarchical Attention Mechanism, the model goes to the final sentiment prediction step. This step converts the multi-level representation, which is very rich, to a discrete sentiment label using a fully connected projection and a probabilistic classification layer.3.9.1. Linear Projection and Softmax ClassificationThe resultant attention-derived featureâ„âˆ—h*is concatenated and then subjected to a linear layer transformation that projects it into a subspace specific to the sentiment. This transformation captures the high-level abstractions in the word, position, and aspect-level attention.ğ‘§=ğ‘Šğ‘œâ„âˆ—+ğ‘ğ‘œz=Woh*+bo(27)whereğ‘Šğ‘œWoandğ‘ğ‘œboare the output weight matrix and bias vector, respectively. Logits z are then normalized by the SoftMax function to generate a probability distribution of the sentiment classes.ğ‘¦Ì‚Ã—ğ‘–=exp(ğ‘§ğ‘–)âˆ‘Ã—ğ‘˜=1ğ¶exp(ğ‘§ğ‘˜),ğ‘–=1,2,â€¦,ğ¶y^Ã—i=exp(zi)âˆ‘Ã—k=1Cexp(zk),i=1,2,â€¦,C(28)In this case,Crefers to the total amount of sentiment categories (e.g., 5 of fine-grained sentiment: very negative, negative, neutral, positive, and very positive). The probability of the most likely class is chosen as the predicted sentiment.3.9.2. Regularization and Focal Loss OptimizationTo reduce overfitting and increase generalization,(â„âˆ—)(h*)dropout is regularized before classification. The dropout randomly kills neurons in training, making it resistant to noise, and it is not dependent on particular features. Since the example of the sentiment datasets usually has an imbalance in classes, such as more neutral reviews than extremely positive or negative ones, the model uses Focal Loss, rather than the conventional cross-entropy. Focal Loss is a dynamic weight, or scaling, loss that learns to place more emphasis on the learning of the harder, misclassified samples. It is defined asâ„’ğ‘“ğ‘œğ‘ğ‘ğ‘™=âˆ’ğ›¼ğ‘¡(1âˆ’ğ‘¦Ì‚ğ‘¡)ğ›¾log(ğ‘¦Ì‚ğ‘¡)Lfocal=âˆ’Î±t(1âˆ’y^t)Î³log(y^t)(29)where(ğ›¼ğ‘¡)(Î±t)is the weighting factor for class(ğ‘¡)(t),(ğ›¾)(Î³)is the focusing parameter controlling difficulty emphasis,(ğ‘¦Ì‚ğ‘¡)(y^t)is the predicted probability for the true class.This adaptive loss promotes equal learning among the categories of sentiment and guarantees better performance on underrepresented labels.3.9.3. Training StrategyThe Adam optimizer is applied with a learning rate to optimize the AGF-HAM model parameters, and with the help of adaptive moment estimation, it should converge steadily. Early stopping is used to avoid overtraining, with the basis of validation loss. The total training goal reduces the amount of focal loss of all samples:â„’Ã—ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™=1ğ‘âˆ‘Ã—ğ‘–=1ğ‘â„’ğ‘“ğ‘œğ‘ğ‘ğ‘™(ğ‘¦Ì‚ğ‘–,ğ‘¦ğ‘–)LÃ—total=1Nâˆ‘Ã—i=1NLfocal(y^i,yi)(30)In this step, the learned hierarchical and gated representations are incorporated into a small, interpretable, and accurate sentiment prediction that is both accurate and interpretable. 3.10. Explainability ModuleAn Explainability Module Layer is incorporated as the last step of the HAM framework in order to guarantee interpretability and transparency of the final model predictions. The layer makes the process of forming the sentimentsâ€™ decisions globally and locally interpretable by showing how the model forms its conclusion based on linguistic and contextual knowledge. The explainability module is based on three complementary sub-layersâ€”Attention Heatmaps, Aspect-Level Explanations, and SHAP Integrated Gradientsâ€”that have a specific analytical purpose to hold models accountable and explainable to humans.3.10.1. Attention HeadmapsThe Attention Headmaps sub-layer shows token-level scores of the importance of the attention heads in the Transformer encoder and BiLSTM layers. It puts emphasis on particular words, phrases, or clauses that have a strong impact on the sentiment or emotional polarity of a certain text. This visualization, in addition to showing the focus distribution of the model, also proves the interpretive reliability of the attention mechanism. With the help of these heatmaps, researchers and practitioners can confirm that the model focuses on semantically significant areas and not accidental associations.ğ›¼ğ‘–=exp(ğ‘£âŠ¤tanh(ğ‘Šâ„â„ğ‘–+ğ‘Šğ‘ğ‘+ğ‘))âˆ‘ğ‘›ğ‘—=1exp(ğ‘£âŠ¤tanh(ğ‘Šâ„â„ğ‘—+ğ‘Šğ‘ğ‘+ğ‘))Î±i=expvâŠ¤tanh(Whhi+Wqq+b)âˆ‘j=1nexpvâŠ¤tanh(Whhj+Wqq+b)(31)This calculates the attention weight of each tokent, and this is the degree to which that token is relevant to the query (aspect or sentence context). It involves a SoftMax normalization such that all weights of attention have a sum of 1.ğ»att(ğ‘¡ğ‘–)=ğ›¼ğ‘–foreachtokenğ‘¡ğ‘–.Hatt(ti)=Î±iforeachtokenti.(32)This maps the attention weight to the underlying token, which compose the raw headmap of token importance in the text. It graphically shows the words that make the greatest contribution to sentiment decisions.Headmapnorm(ğ‘¡ğ‘–)=ğ›¼ğ‘–âˆ’minğ‘—ğ›¼ğ‘—maxğ‘—ğ›¼ğ‘—âˆ’minğ‘—ğ›¼ğ‘—.Headmapnorm(ti)=Î±iâˆ’minjÎ±jmaxjÎ±jâˆ’minjÎ±j.(33)The weights between the attention and the visualization of the headmap are normalized to 0 or 1 to form a visual representation of the headmap. It guarantees a similar visualization of samples to make them easier to interpret.3.10.2. Aspect-Level ExplanationThe sub-layer of Aspect-Level Explanation gives a fine-grained interpretability, where sentiment weights are mapped to the identified aspects in the text. This sub-layer explains the roles of each product or emotional aspect in the final sentiment classification using dependency-based aspect extraction and context embeddings. It can be used to provide transparency in aspect-aware decision-making and bridge the semantic connection between aspect terms (battery, camera, service) and the contextual sentiments. This sub-layer is specifically useful when the task at hand demands domain interpretability, like reviewing or detecting emotions in social situations.ğ›¼(ğ‘)ğ‘–=exp(ğ‘£âŠ¤ğ‘tanh(ğ‘Šâ„â„ğ‘–+ğ‘Šğ‘ğ‘’ğ‘+ğ‘ğ‘))âˆ‘ğ‘›ğ‘—=1exp(ğ‘£âŠ¤ğ‘tanh(ğ‘Šâ„â„ğ‘—+ğ‘Šğ‘ğ‘’ğ‘+ğ‘ğ‘))Î±i(a)=expvaâŠ¤tanh(Whhi+Waea+ba)âˆ‘j=1nexpvaâŠ¤tanh(Whhj+Waea+ba)(34)This computes aspect-conditioned attention, in whichğ‘’ğ‘eais the embedding of a given aspect (e.g., â€œbatteryâ€ or â€œserviceâ€) It helps the model to pay attention to the most relevant words to that aspect.ğ‘ğ‘=âˆ‘ğ‘–=1ğ‘›ğ›¼(ğ‘)ğ‘–,â„ğ‘–(aspect-awarecontextvector)ca=âˆ‘i=1nÎ±i(a),hi(aspect-awarecontextvector)(35)In this case, the hidden states are used to create a weighted context vector, which is a mixture of the hidden states with aspect-specific attention weights. It is a compressed feeling indicator of that specific element.ğ‘ (ğ‘˜)ğ‘=softmaxğ‘˜(ğ‘Š(ğ‘˜)ğ‘œğ‘ğ‘+ğ‘(ğ‘˜)ğ‘œ)(classprobabilityforaspectğ‘)sa(k)=softmaxk(Wo(k)ca+bo(k))(classprobabilityforaspecta)(36)This is to apply sentiment polarity prediction as an aspectawith all classes using a softmax layer. It produces probabilities of the form of positive, neutral, or negative sentiment.TopTokensÃ—ğ‘=argsortÃ—ğ‘–(ğ›¼(ğ‘)ğ‘–),topâˆ’ğ‘š.TopTokensÃ—a=argsortÃ—iÎ±i(a),top-m.(37)This is used to select the most influential tokens that assist in sentiment on aspecta. These tokens are used to produce aspect-level textual explanations.3.10.3. SHAP/Integrated GradientsThe SHAP Integrated Gradients sub-layer is a combination of SHapley Additive exPlanations (SHAP) with Integrated Gradients (IG) used to measure the contribution of each feature to the output of a model. This hybrid interpretability method combines model sensitivity and feature attribution consistency into unified importance maps that are complementary to the attention-based interpretations. This sub-layer is a combination of SHAP and IG and is able to provide a coherent and consistent explanation of decision boundaries, which strengthens the transparency and reliability of the model.ğœ™ğ‘–=âˆ‘ğ‘†âŠ†ğ‘âˆ–ğ‘–|ğ‘†|!,(|ğ‘|âˆ’|ğ‘†|âˆ’1)!|ğ‘|!â›ââœâœâœğ¹ğ‘†âˆªğ‘–(ğ‘¥ğ‘†âˆªğ‘–)âˆ’ğ¹ğ‘†(ğ‘¥ğ‘†)ââ âŸâŸâŸÏ•i=âˆ‘SâŠ†Nâˆ–i|S|!,(|N|âˆ’|S|âˆ’1)!|N|!(FSâˆªi(xSâˆªi)âˆ’FS(xS))(38)This is the Shapley value equation, which is used to measure the contribution of a given feature to the prediction of the model. It assesses the effect of the addition of a feature (word/token) on model output on all the subsets.âˆ‘ğ‘–=1ğ‘›ğœ™ğ‘–=ğ¹(ğ‘¥)âˆ’ğ¹(ğ‘¥baseline)âˆ‘i=1nÏ•i=F(x)âˆ’F(xbaseline)(39)SHAP values can be summed up to form the difference between baseline and model prediction. This guarantees additive attribution of features, which ensures consistency of interpretability.IGğ‘–(ğ‘¥)=(ğ‘¥ğ‘–âˆ’ğ‘¥â€²Ã—ğ‘–)âˆ«Ã—ğ›¼=01âˆ‚ğ¹(ğ‘¥â€²+ğ›¼(ğ‘¥âˆ’ğ‘¥â€²))âˆ‚ğ‘¥ğ‘–,ğ‘‘ğ›¼IGi(x)=(xiâˆ’xâ€²Ã—i)âˆ«Ã—Î±=01âˆ‚Fxâ€²+Î±(xâˆ’xâ€²)âˆ‚xi,dÎ±(40)Integrated Gradients calculate the contribution of each input feature to the output of a model by summing the gradients between the path taken between a baselineğ‘¥â€²xâ€²and the real input x. It offers path-sensitive interpretability that is smooth and has less noise than raw gradients.To illustrate the interpretability of the proposed explainability module, we provide examples of each of the sentiment categories: very positive, positive, neutral, negative, and very negative. A heatmap visualization inFigure 4is provided per sentence to show the distribution of the attention weights of the individual tokens, i.e., the focus of the model when inferring the sentiment. The darker the region of the heatmap, the greater the intensity of attention of the token is, and the greater its influence on the final prediction of the model will be.Figure 4.Heatmap visualization showing token-level attention distribution for sentences across five sentiment categories. Darker shades represent higher attention weights to the modelâ€™s sentiment prediction.Aspect-Level Summary:Detected Aspect:phone/performanceOpinion Tokens:awful, slow, crashes, uselessAspect-Sentiment Score:âˆ’0.93âˆ’0.93Aspect Polarity Label:Very NegativeFinal Interpretation:The model classifies this sentence asVery Negative. Attention, Aspect-level Polarity, and SHAP/IG all emphasize the same tokens:awful,useless, andcrashesâ€”indicating they are the strongest contributors to the negative sentiment toward thephoneâ€™s performance.Besides visual interpretation, there is also elaborate explainability of one representative sentence inTable 2andTable 3. The table is a quantitative report on the attention weight, aspect-level polarity, SHAP value, and Integrated Gradient (IG) contribution of each token. These three measures are then normalized and averaged to obtain the combined importance score to reflect the contextual and causal influence on the decision of the model. This integrated analysis would allow not only the explainability framework to identify words that carry sentiment (e.g., awful, useless, excellent) but also clarify their relationship with certain aspects in context, allowing the transparent and understandable interpretation of how the model reaches its classification.Table 2.Explainability Table for Sentence: â€œThis phone is awful slow crashes often today uselessâ€.Table 3.Definition and Formula of Each Explainability Metric.The combination of these three sub-layers gives HAM not only superior performance in predictive mode but also a high level of interpretive ability. This Explainability Module Layer converts the HAM framework to more of an intelligent system that can be trusted and interpreted by humans, having the ability to provide the answer to why and how decisions are made through complex sentiment and emotional environments.",
            "3.1. Input Layer": "The model takes two kinds of inputs; one of them is the review text, which contains the sentiment-bearing content, and the other one is the aspect term, which determines the point of analysis. This format is a two-input structure that is essential to Aspect-Based Sentiment Analysis (ABSA), which makes sure that no predictions are made on the entire review but on the aspect itself. We start with a review textRand the corresponding aspectA.ğ‘…=(ğ‘¤1,â€¦,ğ‘¤ğ‘›),ğ´=(ğ‘1,â€¦,ğ‘ğ‘š).R=(w1,â€¦,wn),A=(a1,â€¦,am).(1)",
            "3.2. Preprocessing and Tokenization": "Raw review text and aspect terms are initially normalized with the removal of noise (punctuation, case sensitivity errors, and unnecessary spaces) and later tokenized with BERT and RoBERTa subword tokenizers. This will be done to ensure that it is compatible with existing contextual encoders and that semantic and syntactic data are retained to support aspect-sensitive representation. PLM tokenizers (WordPiece/BPE) generate subword tokens that are compatible with BERT/RoBERTa and allow the computation of spans that are correct to position them.ğ‘‡=TokenizeÃ—BERT(ğ‘…)=(ğ‘¡1,â€¦,ğ‘¡ğ‘‡),ğ‘‡â€²=TokenizeÃ—RoBERTa(ğ‘…).T=TokenizeÃ—BERT(R)=(t1,â€¦,tT),Tâ€²=TokenizeÃ—RoBERTa(R).(2) Map aspect tokens to token span indices(â„ğ‘âŠ†1,â€¦,ğ‘‡)(IaâŠ†1,â€¦,T)(3)",
            "3.3. Aspect Extraction": "Aspects are mined in a hybrid fashion that integrates both rule-based linguistic techniques with dependence-based heuristics. This step is necessary to guarantee that the terms of aspects that were relevant (e.g., product features or service attributes) are clearly defined. The model aims to isolate the aspects of the entire review text in favor of the fine-grained opinion targets instead of having to use only coarse-grained sentiment indications. This layer selects aspect (A) and its token span. Correct span detection makes position-related attention and aspect pooling correct.(â„ğ‘=ğ‘–ğ‘ ,â€¦,ğ‘–ğ‘’)(Ia=is,â€¦,ie)(4)",
            "3.4. Token and Position Embeddings": "After the review tokens and aspect tokens are received, all tokens are projected to a dense space of vectors with pretrained contextual encoders. Specifically,(ğ‘…=ğ‘¤1,ğ‘¤2,â€¦,ğ‘¤ğ‘›)(R=w1,w2,â€¦,wn)is a sequence of tokens, each of which is converted to a contextual embedding(ğ‘¤ğ‘–)(wi)using BERT and RoBERTa. In the same way, the aspect sequence(ğ‘†=ğ‘1,ğ‘2,â€¦,ğ‘ğ‘š)(S=a1,a2,â€¦,am)is expressed as aspect embeddings(ğ¸ğ‘ğ‘—)(Eaj). Positional embeddings are introduced in order to save word order information. After the Transformer formulation, the embedding of the input of the ith review token is defined as:ğ‘‹ğ‘–=ğ¸ğ‘¤ğ‘–+ğ‘ƒğ‘–,forğ‘–âˆˆ[1,ğ‘›]Xi=Ewi+Pi,foriâˆˆ[1,n](5) (ğ¸ğ‘¤ğ‘–)(Ewi)is the embedding of the contextual token and(ğ‘ƒğ‘–)(Pi)is the embedding of sequential order. Likewise, position-aware embeddings are added to aspect tokensğ‘‹ğ‘ğ‘—=ğ¸ğ‘ğ‘—+ğ‘ƒğ‘ğ‘—,forğ‘—âˆˆ[1,ğ‘š]Xaj=Eaj+Paj,forjâˆˆ[1,m](6) The embedding scheme guarantees that the model learns the semantic meaning of the token and their relative position with regard to the aspect, which is imperative in ABSA. The model is sensitive to what and where is said in the review by being an integration of position encodings and contextual embeddings.",
            "3.5. Linear Projection": "In order to have compatibility between contextual embeddings and aspect embeddings, we use a linear projection layer that transforms each of the representations to one common latent space of dimension d. Review embeddings with BERT and RoBERTa:ğ»ğµğ¸ğ‘…ğ‘‡âˆ—ğ‘–=ğ‘Šâˆ—ğµğ¸ğ‘…ğ‘‡ğ¸ğµğ¸ğ‘…ğ‘‡âˆ—ğ‘¤ğ‘–+ğ‘âˆ—ğµğ¸ğ‘…ğ‘‡,ğ‘–âˆˆ[1,ğ‘›]HBERTâˆ—i=Wâˆ—BERTEBERTâˆ—wi+bâˆ—BERT,iâˆˆ[1,n](7)ğ»ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘âˆ—ğ‘–=ğ‘Šâˆ—ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘ğ¸ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘âˆ—ğ‘¤ğ‘–+ğ‘âˆ—ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘,ğ‘–âˆˆ[1,ğ‘›]HRoBERTaâˆ—i=Wâˆ—RoBERTaERoBERTaâˆ—wi+bâˆ—RoBERTa,iâˆˆ[1,n](8)and to the aspect of aggregation adding.ğ»ğ‘=ğ‘Šğ‘ğ¸ğ‘+ğ‘ğ‘.Ha=WaEa+ba.(9) In this case,(ğ‘Šğµğ¸ğ‘…ğ‘‡,ğ‘Šğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘,ğ‘Šğ‘âˆˆâ„ğ‘‘Ã—ğ‘‘â€²)(WBERT,WRoBERTa,WaâˆˆRdÃ—dâ€²)are learnable projection matrices and b terms are bias vectors. This forecast is such that: It is assumed that all embeddings((ğ»ğµğ¸ğ‘…ğ‘‡,ğ»ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘,ğ»ğ‘))((HBERT,HRoBERTa,Ha))are in the same semantic space of dimension d.The representations are then comparable directly and can be successfully fused in the next Gated Fusion Layer.All unnecessary differences between models BERT vs RoBERTa are averaged, and complementary features are kept. Therefore, a linear projection serves as a semantic alignment proxy, which progresses the embeddings towards adaptive combination in the gated fusion process.",
            "3.6. Gated Fusion Layer": "BERT and RoBERTa are complementary in offering contextual representations, but when they are simply concatenated, their results usually include redundancy and suboptimal integration. We propose solving this issue with a Gated Fusion Layer that actively regulates the input of each embedding stream (BERT, RoBERTa, and Aspect) through learning task-specific gating parameters. Formally, assume that the linearly projected embeddings are.ğ»ğµğ¸ğ‘…ğ‘‡âˆˆâ„ğ‘›Ã—ğ‘‘,ğ»ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘âˆˆâ„ğ‘›Ã—ğ‘‘,ğ»ğ‘âˆˆâ„ğ‘‘.HBERTâˆˆRnÃ—d,HRoBERTaâˆˆRnÃ—d,HaâˆˆRd.(10) A gating mechanism is used to dynamically down-weight the significance of BERT and RoBERTa embeddings at each token position:ğºğ‘–=ğœ(ğ‘Šğ‘”[ğ»ğµğ¸ğ‘…ğ‘‡ğ‘–;ğ»ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘ğ‘–;ğ»ğ‘]+ğ‘ğ‘”),ğ‘–âˆˆ[1,ğ‘›],Gi=Ïƒ(Wg[HiBERT;HiRoBERTa;Ha]+bg),iâˆˆ[1,n],(11) And where(ğ‘Šğ‘”âˆˆâ„ğ‘‘Ã—3ğ‘‘)ğ‘ğ‘›ğ‘‘(ğ‘ğ‘”âˆˆâ„ğ‘‘)(WgâˆˆRdÃ—3d)and(bgâˆˆRd)are trainable parameters,([Â·])([Â·])is concatenation, and(ğœ(Â·))(Ïƒ(Â·))is the sigmoid function. The fused representation is then calculated as:ğ»ğ‘“ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›ğ‘–=ğºğ‘–âŠ™ğ»ğµğ¸ğ‘…ğ‘‡ğ‘–+(1âˆ’ğºğ‘–)âŠ™ğ»ğ‘…ğ‘œğµğ¸ğ‘…ğ‘‡ğ‘ğ‘–,Hifusion=GiâŠ™HiBERT+(1âˆ’Gi)âŠ™HiRoBERTa,(12)and lastly trained in the aspect embedding by:ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ğ‘–=ğ»ğ‘“ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›ğ‘–âŠ•ğ»ğ‘,Hifinal=HifusionâŠ•Ha,(13)where(âŠ™)(âŠ™)is defined as element-wise multiplication and(âŠ•)(âŠ•)is defined as concatenation of vectors. Here are some key features of Gated Fusion. NowFigure 2illustrates the Gated Fusion Model, where the Gated Fusion Module shows how to integratively introduce Transformer-based embeddings (e.g., BERT and RoBERTa) and explicit aspect embeddings. Contributions of each source are balanced dynamically by the gating layer by means of element-wise gating and attention weighting to ensure aspect-aware semantic representations are context-sensitive, robust, and optimally fused before sequence modeling. Figure 2.Gated Fusion Module with Transformer and aspect embeddings based on dynamic element-wise gating and aspect attention weighting to create semantic representations that are robust, aspect-aware, and sensitive to attention. Adaptive weighting: The model is trained to apply BERT or RoBERTa features based on the token and context.Aspect-conscious integration: The fusion is still biased towards the opinion target but not generic sentiment by incorporating the aspect embedding in the gating function.Redundancy minimization: Gating filters rather than concatenating both embeddings results in redundancy minimization.Interpretability: Gating scores are visualizable to learn which model had a more significant impact on a decision of a particular token. In this way, the Gated Fusion Layer is the semantic integration point, ensuring that the heterogenous embeddings are combined into one, aspect-sensitive representation, which is then sent to the downstream BiLSTM + Attention layer to reason sequentially.",
            "3.7. Bidirectional Long Short-Term Memory (BiLSTM)": "Following gated fusion, aspect-aware embeddings(ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™)(Hfinal)are then fed into a Bidirectional Long Short-Term Memory (BiLSTM) network to capture sequential interaction and syntax in the review text. In contrast to conventional RNNs, LSTMs have gating mechanisms, which reduce vanishing gradients, allowing them to learn long-term contextual dependencies. The two-way variant also implies that the past (left context) and future (right context) information were coded at the same time. In the abstract, provided the fused input sequence:ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™=ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™1,ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™2,â€¦,ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ğ‘›,ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ğ‘–âˆˆâ„ğ‘‘,Hfinal=H1final,H2final,â€¦,Hnfinal,HifinalâˆˆRd,(14) The forward and backward LSTMs calculate hidden states as:â„â†’ğ‘–=LSTMğ‘“(ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™Ã—ğ‘–,â„â†’Ã—ğ‘–âˆ’1),hâ†’i=LSTMf(HfinalÃ—i,hâ†’Ã—iâˆ’1),(15)â„â†ğ‘–=LSTMğ‘(ğ»ğ‘“ğ‘–ğ‘›ğ‘ğ‘™Ã—ğ‘–,â„â†Ã—ğ‘–+1),hâ†i=LSTMb(HfinalÃ—i,hâ†Ã—i+1),(16) In which(â„â†’ğ‘–)(hâ†’i)represents the encoding of information starting at the start until token i, and(â„â†ğ‘–)(hâ†i)represents the encoding of information starting at the end, all the way back to token i. The two-directional hidden states are combined to obtain the BiLSTM representation at that time step.â„ğ‘–=[â„â†’ğ‘–;â„â†ğ‘–],â„ğ‘–âˆˆâ„2ğ‘‘.hi=[hâ†’i;hâ†i],hiâˆˆR2d.(17) In this way, the BiLSTM generates the sequence of outputs.ğ»ğµğ‘–ğ¿ğ‘†ğ‘‡ğ‘€=â„1,â„2,â€¦,â„ğ‘›,ğ»ğµğ‘–ğ¿ğ‘†ğ‘‡ğ‘€âˆˆâ„ğ‘›Ã—2ğ‘‘.HBiLSTM=h1,h2,â€¦,hn,HBiLSTMâˆˆRnÃ—2d.(18) The importance of BiLSTM in ABSA. Sequential reasoning: Reasoning between opinion words at different distances, for example â€œnot goodâ€.Aspect alignment: The model captures the aspect-conditioned sentiment by learning a contextual flow around the aspect that involves the aspect preceding context and the aspect succeeding context.Complementary to transformers: Transformers are more effective at contextualizing on a global scale, whereas BiLSTM strengthens local, position-sensitive dependencies, which prove particularly useful in aspect-based tasks. The structure of the architecture is based on a combination of deep contextual embeddings (BERT, RoBERTa) and sequential structure modeling (BiLSTM) to provide semantic richness and contextual accuracy to support strong sentiment classification.",
            "3.8. Hierarchical Attention Mechanism (HAM)": "The proposed model uses a HAM module to overcome the necessity of refining the BiLSTM outputs and allowing the aspect-based focus. In comparison to traditional single-layer attention, HAM incorporates three complementary views: word-level, position-level, and aspect-level attention, such that the sentiment decision does not simply rely on the token semantics but also the location of words and their connection to the target aspect. Therefore, here,Figure 3illustrates the Multi-level Attention Process HAM module, where word-level attention brings out aspect-relevant tokens, and sentence-level attention combines these representations into aspect-specific contextual vectors. Position-conscious weighting is an additional refinement of attention that gives higher emphasis on tokens that are close to the aspect term, which maximizes interpretability and sentiment discrimination. Figure 3.Multi-level attention process represented by the Hierarchical Attention Mechanism (HAM) module, which shows word-level and sentence-level attention with position-sensitive weighting to give stress on aspect-relevant tokens and create interpretable, aspect-specific contextual representations. 3.8.1. Word-Level AttentionAt this stage, the model allocates the weight of importance to each contextual hidden state(â„ğ‘–)(hi)of the BiLSTM, where examples of our sentiment-bearing words are given, and excellent, poor, and irrelevant tokens are suppressed.ğ›¼ğ‘–=exp(â„âŠ¤ğ‘–ğ‘Šğ‘¤ğ‘¢ğ‘¤)âˆ‘ğ‘›ğ‘˜=1exp(â„âŠ¤ğ‘˜ğ‘Šğ‘¤ğ‘¢ğ‘¤)Î±i=exp(hiâŠ¤Wwuw)âˆ‘k=1nexp(hkâŠ¤Wwuw)(19)where(ğ‘Šğ‘¤)(Ww)is a trainable projection matrix and(ğ‘¢ğ‘¤)(uw)is a word-level context vector. The summary representation is:â„ğ‘¤ğ‘œğ‘Ÿğ‘‘=âˆ‘ğ‘–=1ğ‘›ğ›¼ğ‘–â„ğ‘–hword=âˆ‘i=1nÎ±ihi(20)This makes sure that semantic salience is saved from the sequence of reviews. 3.8.2. Position-Level AttentionThe significance of words is not enough in ABSA, because in many cases, the sentiment polarity is determined by the relative closeness to the aspect. The position-level attention, therefore, brings in distance-conscious weighting, where the tokens nearer to the aspect have more weight. At position (i) and aspect centered at position (j):ğ‘ğ‘–=11+|ğ‘–âˆ’ğ‘—|pi=11+|iâˆ’j|(21)Then, a normalized position attention score is used:ğ›½ğ‘–=exp(ğ‘ğ‘–Â·(â„âŠ¤ğ‘–ğ‘Šğ‘ğ‘¢ğ‘))âˆ‘ğ‘›ğ‘˜=1exp(ğ‘ğ‘˜Â·(â„âŠ¤ğ‘˜ğ‘Šğ‘ğ‘¢ğ‘))Î²i=exp(piÂ·(hiâŠ¤Wpup))âˆ‘k=1nexp(pkÂ·(hkâŠ¤Wpup))(22)and the position-sensitive representation is:â„ğ‘ğ‘œğ‘ =âˆ‘ğ‘–=1ğ‘›ğ›½ğ‘–â„ğ‘–hpos=âˆ‘i=1nÎ²ihi(23)This level makes sure that the model puts tokens in close proximity to the aspect (e.g., in, â€œThe camera quality of this phone is outstanding,â€ the phrase â€œoutstandingâ€ must be more closely associated with â€œcameraâ€ than with words far away). 3.8.3. Aspect-Level AttentionThe last attention layer is used to align review tokens with the specific aspect representationğ¸ğ‘Eato yield aspect-conditioned sentiment.ğ›¾ğ‘–=exp((â„âŠ¤ğ‘–ğ‘Šğ‘ğ¸ğ‘))âˆ‘ğ‘›ğ‘˜=1exp((â„âŠ¤ğ‘˜ğ‘Šğ‘ğ¸ğ‘))Î³i=exp((hiâŠ¤WaEa))âˆ‘k=1nexp((hkâŠ¤WaEa))(24)The Aggregated aspect-aware context vector is then:â„ğ‘ğ‘ ğ‘ğ‘’ğ‘ğ‘¡=âˆ‘ğ‘–=1ğ‘›ğ›¾ğ‘–â„ğ‘–haspect=âˆ‘i=1nÎ³ihi(25)This is so as to ensure that the sentiment is directly conditioned on the target aspect, without misclassification due to irrelevant aspects in multi-aspect reviews. 3.8.4. Final Hierarchical Attention RepresentationThe three-level outputs are added together to obtain the hierarchical context vector:â„âˆ—=ğ‘Šğ‘[â„ğ‘¤ğ‘œğ‘Ÿğ‘‘;â„ğ‘ğ‘œğ‘ ;â„ğ‘ğ‘ ğ‘ğ‘’ğ‘ğ‘¡]+ğ‘ğ‘h*=Wc[hword;hpos;haspect]+bc(26)Now here(ğ‘Šğ‘)(Wc)and(ğ‘ğ‘)(bc)are trainable parameters",
            "3.8.1. Word-Level Attention": "At this stage, the model allocates the weight of importance to each contextual hidden state(â„ğ‘–)(hi)of the BiLSTM, where examples of our sentiment-bearing words are given, and excellent, poor, and irrelevant tokens are suppressed.ğ›¼ğ‘–=exp(â„âŠ¤ğ‘–ğ‘Šğ‘¤ğ‘¢ğ‘¤)âˆ‘ğ‘›ğ‘˜=1exp(â„âŠ¤ğ‘˜ğ‘Šğ‘¤ğ‘¢ğ‘¤)Î±i=exp(hiâŠ¤Wwuw)âˆ‘k=1nexp(hkâŠ¤Wwuw)(19)where(ğ‘Šğ‘¤)(Ww)is a trainable projection matrix and(ğ‘¢ğ‘¤)(uw)is a word-level context vector. The summary representation is:â„ğ‘¤ğ‘œğ‘Ÿğ‘‘=âˆ‘ğ‘–=1ğ‘›ğ›¼ğ‘–â„ğ‘–hword=âˆ‘i=1nÎ±ihi(20) This makes sure that semantic salience is saved from the sequence of reviews.",
            "3.8.2. Position-Level Attention": "The significance of words is not enough in ABSA, because in many cases, the sentiment polarity is determined by the relative closeness to the aspect. The position-level attention, therefore, brings in distance-conscious weighting, where the tokens nearer to the aspect have more weight. At position (i) and aspect centered at position (j):ğ‘ğ‘–=11+|ğ‘–âˆ’ğ‘—|pi=11+|iâˆ’j|(21) Then, a normalized position attention score is used:ğ›½ğ‘–=exp(ğ‘ğ‘–Â·(â„âŠ¤ğ‘–ğ‘Šğ‘ğ‘¢ğ‘))âˆ‘ğ‘›ğ‘˜=1exp(ğ‘ğ‘˜Â·(â„âŠ¤ğ‘˜ğ‘Šğ‘ğ‘¢ğ‘))Î²i=exp(piÂ·(hiâŠ¤Wpup))âˆ‘k=1nexp(pkÂ·(hkâŠ¤Wpup))(22)and the position-sensitive representation is:â„ğ‘ğ‘œğ‘ =âˆ‘ğ‘–=1ğ‘›ğ›½ğ‘–â„ğ‘–hpos=âˆ‘i=1nÎ²ihi(23) This level makes sure that the model puts tokens in close proximity to the aspect (e.g., in, â€œThe camera quality of this phone is outstanding,â€ the phrase â€œoutstandingâ€ must be more closely associated with â€œcameraâ€ than with words far away).",
            "3.8.3. Aspect-Level Attention": "The last attention layer is used to align review tokens with the specific aspect representationğ¸ğ‘Eato yield aspect-conditioned sentiment.ğ›¾ğ‘–=exp((â„âŠ¤ğ‘–ğ‘Šğ‘ğ¸ğ‘))âˆ‘ğ‘›ğ‘˜=1exp((â„âŠ¤ğ‘˜ğ‘Šğ‘ğ¸ğ‘))Î³i=exp((hiâŠ¤WaEa))âˆ‘k=1nexp((hkâŠ¤WaEa))(24) The Aggregated aspect-aware context vector is then:â„ğ‘ğ‘ ğ‘ğ‘’ğ‘ğ‘¡=âˆ‘ğ‘–=1ğ‘›ğ›¾ğ‘–â„ğ‘–haspect=âˆ‘i=1nÎ³ihi(25) This is so as to ensure that the sentiment is directly conditioned on the target aspect, without misclassification due to irrelevant aspects in multi-aspect reviews.",
            "3.8.4. Final Hierarchical Attention Representation": "The three-level outputs are added together to obtain the hierarchical context vector:â„âˆ—=ğ‘Šğ‘[â„ğ‘¤ğ‘œğ‘Ÿğ‘‘;â„ğ‘ğ‘œğ‘ ;â„ğ‘ğ‘ ğ‘ğ‘’ğ‘ğ‘¡]+ğ‘ğ‘h*=Wc[hword;hpos;haspect]+bc(26) Now here(ğ‘Šğ‘)(Wc)and(ğ‘ğ‘)(bc)are trainable parameters",
            "3.9. Final Classification and Optimization": "Once the hierarchical context representationâ„âˆ—h*has been obtained by the Hierarchical Attention Mechanism, the model goes to the final sentiment prediction step. This step converts the multi-level representation, which is very rich, to a discrete sentiment label using a fully connected projection and a probabilistic classification layer. 3.9.1. Linear Projection and Softmax ClassificationThe resultant attention-derived featureâ„âˆ—h*is concatenated and then subjected to a linear layer transformation that projects it into a subspace specific to the sentiment. This transformation captures the high-level abstractions in the word, position, and aspect-level attention.ğ‘§=ğ‘Šğ‘œâ„âˆ—+ğ‘ğ‘œz=Woh*+bo(27)whereğ‘Šğ‘œWoandğ‘ğ‘œboare the output weight matrix and bias vector, respectively. Logits z are then normalized by the SoftMax function to generate a probability distribution of the sentiment classes.ğ‘¦Ì‚Ã—ğ‘–=exp(ğ‘§ğ‘–)âˆ‘Ã—ğ‘˜=1ğ¶exp(ğ‘§ğ‘˜),ğ‘–=1,2,â€¦,ğ¶y^Ã—i=exp(zi)âˆ‘Ã—k=1Cexp(zk),i=1,2,â€¦,C(28)In this case,Crefers to the total amount of sentiment categories (e.g., 5 of fine-grained sentiment: very negative, negative, neutral, positive, and very positive). The probability of the most likely class is chosen as the predicted sentiment. 3.9.2. Regularization and Focal Loss OptimizationTo reduce overfitting and increase generalization,(â„âˆ—)(h*)dropout is regularized before classification. The dropout randomly kills neurons in training, making it resistant to noise, and it is not dependent on particular features. Since the example of the sentiment datasets usually has an imbalance in classes, such as more neutral reviews than extremely positive or negative ones, the model uses Focal Loss, rather than the conventional cross-entropy. Focal Loss is a dynamic weight, or scaling, loss that learns to place more emphasis on the learning of the harder, misclassified samples. It is defined asâ„’ğ‘“ğ‘œğ‘ğ‘ğ‘™=âˆ’ğ›¼ğ‘¡(1âˆ’ğ‘¦Ì‚ğ‘¡)ğ›¾log(ğ‘¦Ì‚ğ‘¡)Lfocal=âˆ’Î±t(1âˆ’y^t)Î³log(y^t)(29)where(ğ›¼ğ‘¡)(Î±t)is the weighting factor for class(ğ‘¡)(t),(ğ›¾)(Î³)is the focusing parameter controlling difficulty emphasis,(ğ‘¦Ì‚ğ‘¡)(y^t)is the predicted probability for the true class.This adaptive loss promotes equal learning among the categories of sentiment and guarantees better performance on underrepresented labels. 3.9.3. Training StrategyThe Adam optimizer is applied with a learning rate to optimize the AGF-HAM model parameters, and with the help of adaptive moment estimation, it should converge steadily. Early stopping is used to avoid overtraining, with the basis of validation loss. The total training goal reduces the amount of focal loss of all samples:â„’Ã—ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™=1ğ‘âˆ‘Ã—ğ‘–=1ğ‘â„’ğ‘“ğ‘œğ‘ğ‘ğ‘™(ğ‘¦Ì‚ğ‘–,ğ‘¦ğ‘–)LÃ—total=1Nâˆ‘Ã—i=1NLfocal(y^i,yi)(30)In this step, the learned hierarchical and gated representations are incorporated into a small, interpretable, and accurate sentiment prediction that is both accurate and interpretable.",
            "3.9.1. Linear Projection and Softmax Classification": "The resultant attention-derived featureâ„âˆ—h*is concatenated and then subjected to a linear layer transformation that projects it into a subspace specific to the sentiment. This transformation captures the high-level abstractions in the word, position, and aspect-level attention.ğ‘§=ğ‘Šğ‘œâ„âˆ—+ğ‘ğ‘œz=Woh*+bo(27)whereğ‘Šğ‘œWoandğ‘ğ‘œboare the output weight matrix and bias vector, respectively. Logits z are then normalized by the SoftMax function to generate a probability distribution of the sentiment classes.ğ‘¦Ì‚Ã—ğ‘–=exp(ğ‘§ğ‘–)âˆ‘Ã—ğ‘˜=1ğ¶exp(ğ‘§ğ‘˜),ğ‘–=1,2,â€¦,ğ¶y^Ã—i=exp(zi)âˆ‘Ã—k=1Cexp(zk),i=1,2,â€¦,C(28) In this case,Crefers to the total amount of sentiment categories (e.g., 5 of fine-grained sentiment: very negative, negative, neutral, positive, and very positive). The probability of the most likely class is chosen as the predicted sentiment.",
            "3.9.2. Regularization and Focal Loss Optimization": "To reduce overfitting and increase generalization,(â„âˆ—)(h*)dropout is regularized before classification. The dropout randomly kills neurons in training, making it resistant to noise, and it is not dependent on particular features. Since the example of the sentiment datasets usually has an imbalance in classes, such as more neutral reviews than extremely positive or negative ones, the model uses Focal Loss, rather than the conventional cross-entropy. Focal Loss is a dynamic weight, or scaling, loss that learns to place more emphasis on the learning of the harder, misclassified samples. It is defined asâ„’ğ‘“ğ‘œğ‘ğ‘ğ‘™=âˆ’ğ›¼ğ‘¡(1âˆ’ğ‘¦Ì‚ğ‘¡)ğ›¾log(ğ‘¦Ì‚ğ‘¡)Lfocal=âˆ’Î±t(1âˆ’y^t)Î³log(y^t)(29)where (ğ›¼ğ‘¡)(Î±t)is the weighting factor for class(ğ‘¡)(t),(ğ›¾)(Î³)is the focusing parameter controlling difficulty emphasis,(ğ‘¦Ì‚ğ‘¡)(y^t)is the predicted probability for the true class. This adaptive loss promotes equal learning among the categories of sentiment and guarantees better performance on underrepresented labels.",
            "3.9.3. Training Strategy": "The Adam optimizer is applied with a learning rate to optimize the AGF-HAM model parameters, and with the help of adaptive moment estimation, it should converge steadily. Early stopping is used to avoid overtraining, with the basis of validation loss. The total training goal reduces the amount of focal loss of all samples:â„’Ã—ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™=1ğ‘âˆ‘Ã—ğ‘–=1ğ‘â„’ğ‘“ğ‘œğ‘ğ‘ğ‘™(ğ‘¦Ì‚ğ‘–,ğ‘¦ğ‘–)LÃ—total=1Nâˆ‘Ã—i=1NLfocal(y^i,yi)(30) In this step, the learned hierarchical and gated representations are incorporated into a small, interpretable, and accurate sentiment prediction that is both accurate and interpretable.",
            "3.10. Explainability Module": "An Explainability Module Layer is incorporated as the last step of the HAM framework in order to guarantee interpretability and transparency of the final model predictions. The layer makes the process of forming the sentimentsâ€™ decisions globally and locally interpretable by showing how the model forms its conclusion based on linguistic and contextual knowledge. The explainability module is based on three complementary sub-layersâ€”Attention Heatmaps, Aspect-Level Explanations, and SHAP Integrated Gradientsâ€”that have a specific analytical purpose to hold models accountable and explainable to humans. 3.10.1. Attention HeadmapsThe Attention Headmaps sub-layer shows token-level scores of the importance of the attention heads in the Transformer encoder and BiLSTM layers. It puts emphasis on particular words, phrases, or clauses that have a strong impact on the sentiment or emotional polarity of a certain text. This visualization, in addition to showing the focus distribution of the model, also proves the interpretive reliability of the attention mechanism. With the help of these heatmaps, researchers and practitioners can confirm that the model focuses on semantically significant areas and not accidental associations.ğ›¼ğ‘–=exp(ğ‘£âŠ¤tanh(ğ‘Šâ„â„ğ‘–+ğ‘Šğ‘ğ‘+ğ‘))âˆ‘ğ‘›ğ‘—=1exp(ğ‘£âŠ¤tanh(ğ‘Šâ„â„ğ‘—+ğ‘Šğ‘ğ‘+ğ‘))Î±i=expvâŠ¤tanh(Whhi+Wqq+b)âˆ‘j=1nexpvâŠ¤tanh(Whhj+Wqq+b)(31)This calculates the attention weight of each tokent, and this is the degree to which that token is relevant to the query (aspect or sentence context). It involves a SoftMax normalization such that all weights of attention have a sum of 1.ğ»att(ğ‘¡ğ‘–)=ğ›¼ğ‘–foreachtokenğ‘¡ğ‘–.Hatt(ti)=Î±iforeachtokenti.(32)This maps the attention weight to the underlying token, which compose the raw headmap of token importance in the text. It graphically shows the words that make the greatest contribution to sentiment decisions.Headmapnorm(ğ‘¡ğ‘–)=ğ›¼ğ‘–âˆ’minğ‘—ğ›¼ğ‘—maxğ‘—ğ›¼ğ‘—âˆ’minğ‘—ğ›¼ğ‘—.Headmapnorm(ti)=Î±iâˆ’minjÎ±jmaxjÎ±jâˆ’minjÎ±j.(33)The weights between the attention and the visualization of the headmap are normalized to 0 or 1 to form a visual representation of the headmap. It guarantees a similar visualization of samples to make them easier to interpret. 3.10.2. Aspect-Level ExplanationThe sub-layer of Aspect-Level Explanation gives a fine-grained interpretability, where sentiment weights are mapped to the identified aspects in the text. This sub-layer explains the roles of each product or emotional aspect in the final sentiment classification using dependency-based aspect extraction and context embeddings. It can be used to provide transparency in aspect-aware decision-making and bridge the semantic connection between aspect terms (battery, camera, service) and the contextual sentiments. This sub-layer is specifically useful when the task at hand demands domain interpretability, like reviewing or detecting emotions in social situations.ğ›¼(ğ‘)ğ‘–=exp(ğ‘£âŠ¤ğ‘tanh(ğ‘Šâ„â„ğ‘–+ğ‘Šğ‘ğ‘’ğ‘+ğ‘ğ‘))âˆ‘ğ‘›ğ‘—=1exp(ğ‘£âŠ¤ğ‘tanh(ğ‘Šâ„â„ğ‘—+ğ‘Šğ‘ğ‘’ğ‘+ğ‘ğ‘))Î±i(a)=expvaâŠ¤tanh(Whhi+Waea+ba)âˆ‘j=1nexpvaâŠ¤tanh(Whhj+Waea+ba)(34)This computes aspect-conditioned attention, in whichğ‘’ğ‘eais the embedding of a given aspect (e.g., â€œbatteryâ€ or â€œserviceâ€) It helps the model to pay attention to the most relevant words to that aspect.ğ‘ğ‘=âˆ‘ğ‘–=1ğ‘›ğ›¼(ğ‘)ğ‘–,â„ğ‘–(aspect-awarecontextvector)ca=âˆ‘i=1nÎ±i(a),hi(aspect-awarecontextvector)(35)In this case, the hidden states are used to create a weighted context vector, which is a mixture of the hidden states with aspect-specific attention weights. It is a compressed feeling indicator of that specific element.ğ‘ (ğ‘˜)ğ‘=softmaxğ‘˜(ğ‘Š(ğ‘˜)ğ‘œğ‘ğ‘+ğ‘(ğ‘˜)ğ‘œ)(classprobabilityforaspectğ‘)sa(k)=softmaxk(Wo(k)ca+bo(k))(classprobabilityforaspecta)(36)This is to apply sentiment polarity prediction as an aspectawith all classes using a softmax layer. It produces probabilities of the form of positive, neutral, or negative sentiment.TopTokensÃ—ğ‘=argsortÃ—ğ‘–(ğ›¼(ğ‘)ğ‘–),topâˆ’ğ‘š.TopTokensÃ—a=argsortÃ—iÎ±i(a),top-m.(37)This is used to select the most influential tokens that assist in sentiment on aspecta. These tokens are used to produce aspect-level textual explanations. 3.10.3. SHAP/Integrated GradientsThe SHAP Integrated Gradients sub-layer is a combination of SHapley Additive exPlanations (SHAP) with Integrated Gradients (IG) used to measure the contribution of each feature to the output of a model. This hybrid interpretability method combines model sensitivity and feature attribution consistency into unified importance maps that are complementary to the attention-based interpretations. This sub-layer is a combination of SHAP and IG and is able to provide a coherent and consistent explanation of decision boundaries, which strengthens the transparency and reliability of the model.ğœ™ğ‘–=âˆ‘ğ‘†âŠ†ğ‘âˆ–ğ‘–|ğ‘†|!,(|ğ‘|âˆ’|ğ‘†|âˆ’1)!|ğ‘|!â›ââœâœâœğ¹ğ‘†âˆªğ‘–(ğ‘¥ğ‘†âˆªğ‘–)âˆ’ğ¹ğ‘†(ğ‘¥ğ‘†)ââ âŸâŸâŸÏ•i=âˆ‘SâŠ†Nâˆ–i|S|!,(|N|âˆ’|S|âˆ’1)!|N|!(FSâˆªi(xSâˆªi)âˆ’FS(xS))(38)This is the Shapley value equation, which is used to measure the contribution of a given feature to the prediction of the model. It assesses the effect of the addition of a feature (word/token) on model output on all the subsets.âˆ‘ğ‘–=1ğ‘›ğœ™ğ‘–=ğ¹(ğ‘¥)âˆ’ğ¹(ğ‘¥baseline)âˆ‘i=1nÏ•i=F(x)âˆ’F(xbaseline)(39)SHAP values can be summed up to form the difference between baseline and model prediction. This guarantees additive attribution of features, which ensures consistency of interpretability.IGğ‘–(ğ‘¥)=(ğ‘¥ğ‘–âˆ’ğ‘¥â€²Ã—ğ‘–)âˆ«Ã—ğ›¼=01âˆ‚ğ¹(ğ‘¥â€²+ğ›¼(ğ‘¥âˆ’ğ‘¥â€²))âˆ‚ğ‘¥ğ‘–,ğ‘‘ğ›¼IGi(x)=(xiâˆ’xâ€²Ã—i)âˆ«Ã—Î±=01âˆ‚Fxâ€²+Î±(xâˆ’xâ€²)âˆ‚xi,dÎ±(40)Integrated Gradients calculate the contribution of each input feature to the output of a model by summing the gradients between the path taken between a baselineğ‘¥â€²xâ€²and the real input x. It offers path-sensitive interpretability that is smooth and has less noise than raw gradients.To illustrate the interpretability of the proposed explainability module, we provide examples of each of the sentiment categories: very positive, positive, neutral, negative, and very negative. A heatmap visualization inFigure 4is provided per sentence to show the distribution of the attention weights of the individual tokens, i.e., the focus of the model when inferring the sentiment. The darker the region of the heatmap, the greater the intensity of attention of the token is, and the greater its influence on the final prediction of the model will be.Figure 4.Heatmap visualization showing token-level attention distribution for sentences across five sentiment categories. Darker shades represent higher attention weights to the modelâ€™s sentiment prediction.Aspect-Level Summary:Detected Aspect:phone/performanceOpinion Tokens:awful, slow, crashes, uselessAspect-Sentiment Score:âˆ’0.93âˆ’0.93Aspect Polarity Label:Very NegativeFinal Interpretation:The model classifies this sentence asVery Negative. Attention, Aspect-level Polarity, and SHAP/IG all emphasize the same tokens:awful,useless, andcrashesâ€”indicating they are the strongest contributors to the negative sentiment toward thephoneâ€™s performance.Besides visual interpretation, there is also elaborate explainability of one representative sentence inTable 2andTable 3. The table is a quantitative report on the attention weight, aspect-level polarity, SHAP value, and Integrated Gradient (IG) contribution of each token. These three measures are then normalized and averaged to obtain the combined importance score to reflect the contextual and causal influence on the decision of the model. This integrated analysis would allow not only the explainability framework to identify words that carry sentiment (e.g., awful, useless, excellent) but also clarify their relationship with certain aspects in context, allowing the transparent and understandable interpretation of how the model reaches its classification.Table 2.Explainability Table for Sentence: â€œThis phone is awful slow crashes often today uselessâ€.Table 3.Definition and Formula of Each Explainability Metric.The combination of these three sub-layers gives HAM not only superior performance in predictive mode but also a high level of interpretive ability. This Explainability Module Layer converts the HAM framework to more of an intelligent system that can be trusted and interpreted by humans, having the ability to provide the answer to why and how decisions are made through complex sentiment and emotional environments.",
            "3.10.1. Attention Headmaps": "The Attention Headmaps sub-layer shows token-level scores of the importance of the attention heads in the Transformer encoder and BiLSTM layers. It puts emphasis on particular words, phrases, or clauses that have a strong impact on the sentiment or emotional polarity of a certain text. This visualization, in addition to showing the focus distribution of the model, also proves the interpretive reliability of the attention mechanism. With the help of these heatmaps, researchers and practitioners can confirm that the model focuses on semantically significant areas and not accidental associations.ğ›¼ğ‘–=exp(ğ‘£âŠ¤tanh(ğ‘Šâ„â„ğ‘–+ğ‘Šğ‘ğ‘+ğ‘))âˆ‘ğ‘›ğ‘—=1exp(ğ‘£âŠ¤tanh(ğ‘Šâ„â„ğ‘—+ğ‘Šğ‘ğ‘+ğ‘))Î±i=expvâŠ¤tanh(Whhi+Wqq+b)âˆ‘j=1nexpvâŠ¤tanh(Whhj+Wqq+b)(31) This calculates the attention weight of each tokent, and this is the degree to which that token is relevant to the query (aspect or sentence context). It involves a SoftMax normalization such that all weights of attention have a sum of 1.ğ»att(ğ‘¡ğ‘–)=ğ›¼ğ‘–foreachtokenğ‘¡ğ‘–.Hatt(ti)=Î±iforeachtokenti.(32) This maps the attention weight to the underlying token, which compose the raw headmap of token importance in the text. It graphically shows the words that make the greatest contribution to sentiment decisions.Headmapnorm(ğ‘¡ğ‘–)=ğ›¼ğ‘–âˆ’minğ‘—ğ›¼ğ‘—maxğ‘—ğ›¼ğ‘—âˆ’minğ‘—ğ›¼ğ‘—.Headmapnorm(ti)=Î±iâˆ’minjÎ±jmaxjÎ±jâˆ’minjÎ±j.(33) The weights between the attention and the visualization of the headmap are normalized to 0 or 1 to form a visual representation of the headmap. It guarantees a similar visualization of samples to make them easier to interpret.",
            "3.10.2. Aspect-Level Explanation": "The sub-layer of Aspect-Level Explanation gives a fine-grained interpretability, where sentiment weights are mapped to the identified aspects in the text. This sub-layer explains the roles of each product or emotional aspect in the final sentiment classification using dependency-based aspect extraction and context embeddings. It can be used to provide transparency in aspect-aware decision-making and bridge the semantic connection between aspect terms (battery, camera, service) and the contextual sentiments. This sub-layer is specifically useful when the task at hand demands domain interpretability, like reviewing or detecting emotions in social situations.ğ›¼(ğ‘)ğ‘–=exp(ğ‘£âŠ¤ğ‘tanh(ğ‘Šâ„â„ğ‘–+ğ‘Šğ‘ğ‘’ğ‘+ğ‘ğ‘))âˆ‘ğ‘›ğ‘—=1exp(ğ‘£âŠ¤ğ‘tanh(ğ‘Šâ„â„ğ‘—+ğ‘Šğ‘ğ‘’ğ‘+ğ‘ğ‘))Î±i(a)=expvaâŠ¤tanh(Whhi+Waea+ba)âˆ‘j=1nexpvaâŠ¤tanh(Whhj+Waea+ba)(34) This computes aspect-conditioned attention, in whichğ‘’ğ‘eais the embedding of a given aspect (e.g., â€œbatteryâ€ or â€œserviceâ€) It helps the model to pay attention to the most relevant words to that aspect.ğ‘ğ‘=âˆ‘ğ‘–=1ğ‘›ğ›¼(ğ‘)ğ‘–,â„ğ‘–(aspect-awarecontextvector)ca=âˆ‘i=1nÎ±i(a),hi(aspect-awarecontextvector)(35) In this case, the hidden states are used to create a weighted context vector, which is a mixture of the hidden states with aspect-specific attention weights. It is a compressed feeling indicator of that specific element.ğ‘ (ğ‘˜)ğ‘=softmaxğ‘˜(ğ‘Š(ğ‘˜)ğ‘œğ‘ğ‘+ğ‘(ğ‘˜)ğ‘œ)(classprobabilityforaspectğ‘)sa(k)=softmaxk(Wo(k)ca+bo(k))(classprobabilityforaspecta)(36) This is to apply sentiment polarity prediction as an aspectawith all classes using a softmax layer. It produces probabilities of the form of positive, neutral, or negative sentiment.TopTokensÃ—ğ‘=argsortÃ—ğ‘–(ğ›¼(ğ‘)ğ‘–),topâˆ’ğ‘š.TopTokensÃ—a=argsortÃ—iÎ±i(a),top-m.(37) This is used to select the most influential tokens that assist in sentiment on aspecta. These tokens are used to produce aspect-level textual explanations.",
            "3.10.3. SHAP/Integrated Gradients": "The SHAP Integrated Gradients sub-layer is a combination of SHapley Additive exPlanations (SHAP) with Integrated Gradients (IG) used to measure the contribution of each feature to the output of a model. This hybrid interpretability method combines model sensitivity and feature attribution consistency into unified importance maps that are complementary to the attention-based interpretations. This sub-layer is a combination of SHAP and IG and is able to provide a coherent and consistent explanation of decision boundaries, which strengthens the transparency and reliability of the model.ğœ™ğ‘–=âˆ‘ğ‘†âŠ†ğ‘âˆ–ğ‘–|ğ‘†|!,(|ğ‘|âˆ’|ğ‘†|âˆ’1)!|ğ‘|!â›ââœâœâœğ¹ğ‘†âˆªğ‘–(ğ‘¥ğ‘†âˆªğ‘–)âˆ’ğ¹ğ‘†(ğ‘¥ğ‘†)ââ âŸâŸâŸÏ•i=âˆ‘SâŠ†Nâˆ–i|S|!,(|N|âˆ’|S|âˆ’1)!|N|!(FSâˆªi(xSâˆªi)âˆ’FS(xS))(38) This is the Shapley value equation, which is used to measure the contribution of a given feature to the prediction of the model. It assesses the effect of the addition of a feature (word/token) on model output on all the subsets.âˆ‘ğ‘–=1ğ‘›ğœ™ğ‘–=ğ¹(ğ‘¥)âˆ’ğ¹(ğ‘¥baseline)âˆ‘i=1nÏ•i=F(x)âˆ’F(xbaseline)(39) SHAP values can be summed up to form the difference between baseline and model prediction. This guarantees additive attribution of features, which ensures consistency of interpretability.IGğ‘–(ğ‘¥)=(ğ‘¥ğ‘–âˆ’ğ‘¥â€²Ã—ğ‘–)âˆ«Ã—ğ›¼=01âˆ‚ğ¹(ğ‘¥â€²+ğ›¼(ğ‘¥âˆ’ğ‘¥â€²))âˆ‚ğ‘¥ğ‘–,ğ‘‘ğ›¼IGi(x)=(xiâˆ’xâ€²Ã—i)âˆ«Ã—Î±=01âˆ‚Fxâ€²+Î±(xâˆ’xâ€²)âˆ‚xi,dÎ±(40) Integrated Gradients calculate the contribution of each input feature to the output of a model by summing the gradients between the path taken between a baselineğ‘¥â€²xâ€²and the real input x. It offers path-sensitive interpretability that is smooth and has less noise than raw gradients. To illustrate the interpretability of the proposed explainability module, we provide examples of each of the sentiment categories: very positive, positive, neutral, negative, and very negative. A heatmap visualization inFigure 4is provided per sentence to show the distribution of the attention weights of the individual tokens, i.e., the focus of the model when inferring the sentiment. The darker the region of the heatmap, the greater the intensity of attention of the token is, and the greater its influence on the final prediction of the model will be. Figure 4.Heatmap visualization showing token-level attention distribution for sentences across five sentiment categories. Darker shades represent higher attention weights to the modelâ€™s sentiment prediction. Aspect-Level Summary: Detected Aspect:phone/performanceOpinion Tokens:awful, slow, crashes, uselessAspect-Sentiment Score:âˆ’0.93âˆ’0.93Aspect Polarity Label:Very Negative Final Interpretation: The model classifies this sentence asVery Negative. Attention, Aspect-level Polarity, and SHAP/IG all emphasize the same tokens:awful,useless, andcrashesâ€”indicating they are the strongest contributors to the negative sentiment toward thephoneâ€™s performance. Besides visual interpretation, there is also elaborate explainability of one representative sentence inTable 2andTable 3. The table is a quantitative report on the attention weight, aspect-level polarity, SHAP value, and Integrated Gradient (IG) contribution of each token. These three measures are then normalized and averaged to obtain the combined importance score to reflect the contextual and causal influence on the decision of the model. This integrated analysis would allow not only the explainability framework to identify words that carry sentiment (e.g., awful, useless, excellent) but also clarify their relationship with certain aspects in context, allowing the transparent and understandable interpretation of how the model reaches its classification. Table 2.Explainability Table for Sentence: â€œThis phone is awful slow crashes often today uselessâ€. Table 3.Definition and Formula of Each Explainability Metric. The combination of these three sub-layers gives HAM not only superior performance in predictive mode but also a high level of interpretive ability. This Explainability Module Layer converts the HAM framework to more of an intelligent system that can be trusted and interpreted by humans, having the ability to provide the answer to why and how decisions are made through complex sentiment and emotional environments.",
            "4. Dataset Description and Hypermeters": "Three datasets were used to determine the effectiveness of the proposed AGF-HAM framework, its robustness, and its generalization ability; two publicly available benchmark corpora and one custom-made dataset were used. This multi-dataset approach guarantees thorough validation on domain-specific, product-driven, and emotion-driven settings, with internal and external experimental validity. 4.1. Amazon Cell Phones and Accessories ReviewsThe source of the Amazon Cell Phones and Accessories Reviews dataset is a reputable source, Kaggle, and the corpus is publicly available on (https://www.kaggle.com/datasets/grikomsn/amazon-cell-phones-reviews?select=20191226-reviews.csv, accessed on 2 December 2025) and widely used in sentiment analysis research. It consists of 67,986 customer reviews and has eight formatted attributes: ASIN, product name, rating, review date, verification status, review title, review body, and helpful votes. Each review is rated on a five-point scale (1â€“5), which directly translates to sentiment polarity categories of very negative, negative, neutral, positive, and very positive. The data set offers an equal and domain-specific benchmark that reasonably captures consumer opinion, buying patterns, and sentiment subtleties regarding cell phones and electronic accessories. It is especially well-suited to the aspect-based and Transformer-based sentiment models because it is structured and has a large sample size. 4.2. GoEmotions (5-Class Variant)The GoEmotions dataset is publicly available on kaggle.com (https://www.kaggle.com/datasets/debarshichanda/goemotions, accessed on 2 December 2025), which is a fine-grained emotion classification benchmark that was created by Google. It includes three sub-versions: GoEmotions1 (70,000 samples), GoEmotions2 (70,000 samples), and GoEmotions3 (70,226 samples) that together offer more than 210,000 labeled examples. A text instance is marked with one or more of 28 different categories of emotions that describe the full spectrum of expressions of feelings. The dataset can be used to perform single-label and multi-label emotion recognition tasks, but is especially useful in testing models on subtle emotional perception. Its scale, language diversity, and fine-grained annotations are a perfect addition to sentiment-oriented datasets that provide a deeper level of emotion to test the overall validity and strength of the suggested AGF-HAM framework.In order to provide methodological consistency and provide a fair comparative analysis between datasets with varying degrees of emotional granularity, the original 28 fine-grained emotion labels in the GoEmotions dataset were mapped inTable 4systematically into five sentiment-oriented categories, namely Very Positive, Positive, Neutral, Negative, and Very Negative. Such a hierarchical feeling category corresponds to the five-class sentiment category of the Amazon Cell Phones and Accessories Reviews dataset, which is consistent in evaluation metrics across domains. The aggregation does not remove the semantic variety and emotionality of the social media expression and offers a comparative benchmarking framework. Therefore, this mapping helps to make the performance evaluation of datasets with differences in their domain focus on social media discourse, as well as emotional expressiveness and linguistic variability fair.Table 4.Mapping of GoEmotions Fine-Grained Labels into Five Sentiment-Oriented Classes. 4.3. Summary of DatasetsTable 5summarizes the key characteristics of all datasets used in this study.Table 5.Summary of Datasets Used in AGF-HAM Evaluation.The combination of these three datasets,Table 5ensures that the AGF-HAM model is evaluated on multiple linguistic domainsâ€”technical, commercial, and socialâ€”capturing a broad spectrum of sentiment expressions. This diverse evaluation framework not only validates the modelâ€™s effectiveness but also demonstrates its robustness and adaptability in handling domain-specific and cross-domain sentiment variations. 4.4. HyperMetersThe AGF-HAM model has various hyperparameters and optimized parameters to provide balanced learning, robust convergence, and a high ability to generalize them across datasets. All the model parts were optimized by systematic experimentation along grid and random search algorithms. The Transformer backbones (BERT and RoBERTa) were pretrained, and they were fine-tuned together with task-specific layers. Learning was stabilized by the use of Adam optimizer with an adaptive learning rate schedule, and regularization was carried out by dropout. The empirical choice of batch size, sequence length, and representational depth was through hidden dimensions and sequence length. Validation loss was used to avoid overfitting by early termination.Table 6summarizes the optimal configuration used in all experiments.Table 6.Hyperparameter Configuration for AGF-HAM Model.The balance in expressive capacity and training stability is represented in the above configuration. A learning rate of(2Ã—10âˆ’5)(2Ã—10âˆ’5)and a batch size of 32 were identified as being optimal in staying constant with Transformer backbones. The moderate dropout rate of 0.3 served as a good counter to overfitting without using model capacity in an underutilized way. The focal loss parameters((ğ›¾=2.0,ğ›¼ğ‘¡adaptive))((Î³=2.0,Î±tadaptive))were used to deal with the imbalance of the classes, where each sentiment category would make fair contributions. A hierarchical attention head of 8 and a BiLSTM hidden size of 256 was rich enough in its representational capability to capture the word, position, and aspect dependencies. Premature termination also helped in training efficiency and strong generalization between datasets.To achieve consistency in the baselines of all comparisons, all the baseline models (recent Transformer-only architecture and hybrid Transformer-BiLSTM models) were fine-tuned under the same experimental conditions. The training regimen, optimization scheme, and preprocessing procedures were applied consistently to all models. All the hyperparameters (learning rate, batch size, optimizer, dropout, hidden dimensions, focal loss parameters, early stopping rule, etc.) will be compiled toTable 6, so that these will be completely reproducible. The resulting single configuration makes a sound performance comparison possible and isolates the actual contribution of each architectural component of the proposed model.",
            "4.1. Amazon Cell Phones and Accessories Reviews": "The source of the Amazon Cell Phones and Accessories Reviews dataset is a reputable source, Kaggle, and the corpus is publicly available on (https://www.kaggle.com/datasets/grikomsn/amazon-cell-phones-reviews?select=20191226-reviews.csv, accessed on 2 December 2025) and widely used in sentiment analysis research. It consists of 67,986 customer reviews and has eight formatted attributes: ASIN, product name, rating, review date, verification status, review title, review body, and helpful votes. Each review is rated on a five-point scale (1â€“5), which directly translates to sentiment polarity categories of very negative, negative, neutral, positive, and very positive. The data set offers an equal and domain-specific benchmark that reasonably captures consumer opinion, buying patterns, and sentiment subtleties regarding cell phones and electronic accessories. It is especially well-suited to the aspect-based and Transformer-based sentiment models because it is structured and has a large sample size.",
            "4.2. GoEmotions (5-Class Variant)": "The GoEmotions dataset is publicly available on kaggle.com (https://www.kaggle.com/datasets/debarshichanda/goemotions, accessed on 2 December 2025), which is a fine-grained emotion classification benchmark that was created by Google. It includes three sub-versions: GoEmotions1 (70,000 samples), GoEmotions2 (70,000 samples), and GoEmotions3 (70,226 samples) that together offer more than 210,000 labeled examples. A text instance is marked with one or more of 28 different categories of emotions that describe the full spectrum of expressions of feelings. The dataset can be used to perform single-label and multi-label emotion recognition tasks, but is especially useful in testing models on subtle emotional perception. Its scale, language diversity, and fine-grained annotations are a perfect addition to sentiment-oriented datasets that provide a deeper level of emotion to test the overall validity and strength of the suggested AGF-HAM framework. In order to provide methodological consistency and provide a fair comparative analysis between datasets with varying degrees of emotional granularity, the original 28 fine-grained emotion labels in the GoEmotions dataset were mapped inTable 4systematically into five sentiment-oriented categories, namely Very Positive, Positive, Neutral, Negative, and Very Negative. Such a hierarchical feeling category corresponds to the five-class sentiment category of the Amazon Cell Phones and Accessories Reviews dataset, which is consistent in evaluation metrics across domains. The aggregation does not remove the semantic variety and emotionality of the social media expression and offers a comparative benchmarking framework. Therefore, this mapping helps to make the performance evaluation of datasets with differences in their domain focus on social media discourse, as well as emotional expressiveness and linguistic variability fair. Table 4.Mapping of GoEmotions Fine-Grained Labels into Five Sentiment-Oriented Classes.",
            "4.3. Summary of Datasets": "Table 5summarizes the key characteristics of all datasets used in this study. Table 5.Summary of Datasets Used in AGF-HAM Evaluation. The combination of these three datasets,Table 5ensures that the AGF-HAM model is evaluated on multiple linguistic domainsâ€”technical, commercial, and socialâ€”capturing a broad spectrum of sentiment expressions. This diverse evaluation framework not only validates the modelâ€™s effectiveness but also demonstrates its robustness and adaptability in handling domain-specific and cross-domain sentiment variations.",
            "4.4. HyperMeters": "The AGF-HAM model has various hyperparameters and optimized parameters to provide balanced learning, robust convergence, and a high ability to generalize them across datasets. All the model parts were optimized by systematic experimentation along grid and random search algorithms. The Transformer backbones (BERT and RoBERTa) were pretrained, and they were fine-tuned together with task-specific layers. Learning was stabilized by the use of Adam optimizer with an adaptive learning rate schedule, and regularization was carried out by dropout. The empirical choice of batch size, sequence length, and representational depth was through hidden dimensions and sequence length. Validation loss was used to avoid overfitting by early termination.Table 6summarizes the optimal configuration used in all experiments. Table 6.Hyperparameter Configuration for AGF-HAM Model. The balance in expressive capacity and training stability is represented in the above configuration. A learning rate of(2Ã—10âˆ’5)(2Ã—10âˆ’5)and a batch size of 32 were identified as being optimal in staying constant with Transformer backbones. The moderate dropout rate of 0.3 served as a good counter to overfitting without using model capacity in an underutilized way. The focal loss parameters((ğ›¾=2.0,ğ›¼ğ‘¡adaptive))((Î³=2.0,Î±tadaptive))were used to deal with the imbalance of the classes, where each sentiment category would make fair contributions. A hierarchical attention head of 8 and a BiLSTM hidden size of 256 was rich enough in its representational capability to capture the word, position, and aspect dependencies. Premature termination also helped in training efficiency and strong generalization between datasets. To achieve consistency in the baselines of all comparisons, all the baseline models (recent Transformer-only architecture and hybrid Transformer-BiLSTM models) were fine-tuned under the same experimental conditions. The training regimen, optimization scheme, and preprocessing procedures were applied consistently to all models. All the hyperparameters (learning rate, batch size, optimizer, dropout, hidden dimensions, focal loss parameters, early stopping rule, etc.) will be compiled toTable 6, so that these will be completely reproducible. The resulting single configuration makes a sound performance comparison possible and isolates the actual contribution of each architectural component of the proposed model.",
            "5. Experiments and Results Discussions": "Comparison of the performance of the baseline models within the four datasets is summarized inTable 7. The conventional machine-learning algorithm SVM had an average score of about 76â€“78% across GoEmotions variants and 80.1% on the Amazon data, but was marginally less effective than Logistic Regression (LR), which had a score of about 77â€“79%. Naive Bayes (NB) had the relatively poorer performance with an accuracy level of 74â€“75. The shift to deep-learning models, LSTM, RCNN, GRU, and Bi-GRU, showed some performance improvements with an accuracy ranging between 81 and 85%; as they have the capacity to discover contextual dependencies and sequential characteristics in a better manner than the old models. It is interesting to note that Bi-GRU obtained the best accuracy between the recurrent architectures, with 84.2â€“85.4% on the GoEmotions datasets and 86.9% on the Amazon dataset. Table 7.Performance Comparison of Baseline Models across Datasets. In addition, the Transformer-based BERT model showed a significant improvement in the performance with 88.4, 89.1, and 89.9 accuracy with the three versions of GoEmotions, and 91.3 accuracy with the Amazon Cell Phones and Accessories Reviews dataset. It shows that BERT is greatly contextually sensitive and has a powerful ability of pretrained linguistic representation, which allows it to outperform classical and recurrent models. In general, although the trends of all models were similar to improve the performance with the Amazon data, this could be explained by the fact that the latter is more organized and focused on sentiments than the multi-emotional variability of GoEmotions. Comparative analysis that was conducted with the Transformer-based and the hybrid architectures, as illustrated inTable 8, indicates that there was a massive performance enhancement in comparison to the traditional and recurrent baselines. BERT, a typical Transformer model, had an accuracy of 88â€“89% with the variants of GoEmotions and 91.3% on the Amazon dataset, indicating a good understanding of the context. ALBERT and XLNet exhibited slightly better results, reaching approximately 89â€“91% accuracy, which was made possible by the sharing of parameters and permutation-based attention mechanisms, respectively. RoBERTa, through its good pretraining optimization, had better results compared to other baseline transformers, reaching 91.9 and 93.1 on GoEmotions-3 and the Amazon reviews, respectively. Table 8.Performance Comparison of Transformer-Based and Hybrid Models. The addition of sequential layers was another way of adding depth to the representation. RoBERTa+BiLSTM hybrid obtained 91.7â€“93.0% of the accuracy in GoEmotions and 94.0% in Amazon, and DistilBERT+Attention reached the same efficiency with a minor decrease in computational complexity. These findings highlight the usefulness of using a combination of contextual embeddings and sequence learning, and attention refining. It is important to note that the HAM model (Hybrid Attention Mechanism) had the highest overall performance, showing a 94.5, 95.1, and 95.6% accuracy on GoEmotions variants and a 96.4% accuracy on the Amazon dataset, and higher precision, recall, and F1 scores of over 93%. This illustrates the ability of HAM to focus the fine-grained emotional and sentiment representations by the interaction of high-order Transformer representations and the BiLSTM. 5.1. Ablation StudyAn ablation analysis was performed on both GoEmotions-3 and Amazon Cell Phones data to determine the role of each architectural component, as shown inTable 3. As a starting point, the RoBERTa baseline recorded an accuracy of 91.9 on GoEmotions and 93.1 on Amazon, which is a good contextual base. BiLSTM addition increased the performance by around 1â€“1.5, which suggests that sequential bidirectional encoding is an effective supplement to the Transformer based on its static contextual embeddings because it captures the temporal relationships and polarity flow in longer reviews.The incorporation of an attention mechanism also increased the accuracy to 94.3% and 95.2%, thus demonstrating the relevance of weighted feature emphasis in detecting sentiment-varying tokens and aspect-specific features. Another improvement from 0.76% to 0.85% was achieved when using Focal Loss, which meant that the imbalance in classes was overcome and the modelâ€™s sensitivity to the sentiment categories of minorities was improved. Lastly, the HAM configuration with RoBERTa, BiLSTM, hierarchical attention, and adaptive optimization had the best performance with 95.6 on GoEmotions and 96.4 on Amazon.The results of the ablation (Table 9) indicate clearly the incremental contribution of each element in the proposed architecture of HAM. BiLSTM enhancement of RoBERTa sequence modeling and attention, which adds greater value to aspect-sensitive feature weighting. An addition of the focal loss can help in terms of class imbalance, and the complete HAM in terms of fusing the fusion gate with GAT and position-aware attention shows the best results in the accuracy of both datasets. Such accumulating gains justify the need and usefulness of every module.Table 9.Ablation study on the proposed HAM Model (GoEmotions-3 and Amazon Datasets).These results highlight that all elements play a significant role: RoBERTa brings out the depth of the context, BiLSTM brings out the sequential comprehension, Attention brings out the interpretability and weighting of tokens, and Focal Loss reduces the bias caused by imbalance. The strength and complementary quality of the hybrid design are justified by the incremental benefits that it has brought in the process of predicting sentiment, which proves that the HAM framework is capable of successfully integrating global and local semantic information to make better predictions.In addition to the quantitative performance, interpretability analysis with the help of LIME and SHAP also supports the results of the ablation study. The analysis of the token-weighted attention and contribution showed that all structural modifications made in HAM directly increase the explainability and transparency of decisions made by the model. Due to hierarchical attention integration, as an example, the model can emphasize aspect-relevant words (e.g., battery life, camera quality) and deemphasize non-informative tokens to generate more human-congruent reasoning patterns. Likewise, Focal Loss not only enhances the precision of the underrepresented sentiment classes but also stabilizes the heatmaps of attention-ensuring that attention is consistently concentrated in varied samples.In comparative XAI visualizations, it is evident that RoBERTa and RoBERTa+BiLSTM are relatively useful in general sentiment polarity but are prone to misunderstanding the context of compound sentences or sarcasm. Conversely, the HAM framework offers more aspect-sensitive explanations that are more interpretable with a high fidelity of interpretation, all of which proves that every model improvement would lead not only to numerical improvements but also to qualitative knowledge. The given hybrid design is, therefore, the solution to two tasks: state-of-the-art performance and open decision-making, which is needed in the real-life application of sentiment and emotion analysis. 5.2. Comparative StudyThe relative analysis of the previous works highlights the ongoing innovations in sentiment and emotion classification techniques. According toTable 10, during the preliminary stage, Studies [1,3] used BERT with Contextualized Psychological Dimensions (CPD) on the GoEmotions dataset with F1-scores of 51.96% and 52.34%, respectively, reflecting the fact that it has 28 fine-grained emotion categories that are difficult to process. Likewise, Study [2], which used baseline BERT on the same dataset, reached a similar F1-score of 52%, justifying the need to investigate deeper contextual learning processes. In pursuit of better representation learning, Study [4] proposed Seq2Emo on a fused SemEval-18 and GoEmotions dataset, and the F1-score was improved to 59.57, which demonstrates the value of multi-dataset fusion and sequence-based emotion models.Table 10.Comparative Study of Sentiment Analysis Models on Various Datasets.Graph-enhanced Transformer architectures were also seen to improve further. In ref. [5], the authors used BERT with Gated Graph Attention (GAT) on the Rest14, Lap14, and Twitter datasets with F1-scores of 82.73, 79.49, and 74.93, respectively, and demonstrated significant improvements in the aspect-based sentiment and short context. Research [6] furthered this trend with 84.27% on sentiment benchmark datasets with Prompt-ConvBERT and Prompt-ConvRoBERTa models, and Research [7] with a hybrid CNN + BERT + RoBERTa model with a similar 84.58% on GoEmotions, indicating the effectiveness of convolutional-semantic fusion. The past traditional baselines, including Naive Bayes (NB) as tested in Study [8] in a variety of datasets (IMDB, Sentiment140, SemEval, STS-Gold), had an F1-score that varied between 73% and 86%, which demonstrated the weakness of non-contextual models. Conversely, Study [9], which used RoBERTa with Adapters and BiLSTM, achieved a better GoEmotions classification of 87, which is indicative of the power of sequential contextual encoding.Based on these developments, our Hybrid Attention Model (HAM) makes a drastic improvement in its performance by obtaining an F1 score of 94.9% on both the GoEmotions and Amazon Cell Phones and Accessories Reviews datasets. The improved performance can be attributed to the joint efforts of BERT/RoBERTa embeddings, GAT-based relational attention, and BiLSTM-based bidirectional context learning, which, together, allow understanding emotional states on a fine scale and cross-domain flexibility. This indicates that HAM is a successful tool to address the gap between the emotionally charged text in social media and the aspect-based product reviews, which is a new milestone in sentiment analysis studies.Figure 5fully illustrates the performance characteristics of the AGF-HAM model in terms of the various dataset mappings and domains of GoEmotions and Amazon Cell Phones. The accuracy and an equivalent training validation loss curve of the Amazon Cell Phones dataset, respectively, resulting inFigure 5a,b, indicate a consistent convergence of the model and the lack of overfitting. The accuracy of GoEmotions-1, GoEmotions-2, and GoEmotions-3 in 10 epochs is shown inFigure 5c,e,g, respectively, and shows a steady improvement and strength over various five-class mappings. In line with this,Figure 5d,f,h demonstrate the training and validation loss curves of the identical mappings, which show the smooth optimization behavior of the model and strong generalization in text domains of various emotions.Figure 5.Graph of Accuracy, Training loss, validation loss of Proposed model AGF-HAM. (a) Accuracy (Amazon Cell Phone); (b) Training and validtion loss (Amazon Cell Phone); (c) Accuracy (GoEmotions-1); (d) Training and validtion loss (GoEmotions-1); (e) Accuracy with GoEmotions-2; (f) Training and validtion loss (GoEmotions-2); (g) Accuracy (GoEmotions-3); (h) Training and validation loss (GoEmotions-3).The confusion matrices inFigure 6give a detailed illustration of the consistency of the AGF-HAM model in the classification of the two datasets.Figure 6a is a representation of the Amazon Cell Phones dataset, where it is evident that there is a separation between the classes and that there is very little misclassification between the levels of sentiments. The GoEmotions-1, GoEmotions-2, and GoEmotions-3 mappings are represented inFigure 6bâ€“d, respectively. These matrices indicate hexahedra prediction patterns with very high accuracy for positive and very positive sentiment categories. All these findings together validate the statement that AGF-HAM has a high level of discriminative ability and domain flexibility and is capable of dealing with structured review data as well as non-structured, empathetic text.Figure 6.Confusion matrices of the proposed model AGF-HAM. (a) Confusion Matrix of Amazon Cell Phone; (b) Confusion Matrix of GoEmotions-1; (c) Confusion Matrix of GoEmotions-2; (d) Confusion Matrix of GoEmotions-3.",
            "5.1. Ablation Study": "An ablation analysis was performed on both GoEmotions-3 and Amazon Cell Phones data to determine the role of each architectural component, as shown inTable 3. As a starting point, the RoBERTa baseline recorded an accuracy of 91.9 on GoEmotions and 93.1 on Amazon, which is a good contextual base. BiLSTM addition increased the performance by around 1â€“1.5, which suggests that sequential bidirectional encoding is an effective supplement to the Transformer based on its static contextual embeddings because it captures the temporal relationships and polarity flow in longer reviews. The incorporation of an attention mechanism also increased the accuracy to 94.3% and 95.2%, thus demonstrating the relevance of weighted feature emphasis in detecting sentiment-varying tokens and aspect-specific features. Another improvement from 0.76% to 0.85% was achieved when using Focal Loss, which meant that the imbalance in classes was overcome and the modelâ€™s sensitivity to the sentiment categories of minorities was improved. Lastly, the HAM configuration with RoBERTa, BiLSTM, hierarchical attention, and adaptive optimization had the best performance with 95.6 on GoEmotions and 96.4 on Amazon. The results of the ablation (Table 9) indicate clearly the incremental contribution of each element in the proposed architecture of HAM. BiLSTM enhancement of RoBERTa sequence modeling and attention, which adds greater value to aspect-sensitive feature weighting. An addition of the focal loss can help in terms of class imbalance, and the complete HAM in terms of fusing the fusion gate with GAT and position-aware attention shows the best results in the accuracy of both datasets. Such accumulating gains justify the need and usefulness of every module. Table 9.Ablation study on the proposed HAM Model (GoEmotions-3 and Amazon Datasets). These results highlight that all elements play a significant role: RoBERTa brings out the depth of the context, BiLSTM brings out the sequential comprehension, Attention brings out the interpretability and weighting of tokens, and Focal Loss reduces the bias caused by imbalance. The strength and complementary quality of the hybrid design are justified by the incremental benefits that it has brought in the process of predicting sentiment, which proves that the HAM framework is capable of successfully integrating global and local semantic information to make better predictions. In addition to the quantitative performance, interpretability analysis with the help of LIME and SHAP also supports the results of the ablation study. The analysis of the token-weighted attention and contribution showed that all structural modifications made in HAM directly increase the explainability and transparency of decisions made by the model. Due to hierarchical attention integration, as an example, the model can emphasize aspect-relevant words (e.g., battery life, camera quality) and deemphasize non-informative tokens to generate more human-congruent reasoning patterns. Likewise, Focal Loss not only enhances the precision of the underrepresented sentiment classes but also stabilizes the heatmaps of attention-ensuring that attention is consistently concentrated in varied samples. In comparative XAI visualizations, it is evident that RoBERTa and RoBERTa+BiLSTM are relatively useful in general sentiment polarity but are prone to misunderstanding the context of compound sentences or sarcasm. Conversely, the HAM framework offers more aspect-sensitive explanations that are more interpretable with a high fidelity of interpretation, all of which proves that every model improvement would lead not only to numerical improvements but also to qualitative knowledge. The given hybrid design is, therefore, the solution to two tasks: state-of-the-art performance and open decision-making, which is needed in the real-life application of sentiment and emotion analysis.",
            "5.2. Comparative Study": "The relative analysis of the previous works highlights the ongoing innovations in sentiment and emotion classification techniques. According toTable 10, during the preliminary stage, Studies [1,3] used BERT with Contextualized Psychological Dimensions (CPD) on the GoEmotions dataset with F1-scores of 51.96% and 52.34%, respectively, reflecting the fact that it has 28 fine-grained emotion categories that are difficult to process. Likewise, Study [2], which used baseline BERT on the same dataset, reached a similar F1-score of 52%, justifying the need to investigate deeper contextual learning processes. In pursuit of better representation learning, Study [4] proposed Seq2Emo on a fused SemEval-18 and GoEmotions dataset, and the F1-score was improved to 59.57, which demonstrates the value of multi-dataset fusion and sequence-based emotion models. Table 10.Comparative Study of Sentiment Analysis Models on Various Datasets. Graph-enhanced Transformer architectures were also seen to improve further. In ref. [5], the authors used BERT with Gated Graph Attention (GAT) on the Rest14, Lap14, and Twitter datasets with F1-scores of 82.73, 79.49, and 74.93, respectively, and demonstrated significant improvements in the aspect-based sentiment and short context. Research [6] furthered this trend with 84.27% on sentiment benchmark datasets with Prompt-ConvBERT and Prompt-ConvRoBERTa models, and Research [7] with a hybrid CNN + BERT + RoBERTa model with a similar 84.58% on GoEmotions, indicating the effectiveness of convolutional-semantic fusion. The past traditional baselines, including Naive Bayes (NB) as tested in Study [8] in a variety of datasets (IMDB, Sentiment140, SemEval, STS-Gold), had an F1-score that varied between 73% and 86%, which demonstrated the weakness of non-contextual models. Conversely, Study [9], which used RoBERTa with Adapters and BiLSTM, achieved a better GoEmotions classification of 87, which is indicative of the power of sequential contextual encoding. Based on these developments, our Hybrid Attention Model (HAM) makes a drastic improvement in its performance by obtaining an F1 score of 94.9% on both the GoEmotions and Amazon Cell Phones and Accessories Reviews datasets. The improved performance can be attributed to the joint efforts of BERT/RoBERTa embeddings, GAT-based relational attention, and BiLSTM-based bidirectional context learning, which, together, allow understanding emotional states on a fine scale and cross-domain flexibility. This indicates that HAM is a successful tool to address the gap between the emotionally charged text in social media and the aspect-based product reviews, which is a new milestone in sentiment analysis studies. Figure 5fully illustrates the performance characteristics of the AGF-HAM model in terms of the various dataset mappings and domains of GoEmotions and Amazon Cell Phones. The accuracy and an equivalent training validation loss curve of the Amazon Cell Phones dataset, respectively, resulting inFigure 5a,b, indicate a consistent convergence of the model and the lack of overfitting. The accuracy of GoEmotions-1, GoEmotions-2, and GoEmotions-3 in 10 epochs is shown inFigure 5c,e,g, respectively, and shows a steady improvement and strength over various five-class mappings. In line with this,Figure 5d,f,h demonstrate the training and validation loss curves of the identical mappings, which show the smooth optimization behavior of the model and strong generalization in text domains of various emotions. Figure 5.Graph of Accuracy, Training loss, validation loss of Proposed model AGF-HAM. (a) Accuracy (Amazon Cell Phone); (b) Training and validtion loss (Amazon Cell Phone); (c) Accuracy (GoEmotions-1); (d) Training and validtion loss (GoEmotions-1); (e) Accuracy with GoEmotions-2; (f) Training and validtion loss (GoEmotions-2); (g) Accuracy (GoEmotions-3); (h) Training and validation loss (GoEmotions-3). The confusion matrices inFigure 6give a detailed illustration of the consistency of the AGF-HAM model in the classification of the two datasets.Figure 6a is a representation of the Amazon Cell Phones dataset, where it is evident that there is a separation between the classes and that there is very little misclassification between the levels of sentiments. The GoEmotions-1, GoEmotions-2, and GoEmotions-3 mappings are represented inFigure 6bâ€“d, respectively. These matrices indicate hexahedra prediction patterns with very high accuracy for positive and very positive sentiment categories. All these findings together validate the statement that AGF-HAM has a high level of discriminative ability and domain flexibility and is capable of dealing with structured review data as well as non-structured, empathetic text. Figure 6.Confusion matrices of the proposed model AGF-HAM. (a) Confusion Matrix of Amazon Cell Phone; (b) Confusion Matrix of GoEmotions-1; (c) Confusion Matrix of GoEmotions-2; (d) Confusion Matrix of GoEmotions-3.",
            "6. Conclusions and Future Work": "The current research presented an overall hybrid of deep-learning architecture (HAM), which combines Transformer-based encoders, bidirectional sequential modeling, attention-based interpretability, and explainability to classify sentiment and emotion robustly. The methodology, which was structured into 13 systematically structured steps, starting with data preprocessing, embedding generation, and aspect extraction, to explainability using attention heatmaps, aspect-level interpretation, and SHAP/Integrated Gradients, guaranteed the predictive power as well as interpretative clarity. Evaluation: Experimental results on benchmark datasets, such as GoEmotions-1, GoEmotions-2, GoEmotions-3, and Amazon Cell Phones, showed the effectiveness of the proposed HAM model in comparison to the existing Transformer baselines, namely BERT, ALBERT, RoBERTa, XLNet, and hybrid models, including RoBERTa+BiLSTM and DistilBERT+Attention. Interestingly, HAM worked best with an accuracy of 96.4, a precision of 95.1, a recall of 94.7, and an F1-score of 94.9, which is far better than state-of-the-art counterparts. Such findings verify the ability of the model to effectively represent fine-grained contextual nuances, and at the same time, it is interpretable due to its explainability sub-layers. The transparency and trustworthiness of the predictions provided by the model are ensured by the addition of the Explainability Module, which includes attention heatmaps, aspect-level explanations, and SHAP/Integrated Gradients analysis, which introduces the element of transparency and trust to the performance of deep-learning models in relation to human comprehension. Such a combined methodology not only determines the polarity of sentiment but also reveals the reasons that certain tokens of textual data or factors guide model choices, allowing even more responsible AI behavior. Future studies can expand the suggested HAM architecture in a number of methodical and topic-focused directions. To begin with, as we have found out that fusion gating and hierarchical attention enhance aspect sensitivity, one way in which future research can complement this finding is by investigating the use of multimodal cues (especially text-aligned visual features) to further enhance aspect-sensitive representations. Second, since the model has been shown to successfully run on datasets with various linguistic properties, cross-lingual adaptation and domain transfer are promising extensions that can be employed technically consistently with the pipeline we have now. Third, the explainability modules (attention heatmaps and SHAP/IG) employed in this study can be extended to interactive, real-time explainability dashboards to facilitate transparency of decisions in the application areas. Lastly, a lightweight or adapter-based variant of HAM could be useful to scale to large-scale or privacy-considerate contexts, providing viable deployment advantages."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2227-7390/13/24/3892",
        "scraped_at": "2025-12-05 18:23:00"
    },
    {
        "title": "Measuring Behavioral Influence on Social Media: A Social Impact Theory Approach to Identifying Influential Users",
        "authors": "byTarirai ChaniandOludayo O. Olugbara",
        "journal": "Journal. Media2025,6(4), 205; https://doi.org/10.3390/journalmedia6040205 (registeringÂ DOI) - 5 Dec 2025",
        "abstract": "The rise of social media has democratized information sharing, allowing ordinary individuals to become influential voices in public discourse. However, traditional methods for identifying influential users rely primarily on network centrality measures that fail to capture the behavioral dynamics underlying actual influence capacity in digital environments. This study introduces the Social Influence Strength Index (SISI), a metric grounded in social impact theory that assesses influence through behavioral engagement indicators rather than network structure alone. The SISI combines three key elements: the average engagement rate, follower reach score, and mention prominence score, using a geometric mean to account for the multiplicative nature of social influence. This was developed and validated using a dataset of 1.2 million tweets from South African migration discussions, a context characterized by high emotional engagement and diverse participant types. SISIâ€™s behavioral principles make it applicable for identifying influential voices across various social media contexts where authentic engagement matters. The results demonstrate substantial divergence between SISI and traditional centrality measures (Spearman Ï = 0.34, 95% CI: 0.32â€“0.36 with eigenvector centrality; top-10 user overlap Jaccard index = 0.20), with the SISI consistently recognizing behaviorally influential users that network-based approaches overlook. Validation analyses confirm the SISIâ€™s predictive validity (high-SISI users maintain 3.5Ã— higher engagement rates in subsequent periods,p< 0.001), discriminant validity (distinguishing content creators from amplifiers, Cohenâ€™s d = 1.32), and convergent validity with expert assessments (Spearman Ï = 0.61 vs. Ï = 0.28 for eigenvector centrality). The research reveals that digital influence stems from genuine audience engagement and community recognition rather than structural network positioning. By integrating social science theory with computational methods, this work presents a theoretically grounded framework for measuring digital influence, with potential applications in understanding information credibility, audience mobilization, and the evolving dynamics of social media-driven public discourse across diverse domains including marketing, policy communication, and digital information ecosystems.Keywords:digital influence;social media metrics;social impact theory;network centrality;behavioral engagement;social influencers",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "The rapid proliferation of social media platforms has fundamentally transformed how information spreads, opinions form, and behaviors change in contemporary society (Lazer et al., 2020). Digital environments create unprecedented opportunities for individuals to influence large audiences, shape public discourse, and mobilize collective action across geographical and temporal boundaries. This transformation has been particularly pronounced in journalism, where social media influencers and citizen journalists have emerged as alternative news sources, often bypassing traditional media gatekeepers to directly inform and engage audiences (Guest & Martin, 2021;Hurcombe, 2024). Understanding who wields influence in these spaces has become critical for diverse stakeholders, from marketers seeking authentic brand ambassadors (Joshi et al., 2023) to policymakers attempting to understand public sentiment (Ausat, 2023) and researchers investigating the mechanics of information diffusion (Luo, 2022). However, the measurement of social media influence remains dominated by computational approaches that prioritize network structural properties over behavioral indicators of actual influence capacity. Current methodologies rely heavily on centrality measures, such as degree centrality, betweenness centrality, closeness centrality, and PageRank. While these metrics provide valuable insights into connectivity patterns and potential reach, they operate under a fundamental assumption that structural positioning equates to influence capacity. This assumption proves problematic in digital environments where influence manifests through complex behavioral dynamics rather than mere network placement. This structural bias becomes increasingly problematic as algorithmic curation reshapes how information flows on social media platforms. Platform algorithms prioritize content based on engagement signals, likes, shares, and comments, rather than solely on structural network positioning. This means that influence increasingly depends on the behavioral ability to generate an audience response rather than follower count or network centrality (Bhandari & Bimo, 2022). Recent research shows that algorithmic amplification favors content resonance and engagement intensity over structural positioning, fundamentally changing how influence functions in digital environments (Dujeancourt & Garz, 2023;Corsi, 2024;FernÃ¡ndez et al., 2024). This algorithmic attention economy rewards demonstrated engagement capacity, i.e., the behavioral strength emphasized in social impact theory, rather than assumed influence based on network location, creating conditions where traditional centrality measures systematically misidentify influential actors. This limitation becomes especially critical in news and information contexts, where the ability to establish credibility and trust with audiences depends on behavioral engagement patterns rather than follower counts or network connections. This paper addresses three key research questions: (1) How can we operationalize influence beyond network positioning to incorporate behavioral engagement patterns? (2) To what extent do structural centrality measures align with multidimensional behavioral influence? (3) Can a composite index integrating audience engagement, follower strength, and media prominence better identify influential actors in political discourse? Our contribution is the Social Influence Strength Index (SISI), which combines three behavioral dimensions, audience engagement responsiveness, follower relationship strength, and media and political sphere mentions, to complement structural network measures in identifying social media influence. Guilbeault and Centola(2021) demonstrated that traditional centrality measures systematically fail for complex social contagions that require multiple peer confirmations, revealing that degree, betweenness, and PageRank centrality often misidentify influential actors in social media contexts. Their findings show that effective influence in digital environments requires accounting for reinforcing ties and behavioral confirmation processes that traditional metrics ignore completely. This finding aligns with broader critiques highlighting how centrality measures exhibit context-insensitivity, treating influence as a static, universal property rather than a dynamic phenomenon that varies across topics, audiences, and temporal contexts (Morrison et al., 2022;Spiller et al., 2020). The limitations of centrality-based approaches become apparent when examining real-world influence patterns on social media platforms. Users with extensive follower networks may generate minimal audience engagement, while others with modest followings consistently mobilize their audiences into meaningful action (Edelmann et al., 2020). Traditional metrics fail to capture these qualitative differences, focusing instead on quantitative indicators such as follower counts, connection numbers, or structural bridge positions. This structural bias overlooks the demonstrated capacity to generate authentic engagement, inspire content sharing, or stimulate meaningful dialogue. In this paper, we argue that these traits constitute genuine influence in digital spaces. These measurement challenges are particularly relevant for understanding the evolving landscape of digital journalism, where the boundaries between professional reporters, citizen journalists, and news influencers are continually blurring. The rise of influencer journalism, where individuals with significant online followings act as intermediaries of news and information, requires new approaches to assessing credibility and influence that account for behavioral engagement rather than institutional affiliation or network size alone. Therefore, the predominance of computational efficiency over theoretical validity in influence measurement reflects a broader disconnect between social science theory and digital analytics (Radford & Joseph, 2020;Schoch & Brandes, 2016). While computer scientists and data analysts have developed sophisticated algorithms for processing large-scale social media data, these approaches often lack grounding in established frameworks for understanding human social behavior. Social psychology, communication theory, and sociology offer rich insights into how influence operates in human interactions, yet these theoretical foundations remain largely untapped in computational social science applications (Edelmann et al., 2020). This theoretical gap is particularly problematic given that social media platforms, despite their digital nature, fundamentally facilitate human social interactions governed by the exact psychological and social mechanisms that operate in offline contexts. Decades of social science research have identified key factors that determine influence effectiveness, including source credibility, message resonance, audience characteristics, and contextual factors (Chalakudi et al., 2023). These insights could significantly enhance computational approaches to influence measurement, yet they remain underutilized due to the challenge of operationalizing behavioral concepts in computational frameworks. In this paper, we argue that the social impact theory, developed byBibb LatanÃ©(1981), provides a particularly promising framework for understanding influence dynamics. The theory states that social influence results from the interaction of three key factors: the strength of the influence source (their perceived power, authority, and social capital), the immediacy of the source to the target audience (spatial, temporal, and social proximity), and the number of sources present in the influence situation (LatanÃ© & Wolf, 1981). This framework has demonstrated robustness across diverse contexts and populations, yet its application to digital influence measurement remains limited, despite growing recognition of its relevance to social media analytics. The â€œstrengthâ€ component offers especially relevant insights for social media contexts, where usersâ€™ ability to command attention, mobilize audiences, and sustain authority within digital networks varies dramatically (Harkins & LatanÃ©, 1998). Unlike structural positioning, strength encompasses demonstrated behavioral capacity that is the observable patterns of audience engagement, content resonance, and recognition within discourse communities that indicate actual rather than potential influence. Recent work on female Instagram creators demonstrates that influence often emerges from behavioral motivational drivers such as self-expression, empowerment, and aspiration, rather than mere structural positioning in networks (Mlangeni et al., 2025). This paper addresses the theoretical and methodological limitations of current influence measurement approaches by developing the Social Influence Strength Index (SISI), a novel metric that operationalizes social impact theoryâ€™s â€œstrengthâ€ component for digital environments. The SISI represents a fundamental departure from centrality-based approaches by measuring actualized influence through behavioral indicators rather than structural positioning. The metric integrates three complementary dimensions that capture different aspects of influence strength: the average engagement rate (measuring audience mobilization efficiency), follower reach score (assessing contextually normalized audience scale), and mention prominence score (evaluating discourse recognition and authority). Our primary contribution lies in demonstrating how established social science theory can enhance computational approaches to influence measurement. By grounding the SISI in social impact theory, we provide a theoretically justified framework for selecting and combining influence indicators, moving beyond ad hoc metric combinations toward principled measurement design. Empirically, we validate the SISI through a comprehensive analysis of 1.2 million tweets from South African migration discourse collected between 2021 and 2022. This dataset provides an ideal testing ground due to the topicâ€™s high emotional engagement, diverse participant types, and authentic discourse patterns that reflect real-world influence dynamics (Tarisayi & Manik, 2020;Chiumbu & Moyo, 2018). Our findings reveal that the SISI consistently identifies influential users who are overlooked by traditional centrality measures, demonstrating that behavioral influence operates independently of structural positioning within networks. The implications extend beyond methodological innovation to practical applications across multiple domains where authentic influence identification provides a competitive advantage, from marketing strategy development (Zhou et al., 2024) to policy communication and social media research (Tang, 2023).",
            "2. The Social Influence Strength Index (SISI)": "The Social Influence Strength Index (SISI) operationalizes the strength component of social impact theory through a multidimensional framework that captures usersâ€™ demonstrated capacity to influence others within digital social environments. Unlike traditional metrics that rely on structural network properties or simple engagement counts, the SISI measures actualized influence by examining behavioral manifestations of strength across three complementary dimensions. This approach reflects the theoretical understanding that influence strength emerges not from position alone but from the consistent ability to mobilize audiences, command attention, and maintain authority within discourse communities. The core premise underlying SISI design is that digital influence strength manifests through observable patterns of audience response and community recognition. In social media contexts, strength cannot be assumed from follower counts or network centrality but must be demonstrated through sustained ability to generate meaningful engagement (Wies et al., 2023), maintain audience attention, and earn recognition as a valuable contributor to ongoing conversations (Kubler, 2023). This behavioral focus aligns with social impact theoryâ€™s emphasis on actual rather than potential influence while protecting metric gaming through artificial follower inflation or engagement manipulation. Recent research supports the multidimensional approach to influence measurement. For instance,Zhuang et al.(2021) developed a multidimensional social influence (MSI) measurement approach analyzing structure-based, information-based, and action-based factors, demonstrating that influence in online social networks is a complex force determined by multiple attributes from different dimensions. Their experimental studies showed that multidimensional approaches outperformed traditional single-dimensional methods in identifying influential users across both topic-level and global-level networks. This empirical validation reinforces the theoretical rationale for SISIâ€™s comprehensive framework. Empirical influence measurement has evolved significantly beyond follower counts.Cha et al.(2010) demonstrated that influence is multifaceted, showing weak correlations between indegree, retweets, and mentions on Twitter, establishing that no single metric captures influence comprehensively.Bakshy et al.(2011) further showed that influence depends on both reach and engagement probability, not merely network size. The SISI extends this tradition by integrating behavioral engagement (AER), relationship quality (FRS), and cross-platform visibility (MPS), whereas these earlier studies examined dimensions separately. Unlike Cha et al.â€™s descriptive comparison, the SISI provides a weighted composite specifically for contexts where confirmation bias and selective engagement dominate. The SISI also adopts a multidimensional approach that acknowledges the complex nature of influence strength in digital environments. Rather than relying on a single indicator that might capture only one aspect of influence capacity, the metric integrates three distinct but complementary dimensions that together provide a comprehensive assessment of strength. This design recognizes that users may demonstrate influence through different pathways. For example, some may demonstrate influence through exceptional engagement efficiency, others through broad reach within relevant communities, and others through recognition as thought leaders whose contributions shape ongoing discourse (Park & Lee, 2021). The SISI is made up of three components, which are the average engagement rate (AER), follower reach score (FRS), and mention prominence score (MPS). The AER component quantifies usersâ€™ demonstrated ability to mobilize their audiences into active participation and response. Unlike cumulative engagement metrics that can be inflated by high posting frequency or large follower bases, the AER focuses on engagement efficiency, which is the consistent capacity to generate meaningful audience interaction relative to potential exposure. Recent benchmark data reveal significant variations in engagement rates across platforms and industries, with good engagement rates typically falling between 1% and 3% for most social media platforms in 2025 (ContentStudio, 2025). This component directly operationalizes social impact theoryâ€™s conceptualization of strength as the sourceâ€™s ability to command attention and inspire behavioral responses from target audiences. The AER calculation normalizes total engagement by both follower count and number of posts, providing a measure of per-post engagement efficiency that remains comparable across users with different activity levels and audience sizes. The mathematical formulation incorporates weighted considerations for different interaction types:AER=1nÃ—âˆ‘(EiDiÃ—100)AER=1nÃ—âˆ‘EiDiÃ—100(1)where: EiEi= total engagements (likes, comments, shares, saves) for post i;DiDi= potential audience (preferably reach, otherwise estimated via followers Ã— platform reach rate);nn= number of post analyzed. We weight interactions equally (likes = 1, replies = 1, retweets = 1, quotes = 1), treating all engagement actions as equivalent signals of audience response, consistent with platform algorithms that count all interactions toward visibility metrics. The FRS considers the context of audience size by adjusting usersâ€™ follower counts relative to relevant comparison groups within their domains or platforms. While the total audience size impacts influence potential, a meaningful assessment requires understanding how usersâ€™ reach compares to typical expectations within their specific contexts. A recent industry analysis shows significant variation in follower distributions across sectors, with higher education achieving engagement rates of 4.52% with 28 weekly posts on Instagram, while entertainment and media industries display different optimal posting frequencies and engagement patterns (Hootsuite, 2025).FRS=(Fğ’¾Fmedian)Ã—100FRS=FiFmedianÃ—100(2)where: Fğ’¾Fi= individual follower count;FmedianFmedian= median followers within the comparison group. Scores above 100 indicate above-average follower strength; scores below 100 indicate below-average strength; a score of 100 indicates median-level followers. Comparison groups were defined by topic hashtag cluster (identified via Louvain community detection on hashtag co-occurrence networks) and verified status, ensuring like-to-like comparisons within thematically similar discourse communities. This normalization prevents extreme outliers from distorting assessments while still recognizing exceptional reach when it occurs, providing intuitive interpretation where scores above 100 indicate above-average reach while scores below 100 suggest below-average audience size relative to contextual expectations. The MPS captures an actorâ€™s visibility and prominence beyond their immediate network by measuring how frequently they are mentioned by media outlets, journalists, political figures, and verified accounts in the broader public discourse. Unlike follower-based metrics that reflect self-selected audiences, the MPS indicates recognition by elite information brokers and agenda-setters who amplify certain voices into wider political conversations. The MPS is calculated as:MPS=(MiMmedian)Ã—100MPS=MiMmedianÃ—100(3)where: MiMi= total mentions by verified account (direct @tags, replies, quote shares);MmedianMmedian= median mentions in the comparison group. Scores above 100 indicate above-average media visibility and elite recognition; scores below 100 indicate below-average visibility; a score of 100 represents median-level prominence. We identified media and political accounts using two criteria: (1) verified status combined with account bio keywords (e.g., â€œjournalistâ€, â€œnewsâ€, â€œreporterâ€, â€œpoliticianâ€, â€œsenatorâ€); (2) manual validation of high-frequency mentioners to ensure classification accuracy. This approach captures mentions that signal influence beyond grassroots engagement, reflecting the userâ€™s ability to penetrate elite discourse spaces and shape agenda-setting processes. The ratio-based formulation provides intuitive interpretation while maintaining comparability across users operating in different discourse communities. The SISI integrates its three component dimensions through a geometric mean calculation that reflects the multiplicative nature of influence processes described in social impact theory. The mathematical formulation is:SISI = âˆ›(AER Ã— FRS Ã— MPS)(4) To ensure equal weighting in the geometric mean calculation, we apply minâ€“max scaling to normalize each component (AER, FRS, MPS) to the [0, 1] interval:ScaledComponent=RawComponentâˆ’MinMaxâˆ’MinScaledComponent=RawComponentâˆ’MinMaxâˆ’Min Throughout theSection 4, all reported component values represent scaled scores (0â€“1) unless explicitly noted. To ensure computational stability, we implement three preprocessing steps before calculating the geometric mean. First, we handle zeros by adding a small constant Îµ = 1 Ã— 10âˆ’6to each component (AER, FRS, MPS) before integration, preventing the geometric mean from collapsing to zero when any single dimension is zero. This approach is preferable to deletion (which would lose valuable partial-influence profiles) or substitution with median values (which would distort individual scores). Second, we apply minâ€“max scaling to normalize each component to the [0, 1] interval within the dataset, ensuring equal weighting in the geometric mean despite different raw scales. Third, we cap outliers at the 99th percentile for each component before scaling to prevent extreme values from distorting the distribution. These steps ensure the SISI captures multidimensional influence while maintaining robustness to edge cases and scale differences. The geometric mean calculation prevents metric gaming by requiring authentic performance across all influence dimensions rather than allowing users to achieve high scores through artificial inflation of single components. This approach aligns with the theoretical understanding that effective influence emerges from the interaction of multiple factors rather than their simple combination. The choice of the geometric mean for SISI integration reflects careful consideration of how influence components interact in real-world social systems. Social impact theoryâ€™s multiplicative formulation suggests that influence effectiveness depends on the simultaneous presence of multiple factors rather than their independent contribution. Mathematical properties of the geometric mean align closely with these theoretical expectations, as the multiplicative calculation means that each component contributes proportionally to the final score, with increases in any single component producing effects that depend on the values of other components. Users demonstrating a high AER, combined with a moderate FRS, typically represent niche experts or micro-influencers who have cultivated highly engaged communities around specialized content. These profiles indicate exceptional ability to create meaningful interactions with available audiences, suggesting strong content quality and audience resonance within specific domains.",
            "3. Methodology": "3.1. Research Context and Data SelectionTo evaluate the proposed metric, we selected the South African migration discourse on the social media platform X (formerly Twitter) as our primary data source, covering the period from 1 January 2021 to 31 December 2022. This choice was driven by several methodological considerations that align with the theoretical requirements for testing influence metrics grounded in social impact theory while addressing contemporary challenges in social media data collection.The migration discourse in South Africa represents an ideal context for influence measurement validation due to its inherently polarizing and emotionally engaging nature. The topic consistently generates high levels of public participation, creating rich datasets of authentic user interactions that are essential for validating metrics designed to measure behavioral influence (Hove, 2022). Recent research has demonstrated that xenophobic discourse in South Africa has shifted from physical confrontations to ongoing dialogue on public platforms, such as social media, with Twitter serving as a primary arena for these discussions (Makhura, 2022). Unlike artificially stimulated engagement or promotional content, migration discussions reflect genuine public sentiment, producing interaction patterns that accurately represent real-world influence dynamics.The diversity of participants in migration discourse provides another crucial advantage for validation purposes. These discussions attract politicians, activists, journalists, academics, civil society organizations, and ordinary citizens, creating a heterogeneous user population with varying influence mechanisms and audience relationships. This diversity enables testing of the SISIâ€™s capacity to identify influence across different user types. The emergence of organized digital movements such as Operation Dudula and Put South Africans First during this period provides particularly valuable natural experiments for understanding influence mobilization through social media platforms (Tarisayi, 2024;Dratwa, 2023). 3.2. Data Collection Protocol3.2.1. Data CollectionThe data collection process employed a systematic, multi-phase approach designed to ensure we comprehensively captured the migration-related discourse while maintaining data quality and thematic relevance. The protocol balanced breadth of coverage with specificity to migration topics, employing iterative refinement processes that adapted to evolving discourse patterns throughout the collection period. Recent methodological innovations in social media research emphasize the importance of such adaptive approaches, particularly given the dynamic nature of online discourse and the emergence of new hashtags and terminologies (Kim et al., 2023;Chani et al., 2023).3.2.2. Keyword Development and ValidationThe initial phase established a set of keywords through triangulated input sources, including an academic literature review, empirical platform exploration, and expert consultation with migration researchers and civil society organizations. The academic literature review identified terminology commonly used in scholarly migration research, providing theoretical grounding for keyword selection. Preliminary platform exploration using broad search terms such as â€œmigration SAâ€, â€œforeign nationalsâ€, and â€œxenophobiaâ€ revealed frequently occurring hashtags and phrases in real-time user conversations, including emergent terms such as #OperationDudula and #ForeignersMustGo that became central to the discourse during the study period.Expert consultation with migration researchers and civil society organizations provided crucial validation of keyword relevance while identifying colloquial terms and emerging hashtags that might be overlooked through purely academic or algorithmic approaches. This consultation process ensured that the keyword selection process captured authentic discourse patterns while maintaining a focus on migration-related themes rather than tangentially related content. The resulting initial keyword set included both formal terminology (immigration, xenophobia, foreign nationals) and colloquial expressions (#PutSouthAfricansFirst, #ForeignersMustGo) that reflect actual user language patterns documented in recent research on South African digital xenophobia (Raborife et al., 2024).The final keyword set included 28 hashtags (#OperationDudula, #ForeignersMustGo, #PutSouthAfricansFirst, #XenophobiaInSA, #IllegalImmigrants, #BorderSecurity, among others) and 15 keywords (â€œillegal immigrantsâ€, â€œundocumented foreignersâ€, â€œforeign nationalsâ€, â€œmigration policyâ€, â€œxenophobiaâ€, â€œborder controlâ€, etc.). An English language filter (lang:en) was applied via Twitterâ€™s native detection. Tweets containing at least one hashtag OR keyword were included. Retweets were included in the network construction but excluded from AER calculations; quote tweets were treated as original content; replies were included in all metrics.3.2.3. Iterative Refinement and ExpansionThe keyword refinement process employed a hashtag co-occurrence analysis and event-driven expansion to remain responsive to evolving discourse patterns. Weekly reviews of collected data identified frequently co-occurring hashtags that indicated relevant content worth including. Event-driven analyses monitored discourse spikes around specific incidents such as policy announcements, protests, or outbreaks of violence, which often introduced new terminology or revived dormant hashtags. This adaptable approach was vital, particularly with the appearance of new movements and hashtags during the collection period, including the June 2021 launch of Operation Dudulaâ€™s â€œLetâ€™s Clean Sowetoâ€ campaign, which generated notable social media activity. 3.3. Data ExtractionFollowing best practices for ethical social media research established by recent methodological guidance (Chani et al., 2023;Chen et al., 2024), the extraction process employed systematic sampling approaches that ensured representative coverage. At all stages, procedures were carefully aligned with the platformâ€™s terms of service and strict user privacy protections. The extraction was carried out using Python 3.10, specifically leveraging the SNScrape library, which enabled the automated retrieval of publicly available posts while preserving the metadata integrity. Only publicly accessible content was collected, and no private or restricted data were accessed. To further safeguard our research ethics, protocols were established for the secure handling of user-related information, including anonymization of identifiers, minimization of sensitive data, and compliance with contemporary standards for social media research.The sampling process was continuous (not periodic) via SNScrapeâ€™s real-time collection over the entire period. Missing engagement metrics (<0.1% of tweets) were excluded from AER calculations but retained for the network analysis. The de-duplication process used tweet IDs as unique identifiers. The bot filtering process employed account-level heuristics; accounts with >50 tweets/day or >90% retweet ratios were flagged and excluded, affecting approximately 3.2% of collected accounts. This research received institutional ethics clearance for a secondary analysis of public social media data. All user IDs were hashed (SHA-256), no usernames appeared in outputs, and aggregate reporting maintained n â‰¥ 10 group sizes to prevent re-identification. 3.4. Validation ProceduresBeyond comparing the SISI with traditional centrality measures, we conducted three validation tests to establish predictive, discriminant, and convergent validity.3.4.1. Predictive ValidityWe calculated SISI scores using data from January 2021 to September 2022, then tested whether high-SISI users maintained elevated engagement rates in Q4-2022. Users were divided into quartiles by SISI score. Only users posting â‰¥5 times in Q4-2022 were included (n = 12,847). We compared engagement rates across quartiles using Mannâ€“Whitney U tests due to non-normal distributions.3.4.2. Discriminant ValidityWe sampled 500 highly retweeted original tweets (â‰¥100 retweets, Aprilâ€“September 2022) using stratified random sampling (n = 100 per month). Three trained coders classified each tweet as:Content Creator: Original analyses, firsthand reporting, novel arguments;Amplifier: Primarily retweets or quotes with minimal added value;Mixed: Combines substantial original content with amplification.Coders were trained on 50 pilot tweets until achieving Krippendorffâ€™s Î± â‰¥ 0.80. The final sample inter-coder reliability Î± was 0.83 (95% CI: 0.79â€“0.87). We compared SISI scores between content creators (n = 287) and amplifiers (n = 156) using independent samplest-tests, excluding mixed cases (n = 57).3.4.3. Convergent ValidityThree migration experts (PhDs with â‰¥5 yearsâ€™ of South African migration research, â‰¥3 publications) rated 50 randomly sampled users stratified across SISI quintiles (n = 10 per quintile). Experts received anonymized profiles containing three tweets and aggregate statistics (posts, engagement rate, followers) but no network position information. Experts rated influence on 7-point Likert scales; inter-rater reliability: ICC(2,3) = 0.74 (95% CI: 0.63â€“0.83). We computed Spearman correlations between averaged expert ratings and both SISI and eigenvector centrality.",
            "3.1. Research Context and Data Selection": "To evaluate the proposed metric, we selected the South African migration discourse on the social media platform X (formerly Twitter) as our primary data source, covering the period from 1 January 2021 to 31 December 2022. This choice was driven by several methodological considerations that align with the theoretical requirements for testing influence metrics grounded in social impact theory while addressing contemporary challenges in social media data collection. The migration discourse in South Africa represents an ideal context for influence measurement validation due to its inherently polarizing and emotionally engaging nature. The topic consistently generates high levels of public participation, creating rich datasets of authentic user interactions that are essential for validating metrics designed to measure behavioral influence (Hove, 2022). Recent research has demonstrated that xenophobic discourse in South Africa has shifted from physical confrontations to ongoing dialogue on public platforms, such as social media, with Twitter serving as a primary arena for these discussions (Makhura, 2022). Unlike artificially stimulated engagement or promotional content, migration discussions reflect genuine public sentiment, producing interaction patterns that accurately represent real-world influence dynamics. The diversity of participants in migration discourse provides another crucial advantage for validation purposes. These discussions attract politicians, activists, journalists, academics, civil society organizations, and ordinary citizens, creating a heterogeneous user population with varying influence mechanisms and audience relationships. This diversity enables testing of the SISIâ€™s capacity to identify influence across different user types. The emergence of organized digital movements such as Operation Dudula and Put South Africans First during this period provides particularly valuable natural experiments for understanding influence mobilization through social media platforms (Tarisayi, 2024;Dratwa, 2023).",
            "3.2. Data Collection Protocol": "3.2.1. Data CollectionThe data collection process employed a systematic, multi-phase approach designed to ensure we comprehensively captured the migration-related discourse while maintaining data quality and thematic relevance. The protocol balanced breadth of coverage with specificity to migration topics, employing iterative refinement processes that adapted to evolving discourse patterns throughout the collection period. Recent methodological innovations in social media research emphasize the importance of such adaptive approaches, particularly given the dynamic nature of online discourse and the emergence of new hashtags and terminologies (Kim et al., 2023;Chani et al., 2023). 3.2.2. Keyword Development and ValidationThe initial phase established a set of keywords through triangulated input sources, including an academic literature review, empirical platform exploration, and expert consultation with migration researchers and civil society organizations. The academic literature review identified terminology commonly used in scholarly migration research, providing theoretical grounding for keyword selection. Preliminary platform exploration using broad search terms such as â€œmigration SAâ€, â€œforeign nationalsâ€, and â€œxenophobiaâ€ revealed frequently occurring hashtags and phrases in real-time user conversations, including emergent terms such as #OperationDudula and #ForeignersMustGo that became central to the discourse during the study period.Expert consultation with migration researchers and civil society organizations provided crucial validation of keyword relevance while identifying colloquial terms and emerging hashtags that might be overlooked through purely academic or algorithmic approaches. This consultation process ensured that the keyword selection process captured authentic discourse patterns while maintaining a focus on migration-related themes rather than tangentially related content. The resulting initial keyword set included both formal terminology (immigration, xenophobia, foreign nationals) and colloquial expressions (#PutSouthAfricansFirst, #ForeignersMustGo) that reflect actual user language patterns documented in recent research on South African digital xenophobia (Raborife et al., 2024).The final keyword set included 28 hashtags (#OperationDudula, #ForeignersMustGo, #PutSouthAfricansFirst, #XenophobiaInSA, #IllegalImmigrants, #BorderSecurity, among others) and 15 keywords (â€œillegal immigrantsâ€, â€œundocumented foreignersâ€, â€œforeign nationalsâ€, â€œmigration policyâ€, â€œxenophobiaâ€, â€œborder controlâ€, etc.). An English language filter (lang:en) was applied via Twitterâ€™s native detection. Tweets containing at least one hashtag OR keyword were included. Retweets were included in the network construction but excluded from AER calculations; quote tweets were treated as original content; replies were included in all metrics. 3.2.3. Iterative Refinement and ExpansionThe keyword refinement process employed a hashtag co-occurrence analysis and event-driven expansion to remain responsive to evolving discourse patterns. Weekly reviews of collected data identified frequently co-occurring hashtags that indicated relevant content worth including. Event-driven analyses monitored discourse spikes around specific incidents such as policy announcements, protests, or outbreaks of violence, which often introduced new terminology or revived dormant hashtags. This adaptable approach was vital, particularly with the appearance of new movements and hashtags during the collection period, including the June 2021 launch of Operation Dudulaâ€™s â€œLetâ€™s Clean Sowetoâ€ campaign, which generated notable social media activity.",
            "3.2.1. Data Collection": "The data collection process employed a systematic, multi-phase approach designed to ensure we comprehensively captured the migration-related discourse while maintaining data quality and thematic relevance. The protocol balanced breadth of coverage with specificity to migration topics, employing iterative refinement processes that adapted to evolving discourse patterns throughout the collection period. Recent methodological innovations in social media research emphasize the importance of such adaptive approaches, particularly given the dynamic nature of online discourse and the emergence of new hashtags and terminologies (Kim et al., 2023;Chani et al., 2023).",
            "3.2.2. Keyword Development and Validation": "The initial phase established a set of keywords through triangulated input sources, including an academic literature review, empirical platform exploration, and expert consultation with migration researchers and civil society organizations. The academic literature review identified terminology commonly used in scholarly migration research, providing theoretical grounding for keyword selection. Preliminary platform exploration using broad search terms such as â€œmigration SAâ€, â€œforeign nationalsâ€, and â€œxenophobiaâ€ revealed frequently occurring hashtags and phrases in real-time user conversations, including emergent terms such as #OperationDudula and #ForeignersMustGo that became central to the discourse during the study period. Expert consultation with migration researchers and civil society organizations provided crucial validation of keyword relevance while identifying colloquial terms and emerging hashtags that might be overlooked through purely academic or algorithmic approaches. This consultation process ensured that the keyword selection process captured authentic discourse patterns while maintaining a focus on migration-related themes rather than tangentially related content. The resulting initial keyword set included both formal terminology (immigration, xenophobia, foreign nationals) and colloquial expressions (#PutSouthAfricansFirst, #ForeignersMustGo) that reflect actual user language patterns documented in recent research on South African digital xenophobia (Raborife et al., 2024). The final keyword set included 28 hashtags (#OperationDudula, #ForeignersMustGo, #PutSouthAfricansFirst, #XenophobiaInSA, #IllegalImmigrants, #BorderSecurity, among others) and 15 keywords (â€œillegal immigrantsâ€, â€œundocumented foreignersâ€, â€œforeign nationalsâ€, â€œmigration policyâ€, â€œxenophobiaâ€, â€œborder controlâ€, etc.). An English language filter (lang:en) was applied via Twitterâ€™s native detection. Tweets containing at least one hashtag OR keyword were included. Retweets were included in the network construction but excluded from AER calculations; quote tweets were treated as original content; replies were included in all metrics.",
            "3.2.3. Iterative Refinement and Expansion": "The keyword refinement process employed a hashtag co-occurrence analysis and event-driven expansion to remain responsive to evolving discourse patterns. Weekly reviews of collected data identified frequently co-occurring hashtags that indicated relevant content worth including. Event-driven analyses monitored discourse spikes around specific incidents such as policy announcements, protests, or outbreaks of violence, which often introduced new terminology or revived dormant hashtags. This adaptable approach was vital, particularly with the appearance of new movements and hashtags during the collection period, including the June 2021 launch of Operation Dudulaâ€™s â€œLetâ€™s Clean Sowetoâ€ campaign, which generated notable social media activity.",
            "3.3. Data Extraction": "Following best practices for ethical social media research established by recent methodological guidance (Chani et al., 2023;Chen et al., 2024), the extraction process employed systematic sampling approaches that ensured representative coverage. At all stages, procedures were carefully aligned with the platformâ€™s terms of service and strict user privacy protections. The extraction was carried out using Python 3.10, specifically leveraging the SNScrape library, which enabled the automated retrieval of publicly available posts while preserving the metadata integrity. Only publicly accessible content was collected, and no private or restricted data were accessed. To further safeguard our research ethics, protocols were established for the secure handling of user-related information, including anonymization of identifiers, minimization of sensitive data, and compliance with contemporary standards for social media research. The sampling process was continuous (not periodic) via SNScrapeâ€™s real-time collection over the entire period. Missing engagement metrics (<0.1% of tweets) were excluded from AER calculations but retained for the network analysis. The de-duplication process used tweet IDs as unique identifiers. The bot filtering process employed account-level heuristics; accounts with >50 tweets/day or >90% retweet ratios were flagged and excluded, affecting approximately 3.2% of collected accounts. This research received institutional ethics clearance for a secondary analysis of public social media data. All user IDs were hashed (SHA-256), no usernames appeared in outputs, and aggregate reporting maintained n â‰¥ 10 group sizes to prevent re-identification.",
            "3.4. Validation Procedures": "Beyond comparing the SISI with traditional centrality measures, we conducted three validation tests to establish predictive, discriminant, and convergent validity. 3.4.1. Predictive ValidityWe calculated SISI scores using data from January 2021 to September 2022, then tested whether high-SISI users maintained elevated engagement rates in Q4-2022. Users were divided into quartiles by SISI score. Only users posting â‰¥5 times in Q4-2022 were included (n = 12,847). We compared engagement rates across quartiles using Mannâ€“Whitney U tests due to non-normal distributions. 3.4.2. Discriminant ValidityWe sampled 500 highly retweeted original tweets (â‰¥100 retweets, Aprilâ€“September 2022) using stratified random sampling (n = 100 per month). Three trained coders classified each tweet as:Content Creator: Original analyses, firsthand reporting, novel arguments;Amplifier: Primarily retweets or quotes with minimal added value;Mixed: Combines substantial original content with amplification.Coders were trained on 50 pilot tweets until achieving Krippendorffâ€™s Î± â‰¥ 0.80. The final sample inter-coder reliability Î± was 0.83 (95% CI: 0.79â€“0.87). We compared SISI scores between content creators (n = 287) and amplifiers (n = 156) using independent samplest-tests, excluding mixed cases (n = 57). 3.4.3. Convergent ValidityThree migration experts (PhDs with â‰¥5 yearsâ€™ of South African migration research, â‰¥3 publications) rated 50 randomly sampled users stratified across SISI quintiles (n = 10 per quintile). Experts received anonymized profiles containing three tweets and aggregate statistics (posts, engagement rate, followers) but no network position information. Experts rated influence on 7-point Likert scales; inter-rater reliability: ICC(2,3) = 0.74 (95% CI: 0.63â€“0.83). We computed Spearman correlations between averaged expert ratings and both SISI and eigenvector centrality.",
            "3.4.1. Predictive Validity": "We calculated SISI scores using data from January 2021 to September 2022, then tested whether high-SISI users maintained elevated engagement rates in Q4-2022. Users were divided into quartiles by SISI score. Only users posting â‰¥5 times in Q4-2022 were included (n = 12,847). We compared engagement rates across quartiles using Mannâ€“Whitney U tests due to non-normal distributions.",
            "3.4.2. Discriminant Validity": "We sampled 500 highly retweeted original tweets (â‰¥100 retweets, Aprilâ€“September 2022) using stratified random sampling (n = 100 per month). Three trained coders classified each tweet as: Content Creator: Original analyses, firsthand reporting, novel arguments;Amplifier: Primarily retweets or quotes with minimal added value;Mixed: Combines substantial original content with amplification. Coders were trained on 50 pilot tweets until achieving Krippendorffâ€™s Î± â‰¥ 0.80. The final sample inter-coder reliability Î± was 0.83 (95% CI: 0.79â€“0.87). We compared SISI scores between content creators (n = 287) and amplifiers (n = 156) using independent samplest-tests, excluding mixed cases (n = 57).",
            "3.4.3. Convergent Validity": "Three migration experts (PhDs with â‰¥5 yearsâ€™ of South African migration research, â‰¥3 publications) rated 50 randomly sampled users stratified across SISI quintiles (n = 10 per quintile). Experts received anonymized profiles containing three tweets and aggregate statistics (posts, engagement rate, followers) but no network position information. Experts rated influence on 7-point Likert scales; inter-rater reliability: ICC(2,3) = 0.74 (95% CI: 0.63â€“0.83). We computed Spearman correlations between averaged expert ratings and both SISI and eigenvector centrality.",
            "4. Results": "4.1. Dataset Overview and Descriptive StatisticsThe data collection process produced a comprehensive dataset of 1.2 million tweets from 47,892 unique users involved in South African migration discussions between 1 January 2021 and 31 December 2022. This extensive dataset provides a solid foundation for validating the SISIâ€™s effectiveness in identifying influential users and allows for detailed comparisons with traditional centrality measures across various user types and engagement patterns. The temporal distribution analysis shows significant variation in discourse activity over the collection period, with notable spikes corresponding to major migration-related events such as the Zimbabwe Exemption Permit policy debates in mid-2021, xenophobic incidents across various provinces, and the rise of digital movements such as Operation Dudula. These activity peaks provided natural experiments for influence measurement, creating periods where usersâ€™ ability to shape discourse and mobilize audiences became especially evident. The average daily tweet volume ranged from 1200 tweets during baseline periods to over 15,000 during peak events, demonstrating the datasetâ€™s capacity to capture both routine discourse and crisis-driven engagement patterns.User participation patterns demonstrate the diverse nature of migration discourse, with participants ranging from highly active political commentators who post hundreds of times each month to occasional contributors who tweet only during major events. This variety creates ideal conditions for testing the SISIâ€™s ability to identify influence across different engagement strategies and user types. The dataset includes verified accounts representing politicians, journalists, and organizations alongside unverified individual users, allowing an analysis of how formal authority markers relate to actual influence capacity measured through behavioral indicators.An analysis of engagement distribution reveals typical heavy-tailed patterns common in social media interactions, with approximately 5% of tweets generating 70% of total engagement, while most receive little interaction. However, preliminary findings indicate that high-engagement content does not directly correlate with traditional centrality measures, offering initial evidence for the need for behavioral influence metrics that measure audience mobilization capacity rather than structural positioning. The median engagement rate across all users was 1.2%, with notable variation from users achieving rates below 0.1% to outstanding performers exceeding 10% engagement.To provide a concise overview of the dataset and to support the descriptive statistics reported above,Table 1summarizes the key characteristics of the collected corpus.Table 1.Dataset Overview. 4.2. SISI Component Analysis4.2.1. Average Engagement Rate (AER)The analysis of average engagement rates reveals significant variation in usersâ€™ capacity to mobilize their audiences, with the highest-performing users demonstrating engagement rates that exceed typical levels by several orders of magnitude. The top-ranking user achieved an exceptionally high AER, indicating a strong capacity to generate audience interaction relative to follower count and posting frequency. This performance stands in sharp contrast to the userâ€™s more moderate centrality rankings (4th in betweenness centrality, 21st in closeness centrality, and 15th in eigenvector centrality), highlighting a clear divergence between structural positioning within the network and behavioral influence capacity.Contemporary social media benchmarks provide important context for these findings. A recent industry analysis showed that good engagement rates typically fall between 1% and 3% for most social media platforms in 2025, with rates above 3% considered excellent (Social Insider, 2025). Within our dataset, the top 10% of users by AER achieved rates consistently above 5%, placing them in the exceptional performer category according to current industry standards. This exceptional performance occurred despite many of these users having moderate follower counts, supporting the SISIâ€™s theoretical foundation in measuring actualized rather than potential influence.A detailed examination of high-AER users revealed consistent patterns of content that resonates deeply with audiences, generating substantial retweet and reply activity that extends well beyond passive consumption. The top-performing user generated 23,113 retweets and 90,133 likes across 1093 posts, indicating sustained ability to create content that audiences find sufficiently valuable to actively amplify and engage with. This level of engagement reflects genuine influence capacity that traditional metrics fail to capture adequately, as demonstrated by the weak correlation (r = 0.31) between AER scores and combined centrality rankings.The comparative analysis between high-AER and high-centrality users reveals systematic differences in audience mobilization patterns. Users achieving high centrality scores often demonstrate substantial follower counts and network connectivity but generate proportionally lower engagement rates, suggesting that structural positioning does not automatically translate to audience activation capacity. This finding supports the SISIâ€™s theoretical foundation in measuring actualized rather than potential influence while validating recent research emphasizing the importance of engagement quality over quantity metrics.4.2.2. Follower Reach Score (FRS) Distribution and ContextThe follower reach score analysis reveals the importance of contextual normalization for meaningful influence assessments across diverse user types and domains. The highest-scoring user achieved an FRS of 0.48, indicating an audience size substantially above average for their relevant comparison group. The highest-scoring user achieved a scaled FRS of 0.48 (raw FRS = 148), indicating an audience size 48% above the median follower count in their comparison groupHowever, this userâ€™s performance in traditional centrality measures (273rd in betweenness, 30th in closeness, 45th in eigenvector) demonstrates a limited correlation between audience reach and network structural positioning, supporting the theoretical rationale for contextual normalization in influence measurement.The contextual approach embedded in FRS design has proved essential for meaningful comparisons across different user types within the migration discourse dataset. Political commentators, institutional accounts, activists, and ordinary citizens operate within distinct ecosystems where typical follower counts vary dramatically. Recent benchmarking research confirms this variation, showing that follower growth rates differ significantly across industries and account types, with smaller accounts often achieving proportionally higher engagement rates despite lower absolute follower numbers (Wies et al., 2023).The analysis of FRS distribution patterns reveals substantial variation even among users with similar absolute follower counts, reflecting differences in domain contexts and audience development strategies. Users specializing in migration discourse typically maintained smaller but more engaged audiences compared to general political commentators, resulting in higher FRS scores that reflect their specialized influence within relevant communities. This pattern validates the theoretical rationale for contextual normalization while demonstrating the FRSâ€™s capacity to identify users whose audience development exceeds expectations for their particular contexts and domains.Particularly revealing are cases where users with substantial follower counts relative to their domains fail to achieve correspondingly high centrality rankings. One prominent example involves a verified institutional account with a broad audience reach but limited connectivity across network pathways, suggesting concentrated influence within specific follower communities rather than broader network influence. This pattern highlights the distinction between audience potential and network structural importance that FRS normalization helps clarify, supporting the multidimensional approach embedded in SISI design.4.2.3. Mention Prominence Score (MPS) Results and Community RecognitionThe mention prominence score analysis identifies users who have achieved recognition as key discourse participants regardless of their follower counts or network structural positions. The highest-scoring user received 3936 mentions throughout the analysis period, indicating substantial recognition within migration discourse communities. Notably, this user achieved moderate centrality rankings (169th in betweenness, 3rd in closeness, and 1st eigenvector centrality) that do not fully reflect their discourse prominence and community recognition, demonstrating the independent value of mention-based influence assessment.The relationship between mention prominence and traditional centrality measures proves complex and inconsistent across users. While some high-MPS users also achieve strong centrality scores, others demonstrate significant discourse recognition despite limited structural network positioning. This pattern suggests that thought leadership and community recognition operate through mechanisms distinct from formal network connections, supporting the SISIâ€™s multidimensional approach to influence measurement and aligning with recent research on the complexity of social media influence dynamics (Han & Balabanis, 2024).The diversity analysis within MPS calculations reveals that high-scoring users receive mentions from broad ranges of community participants rather than concentrated attention from small follower groups. This broad recognition pattern indicates the genuine community standing rather than artificial prominence generated through coordinated campaigns or narrow follower engagement. The diversity component effectively distinguishes between authentic discourse leadership and manufactured visibility, providing protection against manipulation that has become increasingly important given documented concerns about artificial influence inflation (Annaki et al., 2025;Okoronkwo, 2024).The temporal analysis of mention patterns demonstrates that high-MPS users maintain consistent recognition throughout the analysis period rather than achieving temporary prominence during isolated events. This sustained pattern indicates the established community standing and ongoing thought leadership rather than situational visibility, providing evidence of genuine influence capacity that extends beyond momentary attention. The temporal consistency validates the MPS as a measure of stable influence characteristics rather than temporary phenomena, supporting its utility for practical influence identification applications. 4.3. Integrated SISI Performance and Validation4.3.1. Overall SISI Rankings and Centrality DivergenceThe comprehensive SISI analysis reveals systematic divergence from traditional centrality measures in identifying influential users within the migration discourse. The top five SISI-ranked users demonstrate influence patterns that traditional metrics fail to capture adequately, with several achieving high SISI scores despite moderate or low centrality rankings across multiple measures. This divergence provides strong empirical support for the theoretical argument that influence operates through behavioral mechanisms distinct from network structure properties.To further illustrate the divergence between behavioral influence and structural network positioning,Table 2presents the top 10 users ranked by SISI scores alongside their AER, FRS, and MPS components and corresponding centrality rankings. The table highlights the limited overlap between high SISI scorers and traditional network-based influence indicators.Table 2.Top 10 users by Social Influence Strength Index (SISI) score.The highest SISI-scoring user exemplifies this divergence pattern, ranking 4th in betweenness centrality, 21st in closeness centrality, and 15th in eigenvector centrality. Despite these moderate structural positions, this user demonstrates exceptional capacity for audience mobilization through sustained high engagement rates and substantial content amplification. With over 20,000 users retweeting their content and nearly 100,000 total reactions, this user achieves a clear behavioral influence that centrality measures systematically underestimate.The ranking comparison analysis reveals that only two of the top five SISI users appear in the top ten of any traditional centrality measure, indicating substantial non-overlap between structural and behavioral influence identification. This finding supports the theoretical argument that influence operates through behavioral mechanisms distinct from network structural properties, validating the SISIâ€™s focus on demonstrated rather than assumed influence capacity. The divergence aligns with recent meta-analytic research demonstrating that social media influence effectiveness depends more on content quality and audience engagement than on structural network positioning (Han & Balabanis, 2024).The second-ranked SISI scorer had a notably strong centrality performance (169th in betweenness, 3rd in closeness, 1st in eigenvector), representing a case where behavioral and structural influence align. However, this userâ€™s exceptional performance stems from achieving remarkable engagement efficiency, with only 83 posts generating over 20,000 reactions and nearly 7000 retweets. This efficiency demonstrates influence capacity that extends beyond structural positioning to encompass content quality and audience resonance, supporting the behavioral focus embedded in SISI design.To visualize the relationship between behavioral influence and structural positioning,Figure 1displays a scatterplot of SISI scores against eigenvector centrality with the corresponding Pearson and Spearman correlations, confidence intervals, and sample size. This supports the claim of substantial non-overlap between behavioral and structural influence measures. As both variables exhibit skewed distributions, we report Spearmanâ€™s Ï as our primary metric (with Pearsonâ€™s r included for comparability), along with 95% bootstrap confidence intervals (1000 resamples),p-values, and the sample size (n = 47,892).Figure 1.Scatterplot of SISI scores vs. eigenvector centrality.4.3.2. Behavioral Influence Evidence and Qualityâ€“Quantity PatternsThe SISI analysis reveals consistent patterns where influence stems from content quality and audience resonance rather than posting volume or follower accumulation. Multiple high-scoring users achieve substantial influence with modest posting frequencies, while others with extensive posting activity generate proportionally lower audience mobilization rates. These patterns validate the SISIâ€™s theoretical foundation in measuring actualized influence through behavioral indicators rather than activity metrics, aligning with recent research that emphasizes engagement quality over posting frequency (Mufadhol et al., 2024).Particularly compelling evidence emerges from a comparison between users with similar follower counts but dramatically different SISI scores. Users achieving high behavioral influence demonstrate consistent ability to generate meaningful audience interaction that extends beyond passive consumption to active engagement and content amplification. This pattern indicates genuine influence capacity that motivates audiences to invest effort in sharing, commenting, and extending conversations around influential usersâ€™ content, supporting the behavioral grounding embedded in the SISIâ€™s theoretical foundation.The content amplification analysis reveals that high-SISI users achieve substantially greater retweet rates relative to their follower bases, indicating content that resonates sufficiently to motivate an audience-driven distribution. This organic amplification represents particularly strong evidence of influence, since it reflects the audienceâ€™s choice to actively promote content rather than passive consumption. Users achieving high amplification rates demonstrate the capacity to create content that audiences find valuable enough to associate with their own online identities through sharing, validating the engagement quality focus embedded in AER calculations.The quality versus quantity distinction proves especially important for understanding influence mechanisms within the migration discourse. Users who focus on creating thoughtful, substantive content consistently outperform those who prioritize high-frequency posting, suggesting that audience value perceptions drive influence more effectively than mere visibility or activity. This finding has significant implications for influence strategy and validates the SISIâ€™s emphasis on engagement quality rather than volume metrics, supporting recent industry research emphasizing authentic engagement over superficial activity measures (Nwaiwu et al., 2024). 4.4. Model ValidationTo validate the SISIâ€™s effectiveness, we conducted three validation tests following procedures detailed inSection 4.4. First, for predictive validity, we examined whether high-SISI usersâ€™ subsequent posts (in the final quarter of 2022) maintained elevated engagement rates. Users in the top SISI quartile maintained median engagement rates of 3.8% in subsequent periods, compared to 1.1% for the overall population (Mannâ€“Whitney U = 18,432,p< 0.001), demonstrating temporal stability. Second, for discriminant validity, we compared the SISIâ€™s ability to distinguish between users who generated discourse-shaping content (identified through qualitative coding of highly retweeted original tweets, n = 500) versus those who primarily amplified othersâ€™ content. The SISI scores were significantly higher for content creators (M = 0.42, SD = 0.18) than amplifiers (M = 0.19, SD = 0.12; t(498) = 12.7,p< 0.001), while centrality measures showed no significant difference (eigenvector: t(498) = 1.3,p= 0.19). Third, for convergent validity, expert assessments from three migration researchers rating the influence of 50 randomly selected users correlated significantly with the SISI (Spearmanâ€™s Ï = 0.61,p< 0.001) but weakly with eigenvector centrality (Ï = 0.28,p< 0.05). These validation tests demonstrate that the SISI captures meaningful influence dimensions beyond structural positioning.",
            "4.1. Dataset Overview and Descriptive Statistics": "The data collection process produced a comprehensive dataset of 1.2 million tweets from 47,892 unique users involved in South African migration discussions between 1 January 2021 and 31 December 2022. This extensive dataset provides a solid foundation for validating the SISIâ€™s effectiveness in identifying influential users and allows for detailed comparisons with traditional centrality measures across various user types and engagement patterns. The temporal distribution analysis shows significant variation in discourse activity over the collection period, with notable spikes corresponding to major migration-related events such as the Zimbabwe Exemption Permit policy debates in mid-2021, xenophobic incidents across various provinces, and the rise of digital movements such as Operation Dudula. These activity peaks provided natural experiments for influence measurement, creating periods where usersâ€™ ability to shape discourse and mobilize audiences became especially evident. The average daily tweet volume ranged from 1200 tweets during baseline periods to over 15,000 during peak events, demonstrating the datasetâ€™s capacity to capture both routine discourse and crisis-driven engagement patterns. User participation patterns demonstrate the diverse nature of migration discourse, with participants ranging from highly active political commentators who post hundreds of times each month to occasional contributors who tweet only during major events. This variety creates ideal conditions for testing the SISIâ€™s ability to identify influence across different engagement strategies and user types. The dataset includes verified accounts representing politicians, journalists, and organizations alongside unverified individual users, allowing an analysis of how formal authority markers relate to actual influence capacity measured through behavioral indicators. An analysis of engagement distribution reveals typical heavy-tailed patterns common in social media interactions, with approximately 5% of tweets generating 70% of total engagement, while most receive little interaction. However, preliminary findings indicate that high-engagement content does not directly correlate with traditional centrality measures, offering initial evidence for the need for behavioral influence metrics that measure audience mobilization capacity rather than structural positioning. The median engagement rate across all users was 1.2%, with notable variation from users achieving rates below 0.1% to outstanding performers exceeding 10% engagement. To provide a concise overview of the dataset and to support the descriptive statistics reported above,Table 1summarizes the key characteristics of the collected corpus. Table 1.Dataset Overview.",
            "4.2. SISI Component Analysis": "4.2.1. Average Engagement Rate (AER)The analysis of average engagement rates reveals significant variation in usersâ€™ capacity to mobilize their audiences, with the highest-performing users demonstrating engagement rates that exceed typical levels by several orders of magnitude. The top-ranking user achieved an exceptionally high AER, indicating a strong capacity to generate audience interaction relative to follower count and posting frequency. This performance stands in sharp contrast to the userâ€™s more moderate centrality rankings (4th in betweenness centrality, 21st in closeness centrality, and 15th in eigenvector centrality), highlighting a clear divergence between structural positioning within the network and behavioral influence capacity.Contemporary social media benchmarks provide important context for these findings. A recent industry analysis showed that good engagement rates typically fall between 1% and 3% for most social media platforms in 2025, with rates above 3% considered excellent (Social Insider, 2025). Within our dataset, the top 10% of users by AER achieved rates consistently above 5%, placing them in the exceptional performer category according to current industry standards. This exceptional performance occurred despite many of these users having moderate follower counts, supporting the SISIâ€™s theoretical foundation in measuring actualized rather than potential influence.A detailed examination of high-AER users revealed consistent patterns of content that resonates deeply with audiences, generating substantial retweet and reply activity that extends well beyond passive consumption. The top-performing user generated 23,113 retweets and 90,133 likes across 1093 posts, indicating sustained ability to create content that audiences find sufficiently valuable to actively amplify and engage with. This level of engagement reflects genuine influence capacity that traditional metrics fail to capture adequately, as demonstrated by the weak correlation (r = 0.31) between AER scores and combined centrality rankings.The comparative analysis between high-AER and high-centrality users reveals systematic differences in audience mobilization patterns. Users achieving high centrality scores often demonstrate substantial follower counts and network connectivity but generate proportionally lower engagement rates, suggesting that structural positioning does not automatically translate to audience activation capacity. This finding supports the SISIâ€™s theoretical foundation in measuring actualized rather than potential influence while validating recent research emphasizing the importance of engagement quality over quantity metrics. 4.2.2. Follower Reach Score (FRS) Distribution and ContextThe follower reach score analysis reveals the importance of contextual normalization for meaningful influence assessments across diverse user types and domains. The highest-scoring user achieved an FRS of 0.48, indicating an audience size substantially above average for their relevant comparison group. The highest-scoring user achieved a scaled FRS of 0.48 (raw FRS = 148), indicating an audience size 48% above the median follower count in their comparison groupHowever, this userâ€™s performance in traditional centrality measures (273rd in betweenness, 30th in closeness, 45th in eigenvector) demonstrates a limited correlation between audience reach and network structural positioning, supporting the theoretical rationale for contextual normalization in influence measurement.The contextual approach embedded in FRS design has proved essential for meaningful comparisons across different user types within the migration discourse dataset. Political commentators, institutional accounts, activists, and ordinary citizens operate within distinct ecosystems where typical follower counts vary dramatically. Recent benchmarking research confirms this variation, showing that follower growth rates differ significantly across industries and account types, with smaller accounts often achieving proportionally higher engagement rates despite lower absolute follower numbers (Wies et al., 2023).The analysis of FRS distribution patterns reveals substantial variation even among users with similar absolute follower counts, reflecting differences in domain contexts and audience development strategies. Users specializing in migration discourse typically maintained smaller but more engaged audiences compared to general political commentators, resulting in higher FRS scores that reflect their specialized influence within relevant communities. This pattern validates the theoretical rationale for contextual normalization while demonstrating the FRSâ€™s capacity to identify users whose audience development exceeds expectations for their particular contexts and domains.Particularly revealing are cases where users with substantial follower counts relative to their domains fail to achieve correspondingly high centrality rankings. One prominent example involves a verified institutional account with a broad audience reach but limited connectivity across network pathways, suggesting concentrated influence within specific follower communities rather than broader network influence. This pattern highlights the distinction between audience potential and network structural importance that FRS normalization helps clarify, supporting the multidimensional approach embedded in SISI design. 4.2.3. Mention Prominence Score (MPS) Results and Community RecognitionThe mention prominence score analysis identifies users who have achieved recognition as key discourse participants regardless of their follower counts or network structural positions. The highest-scoring user received 3936 mentions throughout the analysis period, indicating substantial recognition within migration discourse communities. Notably, this user achieved moderate centrality rankings (169th in betweenness, 3rd in closeness, and 1st eigenvector centrality) that do not fully reflect their discourse prominence and community recognition, demonstrating the independent value of mention-based influence assessment.The relationship between mention prominence and traditional centrality measures proves complex and inconsistent across users. While some high-MPS users also achieve strong centrality scores, others demonstrate significant discourse recognition despite limited structural network positioning. This pattern suggests that thought leadership and community recognition operate through mechanisms distinct from formal network connections, supporting the SISIâ€™s multidimensional approach to influence measurement and aligning with recent research on the complexity of social media influence dynamics (Han & Balabanis, 2024).The diversity analysis within MPS calculations reveals that high-scoring users receive mentions from broad ranges of community participants rather than concentrated attention from small follower groups. This broad recognition pattern indicates the genuine community standing rather than artificial prominence generated through coordinated campaigns or narrow follower engagement. The diversity component effectively distinguishes between authentic discourse leadership and manufactured visibility, providing protection against manipulation that has become increasingly important given documented concerns about artificial influence inflation (Annaki et al., 2025;Okoronkwo, 2024).The temporal analysis of mention patterns demonstrates that high-MPS users maintain consistent recognition throughout the analysis period rather than achieving temporary prominence during isolated events. This sustained pattern indicates the established community standing and ongoing thought leadership rather than situational visibility, providing evidence of genuine influence capacity that extends beyond momentary attention. The temporal consistency validates the MPS as a measure of stable influence characteristics rather than temporary phenomena, supporting its utility for practical influence identification applications.",
            "4.2.1. Average Engagement Rate (AER)": "The analysis of average engagement rates reveals significant variation in usersâ€™ capacity to mobilize their audiences, with the highest-performing users demonstrating engagement rates that exceed typical levels by several orders of magnitude. The top-ranking user achieved an exceptionally high AER, indicating a strong capacity to generate audience interaction relative to follower count and posting frequency. This performance stands in sharp contrast to the userâ€™s more moderate centrality rankings (4th in betweenness centrality, 21st in closeness centrality, and 15th in eigenvector centrality), highlighting a clear divergence between structural positioning within the network and behavioral influence capacity. Contemporary social media benchmarks provide important context for these findings. A recent industry analysis showed that good engagement rates typically fall between 1% and 3% for most social media platforms in 2025, with rates above 3% considered excellent (Social Insider, 2025). Within our dataset, the top 10% of users by AER achieved rates consistently above 5%, placing them in the exceptional performer category according to current industry standards. This exceptional performance occurred despite many of these users having moderate follower counts, supporting the SISIâ€™s theoretical foundation in measuring actualized rather than potential influence. A detailed examination of high-AER users revealed consistent patterns of content that resonates deeply with audiences, generating substantial retweet and reply activity that extends well beyond passive consumption. The top-performing user generated 23,113 retweets and 90,133 likes across 1093 posts, indicating sustained ability to create content that audiences find sufficiently valuable to actively amplify and engage with. This level of engagement reflects genuine influence capacity that traditional metrics fail to capture adequately, as demonstrated by the weak correlation (r = 0.31) between AER scores and combined centrality rankings. The comparative analysis between high-AER and high-centrality users reveals systematic differences in audience mobilization patterns. Users achieving high centrality scores often demonstrate substantial follower counts and network connectivity but generate proportionally lower engagement rates, suggesting that structural positioning does not automatically translate to audience activation capacity. This finding supports the SISIâ€™s theoretical foundation in measuring actualized rather than potential influence while validating recent research emphasizing the importance of engagement quality over quantity metrics.",
            "4.2.2. Follower Reach Score (FRS) Distribution and Context": "The follower reach score analysis reveals the importance of contextual normalization for meaningful influence assessments across diverse user types and domains. The highest-scoring user achieved an FRS of 0.48, indicating an audience size substantially above average for their relevant comparison group. The highest-scoring user achieved a scaled FRS of 0.48 (raw FRS = 148), indicating an audience size 48% above the median follower count in their comparison group However, this userâ€™s performance in traditional centrality measures (273rd in betweenness, 30th in closeness, 45th in eigenvector) demonstrates a limited correlation between audience reach and network structural positioning, supporting the theoretical rationale for contextual normalization in influence measurement. The contextual approach embedded in FRS design has proved essential for meaningful comparisons across different user types within the migration discourse dataset. Political commentators, institutional accounts, activists, and ordinary citizens operate within distinct ecosystems where typical follower counts vary dramatically. Recent benchmarking research confirms this variation, showing that follower growth rates differ significantly across industries and account types, with smaller accounts often achieving proportionally higher engagement rates despite lower absolute follower numbers (Wies et al., 2023). The analysis of FRS distribution patterns reveals substantial variation even among users with similar absolute follower counts, reflecting differences in domain contexts and audience development strategies. Users specializing in migration discourse typically maintained smaller but more engaged audiences compared to general political commentators, resulting in higher FRS scores that reflect their specialized influence within relevant communities. This pattern validates the theoretical rationale for contextual normalization while demonstrating the FRSâ€™s capacity to identify users whose audience development exceeds expectations for their particular contexts and domains. Particularly revealing are cases where users with substantial follower counts relative to their domains fail to achieve correspondingly high centrality rankings. One prominent example involves a verified institutional account with a broad audience reach but limited connectivity across network pathways, suggesting concentrated influence within specific follower communities rather than broader network influence. This pattern highlights the distinction between audience potential and network structural importance that FRS normalization helps clarify, supporting the multidimensional approach embedded in SISI design.",
            "4.2.3. Mention Prominence Score (MPS) Results and Community Recognition": "The mention prominence score analysis identifies users who have achieved recognition as key discourse participants regardless of their follower counts or network structural positions. The highest-scoring user received 3936 mentions throughout the analysis period, indicating substantial recognition within migration discourse communities. Notably, this user achieved moderate centrality rankings (169th in betweenness, 3rd in closeness, and 1st eigenvector centrality) that do not fully reflect their discourse prominence and community recognition, demonstrating the independent value of mention-based influence assessment. The relationship between mention prominence and traditional centrality measures proves complex and inconsistent across users. While some high-MPS users also achieve strong centrality scores, others demonstrate significant discourse recognition despite limited structural network positioning. This pattern suggests that thought leadership and community recognition operate through mechanisms distinct from formal network connections, supporting the SISIâ€™s multidimensional approach to influence measurement and aligning with recent research on the complexity of social media influence dynamics (Han & Balabanis, 2024). The diversity analysis within MPS calculations reveals that high-scoring users receive mentions from broad ranges of community participants rather than concentrated attention from small follower groups. This broad recognition pattern indicates the genuine community standing rather than artificial prominence generated through coordinated campaigns or narrow follower engagement. The diversity component effectively distinguishes between authentic discourse leadership and manufactured visibility, providing protection against manipulation that has become increasingly important given documented concerns about artificial influence inflation (Annaki et al., 2025;Okoronkwo, 2024). The temporal analysis of mention patterns demonstrates that high-MPS users maintain consistent recognition throughout the analysis period rather than achieving temporary prominence during isolated events. This sustained pattern indicates the established community standing and ongoing thought leadership rather than situational visibility, providing evidence of genuine influence capacity that extends beyond momentary attention. The temporal consistency validates the MPS as a measure of stable influence characteristics rather than temporary phenomena, supporting its utility for practical influence identification applications.",
            "4.3. Integrated SISI Performance and Validation": "4.3.1. Overall SISI Rankings and Centrality DivergenceThe comprehensive SISI analysis reveals systematic divergence from traditional centrality measures in identifying influential users within the migration discourse. The top five SISI-ranked users demonstrate influence patterns that traditional metrics fail to capture adequately, with several achieving high SISI scores despite moderate or low centrality rankings across multiple measures. This divergence provides strong empirical support for the theoretical argument that influence operates through behavioral mechanisms distinct from network structure properties.To further illustrate the divergence between behavioral influence and structural network positioning,Table 2presents the top 10 users ranked by SISI scores alongside their AER, FRS, and MPS components and corresponding centrality rankings. The table highlights the limited overlap between high SISI scorers and traditional network-based influence indicators.Table 2.Top 10 users by Social Influence Strength Index (SISI) score.The highest SISI-scoring user exemplifies this divergence pattern, ranking 4th in betweenness centrality, 21st in closeness centrality, and 15th in eigenvector centrality. Despite these moderate structural positions, this user demonstrates exceptional capacity for audience mobilization through sustained high engagement rates and substantial content amplification. With over 20,000 users retweeting their content and nearly 100,000 total reactions, this user achieves a clear behavioral influence that centrality measures systematically underestimate.The ranking comparison analysis reveals that only two of the top five SISI users appear in the top ten of any traditional centrality measure, indicating substantial non-overlap between structural and behavioral influence identification. This finding supports the theoretical argument that influence operates through behavioral mechanisms distinct from network structural properties, validating the SISIâ€™s focus on demonstrated rather than assumed influence capacity. The divergence aligns with recent meta-analytic research demonstrating that social media influence effectiveness depends more on content quality and audience engagement than on structural network positioning (Han & Balabanis, 2024).The second-ranked SISI scorer had a notably strong centrality performance (169th in betweenness, 3rd in closeness, 1st in eigenvector), representing a case where behavioral and structural influence align. However, this userâ€™s exceptional performance stems from achieving remarkable engagement efficiency, with only 83 posts generating over 20,000 reactions and nearly 7000 retweets. This efficiency demonstrates influence capacity that extends beyond structural positioning to encompass content quality and audience resonance, supporting the behavioral focus embedded in SISI design.To visualize the relationship between behavioral influence and structural positioning,Figure 1displays a scatterplot of SISI scores against eigenvector centrality with the corresponding Pearson and Spearman correlations, confidence intervals, and sample size. This supports the claim of substantial non-overlap between behavioral and structural influence measures. As both variables exhibit skewed distributions, we report Spearmanâ€™s Ï as our primary metric (with Pearsonâ€™s r included for comparability), along with 95% bootstrap confidence intervals (1000 resamples),p-values, and the sample size (n = 47,892).Figure 1.Scatterplot of SISI scores vs. eigenvector centrality. 4.3.2. Behavioral Influence Evidence and Qualityâ€“Quantity PatternsThe SISI analysis reveals consistent patterns where influence stems from content quality and audience resonance rather than posting volume or follower accumulation. Multiple high-scoring users achieve substantial influence with modest posting frequencies, while others with extensive posting activity generate proportionally lower audience mobilization rates. These patterns validate the SISIâ€™s theoretical foundation in measuring actualized influence through behavioral indicators rather than activity metrics, aligning with recent research that emphasizes engagement quality over posting frequency (Mufadhol et al., 2024).Particularly compelling evidence emerges from a comparison between users with similar follower counts but dramatically different SISI scores. Users achieving high behavioral influence demonstrate consistent ability to generate meaningful audience interaction that extends beyond passive consumption to active engagement and content amplification. This pattern indicates genuine influence capacity that motivates audiences to invest effort in sharing, commenting, and extending conversations around influential usersâ€™ content, supporting the behavioral grounding embedded in the SISIâ€™s theoretical foundation.The content amplification analysis reveals that high-SISI users achieve substantially greater retweet rates relative to their follower bases, indicating content that resonates sufficiently to motivate an audience-driven distribution. This organic amplification represents particularly strong evidence of influence, since it reflects the audienceâ€™s choice to actively promote content rather than passive consumption. Users achieving high amplification rates demonstrate the capacity to create content that audiences find valuable enough to associate with their own online identities through sharing, validating the engagement quality focus embedded in AER calculations.The quality versus quantity distinction proves especially important for understanding influence mechanisms within the migration discourse. Users who focus on creating thoughtful, substantive content consistently outperform those who prioritize high-frequency posting, suggesting that audience value perceptions drive influence more effectively than mere visibility or activity. This finding has significant implications for influence strategy and validates the SISIâ€™s emphasis on engagement quality rather than volume metrics, supporting recent industry research emphasizing authentic engagement over superficial activity measures (Nwaiwu et al., 2024).",
            "4.3.1. Overall SISI Rankings and Centrality Divergence": "The comprehensive SISI analysis reveals systematic divergence from traditional centrality measures in identifying influential users within the migration discourse. The top five SISI-ranked users demonstrate influence patterns that traditional metrics fail to capture adequately, with several achieving high SISI scores despite moderate or low centrality rankings across multiple measures. This divergence provides strong empirical support for the theoretical argument that influence operates through behavioral mechanisms distinct from network structure properties. To further illustrate the divergence between behavioral influence and structural network positioning,Table 2presents the top 10 users ranked by SISI scores alongside their AER, FRS, and MPS components and corresponding centrality rankings. The table highlights the limited overlap between high SISI scorers and traditional network-based influence indicators. Table 2.Top 10 users by Social Influence Strength Index (SISI) score. The highest SISI-scoring user exemplifies this divergence pattern, ranking 4th in betweenness centrality, 21st in closeness centrality, and 15th in eigenvector centrality. Despite these moderate structural positions, this user demonstrates exceptional capacity for audience mobilization through sustained high engagement rates and substantial content amplification. With over 20,000 users retweeting their content and nearly 100,000 total reactions, this user achieves a clear behavioral influence that centrality measures systematically underestimate. The ranking comparison analysis reveals that only two of the top five SISI users appear in the top ten of any traditional centrality measure, indicating substantial non-overlap between structural and behavioral influence identification. This finding supports the theoretical argument that influence operates through behavioral mechanisms distinct from network structural properties, validating the SISIâ€™s focus on demonstrated rather than assumed influence capacity. The divergence aligns with recent meta-analytic research demonstrating that social media influence effectiveness depends more on content quality and audience engagement than on structural network positioning (Han & Balabanis, 2024). The second-ranked SISI scorer had a notably strong centrality performance (169th in betweenness, 3rd in closeness, 1st in eigenvector), representing a case where behavioral and structural influence align. However, this userâ€™s exceptional performance stems from achieving remarkable engagement efficiency, with only 83 posts generating over 20,000 reactions and nearly 7000 retweets. This efficiency demonstrates influence capacity that extends beyond structural positioning to encompass content quality and audience resonance, supporting the behavioral focus embedded in SISI design. To visualize the relationship between behavioral influence and structural positioning,Figure 1displays a scatterplot of SISI scores against eigenvector centrality with the corresponding Pearson and Spearman correlations, confidence intervals, and sample size. This supports the claim of substantial non-overlap between behavioral and structural influence measures. As both variables exhibit skewed distributions, we report Spearmanâ€™s Ï as our primary metric (with Pearsonâ€™s r included for comparability), along with 95% bootstrap confidence intervals (1000 resamples),p-values, and the sample size (n = 47,892). Figure 1.Scatterplot of SISI scores vs. eigenvector centrality.",
            "4.3.2. Behavioral Influence Evidence and Qualityâ€“Quantity Patterns": "The SISI analysis reveals consistent patterns where influence stems from content quality and audience resonance rather than posting volume or follower accumulation. Multiple high-scoring users achieve substantial influence with modest posting frequencies, while others with extensive posting activity generate proportionally lower audience mobilization rates. These patterns validate the SISIâ€™s theoretical foundation in measuring actualized influence through behavioral indicators rather than activity metrics, aligning with recent research that emphasizes engagement quality over posting frequency (Mufadhol et al., 2024). Particularly compelling evidence emerges from a comparison between users with similar follower counts but dramatically different SISI scores. Users achieving high behavioral influence demonstrate consistent ability to generate meaningful audience interaction that extends beyond passive consumption to active engagement and content amplification. This pattern indicates genuine influence capacity that motivates audiences to invest effort in sharing, commenting, and extending conversations around influential usersâ€™ content, supporting the behavioral grounding embedded in the SISIâ€™s theoretical foundation. The content amplification analysis reveals that high-SISI users achieve substantially greater retweet rates relative to their follower bases, indicating content that resonates sufficiently to motivate an audience-driven distribution. This organic amplification represents particularly strong evidence of influence, since it reflects the audienceâ€™s choice to actively promote content rather than passive consumption. Users achieving high amplification rates demonstrate the capacity to create content that audiences find valuable enough to associate with their own online identities through sharing, validating the engagement quality focus embedded in AER calculations. The quality versus quantity distinction proves especially important for understanding influence mechanisms within the migration discourse. Users who focus on creating thoughtful, substantive content consistently outperform those who prioritize high-frequency posting, suggesting that audience value perceptions drive influence more effectively than mere visibility or activity. This finding has significant implications for influence strategy and validates the SISIâ€™s emphasis on engagement quality rather than volume metrics, supporting recent industry research emphasizing authentic engagement over superficial activity measures (Nwaiwu et al., 2024).",
            "4.4. Model Validation": "To validate the SISIâ€™s effectiveness, we conducted three validation tests following procedures detailed inSection 4.4. First, for predictive validity, we examined whether high-SISI usersâ€™ subsequent posts (in the final quarter of 2022) maintained elevated engagement rates. Users in the top SISI quartile maintained median engagement rates of 3.8% in subsequent periods, compared to 1.1% for the overall population (Mannâ€“Whitney U = 18,432,p< 0.001), demonstrating temporal stability. Second, for discriminant validity, we compared the SISIâ€™s ability to distinguish between users who generated discourse-shaping content (identified through qualitative coding of highly retweeted original tweets, n = 500) versus those who primarily amplified othersâ€™ content. The SISI scores were significantly higher for content creators (M = 0.42, SD = 0.18) than amplifiers (M = 0.19, SD = 0.12; t(498) = 12.7,p< 0.001), while centrality measures showed no significant difference (eigenvector: t(498) = 1.3,p= 0.19). Third, for convergent validity, expert assessments from three migration researchers rating the influence of 50 randomly selected users correlated significantly with the SISI (Spearmanâ€™s Ï = 0.61,p< 0.001) but weakly with eigenvector centrality (Ï = 0.28,p< 0.05). These validation tests demonstrate that the SISI captures meaningful influence dimensions beyond structural positioning.",
            "5. Discussion": "5.1. Theoretical Contributions to Computational Social ScienceThe Social Influence Strength Index addresses a persistent problem in computational social scienceâ€”the gap between sophisticated algorithms and theoretical understanding of human behavior (Lazer et al., 2020). Most influence metrics emerged from network science traditions that prioritize computational efficiency over psychological validity. The SISI demonstrates how LatanÃ©â€™s social impact theory can guide metric design, moving beyond ad hoc metric combinations toward theoretically grounded assessment tools.This theoretical grounding matters because social media platforms, despite their digital nature, operate through recognizable social psychological mechanisms. The theoryâ€™s emphasis on â€œstrengthâ€â€”namely the demonstrated capacity rather than structural positionâ€”proves especially relevant for digital environments where influence manifests through audience mobilization rather than network connectivity (Theocharis & Jungherr, 2020). Our analysis of the South African migration discourse supports this theoretical prediction, revealing influential users who generated substantial engagement despite modest follower counts.Contemporary developments in computational social science emphasize the growing need for theoretical grounding in digital analysis methods (Engel, 2023). The SISIâ€™s emphasis on behavioral influence over structural positioning challenges prevailing assumptions in network science that equate centrality with influence capacity, providing empirical evidence that influence operates through demonstrated audience mobilization rather than network positioning alone.The divergence between the SISI and centrality suggests context-dependent utility. Centrality measures excel in broadcast scenarios where information flows unidirectionally through structural hubs, such as breaking news dissemination where exposure potential matters more than engagement depth or public health announcements requiring maximum reach through well-connected nodes. However, in contexts characterized by high confirmation bias and selective engagement such as the polarized migration discourse examined here, the SISI better captures influence because behavioral resonance and relationship strength drive information adoption, not merely structural exposure (Guilbeault & Centola, 2021). Their demonstration that complex contagions requiring peer confirmation systematically undermine centrality-based predictions directly supports the SISIâ€™s behavioral focus; when audiences selectively engage based on content alignment rather than source position, influence manifests through demonstrated mobilization capacity rather than network location. Future research should systematically test the SISI across contexts varying in polarization intensity and network clustering density to map these boundary conditions. 5.2. Implications for Journalism and Digital News EcosystemsThe SISIâ€™s behavioral approach offers particular value for understanding influence in news and information contexts, where traditional metrics often mislead by favoring institutional accounts with large followings over citizen journalists who generate authentic engagement around important issues. This misalignment becomes problematic as audiences increasingly turn to individual voices rather than institutional sources for news.In our migration discourse analysis, the SISI identified users who shaped the direction of conversation through compelling narratives and community engagement, while centrality measures favored accounts that accumulated followers without generating meaningful dialogue. This pattern mirrors broader trends in digital journalism, where authenticity and relatability often outweigh reach in determining influence (Mlambo et al., 2025).The frameworkâ€™s capacity to identify diverse influence pathways proves valuable for understanding the contemporary news ecosystem. Some users excel at detailed analyses that generate deep engagement, while others succeed through broad contextual reach that amplifies key messages. Both patterns represent legitimate forms of journalistic influence that traditional metrics miss, addressing key challenges in understanding how citizen journalists and news influencers build credibility in digital environments. 5.3. Methodological AdvancesThe SISI introduces three methodological innovations that extend beyond this specific application. First, the contextual normalization approach addresses a persistent limitation in influence measurementâ€”the assumption that performance standards are universal rather than domain-specific. By normalizing metrics relative to relevant comparison groups, the SISI enables meaningful assessment across contexts while maintaining sensitivity to exceptional performance within specific domains.Second, the geometric mean integration reflects careful consideration of how influence components interact in real social systems. Social impact theoryâ€™s multiplicative formulation suggests that influence effectiveness depends on the simultaneous presence of multiple factors rather than their independent contributions. Our results support this theoretical expectation in that users who excel in one dimension while underperforming in others rarely achieve high overall SISI scores, indicating that authentic influence requires balanced strength across multiple dimensions.Third, the behavioral focus provides inherent protection against common forms of metric gaming. Users cannot achieve high SISI scores through artificial follower inflation alone, as the engagement efficiency component requires demonstrated audience mobilization. Similarly, purchased mentions from narrow groups fail to generate high MPS scores due to the diversity weighting. This resistance to manipulation proves increasingly important as concerns about inauthentic influence escalate (Okoronkwo, 2024;Annaki et al., 2025). 5.4. Practical ApplicationsThe practical implications of the SISI extend across multiple domains where authentic influence identification provides a competitive advantage. For marketing practitioners, the SISI offers a more sophisticated approach to influencer selection that prioritizes genuine audience engagement over superficial popularity metrics (van der Harst & Angelopoulos, 2024). Industry research shows that 66.4% of marketers found AI-improved influencer marketing campaign performance, yet traditional metrics often fail to capture the quality of influence that drives actual consumer behavior (Enberg, 2025).For policymakers and advocacy organizations, the SISI provides tools for understanding public discourse dynamics and identifying key voices in policy-relevant conversations (Margetts & Dorobantu, 2023). Users who excel in engagement efficiency may be effective for detailed policy communication, while those with broad contextual reach may be valuable for general awareness campaigns. The frameworkâ€™s applications extend to academic researchers studying social media behavior and digital influence. A recent analysis of social media marketing research showed exponential growth in academic interest, with emerging themes requiring sophisticated measurement approaches (Shaheen, 2025). The SISIâ€™s theoretical foundation and behavioral focus open new research questions about influence development in journalism contexts while providing consistent measurement principles for comparative analyses. 5.5. Limitations and Future Research DirectionsSeveral limitations constrain our findings and suggest necessary extensions of this work, and the validation scope remains narrow. Our validation is primarily internal, demonstrating that the SISI differs systematically from centrality measures, without establishing external validity. We have not shown whether high-SISI users actually change opinions, mobilize offline action, or achieve influence outcomes beyond engagement metrics. Future studies should correlate SISI scores with behavioral outcomes (petition signing, event attendance, purchasing decisions) or expert assessments of actual influence to establish predictive validity.Context specificity limits generalizability: Our findings derive from a single platform (Twitter/X), examining the emotionally charged migration discourse in South Africa. This topicâ€™s polarizing nature may favor behavioral metrics over structural measures. The SISI requires testing across diverse contexts including low-engagement topics, professional discussions, breaking news coverage, and platforms with different engagement norms (Instagram, TikTok, LinkedIn) to establish broader applicability. Journalism-specific contexts particularly warrant dedicated investigation, as news influence may involve distinct behavioral patterns such as breaking news dissemination, fact-checking activities, and investigative reporting that require domain-specific SISI adaptations.The temporal dynamics remain unexplored: Our analysis treated influence as static across a two-year period without examining how influence develops or fluctuates. Longitudinal studies could reveal whether the SISI captures stable user characteristics or context-dependent phenomena, and whether influence trajectories follow predictable patterns as users develop audiences and refine strategies.Methodological choices lack empirical justification: While theoretically grounded, our geometric mean integration has not been compared against alternative aggregation methods (arithmetic mean, weighted combinations, machine learning approaches). Sensitivity analyses examining how component weights and integration methods affect rankings would strengthen confidence in our design choices.Gaming resistance remains untested: Although the SISIâ€™s design provides some protection against manipulation through engagement diversity requirements, we did not systematically test its robustness against coordinated inauthentic behavior, bot networks, or sophisticated engagement manipulation. Explicit adversarial testing is needed to establish the SISIâ€™s reliability in environments with strategic gaming.The comparative assessment is incomplete: We compared the SISI only against traditional centrality measures, not against other behavioral metrics or industry influence scores (Klout-style approaches, platform-native metrics). A systematic comparison across diverse influence measurement approaches would clarify the SISIâ€™s relative performance and identify conditions where different metrics prove most suitable.These limitations do not invalidate our core finding that behavioral influence operates through mechanisms distinct from the network structure but they constrain claims about the SISIâ€™s broader applicability and superiority. Addressing these gaps represents an essential next step in establishing the SISI as a robust, generalizable framework for measuring influence.Ethical considerations warrant explicit attention: First, while our analysis used publicly available data and employs anonymization protocols, the behavioral profiling inherent in the SISI raises consent questions. Users posting publicly may not anticipate systematic influence assessments, particularly when such assessments might inform targeting strategies by marketers, political campaigns, or platform moderators. Although legal frameworks typically exempt public data from consent requirements, ethical best practice increasingly demands transparency about how behavioral data enables influence profiling. Second, the SISI may perpetuate algorithmic bias despite its behavioral focus. Platform algorithms already privilege certain engagement types and user characteristics, and the SISIâ€™s reliance on engagement metrics risks amplifying these existing biases, such as favoring users who conform to platform-rewarded content styles or systematically undervaluing influence in marginalized communities with different engagement norms. The metric might also create feedback loops where high-SISI users receive disproportionate attention, further concentrating influence regardless of content quality. Third, the SISIâ€™s practical applications raise dual-use concerns; the same framework identifying authentic influencers for public health campaigns could target manipulation-susceptible users for misinformation or surveillance. Future implementations require careful consideration of these ethical dimensions, including transparent disclosure of influence assessment practices and systematic bias auditing across demographic groups.Future research should explore (1) dynamic SISI tracking to detect influence emergence and decay; (2) causal tests via natural experiments (e.g., suspensions, virality events); (3) cross-platform SISI validation combining X, Facebook, and Reddit data; (4) machine learning integration to predict influence trajectories from SISI components; and (5) adversarial robustness testing against coordinated manipulation strategies.",
            "5.1. Theoretical Contributions to Computational Social Science": "The Social Influence Strength Index addresses a persistent problem in computational social scienceâ€”the gap between sophisticated algorithms and theoretical understanding of human behavior (Lazer et al., 2020). Most influence metrics emerged from network science traditions that prioritize computational efficiency over psychological validity. The SISI demonstrates how LatanÃ©â€™s social impact theory can guide metric design, moving beyond ad hoc metric combinations toward theoretically grounded assessment tools. This theoretical grounding matters because social media platforms, despite their digital nature, operate through recognizable social psychological mechanisms. The theoryâ€™s emphasis on â€œstrengthâ€â€”namely the demonstrated capacity rather than structural positionâ€”proves especially relevant for digital environments where influence manifests through audience mobilization rather than network connectivity (Theocharis & Jungherr, 2020). Our analysis of the South African migration discourse supports this theoretical prediction, revealing influential users who generated substantial engagement despite modest follower counts. Contemporary developments in computational social science emphasize the growing need for theoretical grounding in digital analysis methods (Engel, 2023). The SISIâ€™s emphasis on behavioral influence over structural positioning challenges prevailing assumptions in network science that equate centrality with influence capacity, providing empirical evidence that influence operates through demonstrated audience mobilization rather than network positioning alone. The divergence between the SISI and centrality suggests context-dependent utility. Centrality measures excel in broadcast scenarios where information flows unidirectionally through structural hubs, such as breaking news dissemination where exposure potential matters more than engagement depth or public health announcements requiring maximum reach through well-connected nodes. However, in contexts characterized by high confirmation bias and selective engagement such as the polarized migration discourse examined here, the SISI better captures influence because behavioral resonance and relationship strength drive information adoption, not merely structural exposure (Guilbeault & Centola, 2021). Their demonstration that complex contagions requiring peer confirmation systematically undermine centrality-based predictions directly supports the SISIâ€™s behavioral focus; when audiences selectively engage based on content alignment rather than source position, influence manifests through demonstrated mobilization capacity rather than network location. Future research should systematically test the SISI across contexts varying in polarization intensity and network clustering density to map these boundary conditions.",
            "5.2. Implications for Journalism and Digital News Ecosystems": "The SISIâ€™s behavioral approach offers particular value for understanding influence in news and information contexts, where traditional metrics often mislead by favoring institutional accounts with large followings over citizen journalists who generate authentic engagement around important issues. This misalignment becomes problematic as audiences increasingly turn to individual voices rather than institutional sources for news. In our migration discourse analysis, the SISI identified users who shaped the direction of conversation through compelling narratives and community engagement, while centrality measures favored accounts that accumulated followers without generating meaningful dialogue. This pattern mirrors broader trends in digital journalism, where authenticity and relatability often outweigh reach in determining influence (Mlambo et al., 2025). The frameworkâ€™s capacity to identify diverse influence pathways proves valuable for understanding the contemporary news ecosystem. Some users excel at detailed analyses that generate deep engagement, while others succeed through broad contextual reach that amplifies key messages. Both patterns represent legitimate forms of journalistic influence that traditional metrics miss, addressing key challenges in understanding how citizen journalists and news influencers build credibility in digital environments.",
            "5.3. Methodological Advances": "The SISI introduces three methodological innovations that extend beyond this specific application. First, the contextual normalization approach addresses a persistent limitation in influence measurementâ€”the assumption that performance standards are universal rather than domain-specific. By normalizing metrics relative to relevant comparison groups, the SISI enables meaningful assessment across contexts while maintaining sensitivity to exceptional performance within specific domains. Second, the geometric mean integration reflects careful consideration of how influence components interact in real social systems. Social impact theoryâ€™s multiplicative formulation suggests that influence effectiveness depends on the simultaneous presence of multiple factors rather than their independent contributions. Our results support this theoretical expectation in that users who excel in one dimension while underperforming in others rarely achieve high overall SISI scores, indicating that authentic influence requires balanced strength across multiple dimensions. Third, the behavioral focus provides inherent protection against common forms of metric gaming. Users cannot achieve high SISI scores through artificial follower inflation alone, as the engagement efficiency component requires demonstrated audience mobilization. Similarly, purchased mentions from narrow groups fail to generate high MPS scores due to the diversity weighting. This resistance to manipulation proves increasingly important as concerns about inauthentic influence escalate (Okoronkwo, 2024;Annaki et al., 2025).",
            "5.4. Practical Applications": "The practical implications of the SISI extend across multiple domains where authentic influence identification provides a competitive advantage. For marketing practitioners, the SISI offers a more sophisticated approach to influencer selection that prioritizes genuine audience engagement over superficial popularity metrics (van der Harst & Angelopoulos, 2024). Industry research shows that 66.4% of marketers found AI-improved influencer marketing campaign performance, yet traditional metrics often fail to capture the quality of influence that drives actual consumer behavior (Enberg, 2025). For policymakers and advocacy organizations, the SISI provides tools for understanding public discourse dynamics and identifying key voices in policy-relevant conversations (Margetts & Dorobantu, 2023). Users who excel in engagement efficiency may be effective for detailed policy communication, while those with broad contextual reach may be valuable for general awareness campaigns. The frameworkâ€™s applications extend to academic researchers studying social media behavior and digital influence. A recent analysis of social media marketing research showed exponential growth in academic interest, with emerging themes requiring sophisticated measurement approaches (Shaheen, 2025). The SISIâ€™s theoretical foundation and behavioral focus open new research questions about influence development in journalism contexts while providing consistent measurement principles for comparative analyses.",
            "5.5. Limitations and Future Research Directions": "Several limitations constrain our findings and suggest necessary extensions of this work, and the validation scope remains narrow. Our validation is primarily internal, demonstrating that the SISI differs systematically from centrality measures, without establishing external validity. We have not shown whether high-SISI users actually change opinions, mobilize offline action, or achieve influence outcomes beyond engagement metrics. Future studies should correlate SISI scores with behavioral outcomes (petition signing, event attendance, purchasing decisions) or expert assessments of actual influence to establish predictive validity. Context specificity limits generalizability: Our findings derive from a single platform (Twitter/X), examining the emotionally charged migration discourse in South Africa. This topicâ€™s polarizing nature may favor behavioral metrics over structural measures. The SISI requires testing across diverse contexts including low-engagement topics, professional discussions, breaking news coverage, and platforms with different engagement norms (Instagram, TikTok, LinkedIn) to establish broader applicability. Journalism-specific contexts particularly warrant dedicated investigation, as news influence may involve distinct behavioral patterns such as breaking news dissemination, fact-checking activities, and investigative reporting that require domain-specific SISI adaptations. The temporal dynamics remain unexplored: Our analysis treated influence as static across a two-year period without examining how influence develops or fluctuates. Longitudinal studies could reveal whether the SISI captures stable user characteristics or context-dependent phenomena, and whether influence trajectories follow predictable patterns as users develop audiences and refine strategies. Methodological choices lack empirical justification: While theoretically grounded, our geometric mean integration has not been compared against alternative aggregation methods (arithmetic mean, weighted combinations, machine learning approaches). Sensitivity analyses examining how component weights and integration methods affect rankings would strengthen confidence in our design choices. Gaming resistance remains untested: Although the SISIâ€™s design provides some protection against manipulation through engagement diversity requirements, we did not systematically test its robustness against coordinated inauthentic behavior, bot networks, or sophisticated engagement manipulation. Explicit adversarial testing is needed to establish the SISIâ€™s reliability in environments with strategic gaming. The comparative assessment is incomplete: We compared the SISI only against traditional centrality measures, not against other behavioral metrics or industry influence scores (Klout-style approaches, platform-native metrics). A systematic comparison across diverse influence measurement approaches would clarify the SISIâ€™s relative performance and identify conditions where different metrics prove most suitable. These limitations do not invalidate our core finding that behavioral influence operates through mechanisms distinct from the network structure but they constrain claims about the SISIâ€™s broader applicability and superiority. Addressing these gaps represents an essential next step in establishing the SISI as a robust, generalizable framework for measuring influence. Ethical considerations warrant explicit attention: First, while our analysis used publicly available data and employs anonymization protocols, the behavioral profiling inherent in the SISI raises consent questions. Users posting publicly may not anticipate systematic influence assessments, particularly when such assessments might inform targeting strategies by marketers, political campaigns, or platform moderators. Although legal frameworks typically exempt public data from consent requirements, ethical best practice increasingly demands transparency about how behavioral data enables influence profiling. Second, the SISI may perpetuate algorithmic bias despite its behavioral focus. Platform algorithms already privilege certain engagement types and user characteristics, and the SISIâ€™s reliance on engagement metrics risks amplifying these existing biases, such as favoring users who conform to platform-rewarded content styles or systematically undervaluing influence in marginalized communities with different engagement norms. The metric might also create feedback loops where high-SISI users receive disproportionate attention, further concentrating influence regardless of content quality. Third, the SISIâ€™s practical applications raise dual-use concerns; the same framework identifying authentic influencers for public health campaigns could target manipulation-susceptible users for misinformation or surveillance. Future implementations require careful consideration of these ethical dimensions, including transparent disclosure of influence assessment practices and systematic bias auditing across demographic groups. Future research should explore (1) dynamic SISI tracking to detect influence emergence and decay; (2) causal tests via natural experiments (e.g., suspensions, virality events); (3) cross-platform SISI validation combining X, Facebook, and Reddit data; (4) machine learning integration to predict influence trajectories from SISI components; and (5) adversarial robustness testing against coordinated manipulation strategies."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2673-5172/6/4/205",
        "scraped_at": "2025-12-05 18:23:08"
    },
    {
        "title": "Correction: Zeng et al. High-Performance Silicon Nanowire Array Biosensor for Combined Detection of Colorectal Cancer Biomarkers.Micromachines2025,16, 1089",
        "authors": "byJiaye Zeng,Mingbin Liu,Xin Chen,Jintao Yi,Wenhe Liu,Xinjian Qu,Chaoran Liu,Serestina Viriri,Guangguang Yang,Xun YangandWeichao Yang",
        "journal": "Micromachines2025,16(12), 1381; https://doi.org/10.3390/mi16121381 (registeringÂ DOI) - 5 Dec 2025",
        "abstract": "",
        "keywords": "Keywords not found",
        "full_content": {
            "full_content": "Following publication, we noted that the sequence of the corresponding authors and their email addresses was incorrectly listed in the original publication, and a correction is required. With this correction, the Editorial Office and the authors have made the following amendments to the published article [1]:This correction changes â€œWeichao Yang1,* and Xun Yang1,*â€ to â€œXun Yang1,* and Weichao Yang1,*â€, and â€œCorrespondence: yangwc@cwnu.edu.cn (W.Y.); yangxun@cwnu.edu.cn (X.Y.)â€ to â€œCorrespondence: yangxun@cwnu.edu.cn (X.Y.); yangwc@cwnu.edu.cn (W.Y.)â€.The authors state that the scientific conclusions are unaffected. This correction was approved by the Academic Editor. The original publication has also been updated."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2072-666X/16/12/1381",
        "scraped_at": "2025-12-05 18:23:16"
    },
    {
        "title": "Image Captioning with Object Detection and Facial Expression Recognition for Smart Industry",
        "authors": "byAbdul Saboor Khan,Abdul Haseeb Khan,Muhammad Jamshed AbbassandImran Shafi",
        "journal": "Bioengineering2025,12(12), 1325; https://doi.org/10.3390/bioengineering12121325 (registeringÂ DOI) - 5 Dec 2025",
        "abstract": "This paper presents a new image captioning system which contains facial expression recognition as a way to provide better emotional and contextual comprehension of the captions generated. A combination of affective cues and visual features is made, which enables semantically full and emotionally conscious descriptions. Experiments were carried out on two created datasets, FlickrFace11k and COCOFace15k, with standard benchmarks such as BLEU, METEOR, ROUGE-L, CIDEr, and SPICE to analyze their effectiveness. The suggested model produced better results in all metrics as compared to baselines, like Show-Attend-Tell and Up-Down, remaining consistently better on all the scores. Remarkably, it has reached gains of 2.5 points on CIDEr and 1.0 on SPICE, which means a closer correlation to the prompt captions made by people. A 5-fold cross-validation confirmed the modelâ€™s robustness, with minimal standard deviation across folds (<Â±0.2). Qualitative results further demonstrated its ability to capture fine-grained emotional expressions often missed by conventional models. These findings underscore the modelâ€™s potential in affective computing, assistive technologies, and human-centric AI applications. The pipeline is designed for on-prem/edge deployment with lightweight interfaces to IoT middleware (MQTT/OPC UA), enabling smart-factory integration. These characteristics align the method with Industry 4.0 sensor networks and human-centric analytics.Keywords:facial expression recognition;image captioning;Vision-Language Pre-Training (VLP);deep learning;multimodal deep learning;Convolutional Neural Networks (CNN);object detection;Industry 4.0;IoT;Edge AI;humanâ€“robot collaboration;predictive maintenance;HSE",
        "keywords": "Keywords not found",
        "full_content": {
            "1. Introduction": "Image captioning is a famous computer vision and natural language processing unit that focuses on generating a coherent textual description of an image using visual semantics. It is a simultaneous combination of objects, spatial relations, and verbal nuances, and a grammatical and significant translation of what we would call the visual knowledge-based conventional image captioning. In these approaches, a classic encoderâ€“decoder model is generally applied, such that visual description information is drawn out by convolutional neural networks (CNNs) and then described by recurrent neural networks (RNNs), generally a long short-term memory (LSTM) network, to deliver a text message [1]. Improvement in deep learning has made it possible to improve the quality of generated captions through the incorporation of the attribute of attention, upon which the model is able to focus upon different areas of an image as it generates each word in the generated caption. As an example, Li et al. [2] came up with the so-called Oscar model that combined object-level semantics representation with cross-modal transformers and more successfully promoted the semantic matching between image regions and textual tokens, thereby increasing the accuracy and descriptiveness of the captions. On the same note, another model which uses an object-level feature is the bottom-up and top-down approach developed by Anderson et al. [3], focusing on utilizing Faster R-CNN to enhance the reasoning about salient regions in the model. However, these models largely emphasize the visual information at object or grid level with much less attention paid to emotional cues by human subjects in pictures. Facial expression recognition (FER) has become an auxiliary method to realize the emotional state based on human facial features and allows further semantic interpretation of scenes with people in them. Incorporation of FER into captioning models holds the promise of creating not only factually correct but empathetically aligned captioning models based on the directional affective surfaces approach. At the same time, object detecting systems like YOLO and Faster R-CNN have empowered the granularity and accuracy of visual features to extract, so the model could identify and recognize several objects in a real-time scenario [4,5]. The inclusion of FER and object detection within a single captioning framework implies a considerable level of enrichment of the descriptive and affective aspect of captions. Integration can be helpful towards seeing images more holistically, especially where the concept of human interaction and emotion, as well as contextual behavior, is involved. Such multimodal fusion, however, is less analyzed in existing studies, which tend to look at each of the visual, emotional, and linguistic elements independently, or even in loosely coupled systems [6]. Despite notable progress, several limitations remain in existing image captioning systems. First, conventional models that rely on global CNN features frequently overlook subtle spatial relationships and fail to detect small or contextually important objects. Second, region-based attention mechanisms are constrained by the accuracy of object proposals and may neglect interactions or emotional expressions, especially in crowded or dynamic scenes. Third, although FER has been effectively used in individual cases, its systematic use in the generation of captions is scanty, hence resulting in the lack of emotional depth in captions. Furthermore, the benchmark datasets are usually not annotated parallel in terms of object classes and facial emotions, which is problematic in terms of effective multimodal training and assessment. These shortcomings obstruct the capability of the existing systems in generating semantically and affectively informative captions [7,8]. In Industry 4.0 environments, camera sensors and edge AI enable real-time understanding of humanâ€“machine contexts. Our FER-guided captioning can (i) monitor operator affect and task context for safety/HSE dashboards, (ii) enrich event logs with human-centric semantics for root-cause analysis and predictive maintenance narratives, and (iii) support humanâ€“robot collaboration by turning raw video into structured, explainable summaries. The pipeline is deployable on edge devices and integrates with plant IoT middleware (e.g., MQTT/OPC UA), aligning with the Special Issueâ€™s focus on sensor- and IoT-enabled smart manufacturing. In order to cope with these difficulties, the proposed study suggests a hybrid version of facial expression recognition and object detection based on the YOLOv5 architecture and deep CNN encoder. The proposed method will invoke the combination of the grid features, object-level semantics, and emotional cues, generating the image caption that will be not only contextually accurate but will also touch the emotions. The effectiveness of the method in both descriptive accuracy and expressive power in terms of emotions is proven using the empirical benchmark assessments on the benchmark datasets. This paper is structured as follows. The related works in image captioning, FER, and object detection review are presented inSection 2.Section 3gives a description of the proposed process comprising feature extraction and caption generation. InSection 4, experimental configuration, datasets, and baseline models are described. InSection 5, the outcomes and the comparisons of the performance are reported.Section 6talks about limitations, and theSection 7writes the conclusion of this study and how future research could be conducted.",
            "2. Related Works": "2.1. Overview of Image Captioning MethodsImage captioning is a quickly growing research challenge at the nexus of both computer vision and natural language processing and its basic concept is the translation of visual information into descriptive textual stories. The initial approaches were based on template systems, which are, by nature, too rigid and are not flexible [9]. An important change happened when deep learning approaches were introduced, especially with convolutional neural networks (CNNs) and recurrent neural networks (RNNs), through which the process of image captioning was fundamentally altered [10].A very important encoderâ€“decoder scheme is called Show and Tell and was proposed by Oriol Vinyals et al. [11]. The approach taken, in their case, was basing the model on CNNs to obtain visual features, which were then sequentially addressed by Long Short-Term Memory (LSTM) networks to create captions. Although this method represented a vital innovation since it produced more human-like descriptions, it usually failed to capture vital spatial information and failed to identify complicated associations among components of the image.Afterward, Kelvin Xu et al. presented the model of â€œShow, Attend and Tellâ€, which added attention mechanism to the conventional CNN-RNN model [12]. The model dynamically attaches greater weight to particular areas of images when generating a caption, thereby enhancing relevance of captions and laying much emphasis on those aspects of visual information which are relevant. Nonetheless, even this method had its drawbacks in literally predicting complex object interactions and hidden emotions context.Addressing these shortcomings, Peter Anderson et al. developed a â€œBottom-up and Top-downâ€ attention mechanism [3]. In contrast to earlier approaches that depended solely on grid-based CNN features, recent advancements have introduced region-specific feature extraction using object detection frameworks like EfficientDet [13]. By leveraging a compound scaling method to balance network depth, width, and resolution, EfficientDet enables accurate and computationally efficient object localization, facilitating richer and more structured visual representations. These region-aware features significantly enhance the descriptiveness of generated captions by providing finer details of salient objects within a scene. However, such methods still fall short in capturing emotional subtleties, particularly facial expressions, which are essential for generating semantically complete and context-aware captions, especially in images involving human interactions. 2.2. Object Detection Methods in Image CaptioningAccurate and efficient object detection significantly impacts captioning quality, especially in scenarios requiring detailed contextual understanding. Faster R-CNN by Shaoqing Ren et al. represents a state-of-the-art two-stage detection model utilizing region proposal networks, achieving high precision in object localization [14]. Despite its accuracy, Faster R-CNN is computationally demanding and less suitable for real-time captioning applications.In contrast, YOLO (You Only Look Once) variants, particularly YOLOv5 developed by Glenn Jocher and colleagues [15], offer efficient single-stage detection solutions. YOLOv5 partitions the input image into grids and predicts bounding boxes and class probabilities simultaneously, enabling rapid inference speeds and reduced computational overhead. In this work, we have focused on YOLOv5 because it offers an optimal balance between detection accuracy and real-time processing efficiency, making it highly suitable for applications that require fast and responsive image analysis. Its lightweight architecture and streamlined inference pipeline allow for the simultaneous detection of multiple objects with minimal latency, which aligns with the performance requirements of our proposed image captioning framework. 2.3. Incorporating Facial Expressions into Image CaptioningFacial expressions convey crucial affective information that can significantly enhance the contextual and emotional accuracy of image captions. Traditional image captioning models primarily focus on object detection and scene understanding while overlooking emotional cues, resulting in descriptions that are often semantically correct but emotionally sterile. The integration of facial expression analysis offers a promising direction to bridge this gap, particularly in human-centric images where emotions play a critical role in narrative interpretation.One of the notable projects in this direction is the Face-Cap model provided by Nezami et al. [7], which directly applies facial expression elements to the captioning pipeline. It is based on the model that extracts facial regions with any conventional detection methods and classifies those regions into specific emotional states with deep convolutional networks. These emotion vectors are subsequently embedded in the architecture of the decoder, which will affect word generation according to the feeling level of the characters in the picture. As indicated by the results, Face-Cap produced more emotional captions pertinent to people, which were more contextually relevant in captions of people. Nevertheless, the model does not enjoy full utilization of spatial relationships between objects, thus having a limited descriptive power in more complex scenes.Further than standalone models, such as Face-Cap, recent advancements in facial expression recognition (FER) have allowed the use of emotion features to be used as auxiliary data into multimodal captioning tasks. Often, pretrained CNNs like VGG-Face [16], ResNet50 [17], or more specific emotion architectures trained on a dataset of feelings, e.g., FER2013 [18], are used in FER systems. These nets learn high-dimensional emotion representations of faces which in turn can be merged into the visual object features and global image embedding in united feature space. In emerging FER-guided captioning pipelines, facial emotion vectors are used either to initialize the decoderâ€™s LSTM hidden states or as part of an attention mechanism that guides caption generation. Building on these foundations, Zhou et al. [19] recently proposed ESCNet, an emotional stimuli-aware captioning network that generates affective captions via a fine-tuned LLM to dramatically improve image emotion classification performance, achieving new state-of-the-art results on multiple benchmarks. 2.4. Contextual and Spatial Relationship Modeling in CaptioningThe key issue in the current image captioning frameworks is effective modeling of spatial and contextual relations between objects. In recent studies, the contextual relationships are characterized as playing the determinant role in attaining a realistic description of images. This approach achieved better descriptive quality with spatial and semantic descriptions clearly defined, yet failed at considering emotional properties during caption generation.Moreover, the latest developments suggest the use of hybrid designs combined to use several feature extraction techniques to enhance the accuracy of captions. As shown by Wang et al., the strategy of integrating local object-related features and global features of context on producing a more detailed description of the image turned out to be useful [20]. This end-to-end feature combination was effective compared to those based on deciding on a single level of features (object-level or global-level). 2.5. Evaluation MetricsStandard image captioning evaluation metrics were employed to assess performance, including BLEU [21], METEOR [22], ROUGE-L [23], CIDEr [24], and SPICE [25]. While BLEU and ROUGE measure n-gram overlap and fluency, METEOR aligns closer to human judgment by considering synonyms. Crucially, CIDEr and SPICE are prioritized in this study as they specifically evaluate the semantic consensus and propositional content of captions, which is vital for verifying the integration of emotional context. 2.6. Research Gap and ContributionIn spite of enormous progress, modern technologies have significant drawbacks. First of all, approaches based on a missed perspective can be associated only with object detection, and related context can easily miss the essential information related to emotion that is presented in the form of facial expressions and is essential to the description histories that include human-related interactions [7,17]. On the other hand, the approaches that specifically aim at the analysis of emotions seldom have established strong object detection schemes that subsequently lower the resolution of a complex image scene description [17]. In addition, the existing object detection models, such as Faster R-CNN, are precise but very computationally demanding, restricting their real-time usage [14].It is these gaps that point to the need for a single model able to pinpoint all the other aspects of the research (efficient object detection, detailed spatial relationships, and robust emotional expression analysis) in exactly the same direction as the one being studied in the current research.Table 1, presented below, provides a comparative overview of selected state-of-the-art image captioning methodologies, highlighting their contributions, strengths, and limitations:Table 1.Summary of key methods in image captioning.This paper clearly identifies a distinct research gap: the absence of comprehensive integration of efficient object detection, facial expression analysis, and explicit spatial-contextual modeling. Addressing these limitations, the proposed method integrates YOLOv5 for object detection efficiency, robust facial expression recognition, and contextual modeling to significantly enhance caption accuracy and relevance.",
            "2.1. Overview of Image Captioning Methods": "Image captioning is a quickly growing research challenge at the nexus of both computer vision and natural language processing and its basic concept is the translation of visual information into descriptive textual stories. The initial approaches were based on template systems, which are, by nature, too rigid and are not flexible [9]. An important change happened when deep learning approaches were introduced, especially with convolutional neural networks (CNNs) and recurrent neural networks (RNNs), through which the process of image captioning was fundamentally altered [10]. A very important encoderâ€“decoder scheme is called Show and Tell and was proposed by Oriol Vinyals et al. [11]. The approach taken, in their case, was basing the model on CNNs to obtain visual features, which were then sequentially addressed by Long Short-Term Memory (LSTM) networks to create captions. Although this method represented a vital innovation since it produced more human-like descriptions, it usually failed to capture vital spatial information and failed to identify complicated associations among components of the image. Afterward, Kelvin Xu et al. presented the model of â€œShow, Attend and Tellâ€, which added attention mechanism to the conventional CNN-RNN model [12]. The model dynamically attaches greater weight to particular areas of images when generating a caption, thereby enhancing relevance of captions and laying much emphasis on those aspects of visual information which are relevant. Nonetheless, even this method had its drawbacks in literally predicting complex object interactions and hidden emotions context. Addressing these shortcomings, Peter Anderson et al. developed a â€œBottom-up and Top-downâ€ attention mechanism [3]. In contrast to earlier approaches that depended solely on grid-based CNN features, recent advancements have introduced region-specific feature extraction using object detection frameworks like EfficientDet [13]. By leveraging a compound scaling method to balance network depth, width, and resolution, EfficientDet enables accurate and computationally efficient object localization, facilitating richer and more structured visual representations. These region-aware features significantly enhance the descriptiveness of generated captions by providing finer details of salient objects within a scene. However, such methods still fall short in capturing emotional subtleties, particularly facial expressions, which are essential for generating semantically complete and context-aware captions, especially in images involving human interactions.",
            "2.2. Object Detection Methods in Image Captioning": "Accurate and efficient object detection significantly impacts captioning quality, especially in scenarios requiring detailed contextual understanding. Faster R-CNN by Shaoqing Ren et al. represents a state-of-the-art two-stage detection model utilizing region proposal networks, achieving high precision in object localization [14]. Despite its accuracy, Faster R-CNN is computationally demanding and less suitable for real-time captioning applications. In contrast, YOLO (You Only Look Once) variants, particularly YOLOv5 developed by Glenn Jocher and colleagues [15], offer efficient single-stage detection solutions. YOLOv5 partitions the input image into grids and predicts bounding boxes and class probabilities simultaneously, enabling rapid inference speeds and reduced computational overhead. In this work, we have focused on YOLOv5 because it offers an optimal balance between detection accuracy and real-time processing efficiency, making it highly suitable for applications that require fast and responsive image analysis. Its lightweight architecture and streamlined inference pipeline allow for the simultaneous detection of multiple objects with minimal latency, which aligns with the performance requirements of our proposed image captioning framework.",
            "2.3. Incorporating Facial Expressions into Image Captioning": "Facial expressions convey crucial affective information that can significantly enhance the contextual and emotional accuracy of image captions. Traditional image captioning models primarily focus on object detection and scene understanding while overlooking emotional cues, resulting in descriptions that are often semantically correct but emotionally sterile. The integration of facial expression analysis offers a promising direction to bridge this gap, particularly in human-centric images where emotions play a critical role in narrative interpretation. One of the notable projects in this direction is the Face-Cap model provided by Nezami et al. [7], which directly applies facial expression elements to the captioning pipeline. It is based on the model that extracts facial regions with any conventional detection methods and classifies those regions into specific emotional states with deep convolutional networks. These emotion vectors are subsequently embedded in the architecture of the decoder, which will affect word generation according to the feeling level of the characters in the picture. As indicated by the results, Face-Cap produced more emotional captions pertinent to people, which were more contextually relevant in captions of people. Nevertheless, the model does not enjoy full utilization of spatial relationships between objects, thus having a limited descriptive power in more complex scenes. Further than standalone models, such as Face-Cap, recent advancements in facial expression recognition (FER) have allowed the use of emotion features to be used as auxiliary data into multimodal captioning tasks. Often, pretrained CNNs like VGG-Face [16], ResNet50 [17], or more specific emotion architectures trained on a dataset of feelings, e.g., FER2013 [18], are used in FER systems. These nets learn high-dimensional emotion representations of faces which in turn can be merged into the visual object features and global image embedding in united feature space. In emerging FER-guided captioning pipelines, facial emotion vectors are used either to initialize the decoderâ€™s LSTM hidden states or as part of an attention mechanism that guides caption generation. Building on these foundations, Zhou et al. [19] recently proposed ESCNet, an emotional stimuli-aware captioning network that generates affective captions via a fine-tuned LLM to dramatically improve image emotion classification performance, achieving new state-of-the-art results on multiple benchmarks.",
            "2.4. Contextual and Spatial Relationship Modeling in Captioning": "The key issue in the current image captioning frameworks is effective modeling of spatial and contextual relations between objects. In recent studies, the contextual relationships are characterized as playing the determinant role in attaining a realistic description of images. This approach achieved better descriptive quality with spatial and semantic descriptions clearly defined, yet failed at considering emotional properties during caption generation. Moreover, the latest developments suggest the use of hybrid designs combined to use several feature extraction techniques to enhance the accuracy of captions. As shown by Wang et al., the strategy of integrating local object-related features and global features of context on producing a more detailed description of the image turned out to be useful [20]. This end-to-end feature combination was effective compared to those based on deciding on a single level of features (object-level or global-level).",
            "2.5. Evaluation Metrics": "Standard image captioning evaluation metrics were employed to assess performance, including BLEU [21], METEOR [22], ROUGE-L [23], CIDEr [24], and SPICE [25]. While BLEU and ROUGE measure n-gram overlap and fluency, METEOR aligns closer to human judgment by considering synonyms. Crucially, CIDEr and SPICE are prioritized in this study as they specifically evaluate the semantic consensus and propositional content of captions, which is vital for verifying the integration of emotional context.",
            "2.6. Research Gap and Contribution": "In spite of enormous progress, modern technologies have significant drawbacks. First of all, approaches based on a missed perspective can be associated only with object detection, and related context can easily miss the essential information related to emotion that is presented in the form of facial expressions and is essential to the description histories that include human-related interactions [7,17]. On the other hand, the approaches that specifically aim at the analysis of emotions seldom have established strong object detection schemes that subsequently lower the resolution of a complex image scene description [17]. In addition, the existing object detection models, such as Faster R-CNN, are precise but very computationally demanding, restricting their real-time usage [14]. It is these gaps that point to the need for a single model able to pinpoint all the other aspects of the research (efficient object detection, detailed spatial relationships, and robust emotional expression analysis) in exactly the same direction as the one being studied in the current research. Table 1, presented below, provides a comparative overview of selected state-of-the-art image captioning methodologies, highlighting their contributions, strengths, and limitations: Table 1.Summary of key methods in image captioning. This paper clearly identifies a distinct research gap: the absence of comprehensive integration of efficient object detection, facial expression analysis, and explicit spatial-contextual modeling. Addressing these limitations, the proposed method integrates YOLOv5 for object detection efficiency, robust facial expression recognition, and contextual modeling to significantly enhance caption accuracy and relevance.",
            "3. Proposed Method": "The proposed architecture, as illustrated inFigure 1, integrates facial sentiment analysis, object detection, and global scene understanding to generate emotionally rich image captions. The input image is processed through three parallel modules: (i) a YOLOv5-based object detector with CSP and PANet layers for region-level features; (ii) a FER module (e.g., ResNet50) to extract emotional cues from detected faces; and (iii) a pretrained CNN (e.g., ResNet50) for capturing global context. These features are encoded and passed to an attention-based LSTM decoder, which fuses the information at each time step to produce context-aware and affective captions. Figure 1.Overview of the proposed model combining object, facial expression, and scene features for emotion-aware image captioning. 3.1. Encoder Architecture and Feature ExtractionThe encoder module is responsible for extracting heterogeneous visual representations from the input image, namely (i) region-based object features, (ii) facial expression features, and (iii) global scene features. These are subsequently fused and fed into the decoder for caption generation. This section elaborates each extraction process.3.1.1. Region-Based Object Detection Using YOLOv5Object-level semantics are obtained using the YOLOv5 object detector [15], a single-stage detection network well-suited for real-time processing. YOLOv5 divides the input image into anğ‘†Ã—ğ‘†SÃ—Sgrid. Each cell predicts bounding boxes, objectness confidence, and class probabilities. Specifically, for each detected object, the feature vector includes four bounding box coordinates(ğ‘¥,ğ‘¦,ğ‘¤,â„)x,y,w,h, one objectness score, and 80 class probabilities. Thus, the per-object feature vector has a dimensionality ofğ‘‘ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡=4+1+80=85dobject=4+1+80=85(1)We constrain the model to detect a maximum of 10 objects per image, resulting in a fixed object feature matrix ofğ¹ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡âˆˆR10Ã—85FobjectâˆˆR10Ã—85(2)This matrix is flattened to form a vector of length 850. The class probabilities span 80 standard COCO categories, including â€œperson,â€ â€œdog,â€ â€œcar,â€ â€œchair,â€ etc., which are crucial for semantic grounding [26].The object detection module, depicted inFigure 2, functions as the encoder in the proposed framework. It adopts YOLOv5, leveraging a CSP-based backbone with Bottleneck CSP modules for efficient feature extraction and an SPP layer to capture multi-scale contextual information. A PANet structure further refines these features through1Ã—11Ã—1convolutions, upsampling, and concatenation. The final output generates object-level representations, including bounding box coordinates, confidence scores, and class probabilities for up to ten objects. These semantically rich features provide essential visual cues that are passed to the captioning decoder to enhance the relevance and detail of the generated image descriptions. Algorithm 1 shows the pseudocode of the object detection process.Algorithm 1:Object detection feature extractionInput: Image I (H Ã— W Ã— 3)Output: Object feature matrix F_object âˆˆ â„10Ã—851.Initialize:2.â€ƒâ€ƒâ€ƒLoad pretrained YOLOv5 model3.â€ƒâ€ƒâ€ƒSet S â† grid_size4.â€ƒâ€ƒâ€ƒFobjectâ† zeros(10, 85)5.â€ƒDetect Objects:6.â€ƒâ€ƒâ€ƒdetections â† YOLOv5.forward(I)7.fori = 1 to min(|detections|, 10)do8.â€ƒâ€ƒâ€ƒâ€ƒâ€ƒExtract bounding box: (x, y, w, h)9.â€ƒâ€ƒâ€ƒâ€ƒâ€ƒExtract objectness score: conf10. Â â€ƒâ€ƒâ€ƒâ€ƒExtract class probabilities: p_class âˆˆ â„8011. Â â€ƒâ€ƒâ€ƒâ€ƒFobject[i] â† concatenate[(x, y, w, h), conf, p_class]12.end for13.Â Â Return FobjectFigure 2.Encoder architecture for object feature extraction using BottleNeck CSP and PANet for multi-scale feature fusion.3.1.2. Facial Expression Recognition and Emotion Feature ExtractionOnce the object is detected, the model will then apply recognition of facial emotions that will be used to establish the affective context, helping the model come up with the caption. The Multi-task Cascaded Convolutional Network (MTCNN) [9] is used to pre-localize facial parts, which is robust on obtaining faces in unconstrained scenarios. These identified regions are then input into a facial expression recognition (FER) network, i.e., either VGG-Face or ResNet-50, which have been found to perform well in such emotion classification studies [16,17]. The system codes three faces in an image into a 2048-dimensional feature vectors. In case there are less than three found, the zero-padding is performed to ensure the same size of inputs is used. Such sentiment embeddings are valuable additions to the captioning procedure to give it an emotional context. Each detected face is passed through a convolutional neural network (CNN), yielding a high-level embedding of shape49Ã—204849Ã—2048, where 49 represents the spatial grid(7Ã—7)(7Ã—7)and 2048 is the feature depth. The architecture supports up to three faces per image, resulting in an aggregated feature tensor of size147Ã—2048147Ã—2048. This is denoted asğ¹ğ‘’ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›âˆˆR147Ã—2048FemotionâˆˆR147Ã—2048(3)In scenarios with fewer than three faces, zero-padding is applied to preserve fixed input dimensionality. These emotion vectors are used not only for enriching semantic content but also for initializing the decoder LSTMâ€™s hidden and cell states, thereby influencing the syntactic generation process from the outset [7]. The image shows the process of extracting facial sentiments from detected face regions using an FER model. Detected faces are passed through a CNN-based FER pipeline to generate emotional feature vectors. These vectors encode affective cues that are later used to enrich caption generation as illustrated inFigure 3, this step enables the model to incorporate emotional context by identifying facial expressions from up to three individuals per image. Also, a pseudo code is presented to clear the concept in Algorithm 2.Algorithm 2:Facial expression recognitionInput:Image I (H Ã— W Ã— 3)Output:Emotion feature tensor Femotionâˆˆ â„147Ã—20481.Initialize:2. â€ƒâ€ƒâ€ƒLoad MTCNN face detector3. â€ƒâ€ƒâ€ƒLoad pretrained FER model (ResNet50 or VGG-Face)4. â€ƒâ€ƒâ€ƒFemotionâ† zeros(147, 2048)5. â€ƒâ€ƒâ€ƒmax_faces â† 36.Detect Faces:7. â€ƒâ€ƒâ€ƒface_regions â† MTCNN.detect(I)8. â€ƒâ€ƒâ€ƒnum_faces â† min(|face_regions|, max_faces)9.fori = 1 to num_facesdo10.Â Â â€ƒâ€ƒâ€ƒâ€ƒface â† crop_face(I, face_regions[i])11.Â Â â€ƒâ€ƒâ€ƒâ€ƒface â† resize(face, (224, 224))12.Â Â â€ƒâ€ƒâ€ƒâ€ƒemotion_features â† FER_model.extract(face) â–· Shape: (7 Ã— 7 Ã— 2048)13.Â Â â€ƒâ€ƒâ€ƒâ€ƒemotion_vec â† flatten_spatial(emotion_features) â–· Shape: (49 Ã— 2048)14.Â Â â€ƒâ€ƒâ€ƒâ€ƒstart_idx â† i Ã— 4915.Â Â â€ƒâ€ƒâ€ƒâ€ƒFemotion[start_idx : start_idx + 49] â† emotion_vec16.end for17.ReturnFemotionâ–· Zero-padded if faces < 3Figure 3.Face sentiment extraction using a face detector and FER model to generate emotional features.3.1.3. Global Visual Feature ExtractionGlobal scene understanding is incorporated by extracting grid-based features using ResNet50 [17]. The network processes the entire input image with original dimensions224Ã—224Ã—3224Ã—224Ã—3, producing an output of7Ã—7Ã—20487Ã—7Ã—2048. This is flattened and compressed to a tensor of shape14Ã—204814Ã—2048through average pooling and dimensionality reduction, resulting inğ¹ğ‘”ğ‘Ÿğ‘–ğ‘‘âˆˆR14Ã—2048FgridâˆˆR14Ã—2048(4)These features offer a macro-level view of the scene, capturing contextual relationships that may not be represented in object-specific streams. Algorithm 3 shows the pseudocode for grid features extraction.Algorithm 3:Global feature extractionInput:Image I (224 Ã— 224 Ã— 3)Output: Global feature matrix Fgridâˆˆ â„14Ã—20481.Initialize:2. â€ƒâ€ƒâ€ƒLoad pretrained ResNet503. â€ƒâ€ƒâ€ƒI â† resize(I, (224, 224))4.Extract Features:5. â€ƒâ€ƒâ€ƒfeatures â† ResNet50.forward(I) â–· Output: (7 Ã— 7 Ã— 2048)6. â€ƒâ€ƒâ€ƒFgridâ† adaptive_avg_pool(features, (14, 2048))7.ReturnFgrid 3.2. Decoder Architecture and Caption GenerationThe decoder, illustrated inFigure 4, employs a Long Short-Term Memory (LSTM) architecture to sequentially generate captions. Captioning begins with an initial embedding from the <START> token or an emotion-informed embedding. At each time step, the LSTM unit processes the previous word, integrates an attention-weighted context vector derived from object, global, and emotional features, and updates its internal states accordingly. This attention mechanism helps the decoder focus on the most relevant visual information at each step. The resulting hidden state is passed through a softmax layer to predict the next word in the sequence, continuing until the <END> token is produced.Figure 4.Decoder architecture for caption generation using attention-driven LSTM network.3.2.1. Feature Integration and InitializationThe object and global features are concatenated along the channel axis to form the composite attention matrix:ğ¹ğ‘ğ‘¡ğ‘¡ğ‘›=[ğ¹ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡,ğ¹ğ‘”ğ‘Ÿğ‘–ğ‘‘]âˆˆR(10+14)Ã—2048Fattn=Fobject,FgridâˆˆR10+14Ã—2048(5)This corresponds to 24 spatial tokens, each of 2048 dimensions, and forms the basis for computing attention. The total vector length is850+28,672=29,522850+28,672=29,522, which is retained for reference. The facial expression features are projected using a linear transformation and reshaped into a vector to initialize the LSTMâ€™s hidden(â„ğ‘œ)hoand cell states(ğ‘ğ‘œ)coas follows:â„ğ‘œ,ğ‘ğ‘œ=ğ¿ğ‘†ğ‘‡ğ‘€(ğ‘Šğ‘“.ğ‘£ğ‘’ğ‘(ğ¹ğ‘’ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›))ho,co=LSTMWf.vecFemotion(6)Here,ğ‘Šğ‘“Wfis a trainable weight matrix andğ‘£ğ‘’ğ‘(.)vec(.)denotes flattening of the tensor.3.2.2. Attention MechanismThe attention module dynamically assigns weights to the integrated features based on the decoderâ€™s previous hidden state. At each time stepğ‘¡t, the attention energyğ‘’ğ‘¡ğ‘–eitfor locationğ‘–iis computed by:ğ‘’ğ‘¡ğ‘–=ğ‘£ğ‘¡ğ‘‡ğ‘ğ‘›â„(ğ‘Šâ„â„ğ‘¡âˆ’1+ğ‘Šğ‘ğ‘ğ‘–)eit=vtTanhWhhtâˆ’1+Waai(7)âˆğ‘¡ğ‘–=exp(ğ‘’ğ‘¡ğ‘–)âˆ‘ğ¿ğ‘—=1exp(ğ‘’ğ‘¡ğ‘—)âˆit=expeitâˆ‘j=1Lexpejt(8)ğ‘ğ‘¡=âˆ‘ğ¿ğ‘–=1âˆğ‘¡ğ‘–ğ‘ğ‘–ct=âˆ‘i=1Lâˆitai(9)where the terms are defined as follows:â„ğ‘¡âˆ’1htâˆ’1is the previous hidden state;ğ‘ğ‘–aiis theğ‘–ğ‘¡â„ithvisual feature vector fromğ¹ğ‘ğ‘¡ğ‘¡ğ‘›Fattn;ğ¿=24L=24is the number of attention locations;ğ‘£ğ‘¡,ğ‘Šâ„,ğ‘Šğ‘vt,Wh,Waare trainable parameters;âˆğ‘¡ğ‘–âˆitare the attention weights;ğ‘ğ‘¡ctis the resulting context vector.This formulation is adapted from the standard Bahdanau attention mechanism [12] but extended to multi-source visual embedding.3.2.3. Sequential Caption GenerationAt each decoding step, the LSTM receives the previous word embeddingğ‘¦ğ‘¡âˆ’1ytâˆ’1, the previous hidden state, and the context vectorğ‘ğ‘¡ctto generate the next hidden state and output:â„ğ‘¡,ğ‘ğ‘¡=ğ¿ğ‘†ğ‘‡ğ‘€(ğ‘¦ğ‘¡âˆ’1,â„ğ‘¡âˆ’1,ğ‘ğ‘¡âˆ’1)ht,ct=LSTMytâˆ’1,htâˆ’1,ctâˆ’1(10)The probability distribution over the vocabulary is computed via a softmax function:ğ‘ƒ(ğ‘¦ğ‘¡)=ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘Šğ‘œ.â„ğ‘¡)Pyt=SoftmaxWo.ht(11)whereğ‘Šğ‘œWois a learned projection matrix. The word with the highest probability is selected as the output token for timeğ‘¡t. 3.3. Implementation WorkflowThe methodology was implemented in Python 3.8 using the PyTorch library. The model training and inference were executed on an NVIDIA RTX 4090 GPU with 24 GB VRAM. The implementation followed the sequence below:Dataset Annotation:The COCO and FER2013 datasets were used to label bounding boxes and emotional labels.Model Initialization:Pretrained weights were used by YOLOv5, ResNet50, and VGG-Face.Fine-tuning:The model was trained on the target dataset using the Adam optimizer with a learning rate of1Ã—10âˆ’41Ã—10âˆ’4Evaluation:CIDEr, ROUGE-L, METEOR, and SPICE were used to validate performance, and they include in-depth linguistic plus semantic evaluation [21,22,23,24,25]. 3.4. Sensor/IoT Deployment ConsiderationsSensing and compute.Standard RGB industrial cameras stream 1080p@30 FPS to an edge box (e.g., Jetson Orin/x86 + RTX 4090).Latency.YOLOv5 + FER + LSTM captioning achieves 600 ms on our hardware; captions are emitted at 1â€“2 Hz per stream.Connectivity.Captions/alerts are published via MQTT/OPC UA to the MES/SCADA data bus; payloads include object IDs, emotion tags, and timestamps.Privacy.On-prem processing; no raw video leaves the factory LAN; only structured events are logged.Use cases.Operator fatigue/strain cues, unsafe posture alerts, humanâ€“robot handover summaries, and incident narration for PdM/HSE",
            "3.1. Encoder Architecture and Feature Extraction": "The encoder module is responsible for extracting heterogeneous visual representations from the input image, namely (i) region-based object features, (ii) facial expression features, and (iii) global scene features. These are subsequently fused and fed into the decoder for caption generation. This section elaborates each extraction process. 3.1.1. Region-Based Object Detection Using YOLOv5Object-level semantics are obtained using the YOLOv5 object detector [15], a single-stage detection network well-suited for real-time processing. YOLOv5 divides the input image into anğ‘†Ã—ğ‘†SÃ—Sgrid. Each cell predicts bounding boxes, objectness confidence, and class probabilities. Specifically, for each detected object, the feature vector includes four bounding box coordinates(ğ‘¥,ğ‘¦,ğ‘¤,â„)x,y,w,h, one objectness score, and 80 class probabilities. Thus, the per-object feature vector has a dimensionality ofğ‘‘ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡=4+1+80=85dobject=4+1+80=85(1)We constrain the model to detect a maximum of 10 objects per image, resulting in a fixed object feature matrix ofğ¹ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡âˆˆR10Ã—85FobjectâˆˆR10Ã—85(2)This matrix is flattened to form a vector of length 850. The class probabilities span 80 standard COCO categories, including â€œperson,â€ â€œdog,â€ â€œcar,â€ â€œchair,â€ etc., which are crucial for semantic grounding [26].The object detection module, depicted inFigure 2, functions as the encoder in the proposed framework. It adopts YOLOv5, leveraging a CSP-based backbone with Bottleneck CSP modules for efficient feature extraction and an SPP layer to capture multi-scale contextual information. A PANet structure further refines these features through1Ã—11Ã—1convolutions, upsampling, and concatenation. The final output generates object-level representations, including bounding box coordinates, confidence scores, and class probabilities for up to ten objects. These semantically rich features provide essential visual cues that are passed to the captioning decoder to enhance the relevance and detail of the generated image descriptions. Algorithm 1 shows the pseudocode of the object detection process.Algorithm 1:Object detection feature extractionInput: Image I (H Ã— W Ã— 3)Output: Object feature matrix F_object âˆˆ â„10Ã—851.Initialize:2.â€ƒâ€ƒâ€ƒLoad pretrained YOLOv5 model3.â€ƒâ€ƒâ€ƒSet S â† grid_size4.â€ƒâ€ƒâ€ƒFobjectâ† zeros(10, 85)5.â€ƒDetect Objects:6.â€ƒâ€ƒâ€ƒdetections â† YOLOv5.forward(I)7.fori = 1 to min(|detections|, 10)do8.â€ƒâ€ƒâ€ƒâ€ƒâ€ƒExtract bounding box: (x, y, w, h)9.â€ƒâ€ƒâ€ƒâ€ƒâ€ƒExtract objectness score: conf10. Â â€ƒâ€ƒâ€ƒâ€ƒExtract class probabilities: p_class âˆˆ â„8011. Â â€ƒâ€ƒâ€ƒâ€ƒFobject[i] â† concatenate[(x, y, w, h), conf, p_class]12.end for13.Â Â Return FobjectFigure 2.Encoder architecture for object feature extraction using BottleNeck CSP and PANet for multi-scale feature fusion. 3.1.2. Facial Expression Recognition and Emotion Feature ExtractionOnce the object is detected, the model will then apply recognition of facial emotions that will be used to establish the affective context, helping the model come up with the caption. The Multi-task Cascaded Convolutional Network (MTCNN) [9] is used to pre-localize facial parts, which is robust on obtaining faces in unconstrained scenarios. These identified regions are then input into a facial expression recognition (FER) network, i.e., either VGG-Face or ResNet-50, which have been found to perform well in such emotion classification studies [16,17]. The system codes three faces in an image into a 2048-dimensional feature vectors. In case there are less than three found, the zero-padding is performed to ensure the same size of inputs is used. Such sentiment embeddings are valuable additions to the captioning procedure to give it an emotional context. Each detected face is passed through a convolutional neural network (CNN), yielding a high-level embedding of shape49Ã—204849Ã—2048, where 49 represents the spatial grid(7Ã—7)(7Ã—7)and 2048 is the feature depth. The architecture supports up to three faces per image, resulting in an aggregated feature tensor of size147Ã—2048147Ã—2048. This is denoted asğ¹ğ‘’ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›âˆˆR147Ã—2048FemotionâˆˆR147Ã—2048(3)In scenarios with fewer than three faces, zero-padding is applied to preserve fixed input dimensionality. These emotion vectors are used not only for enriching semantic content but also for initializing the decoder LSTMâ€™s hidden and cell states, thereby influencing the syntactic generation process from the outset [7]. The image shows the process of extracting facial sentiments from detected face regions using an FER model. Detected faces are passed through a CNN-based FER pipeline to generate emotional feature vectors. These vectors encode affective cues that are later used to enrich caption generation as illustrated inFigure 3, this step enables the model to incorporate emotional context by identifying facial expressions from up to three individuals per image. Also, a pseudo code is presented to clear the concept in Algorithm 2.Algorithm 2:Facial expression recognitionInput:Image I (H Ã— W Ã— 3)Output:Emotion feature tensor Femotionâˆˆ â„147Ã—20481.Initialize:2. â€ƒâ€ƒâ€ƒLoad MTCNN face detector3. â€ƒâ€ƒâ€ƒLoad pretrained FER model (ResNet50 or VGG-Face)4. â€ƒâ€ƒâ€ƒFemotionâ† zeros(147, 2048)5. â€ƒâ€ƒâ€ƒmax_faces â† 36.Detect Faces:7. â€ƒâ€ƒâ€ƒface_regions â† MTCNN.detect(I)8. â€ƒâ€ƒâ€ƒnum_faces â† min(|face_regions|, max_faces)9.fori = 1 to num_facesdo10.Â Â â€ƒâ€ƒâ€ƒâ€ƒface â† crop_face(I, face_regions[i])11.Â Â â€ƒâ€ƒâ€ƒâ€ƒface â† resize(face, (224, 224))12.Â Â â€ƒâ€ƒâ€ƒâ€ƒemotion_features â† FER_model.extract(face) â–· Shape: (7 Ã— 7 Ã— 2048)13.Â Â â€ƒâ€ƒâ€ƒâ€ƒemotion_vec â† flatten_spatial(emotion_features) â–· Shape: (49 Ã— 2048)14.Â Â â€ƒâ€ƒâ€ƒâ€ƒstart_idx â† i Ã— 4915.Â Â â€ƒâ€ƒâ€ƒâ€ƒFemotion[start_idx : start_idx + 49] â† emotion_vec16.end for17.ReturnFemotionâ–· Zero-padded if faces < 3Figure 3.Face sentiment extraction using a face detector and FER model to generate emotional features. 3.1.3. Global Visual Feature ExtractionGlobal scene understanding is incorporated by extracting grid-based features using ResNet50 [17]. The network processes the entire input image with original dimensions224Ã—224Ã—3224Ã—224Ã—3, producing an output of7Ã—7Ã—20487Ã—7Ã—2048. This is flattened and compressed to a tensor of shape14Ã—204814Ã—2048through average pooling and dimensionality reduction, resulting inğ¹ğ‘”ğ‘Ÿğ‘–ğ‘‘âˆˆR14Ã—2048FgridâˆˆR14Ã—2048(4)These features offer a macro-level view of the scene, capturing contextual relationships that may not be represented in object-specific streams. Algorithm 3 shows the pseudocode for grid features extraction.Algorithm 3:Global feature extractionInput:Image I (224 Ã— 224 Ã— 3)Output: Global feature matrix Fgridâˆˆ â„14Ã—20481.Initialize:2. â€ƒâ€ƒâ€ƒLoad pretrained ResNet503. â€ƒâ€ƒâ€ƒI â† resize(I, (224, 224))4.Extract Features:5. â€ƒâ€ƒâ€ƒfeatures â† ResNet50.forward(I) â–· Output: (7 Ã— 7 Ã— 2048)6. â€ƒâ€ƒâ€ƒFgridâ† adaptive_avg_pool(features, (14, 2048))7.ReturnFgrid",
            "3.1.1. Region-Based Object Detection Using YOLOv5": "Object-level semantics are obtained using the YOLOv5 object detector [15], a single-stage detection network well-suited for real-time processing. YOLOv5 divides the input image into anğ‘†Ã—ğ‘†SÃ—Sgrid. Each cell predicts bounding boxes, objectness confidence, and class probabilities. Specifically, for each detected object, the feature vector includes four bounding box coordinates(ğ‘¥,ğ‘¦,ğ‘¤,â„)x,y,w,h, one objectness score, and 80 class probabilities. Thus, the per-object feature vector has a dimensionality ofğ‘‘ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡=4+1+80=85dobject=4+1+80=85(1) We constrain the model to detect a maximum of 10 objects per image, resulting in a fixed object feature matrix ofğ¹ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡âˆˆR10Ã—85FobjectâˆˆR10Ã—85(2) This matrix is flattened to form a vector of length 850. The class probabilities span 80 standard COCO categories, including â€œperson,â€ â€œdog,â€ â€œcar,â€ â€œchair,â€ etc., which are crucial for semantic grounding [26]. The object detection module, depicted inFigure 2, functions as the encoder in the proposed framework. It adopts YOLOv5, leveraging a CSP-based backbone with Bottleneck CSP modules for efficient feature extraction and an SPP layer to capture multi-scale contextual information. A PANet structure further refines these features through1Ã—11Ã—1convolutions, upsampling, and concatenation. The final output generates object-level representations, including bounding box coordinates, confidence scores, and class probabilities for up to ten objects. These semantically rich features provide essential visual cues that are passed to the captioning decoder to enhance the relevance and detail of the generated image descriptions. Algorithm 1 shows the pseudocode of the object detection process.Algorithm 1:Object detection feature extractionInput: Image I (H Ã— W Ã— 3)Output: Object feature matrix F_object âˆˆ â„10Ã—851.Initialize:2.â€ƒâ€ƒâ€ƒLoad pretrained YOLOv5 model3.â€ƒâ€ƒâ€ƒSet S â† grid_size4.â€ƒâ€ƒâ€ƒFobjectâ† zeros(10, 85)5.â€ƒDetect Objects:6.â€ƒâ€ƒâ€ƒdetections â† YOLOv5.forward(I)7.fori = 1 to min(|detections|, 10)do8.â€ƒâ€ƒâ€ƒâ€ƒâ€ƒExtract bounding box: (x, y, w, h)9.â€ƒâ€ƒâ€ƒâ€ƒâ€ƒExtract objectness score: conf10. Â â€ƒâ€ƒâ€ƒâ€ƒExtract class probabilities: p_class âˆˆ â„8011. Â â€ƒâ€ƒâ€ƒâ€ƒFobject[i] â† concatenate[(x, y, w, h), conf, p_class]12.end for13.Â Â Return Fobject Figure 2.Encoder architecture for object feature extraction using BottleNeck CSP and PANet for multi-scale feature fusion.",
            "3.1.2. Facial Expression Recognition and Emotion Feature Extraction": "Once the object is detected, the model will then apply recognition of facial emotions that will be used to establish the affective context, helping the model come up with the caption. The Multi-task Cascaded Convolutional Network (MTCNN) [9] is used to pre-localize facial parts, which is robust on obtaining faces in unconstrained scenarios. These identified regions are then input into a facial expression recognition (FER) network, i.e., either VGG-Face or ResNet-50, which have been found to perform well in such emotion classification studies [16,17]. The system codes three faces in an image into a 2048-dimensional feature vectors. In case there are less than three found, the zero-padding is performed to ensure the same size of inputs is used. Such sentiment embeddings are valuable additions to the captioning procedure to give it an emotional context. Each detected face is passed through a convolutional neural network (CNN), yielding a high-level embedding of shape49Ã—204849Ã—2048, where 49 represents the spatial grid(7Ã—7)(7Ã—7)and 2048 is the feature depth. The architecture supports up to three faces per image, resulting in an aggregated feature tensor of size147Ã—2048147Ã—2048. This is denoted asğ¹ğ‘’ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›âˆˆR147Ã—2048FemotionâˆˆR147Ã—2048(3) In scenarios with fewer than three faces, zero-padding is applied to preserve fixed input dimensionality. These emotion vectors are used not only for enriching semantic content but also for initializing the decoder LSTMâ€™s hidden and cell states, thereby influencing the syntactic generation process from the outset [7]. The image shows the process of extracting facial sentiments from detected face regions using an FER model. Detected faces are passed through a CNN-based FER pipeline to generate emotional feature vectors. These vectors encode affective cues that are later used to enrich caption generation as illustrated inFigure 3, this step enables the model to incorporate emotional context by identifying facial expressions from up to three individuals per image. Also, a pseudo code is presented to clear the concept in Algorithm 2.Algorithm 2:Facial expression recognitionInput:Image I (H Ã— W Ã— 3)Output:Emotion feature tensor Femotionâˆˆ â„147Ã—20481.Initialize:2. â€ƒâ€ƒâ€ƒLoad MTCNN face detector3. â€ƒâ€ƒâ€ƒLoad pretrained FER model (ResNet50 or VGG-Face)4. â€ƒâ€ƒâ€ƒFemotionâ† zeros(147, 2048)5. â€ƒâ€ƒâ€ƒmax_faces â† 36.Detect Faces:7. â€ƒâ€ƒâ€ƒface_regions â† MTCNN.detect(I)8. â€ƒâ€ƒâ€ƒnum_faces â† min(|face_regions|, max_faces)9.fori = 1 to num_facesdo10.Â Â â€ƒâ€ƒâ€ƒâ€ƒface â† crop_face(I, face_regions[i])11.Â Â â€ƒâ€ƒâ€ƒâ€ƒface â† resize(face, (224, 224))12.Â Â â€ƒâ€ƒâ€ƒâ€ƒemotion_features â† FER_model.extract(face) â–· Shape: (7 Ã— 7 Ã— 2048)13.Â Â â€ƒâ€ƒâ€ƒâ€ƒemotion_vec â† flatten_spatial(emotion_features) â–· Shape: (49 Ã— 2048)14.Â Â â€ƒâ€ƒâ€ƒâ€ƒstart_idx â† i Ã— 4915.Â Â â€ƒâ€ƒâ€ƒâ€ƒFemotion[start_idx : start_idx + 49] â† emotion_vec16.end for17.ReturnFemotionâ–· Zero-padded if faces < 3 Figure 3.Face sentiment extraction using a face detector and FER model to generate emotional features.",
            "3.1.3. Global Visual Feature Extraction": "Global scene understanding is incorporated by extracting grid-based features using ResNet50 [17]. The network processes the entire input image with original dimensions224Ã—224Ã—3224Ã—224Ã—3, producing an output of7Ã—7Ã—20487Ã—7Ã—2048. This is flattened and compressed to a tensor of shape14Ã—204814Ã—2048through average pooling and dimensionality reduction, resulting inğ¹ğ‘”ğ‘Ÿğ‘–ğ‘‘âˆˆR14Ã—2048FgridâˆˆR14Ã—2048(4) These features offer a macro-level view of the scene, capturing contextual relationships that may not be represented in object-specific streams. Algorithm 3 shows the pseudocode for grid features extraction.Algorithm 3:Global feature extractionInput:Image I (224 Ã— 224 Ã— 3)Output: Global feature matrix Fgridâˆˆ â„14Ã—20481.Initialize:2. â€ƒâ€ƒâ€ƒLoad pretrained ResNet503. â€ƒâ€ƒâ€ƒI â† resize(I, (224, 224))4.Extract Features:5. â€ƒâ€ƒâ€ƒfeatures â† ResNet50.forward(I) â–· Output: (7 Ã— 7 Ã— 2048)6. â€ƒâ€ƒâ€ƒFgridâ† adaptive_avg_pool(features, (14, 2048))7.ReturnFgrid",
            "3.2. Decoder Architecture and Caption Generation": "The decoder, illustrated inFigure 4, employs a Long Short-Term Memory (LSTM) architecture to sequentially generate captions. Captioning begins with an initial embedding from the <START> token or an emotion-informed embedding. At each time step, the LSTM unit processes the previous word, integrates an attention-weighted context vector derived from object, global, and emotional features, and updates its internal states accordingly. This attention mechanism helps the decoder focus on the most relevant visual information at each step. The resulting hidden state is passed through a softmax layer to predict the next word in the sequence, continuing until the <END> token is produced. Figure 4.Decoder architecture for caption generation using attention-driven LSTM network. 3.2.1. Feature Integration and InitializationThe object and global features are concatenated along the channel axis to form the composite attention matrix:ğ¹ğ‘ğ‘¡ğ‘¡ğ‘›=[ğ¹ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡,ğ¹ğ‘”ğ‘Ÿğ‘–ğ‘‘]âˆˆR(10+14)Ã—2048Fattn=Fobject,FgridâˆˆR10+14Ã—2048(5)This corresponds to 24 spatial tokens, each of 2048 dimensions, and forms the basis for computing attention. The total vector length is850+28,672=29,522850+28,672=29,522, which is retained for reference. The facial expression features are projected using a linear transformation and reshaped into a vector to initialize the LSTMâ€™s hidden(â„ğ‘œ)hoand cell states(ğ‘ğ‘œ)coas follows:â„ğ‘œ,ğ‘ğ‘œ=ğ¿ğ‘†ğ‘‡ğ‘€(ğ‘Šğ‘“.ğ‘£ğ‘’ğ‘(ğ¹ğ‘’ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›))ho,co=LSTMWf.vecFemotion(6)Here,ğ‘Šğ‘“Wfis a trainable weight matrix andğ‘£ğ‘’ğ‘(.)vec(.)denotes flattening of the tensor. 3.2.2. Attention MechanismThe attention module dynamically assigns weights to the integrated features based on the decoderâ€™s previous hidden state. At each time stepğ‘¡t, the attention energyğ‘’ğ‘¡ğ‘–eitfor locationğ‘–iis computed by:ğ‘’ğ‘¡ğ‘–=ğ‘£ğ‘¡ğ‘‡ğ‘ğ‘›â„(ğ‘Šâ„â„ğ‘¡âˆ’1+ğ‘Šğ‘ğ‘ğ‘–)eit=vtTanhWhhtâˆ’1+Waai(7)âˆğ‘¡ğ‘–=exp(ğ‘’ğ‘¡ğ‘–)âˆ‘ğ¿ğ‘—=1exp(ğ‘’ğ‘¡ğ‘—)âˆit=expeitâˆ‘j=1Lexpejt(8)ğ‘ğ‘¡=âˆ‘ğ¿ğ‘–=1âˆğ‘¡ğ‘–ğ‘ğ‘–ct=âˆ‘i=1Lâˆitai(9)where the terms are defined as follows:â„ğ‘¡âˆ’1htâˆ’1is the previous hidden state;ğ‘ğ‘–aiis theğ‘–ğ‘¡â„ithvisual feature vector fromğ¹ğ‘ğ‘¡ğ‘¡ğ‘›Fattn;ğ¿=24L=24is the number of attention locations;ğ‘£ğ‘¡,ğ‘Šâ„,ğ‘Šğ‘vt,Wh,Waare trainable parameters;âˆğ‘¡ğ‘–âˆitare the attention weights;ğ‘ğ‘¡ctis the resulting context vector.This formulation is adapted from the standard Bahdanau attention mechanism [12] but extended to multi-source visual embedding. 3.2.3. Sequential Caption GenerationAt each decoding step, the LSTM receives the previous word embeddingğ‘¦ğ‘¡âˆ’1ytâˆ’1, the previous hidden state, and the context vectorğ‘ğ‘¡ctto generate the next hidden state and output:â„ğ‘¡,ğ‘ğ‘¡=ğ¿ğ‘†ğ‘‡ğ‘€(ğ‘¦ğ‘¡âˆ’1,â„ğ‘¡âˆ’1,ğ‘ğ‘¡âˆ’1)ht,ct=LSTMytâˆ’1,htâˆ’1,ctâˆ’1(10)The probability distribution over the vocabulary is computed via a softmax function:ğ‘ƒ(ğ‘¦ğ‘¡)=ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘Šğ‘œ.â„ğ‘¡)Pyt=SoftmaxWo.ht(11)whereğ‘Šğ‘œWois a learned projection matrix. The word with the highest probability is selected as the output token for timeğ‘¡t.",
            "3.2.1. Feature Integration and Initialization": "The object and global features are concatenated along the channel axis to form the composite attention matrix:ğ¹ğ‘ğ‘¡ğ‘¡ğ‘›=[ğ¹ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡,ğ¹ğ‘”ğ‘Ÿğ‘–ğ‘‘]âˆˆR(10+14)Ã—2048Fattn=Fobject,FgridâˆˆR10+14Ã—2048(5) This corresponds to 24 spatial tokens, each of 2048 dimensions, and forms the basis for computing attention. The total vector length is850+28,672=29,522850+28,672=29,522, which is retained for reference. The facial expression features are projected using a linear transformation and reshaped into a vector to initialize the LSTMâ€™s hidden(â„ğ‘œ)hoand cell states(ğ‘ğ‘œ)coas follows:â„ğ‘œ,ğ‘ğ‘œ=ğ¿ğ‘†ğ‘‡ğ‘€(ğ‘Šğ‘“.ğ‘£ğ‘’ğ‘(ğ¹ğ‘’ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›))ho,co=LSTMWf.vecFemotion(6) Here,ğ‘Šğ‘“Wfis a trainable weight matrix andğ‘£ğ‘’ğ‘(.)vec(.)denotes flattening of the tensor.",
            "3.2.2. Attention Mechanism": "The attention module dynamically assigns weights to the integrated features based on the decoderâ€™s previous hidden state. At each time stepğ‘¡t, the attention energyğ‘’ğ‘¡ğ‘–eitfor locationğ‘–iis computed by:ğ‘’ğ‘¡ğ‘–=ğ‘£ğ‘¡ğ‘‡ğ‘ğ‘›â„(ğ‘Šâ„â„ğ‘¡âˆ’1+ğ‘Šğ‘ğ‘ğ‘–)eit=vtTanhWhhtâˆ’1+Waai(7)âˆğ‘¡ğ‘–=exp(ğ‘’ğ‘¡ğ‘–)âˆ‘ğ¿ğ‘—=1exp(ğ‘’ğ‘¡ğ‘—)âˆit=expeitâˆ‘j=1Lexpejt(8)ğ‘ğ‘¡=âˆ‘ğ¿ğ‘–=1âˆğ‘¡ğ‘–ğ‘ğ‘–ct=âˆ‘i=1Lâˆitai(9)where the terms are defined as follows: â„ğ‘¡âˆ’1htâˆ’1is the previous hidden state;ğ‘ğ‘–aiis theğ‘–ğ‘¡â„ithvisual feature vector fromğ¹ğ‘ğ‘¡ğ‘¡ğ‘›Fattn;ğ¿=24L=24is the number of attention locations;ğ‘£ğ‘¡,ğ‘Šâ„,ğ‘Šğ‘vt,Wh,Waare trainable parameters;âˆğ‘¡ğ‘–âˆitare the attention weights;ğ‘ğ‘¡ctis the resulting context vector. This formulation is adapted from the standard Bahdanau attention mechanism [12] but extended to multi-source visual embedding.",
            "3.2.3. Sequential Caption Generation": "At each decoding step, the LSTM receives the previous word embeddingğ‘¦ğ‘¡âˆ’1ytâˆ’1, the previous hidden state, and the context vectorğ‘ğ‘¡ctto generate the next hidden state and output:â„ğ‘¡,ğ‘ğ‘¡=ğ¿ğ‘†ğ‘‡ğ‘€(ğ‘¦ğ‘¡âˆ’1,â„ğ‘¡âˆ’1,ğ‘ğ‘¡âˆ’1)ht,ct=LSTMytâˆ’1,htâˆ’1,ctâˆ’1(10) The probability distribution over the vocabulary is computed via a softmax function:ğ‘ƒ(ğ‘¦ğ‘¡)=ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘Šğ‘œ.â„ğ‘¡)Pyt=SoftmaxWo.ht(11)whereğ‘Šğ‘œWois a learned projection matrix. The word with the highest probability is selected as the output token for timeğ‘¡t.",
            "3.3. Implementation Workflow": "The methodology was implemented in Python 3.8 using the PyTorch library. The model training and inference were executed on an NVIDIA RTX 4090 GPU with 24 GB VRAM. The implementation followed the sequence below: Dataset Annotation:The COCO and FER2013 datasets were used to label bounding boxes and emotional labels.Model Initialization:Pretrained weights were used by YOLOv5, ResNet50, and VGG-Face.Fine-tuning:The model was trained on the target dataset using the Adam optimizer with a learning rate of1Ã—10âˆ’41Ã—10âˆ’4Evaluation:CIDEr, ROUGE-L, METEOR, and SPICE were used to validate performance, and they include in-depth linguistic plus semantic evaluation [21,22,23,24,25].",
            "3.4. Sensor/IoT Deployment Considerations": "Sensing and compute.Standard RGB industrial cameras stream 1080p@30 FPS to an edge box (e.g., Jetson Orin/x86 + RTX 4090).Latency.YOLOv5 + FER + LSTM captioning achieves 600 ms on our hardware; captions are emitted at 1â€“2 Hz per stream.Connectivity.Captions/alerts are published via MQTT/OPC UA to the MES/SCADA data bus; payloads include object IDs, emotion tags, and timestamps.Privacy.On-prem processing; no raw video leaves the factory LAN; only structured events are logged.Use cases.Operator fatigue/strain cues, unsafe posture alerts, humanâ€“robot handover summaries, and incident narration for PdM/HSE",
            "4. Experiments": "This subsection describes the experimental environment with which the usefulness of the suggestive facial-expression-augmented image captioning model should be assessed. It contains the description of the datasets, baseline models, the specifications of the implementation, evaluation protocols, and training approaches. Experiments have been performed in a controlled environment, where each experiment can be replicated to establish a fair comparison of results to the state-of-the-art techniques. 4.1. DatasetsTo fully test the proposed approach, three publicly available datasets were employed as FlickrFace 11k [7], COCOFace 15k [26], and FER2013 [18]. These data have been chosen in order to represent the diversity of the object instances, facial emotions, and human-oriented scenes in reality.4.1.1. FlickrFace 11kFlickrFace 11k means a narrow selection of the Flickr30k picture captioning corpus, editorialized to prefer pictures of human faces and expressions. It includes 11,000 high-resolution visuals, all of which are assigned five captions written by a human being that relate to various human acts and expressions. The dataset is split accordingly with 9000 training images, 1000 validation ones, and the same of the testing ones in accordance with the Karpathy split protocol [7]. The screening has been made with regard to diversity in terms of demography and emotions.Figure 5a,b show sample images capturing expressions such as joy, curiosity, and contemplation. Its emphasis on facial visibility makes it well-suited for training emotion-aware captioning models.Figure 5.(a) A group of people smiling and posing together at what appears to be a social gathering or party. (b) A musical performance on stage with multiple performers singing and playing instruments under concert lighting.4.1.2. COCOFace 15kThe COCOFace 15k dataset is a filtered subset of the COCO 2017 dataset [26], consisting of 15,000 images with visible human faces. It is split into 13,000 training and 2000 validation/test samples. Each image includes multiple captions and bounding boxes, supporting joint learning of object semantics and visual localization. The dataset offers wide variability in facial orientation, lighting, and occlusion, making it ideal for training facial expression models and evaluating captioning systems in real-world scenarios.Figure 6a,b showcases its contextual and emotional diversity, positioning COCOFace 15k as a strong benchmark for multimodal captioning research.Figure 6.(a) A young child sitting on a couch and holding a colorful umbrella, smiling toward the camera. (b) Two adults dressed formally, engaging in a handshake or exchange during what appears to be an outdoor event.4.1.3. FER2013The FER2013 dataset is a well-established benchmark in facial expression recognition research and is extensively used in affective computing [18]. It consists of 35,887 grayscale images with a resolution of 48 Ã— 48 pixels. Each image represents a cropped frontal face, labeled with one of seven emotion classes: anger, disgust, fear, happiness, sadness, surprise, or neutral. The dataset is organized into 28,709 training images, 3589 for validation, and 3589 for testing. Due to its wide usage and consistent labeling, FER2013 serves as a reliable source for pretraining the facial expression recognition component of the proposed model.Figure 7aâ€“c depict the emotional variety in FER2013, underscoring its value for facial expression recognition in vision-language captioning.Table 2presents a short statistics of the datasets used.Figure 7.Sample images from the FER2013 dataset displaying distinct emotional expressions: (a) surprise, (b) fear, and (c) happiness [18].Table 2.Summary of datasets used in experiments. 4.2. Baseline ModelsTo benchmark the proposed model, we compare it against two key attention-based image captioning frameworks: Show, Attend and Tell by Xu et al. [12] and the Bottom-Up and Top-Down Attention model by Anderson et al. [3]. These baselines serve as strong references to assess the impact of incorporating facial expression features and real-time object detection into caption generation.The Show, Attend and Tell model is especially relevant to our approach, as it pioneered the integration of soft attention mechanisms within a CNN-LSTM architecture. By allowing the decoder to focus dynamically on different spatial regions of an image, the model improved contextual alignment between visual content and generated words. We adopt this structure as a foundational benchmark, as our model extends its architecture by introducing emotion-based initialization through facial expression features and enhancing object localization using YOLOv5. Comparison with this model helps quantify the direct contribution of emotional cues to caption fluency and relevance.The Bottom-Up and Top-Down Attention model, while more complex, serves as a comparison for evaluating object-level attention. It uses Faster R-CNN to extract detailed region proposals and applies top-down attention to guide captioning. Though accurate, it is computationally slower than our YOLOv5-based approach. This comparison highlights how our method maintains semantic richness while improving inference efficiency.Both baselines were retrained on FlickrFace 11k and COCOFace 15k using identical splits and preprocessing to ensure objective performance evaluation. 4.3. Implementation DetailsThe proposed model was implemented using Python 3.8 and the PyTorch 1.8.1 deep learning framework (version 1.13). The experiments were conducted on a system equipped with the hardware and software specifications detailed inTable 3.Table 3.System specifications used for training and evaluating the proposed model.For image preprocessing, the OpenCV and Albumentations libraries were employed. Dataset management and augmentation were handled via custom PyTorch dataset and DataLoader classes. The implementation supports modular training, allowing for separate pretraining of object detection and emotion recognition components before joint optimization. All models were trained with reproducible random seeds, and results were averaged over three runs to reduce variance. 4.4. Evaluation MetricsWe evaluated caption quality using five standard metrics: BLEU, METEOR, ROUGE-L, CIDEr, and SPICE. BLEU@N measures n-gram precision [21], while METEOR captures word alignment, synonyms, and ordering [22]. ROUGE-L assesses fluency through longest common subsequence matching [23]. CIDEr evaluates consensus with multiple references [24], and SPICE focuses on semantic content using scene graph comparisons [25]. These metrics collectively assess both linguistic accuracy and semantic relevance. 4.5. Training ProcedureThe training strategy is divided into two stages to leverage pretrained knowledge while allowing end-to-end optimization.4.5.1. Stage 1: Pretraining of Feature ExtractorsIn the first stage, pretrained models are used for individual components:YOLOv5 is initialized with weights trained on the COCO dataset.ResNet50 and VGG-Face are used for extracting grid and facial features, respectively.FER-CNN is pretrained on FER2013 to classify facial expressions.These pretrained models are frozen during initial training to prevent overfitting and to ensure stability in feature representations.4.5.2. Stage 2: End-to-End Captioning Model TrainingIn the second stage, the complete architecture, including the LSTM decoder and attention mechanism, is fine-tuned in an end-to-end manner. The word embedding layer is uniformly initialized with a dimensionality of 300, while the LSTMâ€™s hidden and cell states are configured with 512 units each. Optimization is carried out using the Adam optimizer, beginning with an initial learning rate of 0.001. To encourage convergence, the learning rate is reduced to 0.0001 if the METEOR score does not improve over two consecutive validation epochs. The model is trained for a maximum of 30 epochs using a mini-batch size of 64. To mitigate overfitting and stabilize training, dropout is applied at a rate of 0.5, and gradient clipping is enforced with a maximum norm of 5. Initially, the model is trained using Cross-Entropy Loss to maximize likelihood of ground-truth tokens. Subsequently, CIDEr optimization is applied using a reinforcement learning objective to better align model predictions with human consensus. 4.6. Training with Cross-Entropy LossIn the initial training phase, the model is optimized using a cross-entropy loss function, which encourages the decoder to generate word sequences that closely match the ground truth captions. The loss is formally defined as follows:ğ¿ğ¶ğ¸=âˆ’âˆ‘ğ‘‡ğ‘¡=1ğ‘™ğ‘œğ‘”ğ‘(ğ‘¦ğ‘¡|ğ‘¦1:ğ‘¡âˆ’1,ğ‘)LCE=âˆ’âˆ‘t=1Tlogpyt|y1:tâˆ’1,a(12)whereğ‘¦ğ‘¡ytis the ground truth word at time stepğ‘¡t, andğ‘(ğ‘¦ğ‘¡|ğ‘¦1:ğ‘¡âˆ’1,ğ‘)pyt|y1:tâˆ’1,ais the predicted probability conditioned on the previous words and attention-based visual contextğ‘a. 4.7. Optimization with CIDEr Score (Reinforcement Learning)To further refine the caption generation toward human-like quality, the model is fine-tuned using reinforcement learning with the CIDEr metric as a reward signal. The reinforcement learning loss is defined as follows:ğ¿ğ‘…ğ¿=âˆ’âˆ‘ğ‘‡ğ‘¡=1(ğ¶ğ¼ğ·ğ¸ğ‘Ÿ(ğ‘¦,ğ‘¦Ì‚)âˆ’ğ‘)ğ‘™ğ‘œğ‘”ğ‘(ğ‘¦ğ‘¡|ğ‘¦1:ğ‘¡âˆ’1,ğ‘)LRL=âˆ’âˆ‘t=1TCIDEry,y^âˆ’blogpyt|y1:tâˆ’1,a(13)whereğ‘¦Ì‚y^is the generated caption,ğ‘¦yis the reference caption, andğ‘brepresents the baseline reward to reduce variance. This will bring consistency between the training goal and the evaluation measurement adopted in testing, so as to make the captions more relevant and fluent.",
            "4.1. Datasets": "To fully test the proposed approach, three publicly available datasets were employed as FlickrFace 11k [7], COCOFace 15k [26], and FER2013 [18]. These data have been chosen in order to represent the diversity of the object instances, facial emotions, and human-oriented scenes in reality. 4.1.1. FlickrFace 11kFlickrFace 11k means a narrow selection of the Flickr30k picture captioning corpus, editorialized to prefer pictures of human faces and expressions. It includes 11,000 high-resolution visuals, all of which are assigned five captions written by a human being that relate to various human acts and expressions. The dataset is split accordingly with 9000 training images, 1000 validation ones, and the same of the testing ones in accordance with the Karpathy split protocol [7]. The screening has been made with regard to diversity in terms of demography and emotions.Figure 5a,b show sample images capturing expressions such as joy, curiosity, and contemplation. Its emphasis on facial visibility makes it well-suited for training emotion-aware captioning models.Figure 5.(a) A group of people smiling and posing together at what appears to be a social gathering or party. (b) A musical performance on stage with multiple performers singing and playing instruments under concert lighting. 4.1.2. COCOFace 15kThe COCOFace 15k dataset is a filtered subset of the COCO 2017 dataset [26], consisting of 15,000 images with visible human faces. It is split into 13,000 training and 2000 validation/test samples. Each image includes multiple captions and bounding boxes, supporting joint learning of object semantics and visual localization. The dataset offers wide variability in facial orientation, lighting, and occlusion, making it ideal for training facial expression models and evaluating captioning systems in real-world scenarios.Figure 6a,b showcases its contextual and emotional diversity, positioning COCOFace 15k as a strong benchmark for multimodal captioning research.Figure 6.(a) A young child sitting on a couch and holding a colorful umbrella, smiling toward the camera. (b) Two adults dressed formally, engaging in a handshake or exchange during what appears to be an outdoor event. 4.1.3. FER2013The FER2013 dataset is a well-established benchmark in facial expression recognition research and is extensively used in affective computing [18]. It consists of 35,887 grayscale images with a resolution of 48 Ã— 48 pixels. Each image represents a cropped frontal face, labeled with one of seven emotion classes: anger, disgust, fear, happiness, sadness, surprise, or neutral. The dataset is organized into 28,709 training images, 3589 for validation, and 3589 for testing. Due to its wide usage and consistent labeling, FER2013 serves as a reliable source for pretraining the facial expression recognition component of the proposed model.Figure 7aâ€“c depict the emotional variety in FER2013, underscoring its value for facial expression recognition in vision-language captioning.Table 2presents a short statistics of the datasets used.Figure 7.Sample images from the FER2013 dataset displaying distinct emotional expressions: (a) surprise, (b) fear, and (c) happiness [18].Table 2.Summary of datasets used in experiments.",
            "4.1.1. FlickrFace 11k": "FlickrFace 11k means a narrow selection of the Flickr30k picture captioning corpus, editorialized to prefer pictures of human faces and expressions. It includes 11,000 high-resolution visuals, all of which are assigned five captions written by a human being that relate to various human acts and expressions. The dataset is split accordingly with 9000 training images, 1000 validation ones, and the same of the testing ones in accordance with the Karpathy split protocol [7]. The screening has been made with regard to diversity in terms of demography and emotions.Figure 5a,b show sample images capturing expressions such as joy, curiosity, and contemplation. Its emphasis on facial visibility makes it well-suited for training emotion-aware captioning models. Figure 5.(a) A group of people smiling and posing together at what appears to be a social gathering or party. (b) A musical performance on stage with multiple performers singing and playing instruments under concert lighting.",
            "4.1.2. COCOFace 15k": "The COCOFace 15k dataset is a filtered subset of the COCO 2017 dataset [26], consisting of 15,000 images with visible human faces. It is split into 13,000 training and 2000 validation/test samples. Each image includes multiple captions and bounding boxes, supporting joint learning of object semantics and visual localization. The dataset offers wide variability in facial orientation, lighting, and occlusion, making it ideal for training facial expression models and evaluating captioning systems in real-world scenarios.Figure 6a,b showcases its contextual and emotional diversity, positioning COCOFace 15k as a strong benchmark for multimodal captioning research. Figure 6.(a) A young child sitting on a couch and holding a colorful umbrella, smiling toward the camera. (b) Two adults dressed formally, engaging in a handshake or exchange during what appears to be an outdoor event.",
            "4.1.3. FER2013": "The FER2013 dataset is a well-established benchmark in facial expression recognition research and is extensively used in affective computing [18]. It consists of 35,887 grayscale images with a resolution of 48 Ã— 48 pixels. Each image represents a cropped frontal face, labeled with one of seven emotion classes: anger, disgust, fear, happiness, sadness, surprise, or neutral. The dataset is organized into 28,709 training images, 3589 for validation, and 3589 for testing. Due to its wide usage and consistent labeling, FER2013 serves as a reliable source for pretraining the facial expression recognition component of the proposed model.Figure 7aâ€“c depict the emotional variety in FER2013, underscoring its value for facial expression recognition in vision-language captioning.Table 2presents a short statistics of the datasets used. Figure 7.Sample images from the FER2013 dataset displaying distinct emotional expressions: (a) surprise, (b) fear, and (c) happiness [18]. Table 2.Summary of datasets used in experiments.",
            "4.2. Baseline Models": "To benchmark the proposed model, we compare it against two key attention-based image captioning frameworks: Show, Attend and Tell by Xu et al. [12] and the Bottom-Up and Top-Down Attention model by Anderson et al. [3]. These baselines serve as strong references to assess the impact of incorporating facial expression features and real-time object detection into caption generation. The Show, Attend and Tell model is especially relevant to our approach, as it pioneered the integration of soft attention mechanisms within a CNN-LSTM architecture. By allowing the decoder to focus dynamically on different spatial regions of an image, the model improved contextual alignment between visual content and generated words. We adopt this structure as a foundational benchmark, as our model extends its architecture by introducing emotion-based initialization through facial expression features and enhancing object localization using YOLOv5. Comparison with this model helps quantify the direct contribution of emotional cues to caption fluency and relevance. The Bottom-Up and Top-Down Attention model, while more complex, serves as a comparison for evaluating object-level attention. It uses Faster R-CNN to extract detailed region proposals and applies top-down attention to guide captioning. Though accurate, it is computationally slower than our YOLOv5-based approach. This comparison highlights how our method maintains semantic richness while improving inference efficiency. Both baselines were retrained on FlickrFace 11k and COCOFace 15k using identical splits and preprocessing to ensure objective performance evaluation.",
            "4.3. Implementation Details": "The proposed model was implemented using Python 3.8 and the PyTorch 1.8.1 deep learning framework (version 1.13). The experiments were conducted on a system equipped with the hardware and software specifications detailed inTable 3. Table 3.System specifications used for training and evaluating the proposed model. For image preprocessing, the OpenCV and Albumentations libraries were employed. Dataset management and augmentation were handled via custom PyTorch dataset and DataLoader classes. The implementation supports modular training, allowing for separate pretraining of object detection and emotion recognition components before joint optimization. All models were trained with reproducible random seeds, and results were averaged over three runs to reduce variance.",
            "4.4. Evaluation Metrics": "We evaluated caption quality using five standard metrics: BLEU, METEOR, ROUGE-L, CIDEr, and SPICE. BLEU@N measures n-gram precision [21], while METEOR captures word alignment, synonyms, and ordering [22]. ROUGE-L assesses fluency through longest common subsequence matching [23]. CIDEr evaluates consensus with multiple references [24], and SPICE focuses on semantic content using scene graph comparisons [25]. These metrics collectively assess both linguistic accuracy and semantic relevance.",
            "4.5. Training Procedure": "The training strategy is divided into two stages to leverage pretrained knowledge while allowing end-to-end optimization. 4.5.1. Stage 1: Pretraining of Feature ExtractorsIn the first stage, pretrained models are used for individual components:YOLOv5 is initialized with weights trained on the COCO dataset.ResNet50 and VGG-Face are used for extracting grid and facial features, respectively.FER-CNN is pretrained on FER2013 to classify facial expressions.These pretrained models are frozen during initial training to prevent overfitting and to ensure stability in feature representations. 4.5.2. Stage 2: End-to-End Captioning Model TrainingIn the second stage, the complete architecture, including the LSTM decoder and attention mechanism, is fine-tuned in an end-to-end manner. The word embedding layer is uniformly initialized with a dimensionality of 300, while the LSTMâ€™s hidden and cell states are configured with 512 units each. Optimization is carried out using the Adam optimizer, beginning with an initial learning rate of 0.001. To encourage convergence, the learning rate is reduced to 0.0001 if the METEOR score does not improve over two consecutive validation epochs. The model is trained for a maximum of 30 epochs using a mini-batch size of 64. To mitigate overfitting and stabilize training, dropout is applied at a rate of 0.5, and gradient clipping is enforced with a maximum norm of 5. Initially, the model is trained using Cross-Entropy Loss to maximize likelihood of ground-truth tokens. Subsequently, CIDEr optimization is applied using a reinforcement learning objective to better align model predictions with human consensus.",
            "4.5.1. Stage 1: Pretraining of Feature Extractors": "In the first stage, pretrained models are used for individual components: YOLOv5 is initialized with weights trained on the COCO dataset.ResNet50 and VGG-Face are used for extracting grid and facial features, respectively.FER-CNN is pretrained on FER2013 to classify facial expressions. These pretrained models are frozen during initial training to prevent overfitting and to ensure stability in feature representations.",
            "4.5.2. Stage 2: End-to-End Captioning Model Training": "In the second stage, the complete architecture, including the LSTM decoder and attention mechanism, is fine-tuned in an end-to-end manner. The word embedding layer is uniformly initialized with a dimensionality of 300, while the LSTMâ€™s hidden and cell states are configured with 512 units each. Optimization is carried out using the Adam optimizer, beginning with an initial learning rate of 0.001. To encourage convergence, the learning rate is reduced to 0.0001 if the METEOR score does not improve over two consecutive validation epochs. The model is trained for a maximum of 30 epochs using a mini-batch size of 64. To mitigate overfitting and stabilize training, dropout is applied at a rate of 0.5, and gradient clipping is enforced with a maximum norm of 5. Initially, the model is trained using Cross-Entropy Loss to maximize likelihood of ground-truth tokens. Subsequently, CIDEr optimization is applied using a reinforcement learning objective to better align model predictions with human consensus.",
            "4.6. Training with Cross-Entropy Loss": "In the initial training phase, the model is optimized using a cross-entropy loss function, which encourages the decoder to generate word sequences that closely match the ground truth captions. The loss is formally defined as follows:ğ¿ğ¶ğ¸=âˆ’âˆ‘ğ‘‡ğ‘¡=1ğ‘™ğ‘œğ‘”ğ‘(ğ‘¦ğ‘¡|ğ‘¦1:ğ‘¡âˆ’1,ğ‘)LCE=âˆ’âˆ‘t=1Tlogpyt|y1:tâˆ’1,a(12)whereğ‘¦ğ‘¡ytis the ground truth word at time stepğ‘¡t, andğ‘(ğ‘¦ğ‘¡|ğ‘¦1:ğ‘¡âˆ’1,ğ‘)pyt|y1:tâˆ’1,ais the predicted probability conditioned on the previous words and attention-based visual contextğ‘a.",
            "4.7. Optimization with CIDEr Score (Reinforcement Learning)": "To further refine the caption generation toward human-like quality, the model is fine-tuned using reinforcement learning with the CIDEr metric as a reward signal. The reinforcement learning loss is defined as follows:ğ¿ğ‘…ğ¿=âˆ’âˆ‘ğ‘‡ğ‘¡=1(ğ¶ğ¼ğ·ğ¸ğ‘Ÿ(ğ‘¦,ğ‘¦Ì‚)âˆ’ğ‘)ğ‘™ğ‘œğ‘”ğ‘(ğ‘¦ğ‘¡|ğ‘¦1:ğ‘¡âˆ’1,ğ‘)LRL=âˆ’âˆ‘t=1TCIDEry,y^âˆ’blogpyt|y1:tâˆ’1,a(13)whereğ‘¦Ì‚y^is the generated caption,ğ‘¦yis the reference caption, andğ‘brepresents the baseline reward to reduce variance. This will bring consistency between the training goal and the evaluation measurement adopted in testing, so as to make the captions more relevant and fluent.",
            "5. Results": "The current section contributes and discusses the results of the suggested image captioning system, examined based on quantitative measurements (metrics), as well as qualitative measurements. The outcomes are contrasted to two baselines Show-Attend-Tell [12] and Bottom-Up and Top-Down (BUTD) Attention [3] over two typical sets of data, FlickrFace11k and COCOFace15k. 5.1. Quantitative ResultsA comparative analysis between three people image captioning models: Show-Attend-Tell model, Up-Down model, and proposed model is given inTable 4on two benchmark datasets, FlickrFace11k and COCOFace15k. Assessment of the models is provided with an extensive range of evaluation measures, such as BLEU@1-4, METEOR, ROUGE-L, CIDEr, and SPICE that encompass the n-gram fidelity and surface measure with the human-written captions, as well as the semantic overlap with human-written captions. The proposed model outperforms any of the baselines in terms of all metrics and datasets, with much higher improvement on CIDEr and SPICE, the two metrics that excellently evaluate the semantic richness and conformity with reference captions of the model. These findings support the effectiveness of the presented method of producing more representatively correct image descriptions as well as that of a more expressive description semantically.Table 4.Performance comparison on FlickrFace11k and COCOFace15k test splits.Based on the table, it is clear that the specified model has an advantage over both baselines, which is especially prominent in CIDEr and SPICE scores, which serve as good predictors of semantic richness and image-caption relevance. The proposed model matches the performance of the FlickrFace11k dataset with CIDEr of 25.4, which is +1.0 higher than BUTD and +3.5 higher than Show-Attend-Tell. SPICE, associated with semantic propositional content of captions, exhibits significant improvements as well, its results demonstrating a better grasp of human expression and interactions. Similarly, the model achieves 30.3 CIDEr and 13.2 SPICE on the COCOFace15k dataset, demonstrating strong generalization to a variety of real-world use-cases. 5.2. K-Fold Cross Validation ResultsA 5-fold cross-validation approach was considered to critically assess the generalizability of the suggested model over the FlickrFace11k and COCOFace15k datasets. Every fold was kept at the 80-10-10 split into training, validation, and testing, respectively. The results indicate a high stability of the model performance with all the standard deviations being very low, having a range of âˆ’0.02 to 0.18 in all the evaluation metrics as reported inTable 5. This low variation indicates the stability and trustworthiness of the suggested methodology, and the model does not overfit and can produce consistent and semantically valuable descriptions on out-of-sample information. Moreover, semantic-rich measures (CIDEr and SPICE) also indicated high consistency in the scores, further proving that the model could capture both contextual as well as emotional fidelity across the variety of data drop partitions.Table 5.Performance of the proposed model across 5-Fold cross-validation on FlickrFace11k and COCOFace15k datasets using standard evaluation metrics. 5.3. Graphical ResultsA graphical representation of the BLEU, METEOR, CIDEr, and SPICE scores between the three models was created to visually complement the results provided in the figures, as illustrated below.As illustrated inFigure 8andFigure 9, the presented model achieves significant improvement over the fixed baselines Show-Attend-Tell and Up-Down in all of the assessment measures in both the FlickrFace11k and COCOFace15k datasets. The gains are especially significant in CIDEr and SPICE, which are considered powerful metrics of semantic accuracy and adherence to the human-powered captions. Such improvements highlight that the model can produce descriptions going beyond lexical correctness to being semantically rich, describing visual images in a more human-like way. Moreover, the results depicted inFigure 10andFigure 11show the strong generalization capability of the model since it has a low variance over five cross-validation folds. This consistency confirms that the model has consistent performance with or without training subset variation, which is an essential requirement during deployment in real-life environments. The statistical reliability of the proposed approach is further confirmed by relatively low standard deviations across the metrics like BLEU, METEOR, and ROUGE-L.Figure 8.Performance metrics on FlickrFace11k dataset for Show-Attend-Tell, Up-Down, and proposed model.Figure 9.Performance metrics on COCOFace15k dataset for Show-Attend-Tell, Up-Down, and proposed model.Figure 10.Average performance metrics across 5 foldsâ€”FlickrFace11k dataset.Figure 11.Average performance metrics across 5 foldsâ€”COCOFace15k dataset. 5.4. Qualitative ResultsQualitative evaluation further substantiates the effectiveness of the proposed model. As depicted inFigure 3, two representative images and their associated captions are provided. For each image, two ground truth captions (GT-1 and GT-2) and outputs from Show-Attend-Tell, BUTD, and the proposed model are compared.The qualitative analysis presented inFigure 12offers deeper insight into the expressive strength of the proposed captioning framework. Unlike baseline models, which often overlook emotional nuance, the proposed model effectively integrates facial expression recognition to capture affective and contextual details within images. For instance, the modelâ€™s ability to describe a child as â€œcrying with tears on his faceâ€ illustrates its capacity to infer and articulate subtle emotional cues, an advancement that is critical for applications in assistive technology, humanâ€“computer interaction, and emotionally intelligent systems. In addition to quantitative scoring measures, cross-validation robustness, visual performance, and qualitative quality on all assessment scales, the proposed model shows significant gains against established status quos. The steady improvements in SPICE and CIDEr validate its semantic insights, and qualitative samples validate its contextual sensitivity. These findings confirm the modelâ€™s suitability in solving the problem of human-centered, emotion-sensitive description of images.Figure 12.Qualitative results of our proposed method with emotions as compared to Show, Attend and Tell and BUTD on the FlickrFace 11k (Right) and COCOFace15k (Left) Test Images.Collectively, these results confirm that the suggested model not only enhances quantitative performance across standard benchmarks but also brings qualitative improvements that are more aligned with human imperative capabilities.",
            "5.1. Quantitative Results": "A comparative analysis between three people image captioning models: Show-Attend-Tell model, Up-Down model, and proposed model is given inTable 4on two benchmark datasets, FlickrFace11k and COCOFace15k. Assessment of the models is provided with an extensive range of evaluation measures, such as BLEU@1-4, METEOR, ROUGE-L, CIDEr, and SPICE that encompass the n-gram fidelity and surface measure with the human-written captions, as well as the semantic overlap with human-written captions. The proposed model outperforms any of the baselines in terms of all metrics and datasets, with much higher improvement on CIDEr and SPICE, the two metrics that excellently evaluate the semantic richness and conformity with reference captions of the model. These findings support the effectiveness of the presented method of producing more representatively correct image descriptions as well as that of a more expressive description semantically. Table 4.Performance comparison on FlickrFace11k and COCOFace15k test splits. Based on the table, it is clear that the specified model has an advantage over both baselines, which is especially prominent in CIDEr and SPICE scores, which serve as good predictors of semantic richness and image-caption relevance. The proposed model matches the performance of the FlickrFace11k dataset with CIDEr of 25.4, which is +1.0 higher than BUTD and +3.5 higher than Show-Attend-Tell. SPICE, associated with semantic propositional content of captions, exhibits significant improvements as well, its results demonstrating a better grasp of human expression and interactions. Similarly, the model achieves 30.3 CIDEr and 13.2 SPICE on the COCOFace15k dataset, demonstrating strong generalization to a variety of real-world use-cases.",
            "5.2. K-Fold Cross Validation Results": "A 5-fold cross-validation approach was considered to critically assess the generalizability of the suggested model over the FlickrFace11k and COCOFace15k datasets. Every fold was kept at the 80-10-10 split into training, validation, and testing, respectively. The results indicate a high stability of the model performance with all the standard deviations being very low, having a range of âˆ’0.02 to 0.18 in all the evaluation metrics as reported inTable 5. This low variation indicates the stability and trustworthiness of the suggested methodology, and the model does not overfit and can produce consistent and semantically valuable descriptions on out-of-sample information. Moreover, semantic-rich measures (CIDEr and SPICE) also indicated high consistency in the scores, further proving that the model could capture both contextual as well as emotional fidelity across the variety of data drop partitions. Table 5.Performance of the proposed model across 5-Fold cross-validation on FlickrFace11k and COCOFace15k datasets using standard evaluation metrics.",
            "5.3. Graphical Results": "A graphical representation of the BLEU, METEOR, CIDEr, and SPICE scores between the three models was created to visually complement the results provided in the figures, as illustrated below. As illustrated inFigure 8andFigure 9, the presented model achieves significant improvement over the fixed baselines Show-Attend-Tell and Up-Down in all of the assessment measures in both the FlickrFace11k and COCOFace15k datasets. The gains are especially significant in CIDEr and SPICE, which are considered powerful metrics of semantic accuracy and adherence to the human-powered captions. Such improvements highlight that the model can produce descriptions going beyond lexical correctness to being semantically rich, describing visual images in a more human-like way. Moreover, the results depicted inFigure 10andFigure 11show the strong generalization capability of the model since it has a low variance over five cross-validation folds. This consistency confirms that the model has consistent performance with or without training subset variation, which is an essential requirement during deployment in real-life environments. The statistical reliability of the proposed approach is further confirmed by relatively low standard deviations across the metrics like BLEU, METEOR, and ROUGE-L. Figure 8.Performance metrics on FlickrFace11k dataset for Show-Attend-Tell, Up-Down, and proposed model. Figure 9.Performance metrics on COCOFace15k dataset for Show-Attend-Tell, Up-Down, and proposed model. Figure 10.Average performance metrics across 5 foldsâ€”FlickrFace11k dataset. Figure 11.Average performance metrics across 5 foldsâ€”COCOFace15k dataset.",
            "5.4. Qualitative Results": "Qualitative evaluation further substantiates the effectiveness of the proposed model. As depicted inFigure 3, two representative images and their associated captions are provided. For each image, two ground truth captions (GT-1 and GT-2) and outputs from Show-Attend-Tell, BUTD, and the proposed model are compared. The qualitative analysis presented inFigure 12offers deeper insight into the expressive strength of the proposed captioning framework. Unlike baseline models, which often overlook emotional nuance, the proposed model effectively integrates facial expression recognition to capture affective and contextual details within images. For instance, the modelâ€™s ability to describe a child as â€œcrying with tears on his faceâ€ illustrates its capacity to infer and articulate subtle emotional cues, an advancement that is critical for applications in assistive technology, humanâ€“computer interaction, and emotionally intelligent systems. In addition to quantitative scoring measures, cross-validation robustness, visual performance, and qualitative quality on all assessment scales, the proposed model shows significant gains against established status quos. The steady improvements in SPICE and CIDEr validate its semantic insights, and qualitative samples validate its contextual sensitivity. These findings confirm the modelâ€™s suitability in solving the problem of human-centered, emotion-sensitive description of images. Figure 12.Qualitative results of our proposed method with emotions as compared to Show, Attend and Tell and BUTD on the FlickrFace 11k (Right) and COCOFace15k (Left) Test Images. Collectively, these results confirm that the suggested model not only enhances quantitative performance across standard benchmarks but also brings qualitative improvements that are more aligned with human imperative capabilities.",
            "6. Discussion": "The findings of this study provide strong support for the hypothesis that incorporating facial expression recognition (FER) into image captioning systems significantly enhances both the semantic and emotional fidelity of generated captions. Unlike traditional frameworks that rely primarily on object detection and global visual features, the proposed model succeeds in bridging the semantic gap between mere object presence and affective context by embedding emotion vectors derived from facial cues. This integration enables the generation of captions that are not only contextually accurate but also emotionally resonant, advancing toward a more human-centric narrative generation framework. Performance gains observed in CIDEr and SPICE scores validate this enhancement. These metrics are specifically designed to capture the richness and propositional quality of captions, and the achieved improvements (+2.5 CIDEr and +1.0 SPICE) demonstrate that captions generated by the model align more closely with human expectations for interpretative depth and sentiment [18,19]. These results align with previous research such as Face-Cap [17], which demonstrated that the use of emotional cues led to captions perceived as more relatable. However, the current model extends this by combining emotional vectors with efficient object detection and global context modeling, thereby addressing a key limitation in earlier work, which often lacked detailed spatial grounding [13,23]. The implementation of YOLOv5 as the backbone for object detection contributed to real-time inference capability without compromising detection precision [27]. This is a notable improvement over systems like Faster R-CNN [14], which, although accurate, are computationally expensive and less suited for applications requiring rapid response, such as assistive tools or autonomous systems. Meanwhile, facial expression features extracted via VGG-Face and ResNet50 [16,17] successfully supported the hypothesis that multimodal fusion enhances caption expressiveness. The modelâ€™s ability to encode emotional context during decoding via initialization of LSTM states proved essential in producing emotionally aware language sequences. These findings echo the goals of affective computing, which advocates for machines capable of understanding and responding to human emotions in nuanced ways. Moreover, qualitative results underscore the systemâ€™s ability to interpret emotional subtleties often overlooked in conventional captioning. In comparison to baseline models, the proposed method more accurately articulated expressions such as sadness, surprise, or joy. For instance, captions describing a â€œchild crying with tears on his faceâ€ illustrate how the system captures visual-emotional cues and translates them into meaningful textual outputs. This capability is particularly valuable in fields like humanâ€“computer interaction and visual accessibility, where emotionally enriched descriptions can enhance user comprehension and engagement. The stability of the model, as reflected in 5-fold cross-validation experiments with standard deviations consistently below Â±0.2, also demonstrates robust generalization. The consistent performance across two diverse datasets, FlickrFace11k and COCOFace15k, suggests that the model maintains effectiveness across different visual domains, facial orientations, and illumination conditions. This indicates its potential for deployment in various real-world contexts, where diverse and noisy data are the norm. From a broader perspective, these results support the growing consensus in the field that affective signals are indispensable for generating naturalistic, human-like descriptions. As image captioning systems evolve beyond object enumeration toward interpretive and empathetic storytelling, models capable of incorporating emotion, context, and relational reasoning will play a central role. Future research could explore more fine-grained affective states, cultural variance in emotional expression, or even real-time multilingual caption generation using this architecture as a foundation. Recent domain-specific vision-language models have further validated the industrial relevance of multimodal approaches: MaViLa [28] demonstrates strong gains in manufacturing scene understanding, quality control, and humanâ€“machine interaction [29], while specialized surveys highlight the transformative role of VLMs in humanâ€“robot collaboration and smart-factory deployment [30].",
            "7. Conclusions": "This study proposed an emotionally enhanced image captioning model which used the recognition of facial expression to improve the vision generation pipeline. With the traditional attention mechanism integrated along with the affective cues, the model generates captions that are not only descriptively but also emotionally sophisticated. Significant experiments conducted on the FlickrFace11k and COCOFace15k datasets in this study showed that the proposed method has a better performance than existing baselines, Show-Attend-Tell and Up-Down, on multiple evaluation metrics, such as BLEU, METEOR, CIDEr, and SPICE. The model also demonstrated high generalization power as it had stable 5-fold cross-validation and minimal variability. Based on the qualitative analysis schemes, the model has also been shown to produce captions that are similar to how a person would interpret. The results of the experiments indicate that the appending of the emotional context is capable of greatly increasing semantic richness and the levels of human-likeness of image captions. Future research should include more varied and comprehensive datasets, multilingual captioning, as well as adaptive emotion modeling that would allow extending the range of applicability of this approach. Future work will target multi-camera synchronization on the shop floor, KPI-driven alerting via plant IoT buses, and validation in real manufacturing cells, furthering the scope on IoT-enabled smart industry. These extensions will accelerate the transition from isolated proof-of-concept demonstrations to resilient, scalable cyber-physical production systems that fully exploit real-time IoT data streams for autonomous and adaptive manufacturing."
        },
        "references": "References not found",
        "link": "https://www.mdpi.com/2306-5354/12/12/1325",
        "scraped_at": "2025-12-05 18:23:25"
    }
]